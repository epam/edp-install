{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EPAM Delivery Platform \u2693\ufe0e What It Is \u2693\ufe0e EPAM Delivery Platform (EDP) is an open-source cloud-agnostic SaaS/PaaS solution for software development, licensed under Apache License 2.0 . It provides a pre-defined set of CI/CD patterns and tools, which allow a user to start product development quickly with established code review , release , versioning , branching , build processes. These processes include static code analysis, security checks, linters, validators, dynamic feature environments provisioning. EDP consolidates the top Open-Source CI/CD tools by running them on Kubernetes/OpenShift, which enables web/app development either in isolated (on-prem) or cloud environments. EPAM Delivery Platform, which is also called \"The Rocket\" , is a platform that allows shortening the time that is passed before an active development can be started from several months to several hours. EDP consists of the following: The platform based on managed infrastructure and container orchestration Security covering authentication, authorization, and SSO for platform services Development and testing toolset Well-established engineering process and EPAM practices (EngX) reflected in CI/CD pipelines, and delivery analytics Local development with debug capabilities Features \u2693\ufe0e Deployed and configured CI/CD toolset ( Jenkins , Gerrit , Nexus , SonarQube ) Gerrit , GitLab or GitHub as a version control system for your code Jenkins is a pipeline orchestrator CI pipelines for Python, Java 8, Java 11, .Net, Go, React, Terraform, Jenkins Groovy Pipelines, Dockerfile, Helm Build tools: Go, Apache Maven, Apache Gradle Admin Console UI as a single entry point CD pipeline for Microservice Deployment Kubernetes native approach ( CRD, CR ) to declare CI/CD pipelines What's Inside \u2693\ufe0e EPAM Delivery Platform (EDP) is suitable for all aspects of delivery starting from development including the capability to deploy production environment. EDP architecture is represented on a diagram below. Architecture EDP consists of three cross-cutting concerns: Infrastructure as a Service; Container orchestration and centralized services; Security. On the top of these indicated concerns, EDP adds several blocks that include: EDP CI/CD Components . EDP component enables a feature in CI/CD or an instance artifacts storage and distribution (Nexus or Artifactory), static code analysis (Sonar), etc.; EDP Artifacts . This element represents an artifact that is being delivered through EDP and presented as a code. Artifact samples: frontend, backend, mobile, applications, functional and non-functional autotests, workloads for 3rd party components that can be deployed together with applications. EDP development and production environments that share the same logic. Environments wrap a set of artifacts with a specific version, and allow performing SDLC routines in order to be sure of the artifacts quality; Pipelines . Pipelines cover CI/CD process, production rollout and updates. They also connect three elements indicated above via automation allowing SDLC routines to be non-human; Technology Stack \u2693\ufe0e Explore the EDP technology stack diagram Technology stack The EDP IaaS layer supports most popular public clouds AWS, Azure and GCP keeping the capability to be deployed on private/hybrid clouds based on OpenStack. EDP containers are based on Docker technology , orchestrated by Kubernetes compatible solutions. There are two main options for Kubernetes provided by EDP: Managed Kubernetes in Public Clouds to avoid installation and management of Kubernetes cluster, and get all benefits of scaling, reliability of this solution; OpenShift that is a Platform as a Service on the top of Kubernetes from Red Hat. OpenShift is the default option for on-premise installation and it can be considered whether the solution built on the top of EDP should be cloud-agnostic or require enterprise support ; There is no limitation to run EDP on vanilla Kubernetes.","title":"Overview"},{"location":"#epam-delivery-platform","text":"","title":"EPAM Delivery Platform"},{"location":"#what-it-is","text":"EPAM Delivery Platform (EDP) is an open-source cloud-agnostic SaaS/PaaS solution for software development, licensed under Apache License 2.0 . It provides a pre-defined set of CI/CD patterns and tools, which allow a user to start product development quickly with established code review , release , versioning , branching , build processes. These processes include static code analysis, security checks, linters, validators, dynamic feature environments provisioning. EDP consolidates the top Open-Source CI/CD tools by running them on Kubernetes/OpenShift, which enables web/app development either in isolated (on-prem) or cloud environments. EPAM Delivery Platform, which is also called \"The Rocket\" , is a platform that allows shortening the time that is passed before an active development can be started from several months to several hours. EDP consists of the following: The platform based on managed infrastructure and container orchestration Security covering authentication, authorization, and SSO for platform services Development and testing toolset Well-established engineering process and EPAM practices (EngX) reflected in CI/CD pipelines, and delivery analytics Local development with debug capabilities","title":"What It Is"},{"location":"#features","text":"Deployed and configured CI/CD toolset ( Jenkins , Gerrit , Nexus , SonarQube ) Gerrit , GitLab or GitHub as a version control system for your code Jenkins is a pipeline orchestrator CI pipelines for Python, Java 8, Java 11, .Net, Go, React, Terraform, Jenkins Groovy Pipelines, Dockerfile, Helm Build tools: Go, Apache Maven, Apache Gradle Admin Console UI as a single entry point CD pipeline for Microservice Deployment Kubernetes native approach ( CRD, CR ) to declare CI/CD pipelines","title":"Features"},{"location":"#whats-inside","text":"EPAM Delivery Platform (EDP) is suitable for all aspects of delivery starting from development including the capability to deploy production environment. EDP architecture is represented on a diagram below. Architecture EDP consists of three cross-cutting concerns: Infrastructure as a Service; Container orchestration and centralized services; Security. On the top of these indicated concerns, EDP adds several blocks that include: EDP CI/CD Components . EDP component enables a feature in CI/CD or an instance artifacts storage and distribution (Nexus or Artifactory), static code analysis (Sonar), etc.; EDP Artifacts . This element represents an artifact that is being delivered through EDP and presented as a code. Artifact samples: frontend, backend, mobile, applications, functional and non-functional autotests, workloads for 3rd party components that can be deployed together with applications. EDP development and production environments that share the same logic. Environments wrap a set of artifacts with a specific version, and allow performing SDLC routines in order to be sure of the artifacts quality; Pipelines . Pipelines cover CI/CD process, production rollout and updates. They also connect three elements indicated above via automation allowing SDLC routines to be non-human;","title":"What's Inside"},{"location":"#technology-stack","text":"Explore the EDP technology stack diagram Technology stack The EDP IaaS layer supports most popular public clouds AWS, Azure and GCP keeping the capability to be deployed on private/hybrid clouds based on OpenStack. EDP containers are based on Docker technology , orchestrated by Kubernetes compatible solutions. There are two main options for Kubernetes provided by EDP: Managed Kubernetes in Public Clouds to avoid installation and management of Kubernetes cluster, and get all benefits of scaling, reliability of this solution; OpenShift that is a Platform as a Service on the top of Kubernetes from Red Hat. OpenShift is the default option for on-premise installation and it can be considered whether the solution built on the top of EDP should be cloud-agnostic or require enterprise support ; There is no limitation to run EDP on vanilla Kubernetes.","title":"Technology Stack"},{"location":"faq/","text":"FAQ \u2693\ufe0e How Do I Set Parallel Reconciliation for a Number of Codebase Branches? \u2693\ufe0e Set the CODEBASE_BRANCH_MAX_CONCURRENT_RECONCILES Env variable in codebase-operator by updating Deployment template. For example: ... env: - name: WATCH_NAMESPACE ... - name: CODEBASE_BRANCH_MAX_CONCURRENT_RECONCILES value: 10 ... It's not recommended to set the value above 10.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#how-do-i-set-parallel-reconciliation-for-a-number-of-codebase-branches","text":"Set the CODEBASE_BRANCH_MAX_CONCURRENT_RECONCILES Env variable in codebase-operator by updating Deployment template. For example: ... env: - name: WATCH_NAMESPACE ... - name: CODEBASE_BRANCH_MAX_CONCURRENT_RECONCILES value: 10 ... It's not recommended to set the value above 10.","title":"How Do I Set Parallel Reconciliation for a Number of Codebase Branches?"},{"location":"features/","text":"Basic Concepts \u2693\ufe0e Consult EDP Glossary section for definitions mentioned on this page and EDP Toolset to have a full list of tools used with the Platform. The below table contains a full list of features provided by EDP. Features Description Cloud Agnostic EDP runs on Kubernetes cluster, so any Public Cloud Provider which provides Kubernetes can be used. Kubernetes clusters deployed on-premises work as well CI/CD for Microservices EDP is initially designed to support CI/CD for Microservices running as containerized applications inside Kubernetes Cluster. EDP also supports CI for: - Terraform Modules, - Open Policy Rules, - Workflows for Java11, JavaScript (React), .Net, Python, Groovy Pipelines, Go Version Control System (VCS) EDP installs Gerrit as a default Source Code Management (SCM) tool. EDP also supports GitHub and GitLab integration Branching Strategy EDP supports Trunk-based development as well as GitHub/GitLab flow . EDP creates two Pipelines per each codebase branch (see Pipeline Framework ): Code Review and Build Repository Structure EDP provides separate Git repository per each Codebase and doesn't work with Monorepo . However, EDP does support customization and runs helm-lint , dockerfile-lint steps using Monorepo approach. Artifacts Versioning EDP supports two approaches for Artifacts versioning: - default (BRANCH-[TECH_STACK_VERSION]-BUILD_ID) - EDP (MAJOR.MINOR.PATCH-BUILD_ID), which is SemVer . Custom versioning can be created by implementing get-version stage Application Library EDP provides baseline codebase templates for Microservices, Libraries, within create strategy while onboarding new Codebase Stages Library Each EDP Pipeline consists of pre-defined steps (stages). Consult library documentation for more details CI Pipelines EDP provides CI Pipelines (running in Jenkins) for first-class citizens: - Applications (Microservices) based on Java8, Java11, JavaScript (React), .Net, Python, Go - Libraries based on Java8, Java11, JavaScript (React), .Net, Python, Go, Groovy Pipelines, Terraform - Autotests based on Java8, Java11 CD Pipelines EDP provides capabilities to design CD Pipelines (in Admin Console) for Microservices and defines logic for artifacts flow (promotion) from env to env. Artifacts promotion is performed automatically ( Autotests ), manually ( User Approval ) or combining both approaches Autotests EDP provides CI pipeline for autotest implemented in Java. Autotests can be used as Quality Gates in CD Pipelines Custom Pipeline Library EDP can be extended by introducing Custom Pipeline Library Dynamic Environments Each EDP CD Pipeline creates/destroys environment upon user requests","title":"Basic Concepts"},{"location":"features/#basic-concepts","text":"Consult EDP Glossary section for definitions mentioned on this page and EDP Toolset to have a full list of tools used with the Platform. The below table contains a full list of features provided by EDP. Features Description Cloud Agnostic EDP runs on Kubernetes cluster, so any Public Cloud Provider which provides Kubernetes can be used. Kubernetes clusters deployed on-premises work as well CI/CD for Microservices EDP is initially designed to support CI/CD for Microservices running as containerized applications inside Kubernetes Cluster. EDP also supports CI for: - Terraform Modules, - Open Policy Rules, - Workflows for Java11, JavaScript (React), .Net, Python, Groovy Pipelines, Go Version Control System (VCS) EDP installs Gerrit as a default Source Code Management (SCM) tool. EDP also supports GitHub and GitLab integration Branching Strategy EDP supports Trunk-based development as well as GitHub/GitLab flow . EDP creates two Pipelines per each codebase branch (see Pipeline Framework ): Code Review and Build Repository Structure EDP provides separate Git repository per each Codebase and doesn't work with Monorepo . However, EDP does support customization and runs helm-lint , dockerfile-lint steps using Monorepo approach. Artifacts Versioning EDP supports two approaches for Artifacts versioning: - default (BRANCH-[TECH_STACK_VERSION]-BUILD_ID) - EDP (MAJOR.MINOR.PATCH-BUILD_ID), which is SemVer . Custom versioning can be created by implementing get-version stage Application Library EDP provides baseline codebase templates for Microservices, Libraries, within create strategy while onboarding new Codebase Stages Library Each EDP Pipeline consists of pre-defined steps (stages). Consult library documentation for more details CI Pipelines EDP provides CI Pipelines (running in Jenkins) for first-class citizens: - Applications (Microservices) based on Java8, Java11, JavaScript (React), .Net, Python, Go - Libraries based on Java8, Java11, JavaScript (React), .Net, Python, Go, Groovy Pipelines, Terraform - Autotests based on Java8, Java11 CD Pipelines EDP provides capabilities to design CD Pipelines (in Admin Console) for Microservices and defines logic for artifacts flow (promotion) from env to env. Artifacts promotion is performed automatically ( Autotests ), manually ( User Approval ) or combining both approaches Autotests EDP provides CI pipeline for autotest implemented in Java. Autotests can be used as Quality Gates in CD Pipelines Custom Pipeline Library EDP can be extended by introducing Custom Pipeline Library Dynamic Environments Each EDP CD Pipeline creates/destroys environment upon user requests","title":"Basic Concepts"},{"location":"getting-started/","text":"Getting Started \u2693\ufe0e Requirements \u2693\ufe0e Kubernetes cluster 1.18+, or OpenShift 4.6+ kubectl tool helm 3.5.x+ Keycloak 11.0+ Kiosk 0.2.11 Amazon EKS Pod Identity Webhook in case of using AWS ECR as Docker Registry Hardware \u2693\ufe0e Minimal: CPU: 4 Core Memory: 16 Gb EDP Toolset \u2693\ufe0e List of Tools used on the Platform: Domain Related Tools/Solutions Artefacts Management Nexus Repository, AWS ECR AWS Amazon EKS Pod Identity Webhook, AWS ECR, AWS EFS Build .NET, Go, Apache Gradle, Apache Maven, NPM Cluster Backup Velero Code Review Gerrit, GitLab, GitHub Docker Hadolint, kaniko, crane Infrastructure as Code Terraform, TFLint Kubernetes deployment kubectl, helm, ct (Chart Testing) Kubernetes Multitenancy Kiosk Logging EFK, ELK, Loki Monitoring Prometheus, Grafana Pipeline Orchestration Jenkins, GitLab CI (basic) Policies/Rules Open Policy Agent SSO Keycloak, keycloak-proxy Static Code Analysis SonarQube Test Report Tool Allure Install prerequisites \u2693\ufe0e Install EDP \u2693\ufe0e Find below the example of the installation command: helm install edp epamedp / edp - install -- wait -- timeout = 900 s \\ -- version < edp_version > \\ -- set global . edpName =< edp - project > \\ -- set global . dnsWildCard =< cluster_DNS_wilcdard > \\ -- set global . webConsole . url =< kubeconfig . clusters . cluster . server > \\ -- set global . platform =< platform_type > \\ -- set dockerRegistry . url =< aws_account_id > . dkr . ecr . < region > . amazonaws . com \\ -- set keycloak - operator . keycloak . url =< keycloak_endpoint > \\ -- set gerrit - operator . gerrit . sshPort =< gerrit_ssh_port > \\ -- namespace < edp - project > Warning Please be aware that the command above is an example. To install EDP with the necessary parameters, please refer to the Install EDP section of the Operator Guide . Mind the parameters in the EDP installation chart. For details, please refer to the values.yaml .","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#requirements","text":"Kubernetes cluster 1.18+, or OpenShift 4.6+ kubectl tool helm 3.5.x+ Keycloak 11.0+ Kiosk 0.2.11 Amazon EKS Pod Identity Webhook in case of using AWS ECR as Docker Registry","title":"Requirements"},{"location":"getting-started/#hardware","text":"Minimal: CPU: 4 Core Memory: 16 Gb","title":"Hardware"},{"location":"getting-started/#edp-toolset","text":"List of Tools used on the Platform: Domain Related Tools/Solutions Artefacts Management Nexus Repository, AWS ECR AWS Amazon EKS Pod Identity Webhook, AWS ECR, AWS EFS Build .NET, Go, Apache Gradle, Apache Maven, NPM Cluster Backup Velero Code Review Gerrit, GitLab, GitHub Docker Hadolint, kaniko, crane Infrastructure as Code Terraform, TFLint Kubernetes deployment kubectl, helm, ct (Chart Testing) Kubernetes Multitenancy Kiosk Logging EFK, ELK, Loki Monitoring Prometheus, Grafana Pipeline Orchestration Jenkins, GitLab CI (basic) Policies/Rules Open Policy Agent SSO Keycloak, keycloak-proxy Static Code Analysis SonarQube Test Report Tool Allure","title":"EDP Toolset"},{"location":"getting-started/#install-prerequisites","text":"","title":"Install prerequisites"},{"location":"getting-started/#install-edp","text":"Find below the example of the installation command: helm install edp epamedp / edp - install -- wait -- timeout = 900 s \\ -- version < edp_version > \\ -- set global . edpName =< edp - project > \\ -- set global . dnsWildCard =< cluster_DNS_wilcdard > \\ -- set global . webConsole . url =< kubeconfig . clusters . cluster . server > \\ -- set global . platform =< platform_type > \\ -- set dockerRegistry . url =< aws_account_id > . dkr . ecr . < region > . amazonaws . com \\ -- set keycloak - operator . keycloak . url =< keycloak_endpoint > \\ -- set gerrit - operator . gerrit . sshPort =< gerrit_ssh_port > \\ -- namespace < edp - project > Warning Please be aware that the command above is an example. To install EDP with the necessary parameters, please refer to the Install EDP section of the Operator Guide . Mind the parameters in the EDP installation chart. For details, please refer to the values.yaml .","title":"Install EDP"},{"location":"glossary/","text":"Glossary \u2693\ufe0e Get familiar with the definitions and context for the most useful EDP terms presented in table below. Terms Details EDP Component - an item used in CI/CD process Admin Console - an EDP component that helps to manage, set up, and control the business entities. Artifactory - an EDP component that stores all the binary artifacts. NOTE : Nexus is used as a possible implementation of a repository. CI/CD Server - an EDP component that launches pipelines that perform the build, QA, and deployment code logic. NOTE : Jenkins is used as a possible implementation of a CI/CD server. Code Review tool - an EDP component that collaborates with the changes in the codebase. NOTE : Gerrit is used as a possible implementation of a code review tool. Identity Server - an authentication server providing a common way to verify requests to all of the applications. NOTE : Keycloak is used as a possible implementation of an identity server. Security Realm Tenant - a realm in identity server (e.g Keycloak) where all users' accounts and their access permissions are managed. The realm is unique for the identity server instance. Static Code Analyzer - an EDP component that inspects continuously a code quality before the necessary changes appear in a master branch. NOTE : SonarQube is used as a possible implementation of a static code analyzer. VCS (Version Control System) - a replication of the Gerrit repository that displays all the changes made by developers. NOTE : GitHub and GitLab are used as the possible implementation of a repository with the version control system. EDP Business Entity - a part of the CI/CD process (the integration, delivery, and deployment of any codebase changes) Application - a codebase type that is built as the binary artifact and deployable unit with the code that is stored in VCS. As a result, the application becomes a container and can be deployed in an environment. Autotests - a codebase type that inspects a product (e.g. an application set) on a stage. Autotests are not deployed to any container and launched from the respective code stage. CD Pipeline (Continuous Delivery Pipeline) - an EDP business entity that describes the whole delivery process of the selected application set via the respective stages. The main idea of the CD pipeline is to promote the application version between the stages by applying the sequential verification (i.e. the second stage will be available if the verification on the first stage is successfully completed). NOTE : The CD pipeline can include the essential set of applications with its specific stages as well. CD Pipeline Stage - an EDP business entity that is presented as the logical gate required for the application set inspection. Every stage has one OpenShift project where the selected application set is deployed. All stages are sequential and promote applications one-by-one. Codebase - an EDP business entity that possesses a code. Codebase Branch - an EDP business entity that represents a specific version in a Git branch. Every codebase branch has a Codebase Docker Stream entity. Codebase Docker Stream - a deployable component that leads to the application build and displays that the last build was verified on the specific stage. Every CD pipeline stage accepts a set of Codebase Docker Streams (CDS) that are input and output. SAMPLE: if an application1 has a master branch, the input CDS will be named as [app name]-[pipeline name]-[stage name]-[master] and the output after the passing of the DEV stage will be as follows: [app name]-[pipeline name]-[stage name]-[dev]-[verified]. Library - a codebase type that is built as the binary artifact, i.e. it`s stored in the Artifactory and can be uploaded by other applications, autotests or libraries. Quality Gate - an EDP business entity that represents the minimum acceptable results after the testing. Every stage has a quality gate that should be passed to promote the application. The stage quality gate can be a manual approve from a QA specialist OR a successful autotest launch. Quality Gate Type - this value defines trigger type that promotes artifacts (images) to the next environment in CD Pipeline. There are manual and automatic types of quality gates. The manual type means that the promoting process should be confirmed in Jenkins. The automatic type promotes the images automatically in case there are no errors in the Allure Report. NOTE : If any of the test types is not passed, the CD pipeline will fail. Trigger Type - a value that defines a trigger type used for the CD pipeline triggering. There are manual and automatic types of triggering. The manual type means that the CD pipeline should be triggered manually. The automatic type triggers the CD pipeline automatically as soon as the Codebase Docker Stream was changed. EDP CI/CD Pipelines Framework - a library that allows extending the Jenkins pipelines and stages to develop an application. Pipelines are presented as the shared library that can be connected in Jenkins. The library is connected using the Git repository link (a public repository that is supported by EDP) on the GitHub. Allure Report - a tool that represents test results in one brief report in a clear form. Automated Tests - different types of automated tests that can be run on the environment for a specific stage. Build Pipeline - a Jenkins pipeline that builds a corresponding codebase branch in the Codebase. Build Stage - a stage that takes place after the code has been submitted/merged to the repository of the main branch ( the pull request from the feature branch is merged to the main one, the Patch set is submitted in Gerrit ). Code Review Pipeline - a Jenkins pipeline that inspects the code candidate in the Code Review tool. Code Review Stage - a stage where code is reviewed before it goes to the main branch repository of the version control system ( the commit to the feature branch is pushed, the Patch set is created in Gerrit ). Deploy Pipeline - a Jenkins pipeline that is responsible for the CD Pipeline Stage deployment with the full set of applications and autotests. Deployment Stage - a part of the Continuous Delivery where artifacts are being deployed to environments. EDP CI/CD Pipelines - an orchestrator for stages that is responsible for the common technical events, e.g. initialization, in Jenkins pipeline. The set of stages for the pipeline is defined as an input JSON file for the respective Jenkins job. NOTE : There is the ability to create the necessary realization of the library pipeline on your own as well. EDP CI/CD Stages - a repository that is launched in the Jenkins pipeline. Every stage is presented as an individual Groovy file in a corresponding repository. Such single responsibility realization allows rewriting of one essential stage without changing the whole pipeline. Environment - a part of the stage where the built and packed into an image application are deployed for further testing. It`s possible to deploy several applications to several environments (Team and Integration environments) within one stage. Integration Environment - an environment type that is always deployed as soon as the new application version is built in order to launch the integration test and promote images to the next stages. The Integration Environment can be triggered manually or in case a new image appears in the Docker registry. Jenkinsfile - a text file that keeps the definition of a Jenkins Pipeline and is checked into source control. Every Job has its Jenkinsfile that is stored in the specific application repository and in Jenkins as the plain text. Jenkins Node - a machine that is a part of the Jenkins environment that is capable of executing a pipeline. Jenkins Pipeline - a user-defined model of a CD pipeline. The pipeline code defines the entire build process. Jenkins Stage - a part of the whole CI/CD process that should pass the source code in order to be released and deployed on the production. Team Environment - an environment type that can be deployed at any time by the manual trigger of the Deploy pipeline where team or developers can check out their applications. NOTE : The promotion from such kind of environment is prohibited and developed only for the local testing. OpenShift / Kubernetes (K8S) ConfigMap - a resource that stores configuration data and processes the strings that do not contain sensitive information. Docker Container - is a lightweight, standalone, and executable package. Docker Registry - a store for the Docker Container that is created for the application after the Build pipeline performance. OpenShift Web Console - a web console that enables to view, manage, and change OpenShift / K8S resources using browser. Operator Framework - a deployable unit in OpenShift that is responsible for one or a set of resources and performs its life circle (adding, displaying, and provisioning). Path - a route component that helps to find a specified path (e.g. /api) at once and skip the other. Pod - the smallest deployable unit of the large microservice application that is responsible for the application launch. The pod is presented as the one launched Docker container. When the Docker container is collected, it will be kept in Docker Registry and then saved as Pod in the OpenShift project. NOTE : The Deployment Config is responsible for the Pod push, restart, and stop processes. PV (Persistent Volume) - a cluster resource that captures the details of the storage implementation and has an independent lifecycle of any individual pod. PVC (Persistent Volume Claim) - a user request for storage that can request specific size and access mode. PV resources are consumed by PVCs. Route - a resource in OpenShift that allows getting the external access to the pushed application. Secret - an object that stores and manages all the sensitive information (e.g. passwords, tokens, and SSH keys). Service - an external connection point with Pod that is responsible for the network. A specific Service is connected to a specific Pod using labels and redirects all the requests to Pod as well. Site - a route component (link name) that is created from the indicated application name and applies automatically the project name and a wildcard DNS record.","title":"Glossary"},{"location":"glossary/#glossary","text":"Get familiar with the definitions and context for the most useful EDP terms presented in table below. Terms Details EDP Component - an item used in CI/CD process Admin Console - an EDP component that helps to manage, set up, and control the business entities. Artifactory - an EDP component that stores all the binary artifacts. NOTE : Nexus is used as a possible implementation of a repository. CI/CD Server - an EDP component that launches pipelines that perform the build, QA, and deployment code logic. NOTE : Jenkins is used as a possible implementation of a CI/CD server. Code Review tool - an EDP component that collaborates with the changes in the codebase. NOTE : Gerrit is used as a possible implementation of a code review tool. Identity Server - an authentication server providing a common way to verify requests to all of the applications. NOTE : Keycloak is used as a possible implementation of an identity server. Security Realm Tenant - a realm in identity server (e.g Keycloak) where all users' accounts and their access permissions are managed. The realm is unique for the identity server instance. Static Code Analyzer - an EDP component that inspects continuously a code quality before the necessary changes appear in a master branch. NOTE : SonarQube is used as a possible implementation of a static code analyzer. VCS (Version Control System) - a replication of the Gerrit repository that displays all the changes made by developers. NOTE : GitHub and GitLab are used as the possible implementation of a repository with the version control system. EDP Business Entity - a part of the CI/CD process (the integration, delivery, and deployment of any codebase changes) Application - a codebase type that is built as the binary artifact and deployable unit with the code that is stored in VCS. As a result, the application becomes a container and can be deployed in an environment. Autotests - a codebase type that inspects a product (e.g. an application set) on a stage. Autotests are not deployed to any container and launched from the respective code stage. CD Pipeline (Continuous Delivery Pipeline) - an EDP business entity that describes the whole delivery process of the selected application set via the respective stages. The main idea of the CD pipeline is to promote the application version between the stages by applying the sequential verification (i.e. the second stage will be available if the verification on the first stage is successfully completed). NOTE : The CD pipeline can include the essential set of applications with its specific stages as well. CD Pipeline Stage - an EDP business entity that is presented as the logical gate required for the application set inspection. Every stage has one OpenShift project where the selected application set is deployed. All stages are sequential and promote applications one-by-one. Codebase - an EDP business entity that possesses a code. Codebase Branch - an EDP business entity that represents a specific version in a Git branch. Every codebase branch has a Codebase Docker Stream entity. Codebase Docker Stream - a deployable component that leads to the application build and displays that the last build was verified on the specific stage. Every CD pipeline stage accepts a set of Codebase Docker Streams (CDS) that are input and output. SAMPLE: if an application1 has a master branch, the input CDS will be named as [app name]-[pipeline name]-[stage name]-[master] and the output after the passing of the DEV stage will be as follows: [app name]-[pipeline name]-[stage name]-[dev]-[verified]. Library - a codebase type that is built as the binary artifact, i.e. it`s stored in the Artifactory and can be uploaded by other applications, autotests or libraries. Quality Gate - an EDP business entity that represents the minimum acceptable results after the testing. Every stage has a quality gate that should be passed to promote the application. The stage quality gate can be a manual approve from a QA specialist OR a successful autotest launch. Quality Gate Type - this value defines trigger type that promotes artifacts (images) to the next environment in CD Pipeline. There are manual and automatic types of quality gates. The manual type means that the promoting process should be confirmed in Jenkins. The automatic type promotes the images automatically in case there are no errors in the Allure Report. NOTE : If any of the test types is not passed, the CD pipeline will fail. Trigger Type - a value that defines a trigger type used for the CD pipeline triggering. There are manual and automatic types of triggering. The manual type means that the CD pipeline should be triggered manually. The automatic type triggers the CD pipeline automatically as soon as the Codebase Docker Stream was changed. EDP CI/CD Pipelines Framework - a library that allows extending the Jenkins pipelines and stages to develop an application. Pipelines are presented as the shared library that can be connected in Jenkins. The library is connected using the Git repository link (a public repository that is supported by EDP) on the GitHub. Allure Report - a tool that represents test results in one brief report in a clear form. Automated Tests - different types of automated tests that can be run on the environment for a specific stage. Build Pipeline - a Jenkins pipeline that builds a corresponding codebase branch in the Codebase. Build Stage - a stage that takes place after the code has been submitted/merged to the repository of the main branch ( the pull request from the feature branch is merged to the main one, the Patch set is submitted in Gerrit ). Code Review Pipeline - a Jenkins pipeline that inspects the code candidate in the Code Review tool. Code Review Stage - a stage where code is reviewed before it goes to the main branch repository of the version control system ( the commit to the feature branch is pushed, the Patch set is created in Gerrit ). Deploy Pipeline - a Jenkins pipeline that is responsible for the CD Pipeline Stage deployment with the full set of applications and autotests. Deployment Stage - a part of the Continuous Delivery where artifacts are being deployed to environments. EDP CI/CD Pipelines - an orchestrator for stages that is responsible for the common technical events, e.g. initialization, in Jenkins pipeline. The set of stages for the pipeline is defined as an input JSON file for the respective Jenkins job. NOTE : There is the ability to create the necessary realization of the library pipeline on your own as well. EDP CI/CD Stages - a repository that is launched in the Jenkins pipeline. Every stage is presented as an individual Groovy file in a corresponding repository. Such single responsibility realization allows rewriting of one essential stage without changing the whole pipeline. Environment - a part of the stage where the built and packed into an image application are deployed for further testing. It`s possible to deploy several applications to several environments (Team and Integration environments) within one stage. Integration Environment - an environment type that is always deployed as soon as the new application version is built in order to launch the integration test and promote images to the next stages. The Integration Environment can be triggered manually or in case a new image appears in the Docker registry. Jenkinsfile - a text file that keeps the definition of a Jenkins Pipeline and is checked into source control. Every Job has its Jenkinsfile that is stored in the specific application repository and in Jenkins as the plain text. Jenkins Node - a machine that is a part of the Jenkins environment that is capable of executing a pipeline. Jenkins Pipeline - a user-defined model of a CD pipeline. The pipeline code defines the entire build process. Jenkins Stage - a part of the whole CI/CD process that should pass the source code in order to be released and deployed on the production. Team Environment - an environment type that can be deployed at any time by the manual trigger of the Deploy pipeline where team or developers can check out their applications. NOTE : The promotion from such kind of environment is prohibited and developed only for the local testing. OpenShift / Kubernetes (K8S) ConfigMap - a resource that stores configuration data and processes the strings that do not contain sensitive information. Docker Container - is a lightweight, standalone, and executable package. Docker Registry - a store for the Docker Container that is created for the application after the Build pipeline performance. OpenShift Web Console - a web console that enables to view, manage, and change OpenShift / K8S resources using browser. Operator Framework - a deployable unit in OpenShift that is responsible for one or a set of resources and performs its life circle (adding, displaying, and provisioning). Path - a route component that helps to find a specified path (e.g. /api) at once and skip the other. Pod - the smallest deployable unit of the large microservice application that is responsible for the application launch. The pod is presented as the one launched Docker container. When the Docker container is collected, it will be kept in Docker Registry and then saved as Pod in the OpenShift project. NOTE : The Deployment Config is responsible for the Pod push, restart, and stop processes. PV (Persistent Volume) - a cluster resource that captures the details of the storage implementation and has an independent lifecycle of any individual pod. PVC (Persistent Volume Claim) - a user request for storage that can request specific size and access mode. PV resources are consumed by PVCs. Route - a resource in OpenShift that allows getting the external access to the pushed application. Secret - an object that stores and manages all the sensitive information (e.g. passwords, tokens, and SSH keys). Service - an external connection point with Pod that is responsible for the network. A specific Service is connected to a specific Pod using labels and redirects all the requests to Pod as well. Site - a route component (link name) that is created from the indicated application name and applies automatically the project name and a wildcard DNS record.","title":"Glossary"},{"location":"roadmap/","text":"RoadMap \u2693\ufe0e RoadMap consists of three streams: Architecture Building Blocks Admin Console Documentation I. Architecture \u2693\ufe0e Goals: Improve reusability for EDP components Integrate Kubernetes Native Deployment solutions Introduce abstraction layer for CI/CD components Build processes around the GitOps approach Introduce secrets management Kubernetes Multitenancy \u2693\ufe0e Multiple instances of EDP are run in a single Kubernetes cluster. One way to achieve this is to use Multitenancy . Initially, Kiosk was selected as tools that provides this capability. An alternative option that EDP Team took into consideration is Capsule . Another tool which goes far beyond multitenancy is vcluster going a good candidate for e2e testing scenarios where one needs simple lightweight kubernetes cluster in CI pipelines. Microservice Reference Architecture Framework \u2693\ufe0e EDP provides basic Application Templates for a number of technology stacks (Java, .Net, NPM, Python) and Helm is used as a deployment tool. The goal is to extend this library and provide: Application Templates which are built on pre-defined architecture patterns (e.g., Microservice, API Gateway, Circuit Breaker, CQRS, Event Driven) and Deployment Approaches : Canary, Blue/Green. This requires additional tools installation on cluster as well. Policy Enforcement for Kubernetes \u2693\ufe0e Running workload in Kubernetes calls for extra effort from Cluster Administrators to ensure those workloads do follow best practices or specific requirements defined on organization level. Those requirements can be formalized in policies and integrated into: CI Pipelines and Kubernetes Cluster (through Admission Controller approach) - to guarantee proper resource management during development and runtime phases. EDP uses Open Policy Agent (from version 2.8.0), since it supports compliance check for more use-cases: Kubernetes Workloads, Terraform and Java code, HTTP APIs and many others . Kyverno is another option being checked in scope of this activity. Secrets Management \u2693\ufe0e EDP should provide secrets management as a part of platform. There are multiple tools providing secrets management capabilities. The aim is to be aligned with GitOps and Operator Pattern approaches so HashiCorp Vault , Banzaicloud Bank Vaults , Bitnami Sealed Secrets are currently used for internal projects and some of them should be made publicly available - as a part of EDP Deployment. EDP Release 2.12.x External Secret Operator is a recommended secret management tool for the EDP components. Release Management \u2693\ufe0e Conventional Commits and Conventional Changelog are two approaches to be used as part of release process. Today EDP provides only capabilities to manage Release Branches . This activity should address this gap by formalizing and implementing Release Process as a part of EDP. Topics to be covered: Versioning, Tagging, Artifacts Promotion. Kubernetes Native CI/CD Pipelines \u2693\ufe0e EDP uses Jenkins as Pipeline Orchestrator. Jenkins runs workload for CI and CD parts. There is also basic support for GitLab CI , but it provides Docker image build functionality only. EDP works on providing an alternative to Jenkins and use Kubernetes Native Approach for pipeline management. There are a number of tools, which provides such capability: Argo CD Argo Workflows Argo Rollouts Tekton Drone Flux This list is under investigation and solution is going to be implemented in two steps: Introduce tool that provide Continues Delivery/Deployment approach. Argo CD is one of the best to go with. Integrate EDP with tool that provides Continues Integration capabilities. EDP Release 2.12.x Argo CD is suggested as a solution providing the Continuous Delivery capabilities. A new EDP operator called edp-argocd-operator has been developed, which runs as an adapter layer between the EDP Platform and Argo CD. Advanced EDP Role-based Model \u2693\ufe0e EDP has a number of base roles which are used across EDP. In some cases it is necessary to provide more granular permissions for specific users. It is possible to do this using Kubernetes Native approach. Notifications Framework \u2693\ufe0e EDP has a number of components which need to report their statuses: Build/Code Review/Deploy Pipelines, changes in Environments, updates with artifacts. The goal for this activity is to onboard Kubernetes Native approach which provides Notification capabilities with different sources/channels integration (e.g. Email, Slack, MS Teams). Some of these tools are Argo Events , Botkube . Reconciler Component Retirement \u2693\ufe0e Persistent layer, which is based on edp-db (PostgreSQL) and reconciler component should be retired in favour of Kubernetes Custom Resource (CR) . The latest features in EDP are implemented using CR approach. II. Building Blocks \u2693\ufe0e Goals: Introduce best practices from Microservice Reference Architecture deployment and observability using Kubernetes Native Tools Enable integration with the Centralized Test Reporting Frameworks Onboard SAST/DAST tool as a part of CI pipelines and Non-Functional Testing activities EDP Release 2.12.x SAST is introduced as a mandatory part of the CI Pipelines . The list of currently supported SAST scanners and the instruction on how to add them are also available. Infrastructure as Code \u2693\ufe0e EDP Target tool for Infrastructure as Code (IaC) is Terraform . EDP sees two CI/CD scenarios while working with IaC: Module Development and Live Environment Deployment . Today, EDP provides basic capabilities (CI Pipelines) for Terraform Module Development . At the same time, currently EDP doesn't provide Deployment pipelines for Live Environments and the feature is under development. Terragrunt is an option to use in Live Environment deployment. Another Kubernetes Native approach to provision infrastructure components is Crossplane . Database Schema Management \u2693\ufe0e One of the challenges for Application running in Kubernetes is to manage database schema. There are a number of tools which provides such capabilities, e.g. Liquibase , Flyway . Both tools provide versioning control for database schemas. There are different approaches on how to run migration scripts in Kubernetes : in init container , as separate Job or as a separate CD stage. Purpose of this activity is to provide database schema management solution in Kubernetes as a part of EDP. EDP Team investigates SchemaHero tool and use-cases which suits Kubernetes native approach for database schema migrations. Open Policy Agent \u2693\ufe0e Open Policy Agent is introduced in version 2.8.0 . EDP now supports CI for Rego Language , so you can develop your own policies. The next goal is to provide pipeline steps for running compliance policies check for Terraform, Java, Helm Chart as a part of CI process. Report Portal \u2693\ufe0e EDP uses Allure Framework as a Test Report tool . Another option is to integrate Report Portal into EDP ecosystem. Carrier \u2693\ufe0e Carrier provides Non-functional testing capabilities. Java 17 \u2693\ufe0e EDP supports two LTS versions of Java: 8 and 11. The goal is to provide Java 17 (LTS) support. Velero \u2693\ufe0e Velero is used as a cluster backup tool and is deployed as a part of Platform. Currently, Multitenancy/On-premise support for backup capabilities is in process. Istio \u2693\ufe0e Istio is to be used as a Service Mesh and to address challenges for Microservice or Distributed Architectures. Kong \u2693\ufe0e Kong is one of tools which is planned to use as an API Gateway solution provider. Another possible candidate for investigation is Ambassador API Gateway OpenShift 4.X \u2693\ufe0e OpenShift 4.6 is a platform that EDP supports. EDP Release 2.12.x EDP Platform runs on the latest OKD versions: 4.9 and 4.10 . Creating the IAM Roles for Service Account is a recommended way to work with AWS Resources from the OKD cluster. III. Admin Console (UI) \u2693\ufe0e Goals: Improve U\u0425 for different user types to address their concerns in the delivery model Introduce user management capabilities Enrich with traceability metrics for products EDP Release 2.12.x EDP Team has introduced a new UI component called EDP Headlamp , which will replace the EDP Admin Console in future releases. EDP Headlamp is based on the Kinvolk Headlamp UI Client . Users Management \u2693\ufe0e EDP uses Keycloak as Identity and Access provider. EDP roles/groups are managed inside Keycloak realm, then these changes are propagated across EDP Tools. The plan is to provide this functionality in EDP Admin Console using Kubernetes native approach (Custom Resources). The Delivery Pipelines Dashboard \u2693\ufe0e EDP CD Pipeline section in Admin Console provides basic information like: environments, artifact versions deployed per each environment, direct links to namespaces. One option is to enrich this panel with metrics (from prometheus, custom resources, events, etc). Another option is to use existing dashboards and expose EDP metrics to them, e.g. plugin for Lens , OpenShift UI Console Split Jira and Commit Validation Sections \u2693\ufe0e Commit Validate step was initially designed to be aligned with Jira Integration and cannot be used as single feature. Target state is to ensure features CommitMessage Validation and Jira Integration both can be used independently. We also want to add support for Conventional Commits . IV. Documentation as Code \u2693\ufe0e Goal: Transparent documentation and clear development guidelines for EDP customization. Consolidate documentation in a single repository edp-install , use mkdocs tool to generate docs and GitHub Pages as a hosting solution.","title":"RoadMap"},{"location":"roadmap/#roadmap","text":"RoadMap consists of three streams: Architecture Building Blocks Admin Console Documentation","title":"RoadMap"},{"location":"roadmap/#i-architecture","text":"Goals: Improve reusability for EDP components Integrate Kubernetes Native Deployment solutions Introduce abstraction layer for CI/CD components Build processes around the GitOps approach Introduce secrets management","title":"I. Architecture"},{"location":"roadmap/#kubernetes-multitenancy","text":"Multiple instances of EDP are run in a single Kubernetes cluster. One way to achieve this is to use Multitenancy . Initially, Kiosk was selected as tools that provides this capability. An alternative option that EDP Team took into consideration is Capsule . Another tool which goes far beyond multitenancy is vcluster going a good candidate for e2e testing scenarios where one needs simple lightweight kubernetes cluster in CI pipelines.","title":"Kubernetes Multitenancy"},{"location":"roadmap/#microservice-reference-architecture-framework","text":"EDP provides basic Application Templates for a number of technology stacks (Java, .Net, NPM, Python) and Helm is used as a deployment tool. The goal is to extend this library and provide: Application Templates which are built on pre-defined architecture patterns (e.g., Microservice, API Gateway, Circuit Breaker, CQRS, Event Driven) and Deployment Approaches : Canary, Blue/Green. This requires additional tools installation on cluster as well.","title":"Microservice Reference Architecture Framework"},{"location":"roadmap/#policy-enforcement-for-kubernetes","text":"Running workload in Kubernetes calls for extra effort from Cluster Administrators to ensure those workloads do follow best practices or specific requirements defined on organization level. Those requirements can be formalized in policies and integrated into: CI Pipelines and Kubernetes Cluster (through Admission Controller approach) - to guarantee proper resource management during development and runtime phases. EDP uses Open Policy Agent (from version 2.8.0), since it supports compliance check for more use-cases: Kubernetes Workloads, Terraform and Java code, HTTP APIs and many others . Kyverno is another option being checked in scope of this activity.","title":"Policy Enforcement for Kubernetes"},{"location":"roadmap/#secrets-management","text":"EDP should provide secrets management as a part of platform. There are multiple tools providing secrets management capabilities. The aim is to be aligned with GitOps and Operator Pattern approaches so HashiCorp Vault , Banzaicloud Bank Vaults , Bitnami Sealed Secrets are currently used for internal projects and some of them should be made publicly available - as a part of EDP Deployment. EDP Release 2.12.x External Secret Operator is a recommended secret management tool for the EDP components.","title":"Secrets Management"},{"location":"roadmap/#release-management","text":"Conventional Commits and Conventional Changelog are two approaches to be used as part of release process. Today EDP provides only capabilities to manage Release Branches . This activity should address this gap by formalizing and implementing Release Process as a part of EDP. Topics to be covered: Versioning, Tagging, Artifacts Promotion.","title":"Release Management"},{"location":"roadmap/#kubernetes-native-cicd-pipelines","text":"EDP uses Jenkins as Pipeline Orchestrator. Jenkins runs workload for CI and CD parts. There is also basic support for GitLab CI , but it provides Docker image build functionality only. EDP works on providing an alternative to Jenkins and use Kubernetes Native Approach for pipeline management. There are a number of tools, which provides such capability: Argo CD Argo Workflows Argo Rollouts Tekton Drone Flux This list is under investigation and solution is going to be implemented in two steps: Introduce tool that provide Continues Delivery/Deployment approach. Argo CD is one of the best to go with. Integrate EDP with tool that provides Continues Integration capabilities. EDP Release 2.12.x Argo CD is suggested as a solution providing the Continuous Delivery capabilities. A new EDP operator called edp-argocd-operator has been developed, which runs as an adapter layer between the EDP Platform and Argo CD.","title":"Kubernetes Native CI/CD Pipelines"},{"location":"roadmap/#advanced-edp-role-based-model","text":"EDP has a number of base roles which are used across EDP. In some cases it is necessary to provide more granular permissions for specific users. It is possible to do this using Kubernetes Native approach.","title":"Advanced EDP Role-based Model"},{"location":"roadmap/#notifications-framework","text":"EDP has a number of components which need to report their statuses: Build/Code Review/Deploy Pipelines, changes in Environments, updates with artifacts. The goal for this activity is to onboard Kubernetes Native approach which provides Notification capabilities with different sources/channels integration (e.g. Email, Slack, MS Teams). Some of these tools are Argo Events , Botkube .","title":"Notifications Framework"},{"location":"roadmap/#reconciler-component-retirement","text":"Persistent layer, which is based on edp-db (PostgreSQL) and reconciler component should be retired in favour of Kubernetes Custom Resource (CR) . The latest features in EDP are implemented using CR approach.","title":"Reconciler Component Retirement"},{"location":"roadmap/#ii-building-blocks","text":"Goals: Introduce best practices from Microservice Reference Architecture deployment and observability using Kubernetes Native Tools Enable integration with the Centralized Test Reporting Frameworks Onboard SAST/DAST tool as a part of CI pipelines and Non-Functional Testing activities EDP Release 2.12.x SAST is introduced as a mandatory part of the CI Pipelines . The list of currently supported SAST scanners and the instruction on how to add them are also available.","title":"II. Building Blocks"},{"location":"roadmap/#infrastructure-as-code","text":"EDP Target tool for Infrastructure as Code (IaC) is Terraform . EDP sees two CI/CD scenarios while working with IaC: Module Development and Live Environment Deployment . Today, EDP provides basic capabilities (CI Pipelines) for Terraform Module Development . At the same time, currently EDP doesn't provide Deployment pipelines for Live Environments and the feature is under development. Terragrunt is an option to use in Live Environment deployment. Another Kubernetes Native approach to provision infrastructure components is Crossplane .","title":"Infrastructure as Code"},{"location":"roadmap/#database-schema-management","text":"One of the challenges for Application running in Kubernetes is to manage database schema. There are a number of tools which provides such capabilities, e.g. Liquibase , Flyway . Both tools provide versioning control for database schemas. There are different approaches on how to run migration scripts in Kubernetes : in init container , as separate Job or as a separate CD stage. Purpose of this activity is to provide database schema management solution in Kubernetes as a part of EDP. EDP Team investigates SchemaHero tool and use-cases which suits Kubernetes native approach for database schema migrations.","title":"Database Schema Management"},{"location":"roadmap/#open-policy-agent","text":"Open Policy Agent is introduced in version 2.8.0 . EDP now supports CI for Rego Language , so you can develop your own policies. The next goal is to provide pipeline steps for running compliance policies check for Terraform, Java, Helm Chart as a part of CI process.","title":"Open Policy Agent"},{"location":"roadmap/#report-portal","text":"EDP uses Allure Framework as a Test Report tool . Another option is to integrate Report Portal into EDP ecosystem.","title":"Report Portal"},{"location":"roadmap/#carrier","text":"Carrier provides Non-functional testing capabilities.","title":"Carrier"},{"location":"roadmap/#java-17","text":"EDP supports two LTS versions of Java: 8 and 11. The goal is to provide Java 17 (LTS) support.","title":"Java 17"},{"location":"roadmap/#velero","text":"Velero is used as a cluster backup tool and is deployed as a part of Platform. Currently, Multitenancy/On-premise support for backup capabilities is in process.","title":"Velero"},{"location":"roadmap/#istio","text":"Istio is to be used as a Service Mesh and to address challenges for Microservice or Distributed Architectures.","title":"Istio"},{"location":"roadmap/#kong","text":"Kong is one of tools which is planned to use as an API Gateway solution provider. Another possible candidate for investigation is Ambassador API Gateway","title":"Kong"},{"location":"roadmap/#openshift-4x","text":"OpenShift 4.6 is a platform that EDP supports. EDP Release 2.12.x EDP Platform runs on the latest OKD versions: 4.9 and 4.10 . Creating the IAM Roles for Service Account is a recommended way to work with AWS Resources from the OKD cluster.","title":"OpenShift 4.X"},{"location":"roadmap/#iii-admin-console-ui","text":"Goals: Improve U\u0425 for different user types to address their concerns in the delivery model Introduce user management capabilities Enrich with traceability metrics for products EDP Release 2.12.x EDP Team has introduced a new UI component called EDP Headlamp , which will replace the EDP Admin Console in future releases. EDP Headlamp is based on the Kinvolk Headlamp UI Client .","title":"III. Admin Console (UI)"},{"location":"roadmap/#users-management","text":"EDP uses Keycloak as Identity and Access provider. EDP roles/groups are managed inside Keycloak realm, then these changes are propagated across EDP Tools. The plan is to provide this functionality in EDP Admin Console using Kubernetes native approach (Custom Resources).","title":"Users Management"},{"location":"roadmap/#the-delivery-pipelines-dashboard","text":"EDP CD Pipeline section in Admin Console provides basic information like: environments, artifact versions deployed per each environment, direct links to namespaces. One option is to enrich this panel with metrics (from prometheus, custom resources, events, etc). Another option is to use existing dashboards and expose EDP metrics to them, e.g. plugin for Lens , OpenShift UI Console","title":"The Delivery Pipelines Dashboard"},{"location":"roadmap/#split-jira-and-commit-validation-sections","text":"Commit Validate step was initially designed to be aligned with Jira Integration and cannot be used as single feature. Target state is to ensure features CommitMessage Validation and Jira Integration both can be used independently. We also want to add support for Conventional Commits .","title":"Split Jira and Commit Validation Sections"},{"location":"roadmap/#iv-documentation-as-code","text":"Goal: Transparent documentation and clear development guidelines for EDP customization. Consolidate documentation in a single repository edp-install , use mkdocs tool to generate docs and GitHub Pages as a hosting solution.","title":"IV. Documentation as Code"},{"location":"developer-guide/","text":"Overview \u2693\ufe0e The EDP Developer guide is intended for developers and provides details on the necessary actions to extend the EDP functionality.","title":"Overview"},{"location":"developer-guide/#overview","text":"The EDP Developer guide is intended for developers and provides details on the necessary actions to extend the EDP functionality.","title":"Overview"},{"location":"developer-guide/edp-workflow/","text":"EDP Project Rules. Working Process \u2693\ufe0e This page contains the details on the project rules and working process for EDP team and contributors. Explore the main points about working with Gerrit, following the main commit flow, as well as the details about commit types and message below. Project Rules \u2693\ufe0e Before starting the development, please check the project rules: It is highly recommended to become familiar with the Gerrit flow. For details, please refer to the Gerrit official documentation and pay attention to the main points: a. Voting in Gerrit b. Resolution of Merge Conflict c. Comments resolution d. One Jira task should have one Merge Request (MR). If there are many changes within one MR, add the next patch set to the open MR by selecting the Amend commit check box. Only the Assignee is responsible for the MR merge and Jira task status. Every MR should be merged in a timely manner. Log time to Jira ticket. Working Process \u2693\ufe0e With EDP, the main workflow is based on the getting a Jira task and creating a Merge Request according to the rules described below. Workflow Get Jira task \u2192 implement, verify by yourself the results \u2192 create Merge Request (MR) \u2192 send for review \u2192 resolve comments/add changes, ask colleagues for the final review \u2192 track the MR merge \u2192 verify by yourself the results \u2192 change the status in the Jira ticket to CODE COMPLETE or RESOLVED \u2192 share necessary links with a QA specialist in the QA Verification channel \u2192 QA specialist closes the Jira task after his verification \u2192 Jira task should be CLOSED. Commit Flow Get Jira task. Please be aware of the following points: a. Every task has a reporter who can provide more details in case something is not clear. b. The responsible person for the task and code implementation is the assignee who tracks the following: actual Jira task status time logging add comments, attach necessary files in comments, add link that refers to the merged MR (optional, if not related to many repositories) code review and the final merge MS Teams chats - ping other colleagues, answer questions, etc. verification by a QA specialist bug fixing c. Pay attention to the task Status that differs in different entities, the workflow will help to see the whole task processing: View Jira workflow d. There are several entities that are used on the EDP project: Story, Improvement, Task, Bug. Implement feature, improvement, fix and check the results on your own. If it is impossible to check the results of your work before the merge, verify all later. Create a Merge Request, for details, please refer to the Code Review Process . When committing, use the pattern: [EPMDEDP-JIRA Task Number]: commit type: Commit message. a. [EPMDEDP] - is the default part; b. JIRA Task Number - the number of your Jira task; c. commit type: feat : (new feature for the user, not a new feature for build script) fix : (bug fix for the user, not a fix to a build script) docs : (changes to the documentation) style : (formatting, missing semicolons, etc; no production code change) refactor : (refactoring production code, eg. renaming a variable) test : (adding missing tests, refactoring tests; no production code change) chore : (updating grunt tasks etc; no production code change) ! : (added to other commit types to mark breaking changes) For example: [ EPMDEDP - 0000 ]: feat ! : Job provisioner is responsible for the formation of Jenkinsfile BREAKING CHANGE : Job provisioner creates Jenkinsfile and configures it in Jenkins pipeline as a pipeline script . d. Commit message: brief, for example: [EPMDEDP-0000]: fix: Fix Gerrit plugin for Jenkins provisioning or descriptive, for example: [EPMDEDP-0000]: feat: Provide the ability to configure hadolint check *Add configuration files .hadolint.yaml and .hadolint.yml to stash Note Make sure there is a descriptive commit message for a breaking change Merge Request. For example: [EPMDEDP-0000]: feat!: Job provisioner is responsible for the formation of Jenkinsfile BREAKING CHANGE: Job provisioner creates Jenkinsfile and configures it in Jenkins pipeline as a pipeline script. Note If a Merge Request contains both new functionality and breaking changes, make sure the functionality description is placed before the breaking changes. For example: [EPMDEDP-0000]: feat!: Update Gerrit to improve access Implement Developers group creation process Align group permissions BREAKING CHANGES: Update Gerrit config according to groups Related Articles \u2693\ufe0e Conventional Commits Semantic Commit Messages Karma","title":"EDP Project Rules. Working Process"},{"location":"developer-guide/edp-workflow/#edp-project-rules-working-process","text":"This page contains the details on the project rules and working process for EDP team and contributors. Explore the main points about working with Gerrit, following the main commit flow, as well as the details about commit types and message below.","title":"EDP Project Rules. Working Process"},{"location":"developer-guide/edp-workflow/#project-rules","text":"Before starting the development, please check the project rules: It is highly recommended to become familiar with the Gerrit flow. For details, please refer to the Gerrit official documentation and pay attention to the main points: a. Voting in Gerrit b. Resolution of Merge Conflict c. Comments resolution d. One Jira task should have one Merge Request (MR). If there are many changes within one MR, add the next patch set to the open MR by selecting the Amend commit check box. Only the Assignee is responsible for the MR merge and Jira task status. Every MR should be merged in a timely manner. Log time to Jira ticket.","title":"Project Rules"},{"location":"developer-guide/edp-workflow/#working-process","text":"With EDP, the main workflow is based on the getting a Jira task and creating a Merge Request according to the rules described below. Workflow Get Jira task \u2192 implement, verify by yourself the results \u2192 create Merge Request (MR) \u2192 send for review \u2192 resolve comments/add changes, ask colleagues for the final review \u2192 track the MR merge \u2192 verify by yourself the results \u2192 change the status in the Jira ticket to CODE COMPLETE or RESOLVED \u2192 share necessary links with a QA specialist in the QA Verification channel \u2192 QA specialist closes the Jira task after his verification \u2192 Jira task should be CLOSED. Commit Flow Get Jira task. Please be aware of the following points: a. Every task has a reporter who can provide more details in case something is not clear. b. The responsible person for the task and code implementation is the assignee who tracks the following: actual Jira task status time logging add comments, attach necessary files in comments, add link that refers to the merged MR (optional, if not related to many repositories) code review and the final merge MS Teams chats - ping other colleagues, answer questions, etc. verification by a QA specialist bug fixing c. Pay attention to the task Status that differs in different entities, the workflow will help to see the whole task processing: View Jira workflow d. There are several entities that are used on the EDP project: Story, Improvement, Task, Bug. Implement feature, improvement, fix and check the results on your own. If it is impossible to check the results of your work before the merge, verify all later. Create a Merge Request, for details, please refer to the Code Review Process . When committing, use the pattern: [EPMDEDP-JIRA Task Number]: commit type: Commit message. a. [EPMDEDP] - is the default part; b. JIRA Task Number - the number of your Jira task; c. commit type: feat : (new feature for the user, not a new feature for build script) fix : (bug fix for the user, not a fix to a build script) docs : (changes to the documentation) style : (formatting, missing semicolons, etc; no production code change) refactor : (refactoring production code, eg. renaming a variable) test : (adding missing tests, refactoring tests; no production code change) chore : (updating grunt tasks etc; no production code change) ! : (added to other commit types to mark breaking changes) For example: [ EPMDEDP - 0000 ]: feat ! : Job provisioner is responsible for the formation of Jenkinsfile BREAKING CHANGE : Job provisioner creates Jenkinsfile and configures it in Jenkins pipeline as a pipeline script . d. Commit message: brief, for example: [EPMDEDP-0000]: fix: Fix Gerrit plugin for Jenkins provisioning or descriptive, for example: [EPMDEDP-0000]: feat: Provide the ability to configure hadolint check *Add configuration files .hadolint.yaml and .hadolint.yml to stash Note Make sure there is a descriptive commit message for a breaking change Merge Request. For example: [EPMDEDP-0000]: feat!: Job provisioner is responsible for the formation of Jenkinsfile BREAKING CHANGE: Job provisioner creates Jenkinsfile and configures it in Jenkins pipeline as a pipeline script. Note If a Merge Request contains both new functionality and breaking changes, make sure the functionality description is placed before the breaking changes. For example: [EPMDEDP-0000]: feat!: Update Gerrit to improve access Implement Developers group creation process Align group permissions BREAKING CHANGES: Update Gerrit config according to groups","title":"Working Process"},{"location":"developer-guide/edp-workflow/#related-articles","text":"Conventional Commits Semantic Commit Messages Karma","title":"Related Articles"},{"location":"developer-guide/local-development/","text":"Local Development \u2693\ufe0e Requirements \u2693\ufe0e GoLang version higher than 1.13; Note The GOPATH and GOROOT environment variables should be added in PATH. PostgreSQL client version higher than 9.5; Configured access to the VCS; GoLand Intellij IDEA or another IDE. Start Operator \u2693\ufe0e In order to run the operator, follow the steps below: Clone repository; Open folder in GoLand Intellij IDEA, click the button and select the Go Build option: Add configuration In Configuration tab, fill in the following: 3.1. In the Field field, indicate the path to the main.go file; 3.2. In the Working directory field, indicate the path to the operator; 3.3. In the Environment field, specify the platform name (OpenShift/Kubernetes); Build config Create the PostgreSQL database, schema, and a user for the EDP Admin Console operator: Create database with a user: CREATE DATABASE edp-db WITH ENCODING 'UTF8'; CREATE USER postgres WITH PASSWORD 'password'; GRANT ALL PRIVILEGES ON DATABASE 'edp-db' to postgres; Create a schema: CREATE SCHEMA [ IF NOT EXISTS ] ' develop ' ; EDP Admin Console operator supports two modes for running: local and prod. For local deploy, modify edp-admin-console/conf/app.conf and set the following parameters: runmode = local [ local ] dbEnabled = true pgHost = localhost pgPort = 5432 pgDatabase = edp - db pgUser = postgres pgPassword = password edpName = develop Run go build main.go (Shift+F10); After the successful setup, follow the http://localhost:8080 URL address to check the result: Admin console UI Exceptional Cases \u2693\ufe0e After starting the Go build process, the following error will appear: go : finding github . com / openshift / api v3 .9.0 go : finding github . com / openshift / client - go v3 .9.0 go : errors parsing go . mod : C : \\ Users \\ << username >> \\ Desktop \\ EDP \\ edp - admin - console \\ go . mod : 36 : require github . com / openshift / api : version \"v3.9.0\" invalid : unknown revision v3 .9.0 Compilation finished with exit code 1 To resolve the issue, update the go dependency by applying the Golang command: go get github . com / openshift / api @v3 .9.0","title":"Local Development"},{"location":"developer-guide/local-development/#local-development","text":"","title":"Local Development"},{"location":"developer-guide/local-development/#requirements","text":"GoLang version higher than 1.13; Note The GOPATH and GOROOT environment variables should be added in PATH. PostgreSQL client version higher than 9.5; Configured access to the VCS; GoLand Intellij IDEA or another IDE.","title":"Requirements"},{"location":"developer-guide/local-development/#start-operator","text":"In order to run the operator, follow the steps below: Clone repository; Open folder in GoLand Intellij IDEA, click the button and select the Go Build option: Add configuration In Configuration tab, fill in the following: 3.1. In the Field field, indicate the path to the main.go file; 3.2. In the Working directory field, indicate the path to the operator; 3.3. In the Environment field, specify the platform name (OpenShift/Kubernetes); Build config Create the PostgreSQL database, schema, and a user for the EDP Admin Console operator: Create database with a user: CREATE DATABASE edp-db WITH ENCODING 'UTF8'; CREATE USER postgres WITH PASSWORD 'password'; GRANT ALL PRIVILEGES ON DATABASE 'edp-db' to postgres; Create a schema: CREATE SCHEMA [ IF NOT EXISTS ] ' develop ' ; EDP Admin Console operator supports two modes for running: local and prod. For local deploy, modify edp-admin-console/conf/app.conf and set the following parameters: runmode = local [ local ] dbEnabled = true pgHost = localhost pgPort = 5432 pgDatabase = edp - db pgUser = postgres pgPassword = password edpName = develop Run go build main.go (Shift+F10); After the successful setup, follow the http://localhost:8080 URL address to check the result: Admin console UI","title":"Start Operator"},{"location":"developer-guide/local-development/#exceptional-cases","text":"After starting the Go build process, the following error will appear: go : finding github . com / openshift / api v3 .9.0 go : finding github . com / openshift / client - go v3 .9.0 go : errors parsing go . mod : C : \\ Users \\ << username >> \\ Desktop \\ EDP \\ edp - admin - console \\ go . mod : 36 : require github . com / openshift / api : version \"v3.9.0\" invalid : unknown revision v3 .9.0 Compilation finished with exit code 1 To resolve the issue, update the go dependency by applying the Golang command: go get github . com / openshift / api @v3 .9.0","title":"Exceptional Cases"},{"location":"developer-guide/mk-docs-development/","text":"Documentation Flow \u2693\ufe0e This section defines necessary steps to start developing the EDP documentation in the MkDocs Framework. The framework presents a static site generator with documentation written in Markdown. All the docs are configured with a YAML configuration file. Note For more details on the framework, please refer to the MkDocs official website . There are two options for working with MkDocs: Work with MkDocs if Docker is installed Work with MkDocs if Docker is not installed Please see below the detailed description of each options and choose the one that suits you. MkDocs With Docker \u2693\ufe0e Prerequisites: Docker is installed. make utility is installed. Git is installed. Please refer to the Git downloads . To work with MkDocs, take the following steps: Clone the edp-install repository to your local folder. Run the following command: make docs Enter the localhost:8000 address in the browser and check that documentation pages are available. Open the file editor, navigate to edp-install->docs and make necessary changes. Check all the changes at localhost:8000. Create a merge request with changes. MkDocs Without Docker \u2693\ufe0e Prerequisites: Git is installed. Please refer to the Git downloads . Python 3.9.5 is installed. To work with MkDocs without Docker, take the following steps: Clone the edp-install repository to your local folder. Run the following command: pip install -r hack/mkdocs/requirements.txt Run the local development command: mkdocs serve --dev-addr 0.0.0.0:8000 Note This command may not work on Windows, so a quick solution is: python -m mkdocs serve --dev-addr 0.0.0.0:8000 Enter the localhost:8000 address in the browser and check that documentation pages are available. Open the file editor, navigate to edp-install->docs and make necessary changes. Check all the changes at localhost:8000. Create a merge request with changes.","title":"Documentation Flow"},{"location":"developer-guide/mk-docs-development/#documentation-flow","text":"This section defines necessary steps to start developing the EDP documentation in the MkDocs Framework. The framework presents a static site generator with documentation written in Markdown. All the docs are configured with a YAML configuration file. Note For more details on the framework, please refer to the MkDocs official website . There are two options for working with MkDocs: Work with MkDocs if Docker is installed Work with MkDocs if Docker is not installed Please see below the detailed description of each options and choose the one that suits you.","title":"Documentation Flow"},{"location":"developer-guide/mk-docs-development/#mkdocs-with-docker","text":"Prerequisites: Docker is installed. make utility is installed. Git is installed. Please refer to the Git downloads . To work with MkDocs, take the following steps: Clone the edp-install repository to your local folder. Run the following command: make docs Enter the localhost:8000 address in the browser and check that documentation pages are available. Open the file editor, navigate to edp-install->docs and make necessary changes. Check all the changes at localhost:8000. Create a merge request with changes.","title":"MkDocs With Docker"},{"location":"developer-guide/mk-docs-development/#mkdocs-without-docker","text":"Prerequisites: Git is installed. Please refer to the Git downloads . Python 3.9.5 is installed. To work with MkDocs without Docker, take the following steps: Clone the edp-install repository to your local folder. Run the following command: pip install -r hack/mkdocs/requirements.txt Run the local development command: mkdocs serve --dev-addr 0.0.0.0:8000 Note This command may not work on Windows, so a quick solution is: python -m mkdocs serve --dev-addr 0.0.0.0:8000 Enter the localhost:8000 address in the browser and check that documentation pages are available. Open the file editor, navigate to edp-install->docs and make necessary changes. Check all the changes at localhost:8000. Create a merge request with changes.","title":"MkDocs Without Docker"},{"location":"developer-guide/rest-api/","text":"EDP API \u2693\ufe0e Create Codebase Entity \u2693\ufe0e EDP allows you to create three codebase types: Application, Autotest and Library. There are also several strategy types for each codebase: Create, Clone and Import. Depending on the selected codebase type and the respective strategy, you should specify a different set of fields in a request. Note The Route, Database and VCS are optional fields. In accordance with the necessary deploy set, you have to add the necessary fields into request Request \u2693\ufe0e POST /api/v1/edp/codebase Application (Create) \u2693\ufe0e { \"name\": \"app01\", \"type\": \"application\", \"strategy\": \"create\", \"lang\": \"java\", \"framework\": \"springboot\", \"buildTool\": \"maven\", \"multiModule\": false, \"route\": { \"site\": \"api\", \"path\": \"/\" }, \"database\": { \"kind\": \"postgresql\", \"version\": \"postgres:9.6\", \"capacity\": \"1Gi\", \"storage\": \"efs\" }, \"description\": \"Description\", \"gitServer\": \"gerrit\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", \"deploymentScript\": \"openshift-template\" } Application (Clone) \u2693\ufe0e { \" name \" : \" app01 \" , \" type \" : \" application \" , \" strategy \" : \" clone \" , \" lang \" : \" java \" , \" framework \" : \" springboot \" , \" buildTool \" : \" maven \" , \" multiModule \" : false , \" repository \" : { \" url \" : \" http(s)://git.sample.com/sample.git \" , // login and password are required only if repo is private \" login \" : \" login \" , \" password \" : \" password \" }, \" description \" : \" Description \" , \" gitServer \" : \" gerrit \" , \" jenkinsSlave \" : \" maven \" , \" jobProvisioning \" : \" default \" , \" deploymentScript \" : \" openshift-template \" } Application (Import) \u2693\ufe0e { \"type\": \"application\", \"strategy\": \"import\", \"lang\": \"java\", \"framework\": \"springboot\", \"buildTool\": \"maven\", \"multiModule\": false, \"description\": \"Description\", \"gitServer\": \"git-epam\", \"gitUrlPath\": \"/relative/path/to/repo\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", \"deploymentScript\": \"openshift-template\" } Autotests (Clone) \u2693\ufe0e { \" name \" : \" aut01 \" , \" type \" : \" autotests \" , \" strategy \" : \" clone \" , \" lang \" : \" java \" , \" framework \" : \" springboot \" , \" buildTool \" : \" maven \" , \" testReportFramework \" : \" allure \" , \" repository \" : { \" url \" : \" http(s)://git.sample.com/sample.git \" , // login and password are required only if repo is private \" login \" : \" login \" , \" password \" : \" password \" }, \" description \" : \" Description \" , \" jenkinsSlave \" : \" maven \" , \" jobProvisioning \" : \" default \" } Autotests (Import) \u2693\ufe0e { \"type\": \"autotests\", \"strategy\": \"import\", \"lang\": \"java\", \"framework\": \"springboot\", \"buildTool\": \"maven\", \"testReportFramework\": \"allure\", \"description\": \"Description\", \"gitServer\": \"git-epam\", \"gitRelativePath\": \"/relative/path/to/repo\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\" } Library (Create) \u2693\ufe0e { \"name\": \"lib01\", \"type\": \"library\", \"strategy\": \"create\", \"lang\": \"java\", \"buildTool\": \"maven\", \"multiModule\": false, \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", } Library (Clone) \u2693\ufe0e { \" name \" : \" lib01 \" , \" type \" : \" library \" , \" strategy \" : \" clone \" , \" lang \" : \" java \" , \" buildTool \" : \" maven \" , \" multiModule \" : false , \" repository \" : { \" url \" : \" http(s)://git.sample.com/sample.git \" , // login and password are required only if repo is private \" login \" : \" login \" , \" password \" : \" password \" }, \" vcs \" : null , \" jenkinsSlave \" : \" maven \" , \" jobProvisioning \" : \" default \" , } Library (Import) \u2693\ufe0e { \"type\": \"library\", \"strategy\": \"import\", \"lang\": \"java\", \"buildTool\": \"maven\", \"multiModule\": false, \"gitServer\": \"git-epam\", \"gitUrlPath\": \"/relative/path/to/repo\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", } Response \u2693\ufe0e Status 200 OK Get Codebase by Name \u2693\ufe0e Request \u2693\ufe0e GET /api/v1/edp/codebase/{codebaseName} example: localhost/api/v1/edp/codebase/app01 Response \u2693\ufe0e Status 200 OK { \"id\" : 1 , \"name\" : \"app01\" , \"language\" : \"java\" , \"build_tool\" : \"maven\" , \"framework\" : \"springboot\" , \"strategy\" : \"create\" , \"git_url\" : \"\" , \"route_site\" : \"api\" , \"route_path\" : \"/\" , \"type\" : \"application\" , \"status\" : \"active\" , \"testReportFramework\" : \"\" , \"description\" : \"Description\" , \"codebase_branch\" : [ { \"id\" : 1 , \"branchName\" : \"master\" , \"from_commit\" : \"\" , \"status\" : \"active\" , \"branchLink\" : \"\" , \"jenkinsLink\" : \"\" , \"appName\" : \"\" , \"codebaseDockerStream\" : null } ], \"gitServer\" : \"gerrit\" , \"gitProjectPath\" : null , \"jenkinsSlave\" : \"maven\" , \"jobProvisioning\" : \"default\" , \"deploymentScript\" : \"openshift-template\" } Get All Codebases \u2693\ufe0e Request \u2693\ufe0e GET /api/v1/edp/codebase?type={codebaseType} example: localhost/api/v1/edp/codebase?type=application Response \u2693\ufe0e Status 200 OK [ { \"id\" : 1 , \"name\" : \"app01\" , \"language\" : \"java\" , \"build_tool\" : \"maven\" , \"framework\" : \"springboot\" , \"strategy\" : \"create\" , \"git_url\" : \"\" , \"route_site\" : \"api\" , \"route_path\" : \"/\" , \"type\" : \"application\" , \"status\" : \"active\" , \"testReportFramework\" : \"\" , \"description\" : \"Description\" , \"codebase_branch\" : [ { \"id\" : 1 , \"branchName\" : \"master\" , \"from_commit\" : \"\" , \"status\" : \"active\" , \"branchLink\" : \"\" , \"jenkinsLink\" : \"\" , \"appName\" : \"\" , \"codebaseDockerStream\" : [ { \"id\" : 1 , \"ocImageStreamName\" : \"app01-master\" , \"imageLink\" : \"\" , \"jenkinsLink\" : \"\" } ] } ], \"gitServer\" : \"gerrit\" , \"gitProjectPath\" : null , \"jenkinsSlave\" : \"maven\" , \"jobProvisioning\" : \"\" , \"deploymentScript\" : \"openshift-template\" }, { \"id\" : 2 , \"name\" : \"app02\" , \"language\" : \"java\" , \"build_tool\" : \"maven\" , \"framework\" : \"springboot\" , \"strategy\" : \"create\" , \"git_url\" : \"\" , \"route_site\" : \"app\" , \"route_path\" : \"/\" , \"type\" : \"application\" , \"status\" : \"failed\" , \"testReportFramework\" : \"\" , \"description\" : \"\" , \"codebase_branch\" : [ { \"id\" : 2 , \"branchName\" : \"master\" , \"from_commit\" : \"\" , \"status\" : \"inactive\" , \"branchLink\" : \"\" , \"jenkinsLink\" : \"\" , \"appName\" : \"\" , \"codebaseDockerStream\" : [ { \"id\" : 2 , \"ocImageStreamName\" : \"app02-master\" , \"imageLink\" : \"\" , \"jenkinsLink\" : \"\" } ] } ], \"gitServer\" : \"gerrit\" , \"gitProjectPath\" : null , \"jenkinsSlave\" : \"maven\" , \"jobProvisioning\" : \"\" , \"deploymentScript\" : \"openshift-template\" } ] Create CD Pipeline Entity \u2693\ufe0e Request \u2693\ufe0e POST /api/v1/edp/cd-pipeline { \"name\":\"pipe1\", \"applications\":[ { \"appName\":\"app01\", \"inputDockerStream\":\"app01-master\" } ], \"stages\":[ { \"name\":\"sit\", \"description\":\"description-sit\", \"qualityGateType\":\"manual\", \"stepName\":\"approve\", \"triggerType\":\"manual\", \"order\":0, \"qualityGates\": [ { \"qualityGateType\":\"manual\", \"stepName\":\"step-one-one\", \"autotestName\": null, \"branchName\": null }, { \"qualityGateType\":\"manual\", \"stepName\":\"step-two-two\", \"autotestName\": null, \"branchName\": null } ] } ] } Response \u2693\ufe0e Status 200 OK Get CD Pipeline Entity by Name \u2693\ufe0e Request \u2693\ufe0e GET /api/v1/edp/cd-pipeline/{cdPipelineName} example: localhost/api/v1/edp/cd-pipeline/pipe1 Response \u2693\ufe0e Status 200 OK { \"id\": 1, \"name\": \"pipe1\", \"status\": \"active\", \"jenkinsLink\": \"\", \"codebaseBranches\": [ { \"id\": 1, \"branchName\": \"master\", \"from_commit\": \"\", \"status\": \"active\", \"branchLink\": \"\", \"jenkinsLink\": \"\", \"appName\": \"java-springboot-helloworld\", \"codebaseDockerStream\": [ { \"id\": 1, \"ocImageStreamName\": \"java-springboot-helloworld-master\", \"imageLink\": \"\", \"jenkinsLink\": \"\" } ] } ], \"stages\": [ { \"id\": 1, \"name\": \"sit\", \"description\": \"sit\", \"triggerType\": \"manual\", \"order\": 0, \"platformProjectLink\": \"\", \"platformProjectName\": env-am-test-deploy-sit\", \"qualityGates\": [ { \"id\": 1, \"qualityGateType\": \"manual\", \"stepName\": \"manual\", \"cdStageId\": 1, \"autotest\": null, \"codebaseBranch\": null } ], \"source\": { \"type\": \"library\", \"library\": { \"name\": \"lib01\", \"branch\": \"master\" } } } ], \"services\": [], \"applicationsToPromote\": [ \"java-springboot-helloworld\" ] } Get CD Stage Entity by Pipeline and Stage Names \u2693\ufe0e Request \u2693\ufe0e GET /api/v1/edp/cd-pipeline/{cdPipelineName}/stage/{stageName} example: `localhost/api/v1/edp/cd-pipeline/pipe1/stage/sit Response \u2693\ufe0e { \"name\": \"sit\", \"cdPipeline\": \"pipe1\", \"description\": \"sit\", \"triggerType\": \"manual\", \"order\": \"0\", \"applications\": [ { \"name\": \"java-springboot-helloworld\", \"branchName\": \"master\", \"inputIs\": \"java-springboot-helloworld-master\", \"outputIs\": \"am-test-deploy-sit-java-springboot-helloworld-verified\" } ], \"qualityGates\": [ { \"id\": 1, \"qualityGateType\": \"manual\", \"stepName\": \"manual\", \"cdStageId\": 1, \"autotest\": null, \"codebaseBranch\": null } ] } Update CD Pipeline Entity \u2693\ufe0e Request \u2693\ufe0e PUT /api/v1/edp/cd-pipeline/{cdPipelineName}/update example: localhost/api/v1/edp/cd-pipeline/pipe1/update Change Set of Applications \u2693\ufe0e { \"applications\":[ { \"appName\":\"app01\", \"inputDockerStream\":\"app01-master\" }, { \"appName\":\"app02\", \"inputDockerStream\":\"app02\" } ] } Response \u2693\ufe0e 204 No Cont ent Change Set of Stages \u2693\ufe0e { \"stages\": [ { \"name\": \"sit\", \"description\": \"sit\", \"triggerType\": \"manual\", \"order\": 0, \"platformProjectLink\": \"\", \"platformProjectName\": env-deploy-sit\", \"qualityGates\": [ { \"id\": 1, \"qualityGateType\": \"manual\", \"stepName\": \"manual\", \"cdStageId\": 1, \"autotest\": null, \"codebaseBranch\": null } ], \"source\": { \"type\": \"library\", \"library\": { \"name\": \"lib01\", \"branch\": \"master\" } } } ] } Response \u2693\ufe0e 204 No Cont ent","title":"EDP API"},{"location":"developer-guide/rest-api/#edp-api","text":"","title":"EDP API"},{"location":"developer-guide/rest-api/#create-codebase-entity","text":"EDP allows you to create three codebase types: Application, Autotest and Library. There are also several strategy types for each codebase: Create, Clone and Import. Depending on the selected codebase type and the respective strategy, you should specify a different set of fields in a request. Note The Route, Database and VCS are optional fields. In accordance with the necessary deploy set, you have to add the necessary fields into request","title":"Create Codebase Entity"},{"location":"developer-guide/rest-api/#request","text":"POST /api/v1/edp/codebase","title":"Request"},{"location":"developer-guide/rest-api/#application-create","text":"{ \"name\": \"app01\", \"type\": \"application\", \"strategy\": \"create\", \"lang\": \"java\", \"framework\": \"springboot\", \"buildTool\": \"maven\", \"multiModule\": false, \"route\": { \"site\": \"api\", \"path\": \"/\" }, \"database\": { \"kind\": \"postgresql\", \"version\": \"postgres:9.6\", \"capacity\": \"1Gi\", \"storage\": \"efs\" }, \"description\": \"Description\", \"gitServer\": \"gerrit\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", \"deploymentScript\": \"openshift-template\" }","title":"Application (Create)"},{"location":"developer-guide/rest-api/#application-clone","text":"{ \" name \" : \" app01 \" , \" type \" : \" application \" , \" strategy \" : \" clone \" , \" lang \" : \" java \" , \" framework \" : \" springboot \" , \" buildTool \" : \" maven \" , \" multiModule \" : false , \" repository \" : { \" url \" : \" http(s)://git.sample.com/sample.git \" , // login and password are required only if repo is private \" login \" : \" login \" , \" password \" : \" password \" }, \" description \" : \" Description \" , \" gitServer \" : \" gerrit \" , \" jenkinsSlave \" : \" maven \" , \" jobProvisioning \" : \" default \" , \" deploymentScript \" : \" openshift-template \" }","title":"Application (Clone)"},{"location":"developer-guide/rest-api/#application-import","text":"{ \"type\": \"application\", \"strategy\": \"import\", \"lang\": \"java\", \"framework\": \"springboot\", \"buildTool\": \"maven\", \"multiModule\": false, \"description\": \"Description\", \"gitServer\": \"git-epam\", \"gitUrlPath\": \"/relative/path/to/repo\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", \"deploymentScript\": \"openshift-template\" }","title":"Application (Import)"},{"location":"developer-guide/rest-api/#autotests-clone","text":"{ \" name \" : \" aut01 \" , \" type \" : \" autotests \" , \" strategy \" : \" clone \" , \" lang \" : \" java \" , \" framework \" : \" springboot \" , \" buildTool \" : \" maven \" , \" testReportFramework \" : \" allure \" , \" repository \" : { \" url \" : \" http(s)://git.sample.com/sample.git \" , // login and password are required only if repo is private \" login \" : \" login \" , \" password \" : \" password \" }, \" description \" : \" Description \" , \" jenkinsSlave \" : \" maven \" , \" jobProvisioning \" : \" default \" }","title":"Autotests (Clone)"},{"location":"developer-guide/rest-api/#autotests-import","text":"{ \"type\": \"autotests\", \"strategy\": \"import\", \"lang\": \"java\", \"framework\": \"springboot\", \"buildTool\": \"maven\", \"testReportFramework\": \"allure\", \"description\": \"Description\", \"gitServer\": \"git-epam\", \"gitRelativePath\": \"/relative/path/to/repo\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\" }","title":"Autotests (Import)"},{"location":"developer-guide/rest-api/#library-create","text":"{ \"name\": \"lib01\", \"type\": \"library\", \"strategy\": \"create\", \"lang\": \"java\", \"buildTool\": \"maven\", \"multiModule\": false, \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", }","title":"Library (Create)"},{"location":"developer-guide/rest-api/#library-clone","text":"{ \" name \" : \" lib01 \" , \" type \" : \" library \" , \" strategy \" : \" clone \" , \" lang \" : \" java \" , \" buildTool \" : \" maven \" , \" multiModule \" : false , \" repository \" : { \" url \" : \" http(s)://git.sample.com/sample.git \" , // login and password are required only if repo is private \" login \" : \" login \" , \" password \" : \" password \" }, \" vcs \" : null , \" jenkinsSlave \" : \" maven \" , \" jobProvisioning \" : \" default \" , }","title":"Library (Clone)"},{"location":"developer-guide/rest-api/#library-import","text":"{ \"type\": \"library\", \"strategy\": \"import\", \"lang\": \"java\", \"buildTool\": \"maven\", \"multiModule\": false, \"gitServer\": \"git-epam\", \"gitUrlPath\": \"/relative/path/to/repo\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", }","title":"Library (Import)"},{"location":"developer-guide/rest-api/#response","text":"Status 200 OK","title":"Response"},{"location":"developer-guide/rest-api/#get-codebase-by-name","text":"","title":"Get Codebase by Name"},{"location":"developer-guide/rest-api/#request_1","text":"GET /api/v1/edp/codebase/{codebaseName} example: localhost/api/v1/edp/codebase/app01","title":"Request"},{"location":"developer-guide/rest-api/#response_1","text":"Status 200 OK { \"id\" : 1 , \"name\" : \"app01\" , \"language\" : \"java\" , \"build_tool\" : \"maven\" , \"framework\" : \"springboot\" , \"strategy\" : \"create\" , \"git_url\" : \"\" , \"route_site\" : \"api\" , \"route_path\" : \"/\" , \"type\" : \"application\" , \"status\" : \"active\" , \"testReportFramework\" : \"\" , \"description\" : \"Description\" , \"codebase_branch\" : [ { \"id\" : 1 , \"branchName\" : \"master\" , \"from_commit\" : \"\" , \"status\" : \"active\" , \"branchLink\" : \"\" , \"jenkinsLink\" : \"\" , \"appName\" : \"\" , \"codebaseDockerStream\" : null } ], \"gitServer\" : \"gerrit\" , \"gitProjectPath\" : null , \"jenkinsSlave\" : \"maven\" , \"jobProvisioning\" : \"default\" , \"deploymentScript\" : \"openshift-template\" }","title":"Response"},{"location":"developer-guide/rest-api/#get-all-codebases","text":"","title":"Get All Codebases"},{"location":"developer-guide/rest-api/#request_2","text":"GET /api/v1/edp/codebase?type={codebaseType} example: localhost/api/v1/edp/codebase?type=application","title":"Request"},{"location":"developer-guide/rest-api/#response_2","text":"Status 200 OK [ { \"id\" : 1 , \"name\" : \"app01\" , \"language\" : \"java\" , \"build_tool\" : \"maven\" , \"framework\" : \"springboot\" , \"strategy\" : \"create\" , \"git_url\" : \"\" , \"route_site\" : \"api\" , \"route_path\" : \"/\" , \"type\" : \"application\" , \"status\" : \"active\" , \"testReportFramework\" : \"\" , \"description\" : \"Description\" , \"codebase_branch\" : [ { \"id\" : 1 , \"branchName\" : \"master\" , \"from_commit\" : \"\" , \"status\" : \"active\" , \"branchLink\" : \"\" , \"jenkinsLink\" : \"\" , \"appName\" : \"\" , \"codebaseDockerStream\" : [ { \"id\" : 1 , \"ocImageStreamName\" : \"app01-master\" , \"imageLink\" : \"\" , \"jenkinsLink\" : \"\" } ] } ], \"gitServer\" : \"gerrit\" , \"gitProjectPath\" : null , \"jenkinsSlave\" : \"maven\" , \"jobProvisioning\" : \"\" , \"deploymentScript\" : \"openshift-template\" }, { \"id\" : 2 , \"name\" : \"app02\" , \"language\" : \"java\" , \"build_tool\" : \"maven\" , \"framework\" : \"springboot\" , \"strategy\" : \"create\" , \"git_url\" : \"\" , \"route_site\" : \"app\" , \"route_path\" : \"/\" , \"type\" : \"application\" , \"status\" : \"failed\" , \"testReportFramework\" : \"\" , \"description\" : \"\" , \"codebase_branch\" : [ { \"id\" : 2 , \"branchName\" : \"master\" , \"from_commit\" : \"\" , \"status\" : \"inactive\" , \"branchLink\" : \"\" , \"jenkinsLink\" : \"\" , \"appName\" : \"\" , \"codebaseDockerStream\" : [ { \"id\" : 2 , \"ocImageStreamName\" : \"app02-master\" , \"imageLink\" : \"\" , \"jenkinsLink\" : \"\" } ] } ], \"gitServer\" : \"gerrit\" , \"gitProjectPath\" : null , \"jenkinsSlave\" : \"maven\" , \"jobProvisioning\" : \"\" , \"deploymentScript\" : \"openshift-template\" } ]","title":"Response"},{"location":"developer-guide/rest-api/#create-cd-pipeline-entity","text":"","title":"Create CD Pipeline Entity"},{"location":"developer-guide/rest-api/#request_3","text":"POST /api/v1/edp/cd-pipeline { \"name\":\"pipe1\", \"applications\":[ { \"appName\":\"app01\", \"inputDockerStream\":\"app01-master\" } ], \"stages\":[ { \"name\":\"sit\", \"description\":\"description-sit\", \"qualityGateType\":\"manual\", \"stepName\":\"approve\", \"triggerType\":\"manual\", \"order\":0, \"qualityGates\": [ { \"qualityGateType\":\"manual\", \"stepName\":\"step-one-one\", \"autotestName\": null, \"branchName\": null }, { \"qualityGateType\":\"manual\", \"stepName\":\"step-two-two\", \"autotestName\": null, \"branchName\": null } ] } ] }","title":"Request"},{"location":"developer-guide/rest-api/#response_3","text":"Status 200 OK","title":"Response"},{"location":"developer-guide/rest-api/#get-cd-pipeline-entity-by-name","text":"","title":"Get CD Pipeline Entity by Name"},{"location":"developer-guide/rest-api/#request_4","text":"GET /api/v1/edp/cd-pipeline/{cdPipelineName} example: localhost/api/v1/edp/cd-pipeline/pipe1","title":"Request"},{"location":"developer-guide/rest-api/#response_4","text":"Status 200 OK { \"id\": 1, \"name\": \"pipe1\", \"status\": \"active\", \"jenkinsLink\": \"\", \"codebaseBranches\": [ { \"id\": 1, \"branchName\": \"master\", \"from_commit\": \"\", \"status\": \"active\", \"branchLink\": \"\", \"jenkinsLink\": \"\", \"appName\": \"java-springboot-helloworld\", \"codebaseDockerStream\": [ { \"id\": 1, \"ocImageStreamName\": \"java-springboot-helloworld-master\", \"imageLink\": \"\", \"jenkinsLink\": \"\" } ] } ], \"stages\": [ { \"id\": 1, \"name\": \"sit\", \"description\": \"sit\", \"triggerType\": \"manual\", \"order\": 0, \"platformProjectLink\": \"\", \"platformProjectName\": env-am-test-deploy-sit\", \"qualityGates\": [ { \"id\": 1, \"qualityGateType\": \"manual\", \"stepName\": \"manual\", \"cdStageId\": 1, \"autotest\": null, \"codebaseBranch\": null } ], \"source\": { \"type\": \"library\", \"library\": { \"name\": \"lib01\", \"branch\": \"master\" } } } ], \"services\": [], \"applicationsToPromote\": [ \"java-springboot-helloworld\" ] }","title":"Response"},{"location":"developer-guide/rest-api/#get-cd-stage-entity-by-pipeline-and-stage-names","text":"","title":"Get CD Stage Entity by Pipeline and Stage Names"},{"location":"developer-guide/rest-api/#request_5","text":"GET /api/v1/edp/cd-pipeline/{cdPipelineName}/stage/{stageName} example: `localhost/api/v1/edp/cd-pipeline/pipe1/stage/sit","title":"Request"},{"location":"developer-guide/rest-api/#response_5","text":"{ \"name\": \"sit\", \"cdPipeline\": \"pipe1\", \"description\": \"sit\", \"triggerType\": \"manual\", \"order\": \"0\", \"applications\": [ { \"name\": \"java-springboot-helloworld\", \"branchName\": \"master\", \"inputIs\": \"java-springboot-helloworld-master\", \"outputIs\": \"am-test-deploy-sit-java-springboot-helloworld-verified\" } ], \"qualityGates\": [ { \"id\": 1, \"qualityGateType\": \"manual\", \"stepName\": \"manual\", \"cdStageId\": 1, \"autotest\": null, \"codebaseBranch\": null } ] }","title":"Response"},{"location":"developer-guide/rest-api/#update-cd-pipeline-entity","text":"","title":"Update CD Pipeline Entity"},{"location":"developer-guide/rest-api/#request_6","text":"PUT /api/v1/edp/cd-pipeline/{cdPipelineName}/update example: localhost/api/v1/edp/cd-pipeline/pipe1/update","title":"Request"},{"location":"developer-guide/rest-api/#change-set-of-applications","text":"{ \"applications\":[ { \"appName\":\"app01\", \"inputDockerStream\":\"app01-master\" }, { \"appName\":\"app02\", \"inputDockerStream\":\"app02\" } ] }","title":"Change Set of Applications"},{"location":"developer-guide/rest-api/#response_6","text":"204 No Cont ent","title":"Response"},{"location":"developer-guide/rest-api/#change-set-of-stages","text":"{ \"stages\": [ { \"name\": \"sit\", \"description\": \"sit\", \"triggerType\": \"manual\", \"order\": 0, \"platformProjectLink\": \"\", \"platformProjectName\": env-deploy-sit\", \"qualityGates\": [ { \"id\": 1, \"qualityGateType\": \"manual\", \"stepName\": \"manual\", \"cdStageId\": 1, \"autotest\": null, \"codebaseBranch\": null } ], \"source\": { \"type\": \"library\", \"library\": { \"name\": \"lib01\", \"branch\": \"master\" } } } ] }","title":"Change Set of Stages"},{"location":"developer-guide/rest-api/#response_7","text":"204 No Cont ent","title":"Response"},{"location":"operator-guide/","text":"Overview \u2693\ufe0e The EDP Operator guide is intended for DevOps and provides information on EDP installation, configuration and customization, as well as the platform support. Inspect the documentation to adjust the EPAM Delivery Platform according to your business needs: The Installation section provides the prerequisites for EDP installation, including Kubernetes or OpenShift cluster setup, Keycloak , DefectDojo , Kiosk , and Ingress-nginx setup as well as the subsequent deployment of EPAM Delivery Platform . The Configuration section indicates the options to set the project with adding a code language , backup , VCS import strategy , managing Jenkins pipelines , and logging . The Integration section comprises the AWS , GitHub , GitLab , Jira , and Logsight integration options. The Tutorials section provides information on working with various aspects, for example, using cert-manager in OpenShift , deploying AWS EKS cluster , deploying OKD 4.9 cluster , deploying OKD 4.10 cluster , managing Jenkins agent , and upgrading Keycloak v.17.0.x-legacy to v.19.0.x on Kubernetes .","title":"Overview"},{"location":"operator-guide/#overview","text":"The EDP Operator guide is intended for DevOps and provides information on EDP installation, configuration and customization, as well as the platform support. Inspect the documentation to adjust the EPAM Delivery Platform according to your business needs: The Installation section provides the prerequisites for EDP installation, including Kubernetes or OpenShift cluster setup, Keycloak , DefectDojo , Kiosk , and Ingress-nginx setup as well as the subsequent deployment of EPAM Delivery Platform . The Configuration section indicates the options to set the project with adding a code language , backup , VCS import strategy , managing Jenkins pipelines , and logging . The Integration section comprises the AWS , GitHub , GitLab , Jira , and Logsight integration options. The Tutorials section provides information on working with various aspects, for example, using cert-manager in OpenShift , deploying AWS EKS cluster , deploying OKD 4.9 cluster , deploying OKD 4.10 cluster , managing Jenkins agent , and upgrading Keycloak v.17.0.x-legacy to v.19.0.x on Kubernetes .","title":"Overview"},{"location":"operator-guide/add-jenkins-agent/","text":"Manage Jenkins Agent \u2693\ufe0e Inspect the main steps to add and update Jenkins agent. Create/Update Jenkins Agent \u2693\ufe0e Every Jenkins agent is based on epamedp/edp-jenkins-base-agent. Check DockerHub for the latest version. Use it to create a new agent (or update an old one). See the example with Dockerfile of gradle-java11-agent below: View: Dockerfile # Copyright 2021 EPAM Systems. # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # http://www.apache.org/licenses/LICENSE-2.0 # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. FROM epamedp/edp-jenkins-base-agent:1.0.1 SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"] ENV GRADLE_VERSION=7.1 \\ PATH=$PATH:/opt/gradle/bin # Install Gradle RUN curl -skL -o /tmp/gradle-bin.zip https://services.gradle.org/distributions/gradle-$GRADLE_VERSION-bin.zip && \\ mkdir -p /opt/gradle && \\ unzip -q /tmp/gradle-bin.zip -d /opt/gradle && \\ ln -sf /opt/gradle/gradle-$GRADLE_VERSION/bin/gradle /usr/local/bin/gradle RUN yum install java-11-openjdk-devel.x86_64 -y && \\ rpm -V java-11-openjdk-devel.x86_64 && \\ yum clean all -y WORKDIR $HOME/.gradle RUN chown -R \"1001:0\" \"$HOME\" && \\ chmod -R \"g+rw\" \"$HOME\" USER 1001 After the Docker agent update/creation, build and load the image into the project registry (e.g. DockerHub , AWS ECR , etc.). Add Jenkins Agent Configuration \u2693\ufe0e To add a new Jenkins agent, take the steps below: Run the following command. Please be aware that \u2039edp-project\u203a is the name of the EDP tenant. kubectl edit configmap jenkins-slaves -n <edp-project> Note On an OpenShift cluster, run the oc command instead of kubectl one. Add new agent template. View: ConfigMap jenkins-slaves data: docker-template: |- <org.csanchez.jenkins.plugins.kubernetes.PodTemplate> <inheritFrom></inheritFrom> <name> docker </name> <namespace></namespace> <privileged> false </privileged> <alwaysPullImage> false </alwaysPullImage> <instanceCap> 2147483647 </instanceCap> <slaveConnectTimeout> 100 </slaveConnectTimeout> <idleMinutes> 5 </idleMinutes> <activeDeadlineSeconds> 0 </activeDeadlineSeconds> <label> docker </label> <serviceAccount> jenkins </serviceAccount> <nodeSelector> beta.kubernetes.io/os=linux </nodeSelector> <nodeUsageMode> NORMAL </nodeUsageMode> <workspaceVolume class= \"org.csanchez.jenkins.plugins.kubernetes.volumes.workspace.EmptyDirWorkspaceVolume\" > <memory> false </memory> </workspaceVolume> <volumes/> <containers> <org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> <name> jnlp </name> <image> IMAGE_NAME:IMAGE_TAG </image> <privileged> false </privileged> <alwaysPullImage> false </alwaysPullImage> <workingDir> /tmp </workingDir> <command></command> <args> ${ computer . jnlpmac } ${ computer . name } </args> <ttyEnabled> false </ttyEnabled> <resourceRequestCpu></resourceRequestCpu> <resourceRequestMemory></resourceRequestMemory> <resourceLimitCpu></resourceLimitCpu> <resourceLimitMemory></resourceLimitMemory> <envVars> <org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> <key> JAVA_TOOL_OPTIONS </key> <value> -XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true </value> </org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> </envVars> <ports/> </org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> </containers> <envVars/> <annotations/> <imagePullSecrets/> <podRetention class= \"org.csanchez.jenkins.plugins.kubernetes.pod.retention.Default\"/ > </org.csanchez.jenkins.plugins.kubernetes.PodTemplate> Note The name and label properties should be unique( docker in the example above). Insert image name and tag instead of IMAGE_NAME:IMAGE_TAG . Open Jenkins to ensure that everything is added correctly. Click the Manage Jenkins option, navigate to the Manage Nodes and Clouds -> Configure Clouds -> Kubernetes -> Pod Templates... , and scroll down to find new Jenkins agent Pod Template details... : Jenkins pod template As a result, the newly added Jenkins agent will be available in the Advanced Settings block of the Admin Console tool during the codebase creation: Advanced settings Modify Existing Agent Configuration \u2693\ufe0e If your application is integrated with EDP, take the steps below to change an existing agent configuration: Run the following command. Please be aware that \u2039edp-project\u203a is the name of the EDP tenant. kubectl edit configmap jenkins-slaves -n <edp-project> Note On an OpenShift cluster, run the oc command instead of kubectl one. Find the agent template in use and change and change the parameters. Open Jenkins and check the correct addition. Click the Manage Jenkins option, navigate to the Manage Nodes and Clouds -> Configure Clouds -> Kubernetes -> Pod Templates... , and scroll down to Pod Template details... with the necessary data.","title":"Manage Jenkins Agent"},{"location":"operator-guide/add-jenkins-agent/#manage-jenkins-agent","text":"Inspect the main steps to add and update Jenkins agent.","title":"Manage Jenkins Agent"},{"location":"operator-guide/add-jenkins-agent/#createupdate-jenkins-agent","text":"Every Jenkins agent is based on epamedp/edp-jenkins-base-agent. Check DockerHub for the latest version. Use it to create a new agent (or update an old one). See the example with Dockerfile of gradle-java11-agent below: View: Dockerfile # Copyright 2021 EPAM Systems. # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # http://www.apache.org/licenses/LICENSE-2.0 # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. FROM epamedp/edp-jenkins-base-agent:1.0.1 SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"] ENV GRADLE_VERSION=7.1 \\ PATH=$PATH:/opt/gradle/bin # Install Gradle RUN curl -skL -o /tmp/gradle-bin.zip https://services.gradle.org/distributions/gradle-$GRADLE_VERSION-bin.zip && \\ mkdir -p /opt/gradle && \\ unzip -q /tmp/gradle-bin.zip -d /opt/gradle && \\ ln -sf /opt/gradle/gradle-$GRADLE_VERSION/bin/gradle /usr/local/bin/gradle RUN yum install java-11-openjdk-devel.x86_64 -y && \\ rpm -V java-11-openjdk-devel.x86_64 && \\ yum clean all -y WORKDIR $HOME/.gradle RUN chown -R \"1001:0\" \"$HOME\" && \\ chmod -R \"g+rw\" \"$HOME\" USER 1001 After the Docker agent update/creation, build and load the image into the project registry (e.g. DockerHub , AWS ECR , etc.).","title":"Create/Update Jenkins Agent"},{"location":"operator-guide/add-jenkins-agent/#add-jenkins-agent-configuration","text":"To add a new Jenkins agent, take the steps below: Run the following command. Please be aware that \u2039edp-project\u203a is the name of the EDP tenant. kubectl edit configmap jenkins-slaves -n <edp-project> Note On an OpenShift cluster, run the oc command instead of kubectl one. Add new agent template. View: ConfigMap jenkins-slaves data: docker-template: |- <org.csanchez.jenkins.plugins.kubernetes.PodTemplate> <inheritFrom></inheritFrom> <name> docker </name> <namespace></namespace> <privileged> false </privileged> <alwaysPullImage> false </alwaysPullImage> <instanceCap> 2147483647 </instanceCap> <slaveConnectTimeout> 100 </slaveConnectTimeout> <idleMinutes> 5 </idleMinutes> <activeDeadlineSeconds> 0 </activeDeadlineSeconds> <label> docker </label> <serviceAccount> jenkins </serviceAccount> <nodeSelector> beta.kubernetes.io/os=linux </nodeSelector> <nodeUsageMode> NORMAL </nodeUsageMode> <workspaceVolume class= \"org.csanchez.jenkins.plugins.kubernetes.volumes.workspace.EmptyDirWorkspaceVolume\" > <memory> false </memory> </workspaceVolume> <volumes/> <containers> <org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> <name> jnlp </name> <image> IMAGE_NAME:IMAGE_TAG </image> <privileged> false </privileged> <alwaysPullImage> false </alwaysPullImage> <workingDir> /tmp </workingDir> <command></command> <args> ${ computer . jnlpmac } ${ computer . name } </args> <ttyEnabled> false </ttyEnabled> <resourceRequestCpu></resourceRequestCpu> <resourceRequestMemory></resourceRequestMemory> <resourceLimitCpu></resourceLimitCpu> <resourceLimitMemory></resourceLimitMemory> <envVars> <org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> <key> JAVA_TOOL_OPTIONS </key> <value> -XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true </value> </org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> </envVars> <ports/> </org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> </containers> <envVars/> <annotations/> <imagePullSecrets/> <podRetention class= \"org.csanchez.jenkins.plugins.kubernetes.pod.retention.Default\"/ > </org.csanchez.jenkins.plugins.kubernetes.PodTemplate> Note The name and label properties should be unique( docker in the example above). Insert image name and tag instead of IMAGE_NAME:IMAGE_TAG . Open Jenkins to ensure that everything is added correctly. Click the Manage Jenkins option, navigate to the Manage Nodes and Clouds -> Configure Clouds -> Kubernetes -> Pod Templates... , and scroll down to find new Jenkins agent Pod Template details... : Jenkins pod template As a result, the newly added Jenkins agent will be available in the Advanced Settings block of the Admin Console tool during the codebase creation: Advanced settings","title":"Add Jenkins Agent Configuration"},{"location":"operator-guide/add-jenkins-agent/#modify-existing-agent-configuration","text":"If your application is integrated with EDP, take the steps below to change an existing agent configuration: Run the following command. Please be aware that \u2039edp-project\u203a is the name of the EDP tenant. kubectl edit configmap jenkins-slaves -n <edp-project> Note On an OpenShift cluster, run the oc command instead of kubectl one. Find the agent template in use and change and change the parameters. Open Jenkins and check the correct addition. Click the Manage Jenkins option, navigate to the Manage Nodes and Clouds -> Configure Clouds -> Kubernetes -> Pod Templates... , and scroll down to Pod Template details... with the necessary data.","title":"Modify Existing Agent Configuration"},{"location":"operator-guide/add-other-code-language/","text":"Add Other Code Language \u2693\ufe0e There is an ability to extend the default code languages when creating a codebase with the Clone or Import strategy. Other code language Warning The Create strategy does not allow to customize the default code language set. To customize the Build Tool list, perform the following: Edit the edp-admin-console deployment by adding the necessary code language into the BUILD TOOLS field: kubectl edit deployment edp-admin-console -n <edp-project> Note Using an OpenShift cluster, run the oc command instead of kubectl one. Info \u2039edp-project\u203a is the name of the EDP tenant here and in all the following steps. View: edp-admin-console deployment ... spec : containers : - env : ... - name : BUILD_TOOLS value : docker # List of custom build tools in Admin Console, e.g. 'docker,helm'; ... ... Add the Jenkins agent by following the instruction . Add the Custom CI pipeline provisioner by following the instruction . As a result, the newly added Jenkins agent will be available in the Select Jenkins Slave dropdown list of the Advanced Settings block during the codebase creation: Advanced settings If it is necessary to create Code Review and Build pipelines, add corresponding entries (e.g. stages[Build-application-docker], [Code-review-application-docker]). See the example below: ... stages [ ' Code - review - application - docker ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"sonar\" } ] ' stages [ ' Build - application - docker ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"sonar\" }, ' + ' { \"name\" : \"build-image-kaniko\" } ' + ' ,{ \"name\" : \"git-tag\" } ] ' ... Jenkins job provisioner Note Application is one of the available options. Another option might be to add a library. Please refer to the Add Library page for details. Related Articles \u2693\ufe0e Add Application Add Library Manage Jenkins Agent Manage Jenkins CI Pipeline Job Provisioner","title":"Add Other Code Language"},{"location":"operator-guide/add-other-code-language/#add-other-code-language","text":"There is an ability to extend the default code languages when creating a codebase with the Clone or Import strategy. Other code language Warning The Create strategy does not allow to customize the default code language set. To customize the Build Tool list, perform the following: Edit the edp-admin-console deployment by adding the necessary code language into the BUILD TOOLS field: kubectl edit deployment edp-admin-console -n <edp-project> Note Using an OpenShift cluster, run the oc command instead of kubectl one. Info \u2039edp-project\u203a is the name of the EDP tenant here and in all the following steps. View: edp-admin-console deployment ... spec : containers : - env : ... - name : BUILD_TOOLS value : docker # List of custom build tools in Admin Console, e.g. 'docker,helm'; ... ... Add the Jenkins agent by following the instruction . Add the Custom CI pipeline provisioner by following the instruction . As a result, the newly added Jenkins agent will be available in the Select Jenkins Slave dropdown list of the Advanced Settings block during the codebase creation: Advanced settings If it is necessary to create Code Review and Build pipelines, add corresponding entries (e.g. stages[Build-application-docker], [Code-review-application-docker]). See the example below: ... stages [ ' Code - review - application - docker ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"sonar\" } ] ' stages [ ' Build - application - docker ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"sonar\" }, ' + ' { \"name\" : \"build-image-kaniko\" } ' + ' ,{ \"name\" : \"git-tag\" } ] ' ... Jenkins job provisioner Note Application is one of the available options. Another option might be to add a library. Please refer to the Add Library page for details.","title":"Add Other Code Language"},{"location":"operator-guide/add-other-code-language/#related-articles","text":"Add Application Add Library Manage Jenkins Agent Manage Jenkins CI Pipeline Job Provisioner","title":"Related Articles"},{"location":"operator-guide/add-security-scanner/","text":"Add Security Scanner \u2693\ufe0e In order to add a new security scanner, perform the steps below: Select a pipeline customization option from the Customize CI Pipeline article . Follow the steps described in this article, to create a new repository. Note This tutorial will focus on adding a new stage using shared library via the custom global pipeline libraries . Open the new repository and create a directory with the /src/com/epam/edp/customStages/impl/ci/impl/stageName/ name in the library repository, for example: /src/com/epam/edp/customStages/impl/ci/impl/security/ . After that, add a Groovy file with another name to the same stages catalog, for example: CustomSAST.groovy . Copy the logic from SASTMavenGradleGoApplication.groovy stage into the new CustomSAST.groovy stage. Add a new runGoSecScanner function to the stage: @Stage ( name = \"sast-custom\" , buildTool = [ \"maven\" , \"gradle\" , \"go\" ], type = [ ProjectType . APPLICATION ]) class CustomSAST { ... def runGoSecScanner ( context ) { def edpName = context . platform . getJsonPathValue ( \"cm\" , \"edp-config\" , \".data.edp_name\" ) def reportData = [:] reportData . active = \"true\" reportData . verified = \"false\" reportData . path = \"sast-gosec-report.json\" reportData . type = \"Gosec Scanner\" reportData . productTypeName = \"Tenant\" reportData . productName = \"${edpName}\" reportData . engagementName = \"${context.codebase.name}-${context.git.branch}\" reportData . autoCreateContext = \"true\" reportData . closeOldFindings = \"true\" reportData . pushToJira = \"false\" reportData . environment = \"Development\" reportData . testTitle = \"SAST\" script . sh ( script: \"\"\" set -ex gosec -fmt=json -out=${reportData.path} ./... \"\"\" ) return reportData } ... } Add function calls for the runGoSecScanner and publishReport functions: ... script . node ( \"sast\" ) { script . dir ( \"${testDir}\" ) { script . unstash 'all-repo' ... def dataFromGoSecScanner = runGoSecScanner ( context ) publishReport ( defectDojoCredentials , dataFromGoSecScanner ) } } ... Gosec scanner will be installed on the Jenkins SAST agent. It is based on the epamedp/edp-jenkins-base-agent . Please check DockerHub for its latest version. See below an example of the edp-jenkins-sast-agent Dockerfile: View: Default Dockerfile # Copyright 2022 EPAM Systems. # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # http://www.apache.org/licenses/LICENSE-2.0 # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. FROM epamedp/edp-jenkins-base-agent:1.0.31 SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"] USER root ENV SEMGREP_SCANNER_VERSION=0.106.0 \\ GOSEC_SCANNER_VERSION=2.12.0 RUN apk --no-cache add \\ curl=7.79.1-r2 \\ build-base=0.5-r3 \\ python3-dev=3.9.5-r2 \\ py3-pip=20.3.4-r1 \\ go=1.16.15-r0 # hadolint ignore=DL3059 RUN pip3 install --no-cache-dir --upgrade --ignore-installed \\ pip==22.2.1 \\ ruamel.yaml==0.17.21 \\ semgrep==${SEMGREP_SCANNER_VERSION} # Install GOSEC RUN curl -Lo /tmp/gosec.tar.gz https://github.com/securego/gosec/releases/download/v${GOSEC_SCANNER_VERSION}/gosec_${GOSEC_SCANNER_VERSION}_linux_amd64.tar.gz && \\ tar xf /tmp/gosec.tar.gz && \\ rm -f /tmp/gosec.tar.gz && \\ mv gosec /bin/gosec RUN chown -R \"1001:0\" \"$HOME\" && \\ chmod -R \"g+rw\" \"$HOME\" USER 1001 Related Articles \u2693\ufe0e Customize CI Pipeline Static Application Security Testing Overview Semgrep","title":"Add Security Scanner"},{"location":"operator-guide/add-security-scanner/#add-security-scanner","text":"In order to add a new security scanner, perform the steps below: Select a pipeline customization option from the Customize CI Pipeline article . Follow the steps described in this article, to create a new repository. Note This tutorial will focus on adding a new stage using shared library via the custom global pipeline libraries . Open the new repository and create a directory with the /src/com/epam/edp/customStages/impl/ci/impl/stageName/ name in the library repository, for example: /src/com/epam/edp/customStages/impl/ci/impl/security/ . After that, add a Groovy file with another name to the same stages catalog, for example: CustomSAST.groovy . Copy the logic from SASTMavenGradleGoApplication.groovy stage into the new CustomSAST.groovy stage. Add a new runGoSecScanner function to the stage: @Stage ( name = \"sast-custom\" , buildTool = [ \"maven\" , \"gradle\" , \"go\" ], type = [ ProjectType . APPLICATION ]) class CustomSAST { ... def runGoSecScanner ( context ) { def edpName = context . platform . getJsonPathValue ( \"cm\" , \"edp-config\" , \".data.edp_name\" ) def reportData = [:] reportData . active = \"true\" reportData . verified = \"false\" reportData . path = \"sast-gosec-report.json\" reportData . type = \"Gosec Scanner\" reportData . productTypeName = \"Tenant\" reportData . productName = \"${edpName}\" reportData . engagementName = \"${context.codebase.name}-${context.git.branch}\" reportData . autoCreateContext = \"true\" reportData . closeOldFindings = \"true\" reportData . pushToJira = \"false\" reportData . environment = \"Development\" reportData . testTitle = \"SAST\" script . sh ( script: \"\"\" set -ex gosec -fmt=json -out=${reportData.path} ./... \"\"\" ) return reportData } ... } Add function calls for the runGoSecScanner and publishReport functions: ... script . node ( \"sast\" ) { script . dir ( \"${testDir}\" ) { script . unstash 'all-repo' ... def dataFromGoSecScanner = runGoSecScanner ( context ) publishReport ( defectDojoCredentials , dataFromGoSecScanner ) } } ... Gosec scanner will be installed on the Jenkins SAST agent. It is based on the epamedp/edp-jenkins-base-agent . Please check DockerHub for its latest version. See below an example of the edp-jenkins-sast-agent Dockerfile: View: Default Dockerfile # Copyright 2022 EPAM Systems. # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # http://www.apache.org/licenses/LICENSE-2.0 # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. FROM epamedp/edp-jenkins-base-agent:1.0.31 SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"] USER root ENV SEMGREP_SCANNER_VERSION=0.106.0 \\ GOSEC_SCANNER_VERSION=2.12.0 RUN apk --no-cache add \\ curl=7.79.1-r2 \\ build-base=0.5-r3 \\ python3-dev=3.9.5-r2 \\ py3-pip=20.3.4-r1 \\ go=1.16.15-r0 # hadolint ignore=DL3059 RUN pip3 install --no-cache-dir --upgrade --ignore-installed \\ pip==22.2.1 \\ ruamel.yaml==0.17.21 \\ semgrep==${SEMGREP_SCANNER_VERSION} # Install GOSEC RUN curl -Lo /tmp/gosec.tar.gz https://github.com/securego/gosec/releases/download/v${GOSEC_SCANNER_VERSION}/gosec_${GOSEC_SCANNER_VERSION}_linux_amd64.tar.gz && \\ tar xf /tmp/gosec.tar.gz && \\ rm -f /tmp/gosec.tar.gz && \\ mv gosec /bin/gosec RUN chown -R \"1001:0\" \"$HOME\" && \\ chmod -R \"g+rw\" \"$HOME\" USER 1001","title":"Add Security Scanner"},{"location":"operator-guide/add-security-scanner/#related-articles","text":"Customize CI Pipeline Static Application Security Testing Overview Semgrep","title":"Related Articles"},{"location":"operator-guide/argocd-integration/","text":"Argo CD Integration \u2693\ufe0e EDP uses Jenkins Pipeline as a part of the Continues Delivery/Continues Deployment implementation. Another approach is to use Argo CD tool as an alternative to Jenkins. Argo CD follows the best GitOps practices, uses Kubernetes native approach for the Deployment Management, has rich UI and required RBAC capabilities. Argo CD Deployment Approach in EDP \u2693\ufe0e Argo CD can be installed using two different approaches : Cluster-wide scope with the cluster-admin access Namespaced scope with the single namespace access Both approaches can be deployed with High Availability (HA) or Non High Availability (non HA) installation manifests. EDP uses the HA deployment with the cluster-admin permissions, to minimize cluster resources consumption by sharing single Argo CD instance across multiple EDP Tenants. Please follow the installation instructions , to deploy Argo CD. EDP Argo CD Operator \u2693\ufe0e EDP Argo CD Operator works as a proxy between the EDP Tenant used in the EDP installation process and the centralized Argo CD . It monitors the EDP Custom Resource ArgoApplication and Secrets with the argocd.edp.epam.com/secret-type: repository label in EDP namespaces and creates related entities in Argo CD using an API (the JWT token and Argo CD URL must be defined to start the operator successfully). There is one-to-one mapping between the EDP and Argo CD custom resources: argoapplications.v1.edp.epam.com and applications.argoproj.io Secret with labels argocd.edp.epam.com/secret-type: repository and argocd.argoproj.io/secret-type=repository See a diagram below for the details: Argo CD Diagram Argo CD is deployed in a separate argocd namespace. Argo CD uses a cluster-admin role for managing cluster-scope resources. The control-plane application is created using the App of Apps approach, and its code is managed by the control-plane members. The control-plane is used to onboard new Argo CD Tenants (Argo CD Projects - AppProject). The control-plane admin provides JWT Token for each EDP Tenant . The EDP Tenant deploys edp-argocd-operator in its edpTenant EDP namespace, and uses JWT Token and URL for Argo CD Instance provided by control-plane admin . The EDP Tenant Member manages Argo CD Repositories and Argo CD Applications using kind: Secret and kind: ArgoApplication in the edpTenant namespace. Please find the information about the advanced operator deployment in the Helm chart repository for the EDP Argo CD Operator . Configuration \u2693\ufe0e Note Make sure that both EDP and Argo CD are installed , and that SSO is enabled. To start using Argo CD with EDP, perform the following steps: The App Of Apps approach is used to manage the EDP Tenants . Inspect the edp-grub repository structure that is used to provide the EDP Tenants for the Argo CD Projects: edp-grub \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 apps ### All Argo CD Applications are stored here \u2502 \u251c\u2500\u2500 grub-argocd.yaml # Application that provisions Argo CD Resources - Argo Projects (EDP Tenants) \u2502 \u2514\u2500\u2500 grub-keycloak.yaml # Application that provisions Keycloak Resources - Argo CD Groups (EDP Tenants) \u251c\u2500\u2500 apps-configs \u2502 \u2514\u2500\u2500 grub \u2502 \u251c\u2500\u2500 argocd ### Argo CD resources definition \u2502 \u2502 \u251c\u2500\u2500 team-bar.yaml \u2502 \u2502 \u2514\u2500\u2500 team-foo.yaml \u2502 \u2514\u2500\u2500 keycloak ### Keycloak resources definition \u2502 \u251c\u2500\u2500 team-bar.yaml \u2502 \u2514\u2500\u2500 team-foo.yaml \u251c\u2500\u2500 bootstrap \u2502 \u2514\u2500\u2500 root.yaml ### Root application in App of Apps, which provision Applications from /apps \u2514\u2500\u2500 examples ### Examples \u2514\u2500\u2500 tenant \u2514\u2500\u2500 foo-petclinic.yaml The Root Application must be created under the control-plane scope. Create an Argo CD Project (EDP Tenant), for example, with the team-foo name. Two resources must be created: KeycloakRealmGroup apiVersion : v1.edp.epam.com/v1 kind : KeycloakRealmGroup metadata : name : argocd-team-foo-users spec : name : ArgoCD-team-foo-users realm : main AppProject apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : team-foo namespace : argocd # Finalizer that ensures that project is not deleted until it is not referenced by any application finalizers : - resources-finalizer.argocd.argoproj.io spec : description : CD pipelines for team-foo roles : - name : developer description : Users for team-foo tenant policies : - p, proj:team-foo:developer, applications, create, team-foo/*, allow - p, proj:team-foo:developer, applications, delete, team-foo/*, allow - p, proj:team-foo:developer, applications, get, team-foo/*, allow - p, proj:team-foo:developer, applications, override, team-foo/*, allow - p, proj:team-foo:developer, applications, sync, team-foo/*, allow - p, proj:team-foo:developer, applications, update, team-foo/*, allow - p, proj:team-foo:developer, repositories, create, team-foo/*, allow - p, proj:team-foo:developer, repositories, delete, team-foo/*, allow - p, proj:team-foo:developer, repositories, update, team-foo/*, allow - p, proj:team-foo:developer, repositories, get, team-foo/*, allow - p, proj:team-foo:developer, clusters, create, team-foo/*, allow - p, proj:team-foo:developer, clusters, delete, team-foo/*, allow - p, proj:team-foo:developer, clusters, update, team-foo/*, allow - p, proj:team-foo:developer, clusters, get, team-foo/*, allow groups : # Keycloak Group name - ArgoCD-team-foo-users destinations : # ensure we can deploy to ns with tenant prefix - namespace : 'team-foo-*' # allow to deploy to specific server (local in our case) server : https://kubernetes.default.svc # Deny all cluster-scoped resources from being created, except for Namespace clusterResourceWhitelist : - group : '' kind : Namespace # Allow all namespaced-scoped resources to be created, except for ResourceQuota, LimitRange, NetworkPolicy namespaceResourceBlacklist : - group : '' kind : ResourceQuota - group : '' kind : LimitRange - group : '' kind : NetworkPolicy # we are ok to create any resources inside namespace namespaceResourceWhitelist : - group : '*' kind : '*' # we are ok to deploy from any repo sourceRepos : - '*' Get a JWT Token for the team-foo project in the Argo CD -> Settings -> Projects -> team-foo -> Roles -> developer section. Please find more information about the automation tokens generation in the Authentication section of the Argo CD official documentation . The generated JWT Token and Argo CD URL must be stored in the EDP namespace under the argocd-access secret name and used for the edp-argocd-operator deployment . In Keycloak, add users to the ArgoCD-team-foo-users Keycloak Group. Deploy a test EDP Application with the demo name, which is stored in a Gerrit private repository, as Gerrit is a part of EDP. Repository: apiVersion : v1 kind : Secret metadata : name : demo labels : # must be type of repository argocd.edp.epam.com/secret-type : repository stringData : type : git url : ssh://argocd@gerrit.edpnamespace:30007/demo.git # Our Tenant name project : team-foo # Use insecure to work with privately hosted Git services over SSH. # If true, it is the same as use --insecure-skip-server-verification. # Optional, default - \"false\". # See: https://argo-cd.readthedocs.io/en/release-1.8/user-guide/private-repositories/#unknown-ssh-hosts insecure : \"true\" sshPrivateKey : | -----BEGIN OPENSSH PRIVATE KEY----- YOUR_PRIVATE_SSH_KEY -----END OPENSSH PRIVATE KEY----- ArgoApplication: apiVersion : v1.edp.epam.com/v1alpha1 kind : ArgoApplication metadata : name : demo spec : project : team-foo destination : namespace : team-foo-demo server : https://kubernetes.default.svc source : helm : parameters : - name : image.tag value : master-0.1.0-1 - name : image.repository value : image-repo path : deploy-templates repoURL : ssh://argocd@gerrit.edpnamespace:30007/demo.git targetRevision : master syncPolicy : syncOptions : - CreateNamespace=true automated : selfHeal : true prune : true Check that your new Repository and Application are added under the team-foo Project scope in the Argo CD UI. Related Articles \u2693\ufe0e Install Argo CD","title":"Argo CD Integration"},{"location":"operator-guide/argocd-integration/#argo-cd-integration","text":"EDP uses Jenkins Pipeline as a part of the Continues Delivery/Continues Deployment implementation. Another approach is to use Argo CD tool as an alternative to Jenkins. Argo CD follows the best GitOps practices, uses Kubernetes native approach for the Deployment Management, has rich UI and required RBAC capabilities.","title":"Argo CD Integration"},{"location":"operator-guide/argocd-integration/#argo-cd-deployment-approach-in-edp","text":"Argo CD can be installed using two different approaches : Cluster-wide scope with the cluster-admin access Namespaced scope with the single namespace access Both approaches can be deployed with High Availability (HA) or Non High Availability (non HA) installation manifests. EDP uses the HA deployment with the cluster-admin permissions, to minimize cluster resources consumption by sharing single Argo CD instance across multiple EDP Tenants. Please follow the installation instructions , to deploy Argo CD.","title":"Argo CD Deployment Approach in EDP"},{"location":"operator-guide/argocd-integration/#edp-argo-cd-operator","text":"EDP Argo CD Operator works as a proxy between the EDP Tenant used in the EDP installation process and the centralized Argo CD . It monitors the EDP Custom Resource ArgoApplication and Secrets with the argocd.edp.epam.com/secret-type: repository label in EDP namespaces and creates related entities in Argo CD using an API (the JWT token and Argo CD URL must be defined to start the operator successfully). There is one-to-one mapping between the EDP and Argo CD custom resources: argoapplications.v1.edp.epam.com and applications.argoproj.io Secret with labels argocd.edp.epam.com/secret-type: repository and argocd.argoproj.io/secret-type=repository See a diagram below for the details: Argo CD Diagram Argo CD is deployed in a separate argocd namespace. Argo CD uses a cluster-admin role for managing cluster-scope resources. The control-plane application is created using the App of Apps approach, and its code is managed by the control-plane members. The control-plane is used to onboard new Argo CD Tenants (Argo CD Projects - AppProject). The control-plane admin provides JWT Token for each EDP Tenant . The EDP Tenant deploys edp-argocd-operator in its edpTenant EDP namespace, and uses JWT Token and URL for Argo CD Instance provided by control-plane admin . The EDP Tenant Member manages Argo CD Repositories and Argo CD Applications using kind: Secret and kind: ArgoApplication in the edpTenant namespace. Please find the information about the advanced operator deployment in the Helm chart repository for the EDP Argo CD Operator .","title":"EDP Argo CD Operator"},{"location":"operator-guide/argocd-integration/#configuration","text":"Note Make sure that both EDP and Argo CD are installed , and that SSO is enabled. To start using Argo CD with EDP, perform the following steps: The App Of Apps approach is used to manage the EDP Tenants . Inspect the edp-grub repository structure that is used to provide the EDP Tenants for the Argo CD Projects: edp-grub \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 apps ### All Argo CD Applications are stored here \u2502 \u251c\u2500\u2500 grub-argocd.yaml # Application that provisions Argo CD Resources - Argo Projects (EDP Tenants) \u2502 \u2514\u2500\u2500 grub-keycloak.yaml # Application that provisions Keycloak Resources - Argo CD Groups (EDP Tenants) \u251c\u2500\u2500 apps-configs \u2502 \u2514\u2500\u2500 grub \u2502 \u251c\u2500\u2500 argocd ### Argo CD resources definition \u2502 \u2502 \u251c\u2500\u2500 team-bar.yaml \u2502 \u2502 \u2514\u2500\u2500 team-foo.yaml \u2502 \u2514\u2500\u2500 keycloak ### Keycloak resources definition \u2502 \u251c\u2500\u2500 team-bar.yaml \u2502 \u2514\u2500\u2500 team-foo.yaml \u251c\u2500\u2500 bootstrap \u2502 \u2514\u2500\u2500 root.yaml ### Root application in App of Apps, which provision Applications from /apps \u2514\u2500\u2500 examples ### Examples \u2514\u2500\u2500 tenant \u2514\u2500\u2500 foo-petclinic.yaml The Root Application must be created under the control-plane scope. Create an Argo CD Project (EDP Tenant), for example, with the team-foo name. Two resources must be created: KeycloakRealmGroup apiVersion : v1.edp.epam.com/v1 kind : KeycloakRealmGroup metadata : name : argocd-team-foo-users spec : name : ArgoCD-team-foo-users realm : main AppProject apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : team-foo namespace : argocd # Finalizer that ensures that project is not deleted until it is not referenced by any application finalizers : - resources-finalizer.argocd.argoproj.io spec : description : CD pipelines for team-foo roles : - name : developer description : Users for team-foo tenant policies : - p, proj:team-foo:developer, applications, create, team-foo/*, allow - p, proj:team-foo:developer, applications, delete, team-foo/*, allow - p, proj:team-foo:developer, applications, get, team-foo/*, allow - p, proj:team-foo:developer, applications, override, team-foo/*, allow - p, proj:team-foo:developer, applications, sync, team-foo/*, allow - p, proj:team-foo:developer, applications, update, team-foo/*, allow - p, proj:team-foo:developer, repositories, create, team-foo/*, allow - p, proj:team-foo:developer, repositories, delete, team-foo/*, allow - p, proj:team-foo:developer, repositories, update, team-foo/*, allow - p, proj:team-foo:developer, repositories, get, team-foo/*, allow - p, proj:team-foo:developer, clusters, create, team-foo/*, allow - p, proj:team-foo:developer, clusters, delete, team-foo/*, allow - p, proj:team-foo:developer, clusters, update, team-foo/*, allow - p, proj:team-foo:developer, clusters, get, team-foo/*, allow groups : # Keycloak Group name - ArgoCD-team-foo-users destinations : # ensure we can deploy to ns with tenant prefix - namespace : 'team-foo-*' # allow to deploy to specific server (local in our case) server : https://kubernetes.default.svc # Deny all cluster-scoped resources from being created, except for Namespace clusterResourceWhitelist : - group : '' kind : Namespace # Allow all namespaced-scoped resources to be created, except for ResourceQuota, LimitRange, NetworkPolicy namespaceResourceBlacklist : - group : '' kind : ResourceQuota - group : '' kind : LimitRange - group : '' kind : NetworkPolicy # we are ok to create any resources inside namespace namespaceResourceWhitelist : - group : '*' kind : '*' # we are ok to deploy from any repo sourceRepos : - '*' Get a JWT Token for the team-foo project in the Argo CD -> Settings -> Projects -> team-foo -> Roles -> developer section. Please find more information about the automation tokens generation in the Authentication section of the Argo CD official documentation . The generated JWT Token and Argo CD URL must be stored in the EDP namespace under the argocd-access secret name and used for the edp-argocd-operator deployment . In Keycloak, add users to the ArgoCD-team-foo-users Keycloak Group. Deploy a test EDP Application with the demo name, which is stored in a Gerrit private repository, as Gerrit is a part of EDP. Repository: apiVersion : v1 kind : Secret metadata : name : demo labels : # must be type of repository argocd.edp.epam.com/secret-type : repository stringData : type : git url : ssh://argocd@gerrit.edpnamespace:30007/demo.git # Our Tenant name project : team-foo # Use insecure to work with privately hosted Git services over SSH. # If true, it is the same as use --insecure-skip-server-verification. # Optional, default - \"false\". # See: https://argo-cd.readthedocs.io/en/release-1.8/user-guide/private-repositories/#unknown-ssh-hosts insecure : \"true\" sshPrivateKey : | -----BEGIN OPENSSH PRIVATE KEY----- YOUR_PRIVATE_SSH_KEY -----END OPENSSH PRIVATE KEY----- ArgoApplication: apiVersion : v1.edp.epam.com/v1alpha1 kind : ArgoApplication metadata : name : demo spec : project : team-foo destination : namespace : team-foo-demo server : https://kubernetes.default.svc source : helm : parameters : - name : image.tag value : master-0.1.0-1 - name : image.repository value : image-repo path : deploy-templates repoURL : ssh://argocd@gerrit.edpnamespace:30007/demo.git targetRevision : master syncPolicy : syncOptions : - CreateNamespace=true automated : selfHeal : true prune : true Check that your new Repository and Application are added under the team-foo Project scope in the Argo CD UI.","title":"Configuration"},{"location":"operator-guide/argocd-integration/#related-articles","text":"Install Argo CD","title":"Related Articles"},{"location":"operator-guide/configure-keycloak-oidc-eks/","text":"EKS OIDC With Keycloak \u2693\ufe0e This article provides the instruction of configuring Keycloak as OIDC Identity Provider for EKS. The example is written on Terraform (HCL). Prerequisites \u2693\ufe0e To follow the instruction, check the following prerequisites: terraform 0.14.10 hashicorp/aws = 4.8.0 mrparkers/keycloak >= 3.0.0 hashicorp/kubernetes ~> 2.9.0 kubectl = 1.22 kubelogin >= v1.25.1 Ensure that Keycloak has network availability for AWS (not in a private network). Note To connect OIDC with a cluster, install and configure the kubelogin plugin. For Windows, it is recommended to download the kubelogin as a binary and add it to your PATH. Solution Overview \u2693\ufe0e The solution includes three types of the resources - AWS (EKS), Keycloak, Kubernetes. The left part of Keycloak resources remain unchanged after creation, thus allowing us to associate a claim for a user group membership. Other resources can be created, deleted or changed if needed. The most crucial from Kubernetes permissions are Kubernetes RoleBindings and ClusterRoles/Roles. Roles present a set of permissions, in turn RoleBindings map Kubernetes Role to representative Keycloak groups, so a group member can have just appropriate permissions. EKS Keycloak OIDC Keycloak Configuration \u2693\ufe0e To configure Keycloak, follow the steps described below. Create a client: resource \"keycloak_openid_client\" \"openid_client\" { realm_id = <realm_id> client_id = <client_id> access_type = \"CONFIDENTIAL\" standard_flow_enabled = true implicit_flow_enabled = false direct_access_grants_enabled = true service_accounts_enabled = true oauth2_device_authorization_grant_enabled = true backchannel_logout_session_required = true root_url = \"http://localhost:8000/\" base_url = \"http://localhost:8000/\" admin_url = \"http://localhost:8000/\" web_origins = [ \"*\" ] valid_redirect_uris = [ \"http://localhost:8000/*\" ] } Create the client scope: resource \"keycloak_openid_client_scope\" \"openid_client_scope\" { realm_id = <realm_id> name = \"groups\" description = \"When requested, this scope will map a user's group memberships to a claim\" include_in_token_scope = true consent_screen_text = false } Add scope to the client by selecting all default client scope: resource \"keycloak_openid_client_default_scopes\" \"client_default_scopes\" { realm_id = <realm_id> client_id = keycloak_openid_client.openid_client.id default_scopes = [ \"profile\" , \"email\" , \"roles\" , \"web-origins\" , keycloak_openid_client_scope.openid_client_scope.name , ] } Add the following mapper to the client scope: resource \"keycloak_openid_group_membership_protocol_mapper\" \"group_membership_mapper\" { realm_id = <realm_id> client_scope_id = keycloak_openid_client_scope.openid_client_scope.id name = \"group-membership-mapper\" add_to_id_token = true add_to_access_token = true add_to_userinfo = true full_path = false claim_name = \"groups\" } In the authorization token, get groups membership field with the list of group membership in the realm: ... \"email_verified\" : false , \"name\" : \"An User\" , \"groups\" : [ \"<env_prefix_name>-oidc-viewers\" , \"<env_prefix_name>-oidc-cluster-admins\" ], \"preferred_username\" : \"an_user@example.com\" , \"given_name\" : \"An\" , \"family_name\" : \"User\" , \"email\" : \"an_user@example.com\" ... Create group/groups, e.g. admin group: resource \"keycloak_group\" \"oidc_tenant_admin\" { realm_id = <realm_id> name = \"<env_prefix_name>-oidc-admins\" } EKS Configuration \u2693\ufe0e To configure EKS, follow the steps described below. In AWS Console, open EKS home page -> Choose a cluster -> Configuration tab -> Authentication tab. The Terraform code for association with Keycloak: terraform.tfvars ... cluster_identity_providers = { keycloak = { client_id = <keycloak_client_id> identity_provider_config_name = \"Keycloak\" issuer_url = \"https://<keycloak_url>/auth/realms/<realm_name>\" groups_claim = \"groups\" } ... the resource code resource \"aws_eks_identity_provider_config\" \"keycloak\" { for_each = { for k , v in var.cluster_identity_providers : k = > v if true } cluster_name = var.platform_name oidc { client_id = each.value.client_id groups_claim = lookup ( each.value , \"groups_claim\" , null ) groups_prefix = lookup ( each.value , \"groups_prefix\" , null ) identity_provider_config_name = try ( each.value.identity_provider_config_name , each.key ) issuer_url = each.value.issuer_url required_claims = lookup ( each.value , \"required_claims\" , null ) username_claim = lookup ( each.value , \"username_claim\" , null ) username_prefix = lookup ( each.value , \"username_prefix\" , null ) } tags = var.tags } Note The resource creation takes around 20-30 minutes. The resource doesn't support updating, so each change will lead to deletion of the old instance and creation of a new instance instead. Kubernetes Configuration \u2693\ufe0e To connect the created Keycloak resources with permissions, it is necessary to create Kubernetes Roles and RoleBindings: ClusterRole resource \"kubernetes_cluster_role_v1\" \"oidc_tenant_admin\" { metadata { name = \"oidc-admin\" } rule { api_groups = [ \"*\" ] resources = [ \"*\" ] verbs = [ \"*\" ] } } ClusterRoleBinding resource \"kubernetes_cluster_role_binding_v1\" \"oidc_cluster_rb\" { metadata { name = \"oidc-cluster-admin\" } role_ref { api_group = \"rbac.authorization.k8s.io\" kind = \"ClusterRole\" name = kubernetes_cluster_role_v1.oidc_tenant_admin.metadata[0].name } subject { kind = \"Group\" name = keycloak_group.oidc_tenant_admin.name api_group = \"rbac.authorization.k8s.io\" # work-around due https://github.com/hashicorp/terraform-provider-kubernetes/issues/710 namespace = \"\" } } Note When creating the Keycloak group, ClusterRole, and ClusterRoleBinding, a user receives cluster admin permissions. There is also an option to provide admin permissions just to a particular namespace or another resources set in another namespace. For details, please refer to the Mixing Kubernetes Roles page. Kubeconfig \u2693\ufe0e Template for kubeconfig: apiVersion : v1 preferences : {} kind : Config clusters : - cluster : server : https://<eks_url>.eks.amazonaws.com certificate-authority-data : <certificate_authtority_data> name : <cluster_name> contexts : - context : cluster : <cluster_name> user : <keycloak_user_email> name : <cluster_name> current-context : <cluster_name> users : - name : <keycloak_user_email> user : exec : apiVersion : client.authentication.k8s.io/v1beta1 command : kubectl args : - oidc-login - get-token - -v1 - --oidc-issuer-url=https://<keycloak_url>/auth/realms/<realm> - --oidc-client-id=<keycloak_client_id> - --oidc-client-secret=<keycloak_client_secret> Flag -v1 can be used for debug, in a common case it's not needed and can be deleted. To find the client secret: Open Keycloak Choose realm Find keycloak_client_id that was previously created Open Credentials tab Copy Secret Testing \u2693\ufe0e Before testing, ensure that a user is a member of the correct Keycloak group. To add a user to a Keycloak group: Open Keycloak Choose realm Open user screen with search field Find a user and open the configuration Open Groups tab In Available Groups, choose an appropriate group Click the Join button The group should appear in the Group Membership list Follow the steps below to test the configuration: Run kubectl command, it is important to specify the correct kubeconfig: KUBECONFIG = <path_to_oidc_kubeconfig> kubectl get ingresses -n <namespace_name> After the first run and redirection to the Keycloak login page, log in using credentials (login:password) or using SSO Provider. In case of the successful login, you will receive the following notification that can be closed: OIDC Successful Login As the result, a respective response from the Kubernetes will appear in the console in case a user is configured correctly and is a member of the correct group and Roles/RoleBindings. If something is not set up correctly, the following output error will be displayed: Error from server ( Forbidden ) : ingresses.networking.k8s.io is forbidden: User \"https://<keycloak_url>/auth/realms/<realm>#<keycloak_user_id>\" cannot list resource \"ingresses\" in API group \"networking.k8s.io\" in the namespace \"<namespace_name>\" Session Update \u2693\ufe0e To update the session, clear cache. The default location for the login cache: rm -rf ~/.kube/cache Access Cluster via Lens \u2693\ufe0e To access the Kubernetes cluster via Lens , follow the steps below to configure it: Add a new kubeconfig to the location where Lens has access. The default location of the kubeconfig is ~/.kube/config but it can be changed by navigating to File -> Preferences -> Kubernetes -> Kubeconfig Syncs ; (Optional) Using Windows, it is recommended to reboot the system after adding a new kubeconfig. Authenticate on the Keycloak login page to be able to access the cluster; Note Lens does not add namespaces of the project automatically, so it is necessary to add them manually, simply go to Settings -> Namespaces and add the namespaces of a project.","title":"EKS OIDC With Keycloak"},{"location":"operator-guide/configure-keycloak-oidc-eks/#eks-oidc-with-keycloak","text":"This article provides the instruction of configuring Keycloak as OIDC Identity Provider for EKS. The example is written on Terraform (HCL).","title":"EKS OIDC With Keycloak"},{"location":"operator-guide/configure-keycloak-oidc-eks/#prerequisites","text":"To follow the instruction, check the following prerequisites: terraform 0.14.10 hashicorp/aws = 4.8.0 mrparkers/keycloak >= 3.0.0 hashicorp/kubernetes ~> 2.9.0 kubectl = 1.22 kubelogin >= v1.25.1 Ensure that Keycloak has network availability for AWS (not in a private network). Note To connect OIDC with a cluster, install and configure the kubelogin plugin. For Windows, it is recommended to download the kubelogin as a binary and add it to your PATH.","title":"Prerequisites"},{"location":"operator-guide/configure-keycloak-oidc-eks/#solution-overview","text":"The solution includes three types of the resources - AWS (EKS), Keycloak, Kubernetes. The left part of Keycloak resources remain unchanged after creation, thus allowing us to associate a claim for a user group membership. Other resources can be created, deleted or changed if needed. The most crucial from Kubernetes permissions are Kubernetes RoleBindings and ClusterRoles/Roles. Roles present a set of permissions, in turn RoleBindings map Kubernetes Role to representative Keycloak groups, so a group member can have just appropriate permissions. EKS Keycloak OIDC","title":"Solution Overview"},{"location":"operator-guide/configure-keycloak-oidc-eks/#keycloak-configuration","text":"To configure Keycloak, follow the steps described below. Create a client: resource \"keycloak_openid_client\" \"openid_client\" { realm_id = <realm_id> client_id = <client_id> access_type = \"CONFIDENTIAL\" standard_flow_enabled = true implicit_flow_enabled = false direct_access_grants_enabled = true service_accounts_enabled = true oauth2_device_authorization_grant_enabled = true backchannel_logout_session_required = true root_url = \"http://localhost:8000/\" base_url = \"http://localhost:8000/\" admin_url = \"http://localhost:8000/\" web_origins = [ \"*\" ] valid_redirect_uris = [ \"http://localhost:8000/*\" ] } Create the client scope: resource \"keycloak_openid_client_scope\" \"openid_client_scope\" { realm_id = <realm_id> name = \"groups\" description = \"When requested, this scope will map a user's group memberships to a claim\" include_in_token_scope = true consent_screen_text = false } Add scope to the client by selecting all default client scope: resource \"keycloak_openid_client_default_scopes\" \"client_default_scopes\" { realm_id = <realm_id> client_id = keycloak_openid_client.openid_client.id default_scopes = [ \"profile\" , \"email\" , \"roles\" , \"web-origins\" , keycloak_openid_client_scope.openid_client_scope.name , ] } Add the following mapper to the client scope: resource \"keycloak_openid_group_membership_protocol_mapper\" \"group_membership_mapper\" { realm_id = <realm_id> client_scope_id = keycloak_openid_client_scope.openid_client_scope.id name = \"group-membership-mapper\" add_to_id_token = true add_to_access_token = true add_to_userinfo = true full_path = false claim_name = \"groups\" } In the authorization token, get groups membership field with the list of group membership in the realm: ... \"email_verified\" : false , \"name\" : \"An User\" , \"groups\" : [ \"<env_prefix_name>-oidc-viewers\" , \"<env_prefix_name>-oidc-cluster-admins\" ], \"preferred_username\" : \"an_user@example.com\" , \"given_name\" : \"An\" , \"family_name\" : \"User\" , \"email\" : \"an_user@example.com\" ... Create group/groups, e.g. admin group: resource \"keycloak_group\" \"oidc_tenant_admin\" { realm_id = <realm_id> name = \"<env_prefix_name>-oidc-admins\" }","title":"Keycloak Configuration"},{"location":"operator-guide/configure-keycloak-oidc-eks/#eks-configuration","text":"To configure EKS, follow the steps described below. In AWS Console, open EKS home page -> Choose a cluster -> Configuration tab -> Authentication tab. The Terraform code for association with Keycloak: terraform.tfvars ... cluster_identity_providers = { keycloak = { client_id = <keycloak_client_id> identity_provider_config_name = \"Keycloak\" issuer_url = \"https://<keycloak_url>/auth/realms/<realm_name>\" groups_claim = \"groups\" } ... the resource code resource \"aws_eks_identity_provider_config\" \"keycloak\" { for_each = { for k , v in var.cluster_identity_providers : k = > v if true } cluster_name = var.platform_name oidc { client_id = each.value.client_id groups_claim = lookup ( each.value , \"groups_claim\" , null ) groups_prefix = lookup ( each.value , \"groups_prefix\" , null ) identity_provider_config_name = try ( each.value.identity_provider_config_name , each.key ) issuer_url = each.value.issuer_url required_claims = lookup ( each.value , \"required_claims\" , null ) username_claim = lookup ( each.value , \"username_claim\" , null ) username_prefix = lookup ( each.value , \"username_prefix\" , null ) } tags = var.tags } Note The resource creation takes around 20-30 minutes. The resource doesn't support updating, so each change will lead to deletion of the old instance and creation of a new instance instead.","title":"EKS Configuration"},{"location":"operator-guide/configure-keycloak-oidc-eks/#kubernetes-configuration","text":"To connect the created Keycloak resources with permissions, it is necessary to create Kubernetes Roles and RoleBindings: ClusterRole resource \"kubernetes_cluster_role_v1\" \"oidc_tenant_admin\" { metadata { name = \"oidc-admin\" } rule { api_groups = [ \"*\" ] resources = [ \"*\" ] verbs = [ \"*\" ] } } ClusterRoleBinding resource \"kubernetes_cluster_role_binding_v1\" \"oidc_cluster_rb\" { metadata { name = \"oidc-cluster-admin\" } role_ref { api_group = \"rbac.authorization.k8s.io\" kind = \"ClusterRole\" name = kubernetes_cluster_role_v1.oidc_tenant_admin.metadata[0].name } subject { kind = \"Group\" name = keycloak_group.oidc_tenant_admin.name api_group = \"rbac.authorization.k8s.io\" # work-around due https://github.com/hashicorp/terraform-provider-kubernetes/issues/710 namespace = \"\" } } Note When creating the Keycloak group, ClusterRole, and ClusterRoleBinding, a user receives cluster admin permissions. There is also an option to provide admin permissions just to a particular namespace or another resources set in another namespace. For details, please refer to the Mixing Kubernetes Roles page.","title":"Kubernetes Configuration"},{"location":"operator-guide/configure-keycloak-oidc-eks/#kubeconfig","text":"Template for kubeconfig: apiVersion : v1 preferences : {} kind : Config clusters : - cluster : server : https://<eks_url>.eks.amazonaws.com certificate-authority-data : <certificate_authtority_data> name : <cluster_name> contexts : - context : cluster : <cluster_name> user : <keycloak_user_email> name : <cluster_name> current-context : <cluster_name> users : - name : <keycloak_user_email> user : exec : apiVersion : client.authentication.k8s.io/v1beta1 command : kubectl args : - oidc-login - get-token - -v1 - --oidc-issuer-url=https://<keycloak_url>/auth/realms/<realm> - --oidc-client-id=<keycloak_client_id> - --oidc-client-secret=<keycloak_client_secret> Flag -v1 can be used for debug, in a common case it's not needed and can be deleted. To find the client secret: Open Keycloak Choose realm Find keycloak_client_id that was previously created Open Credentials tab Copy Secret","title":"Kubeconfig"},{"location":"operator-guide/configure-keycloak-oidc-eks/#testing","text":"Before testing, ensure that a user is a member of the correct Keycloak group. To add a user to a Keycloak group: Open Keycloak Choose realm Open user screen with search field Find a user and open the configuration Open Groups tab In Available Groups, choose an appropriate group Click the Join button The group should appear in the Group Membership list Follow the steps below to test the configuration: Run kubectl command, it is important to specify the correct kubeconfig: KUBECONFIG = <path_to_oidc_kubeconfig> kubectl get ingresses -n <namespace_name> After the first run and redirection to the Keycloak login page, log in using credentials (login:password) or using SSO Provider. In case of the successful login, you will receive the following notification that can be closed: OIDC Successful Login As the result, a respective response from the Kubernetes will appear in the console in case a user is configured correctly and is a member of the correct group and Roles/RoleBindings. If something is not set up correctly, the following output error will be displayed: Error from server ( Forbidden ) : ingresses.networking.k8s.io is forbidden: User \"https://<keycloak_url>/auth/realms/<realm>#<keycloak_user_id>\" cannot list resource \"ingresses\" in API group \"networking.k8s.io\" in the namespace \"<namespace_name>\"","title":"Testing"},{"location":"operator-guide/configure-keycloak-oidc-eks/#session-update","text":"To update the session, clear cache. The default location for the login cache: rm -rf ~/.kube/cache","title":"Session Update"},{"location":"operator-guide/configure-keycloak-oidc-eks/#access-cluster-via-lens","text":"To access the Kubernetes cluster via Lens , follow the steps below to configure it: Add a new kubeconfig to the location where Lens has access. The default location of the kubeconfig is ~/.kube/config but it can be changed by navigating to File -> Preferences -> Kubernetes -> Kubeconfig Syncs ; (Optional) Using Windows, it is recommended to reboot the system after adding a new kubeconfig. Authenticate on the Keycloak login page to be able to access the cluster; Note Lens does not add namespaces of the project automatically, so it is necessary to add them manually, simply go to Settings -> Namespaces and add the namespaces of a project.","title":"Access Cluster via Lens"},{"location":"operator-guide/delete-jenkins-job-provision/","text":"Delete Jenkins Job Provision \u2693\ufe0e To delete the job provisioner, take the following steps: Delete the job provisioner from Jenkins. Navigate to Admin Console->Jenkins->jobs->job-provisions folder, select the necessary provisioner and click the drop-down right to the provisioner name. Select Delete project . Delete job provisioner Run the commands below in the 'edp-db' pod: psql edp-db admin # Replace admin with the admin login from the db-admin-console secrets SET search_path to <NAMESPACE_NAME>; SELECT * FROM job_provisioning; Check the id of the necessary provisioner. Run the deletion command: DELETE FROM job_provisioning WHERE id=<PROVISIONER_ID>;","title":"Delete Jenkins Job Provision"},{"location":"operator-guide/delete-jenkins-job-provision/#delete-jenkins-job-provision","text":"To delete the job provisioner, take the following steps: Delete the job provisioner from Jenkins. Navigate to Admin Console->Jenkins->jobs->job-provisions folder, select the necessary provisioner and click the drop-down right to the provisioner name. Select Delete project . Delete job provisioner Run the commands below in the 'edp-db' pod: psql edp-db admin # Replace admin with the admin login from the db-admin-console secrets SET search_path to <NAMESPACE_NAME>; SELECT * FROM job_provisioning; Check the id of the necessary provisioner. Run the deletion command: DELETE FROM job_provisioning WHERE id=<PROVISIONER_ID>;","title":"Delete Jenkins Job Provision"},{"location":"operator-guide/deploy-aws-eks/","text":"Deploy AWS EKS Cluster \u2693\ufe0e This instruction provides detailed information on the Amazon Elastic Kubernetes Service cluster deployment and contains the additional setup necessary for the managed infrastructure. Prerequisites \u2693\ufe0e Before the EKS cluster deployment and configuration, make sure to check the prerequisites. Required Tools \u2693\ufe0e Install the required tools listed below: Git tfenv AWS CLI kubectl helm lens (optional) To check the correct tools installation, run the following commands: $ git --version $ tfenv --version $ aws --version $ kubectl version $ helm version AWS Account and IAM Roles \u2693\ufe0e Make sure the AWS account is active. Create the AWS IAM role: EKSDeployerRole to deploy EKS cluster on the project side. The provided resources will allow to use cross-account deployment by assuming the created EKSDeployerRole from the root AWS account. Take the following steps: Clone git repo with the edp-terraform-aws-platform.git ism-deployer project, and rename it according to the project name. clone project $ git clone https://github.com/epmd-edp/edp-terraform-aws-platform.git $ mv edp-terraform-aws-platform edp-terraform-aws-platform-<PROJECT_NAME> $ cd edp-terraform-aws-platform-<PROJECT_NAME>/iam-deployer where: \u2039PROJECT_NAME\u203a - is a project name or a unique platform identifier, for example, shared or test-eks . Fill in the input variables for Terraform run in the \u2039iam-deployer/terraform.tfvars\u203a file. Use the iam-deployer/template.tfvars as an example. Please find the detailed description of the variables in the iam-deployer/variables.tf file. terraform.tfvars file example aws_profile = \"aws_user\" region = \"eu-central-1\" tags = { \"SysName\" = \"EKS\" \"SysOwner\" = \"owner@example.com\" \"Environment\" = \"EKS-TEST-CLUSTER\" \"CostCenter\" = \"0000\" \"BusinessUnit\" = \"BU\" \"Department\" = \"DEPARTMENT\" } Run the terraform apply command. Then initialize the backend and apply the changes. apply the changes $ terraform init $ terraform apply ... Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes aws_iam_role.deployer: Creating... aws_iam_role.deployer: Creation complete after 4s [ id = EKSDeployerRole ] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: deployer_iam_role_arn = \"arn:aws:iam::012345678910:role/EKSDeployerRole\" deployer_iam_role_id = \"EKSDeployerRole\" deployer_iam_role_name = \"EKSDeployerRole\" Commit the local state. At this run, Terraform will use the local backend to store the state on the local filesystem. Terraform locks that state using system APIs and performs operations locally. It is not mandatory to store the resulted state file in Git, but this option can be used since the file data is not sensitive. Optionally, commit the state of the s3-backend project. $ git add iam-deployer/terraform.tfstate iam-deployer/terraform.tfvars $ git commit -m \"Terraform state for IAM deployer role\" Create the AWS IAM role: ServiceRoleForEKS WorkerNode to connect to the EKS cluster. Take the following steps: Use the local state file or the AWS S3 bucket for saving the state file. The AWS S3 bucket creation is described in the Terraform Backend section. Go to the folder with the iam-workernode role edp-terraform-aws-platform.git , and rename it according to the project name. go to the iam-workernode folder $ cd edp-terraform-aws-platform-<PROJECT_NAME>/iam-workernode where: \u2039PROJECT_NAME\u203a - is a project name or a unique platform identifier, for example, shared or test-eks . Fill in the input variables for Terraform run in the \u2039iam-workernode/terraform.tfvars\u203a file, use the iam-workernode/template.tfvars as an example. Please find the detailed description of the variables in the iam-workernode/variables.tf file. terraform.tfvars file example role_arn = \"arn:aws:iam::012345678910:role/EKSDeployerRole\" platform_name = \"<PROJECT_NAME>\" iam_permissions_boundary_policy_arn = \"arn:aws:iam::012345678910:policy/some_role_boundary\" region = \"eu-central-1\" tags = { \"SysName\" = \"EKS\" \"SysOwner\" = \"owner@example.com\" \"Environment\" = \"EKS-TEST-CLUSTER\" \"CostCenter\" = \"0000\" \"BusinessUnit\" = \"BU\" \"Department\" = \"DEPARTMENT\" } Run the terraform apply command. Then initialize the backend and apply the changes. apply the changes $ terraform init $ terraform apply ... Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Create the AWS IAM role: ServiceRoleForEKSShared for the EKS cluster. Take the following steps: Create the AWS IAM role: ServiceRoleForEKSShared Attach the following policies: \"AmazonEKSClusterPolicy\" and \"AmazonEKSServicePolicy\" Configure AWS profile for deployment from the local node. Please, refer to the AWS documentation for detailed guide to configure profiles. Create AWS Key pair for EKS cluster nodes access. Please refer to the AWS documentation for detailed guide to create a Key pair. Create a public Hosted Zone if there is no any to provide for EKS cluster deployment. Please, refer to the AWS documentation for detailed guide to create a Hosted zone. Terraform Backend \u2693\ufe0e The Terraform configuration for EKS cluster deployment has a backend block, which defines where and how the operations are performed, and where the state snapshots are stored. Currently, the best practice is to store the state as a given key in a given bucket on Amazon S3. This backend also supports state locking and consistency checking via Dynamo DB, which can be enabled by setting the dynamodb_table field to an existing DynamoDB table name. In the following configuration a single DynamoDB table can be used to lock multiple remote state files. Terraform generates key names that include the values of the bucket and key variables. In the edp-terraform-aws-platform.git repo an optional project is provided to create initial resources to start using Terraform from the scratch. The provided resources will allow to use the following Terraform options : to store Terraform states remotely in the Amazon S3 bucket; to manage remote state access with S3 bucket policy; to support state locking and consistency checking via DynamoDB. After Terraform run the following AWS resources will be created: S3 bucket: terraform-states-\u2039AWS_ACCOUNT_ID\u203a S3 bucket policy: terraform-states-\u2039AWS_ACCOUNT_ID\u203a DynamoDB lock table: terraform_locks Please, skip this section if you already have the listed resources for further Terraform remote backend usage. To create the required resources, do the following: Clone git repo with s3-backend project edp-terraform-aws-platform.git , rename it in the correspondence with project name. clone project $ git clone https://github.com/epmd-edp/edp-terraform-aws-platform.git $ mv edp-terraform-aws-platform tedp-terraform-aws-platform-<PROJECT_NAME> $ cd edp-terraform-aws-platform-<PROJECT_NAME>/s3-backend where: \u2039PROJECT_NAME\u203a - is a project name, a unique platform identifier, e.g. shared, test-eks etc. Fill the input variables for Terraform run in the \u2039s3-backend/terraform.tfvars\u203a file, refer to the s3-backend/template.tfvars as an example. terraform.tfvars file example region = \"eu-central-1\" s3_states_bucket_name = \"terraform-states\" table_name = \"terraform_locks\" tags = { \"SysName\" = \"EKS\" \"SysOwner\" = \"owner@example.com\" \"Environment\" = \"EKS-TEST-CLUSTER\" \"CostCenter\" = \"0000\" \"BusinessUnit\" = \"BU\" \"Department\" = \"DEPARTMENT\" } Find the detailed description of the variables in the s3-backend/variables.tf file. Run Terraform apply. Initialize the backend and apply the changes. apply the changes $ terraform init $ terraform apply ... Do you want to perform these actions ? Terraform will perform the actions described above . Only ' yes ' will be accepted to approve . Enter a value : yes aws_dynamodb_table . terraform_lock_table : Creating ... aws_s3_bucket . terraform_states : Creating ... aws_dynamodb_table . terraform_lock_table : Creation complete after 27 s [ id = terraform - locks - test ] aws_s3_bucket . terraform_states : Creation complete after 1 m10s [ id = terraform - states - test - 012345678910 ] aws_s3_bucket_policy . terraform_states : Creating ... aws_s3_bucket_policy . terraform_states : Creation complete after 1 s [ id = terraform - states - test - 012345678910 ] Apply complete ! Resources : 3 added , 0 changed , 0 destroyed . Outputs : terraform_lock_table_dynamodb_id = \" terraform_locks \" terraform_states_s3_bucket_name = \" terraform-states-012345678910 \" Commit the local state. At this run Terraform will use the local backend to store state on the local filesystem, locks that state using system APIs, and performs operations locally. There is no strong requirements to store the resulted state file in the git, but it's possible at will since there is no sensitive data. On your choice, commit the state of the s3-backend project or not. $ git add s3 - backend / terraform . tfstate $ git commit - m \" Terraform state for s3-backend \" As a result, the projects that run Terraform can use the following definition for remote state configuration: providers.tf - terraform backend configuration block terraform { backend \"s3\" { bucket = \"terraform-states-<AWS_ACCOUNT_ID>\" key = \"<PROJECT_NAME>/<REGION>/terraform/terraform.tfstate\" region = \"<REGION>\" acl = \"bucket-owner-full-control\" dynamodb_table = \"terraform_locks\" encrypt = true } } where: AWS_ACCOUNT_ID - is AWS account id, e.g. 012345678910, REGION - is AWS region, e.g. eu-central-1, PROJECT_NAME - is a project name, a unique platform identifier, e.g. shared, test-eks etc. View: providers.tf - terraform backend configuration example terraform { backend \"s3\" { bucket = \"terraform-states-012345678910\" key = \"test-eks/eu-central-1/terraform/terraform.tfstate\" region = \"eu-central-1\" acl = \"bucket-owner-full-control\" dynamodb_table = \"terraform_locks\" encrypt = true } } Note At the moment, it is recommended to use common s3 bucket and Dynamo DB in the root EDP account both for Shared and Standalone clusters deployment. Deploy EKS Cluster \u2693\ufe0e To deploy the EKS cluster, make sure that all the above-mentioned Prerequisites are ready to be used. EKS Cluster Deployment with Terraform \u2693\ufe0e Clone git repo with the Terraform project for EKS infrastructure edp-terraform-aws-platform.git and rename it in the correspondence with project name if not yet. clone project $ git clone https://github.com/epmd-edp/edp-terraform-aws-platform.git $ mv edp-terraform-aws-platform edp-terraform-aws-platform-<PROJECT_NAME> $ cd edp-terraform-aws-platform-<PROJECT_NAME> where: \u2039PROJECT_NAME\u203a - is a project name, a unique platform identifier, e.g. shared, test-eks etc. Configure Terraform backend according to your project needs or use instructions from the Configure Terraform backend section. Fill the input variables for Terraform run in the \u2039terraform.tfvars\u203a file, refer to the template.tfvars file and apply the changes. See details below. Be sure to put the correct values of the variables created in the Prerequisites section. Find the detailed description of the variables in the variables.tf file. Warning Please, do not use upper case in the input variables. It can lead to unexpected issues. template.tfvars file template # Check out all the inputs based on the comments below and fill the gaps instead <...> # More details on each variable can be found in the variables.tf file create_elb = true # set to true if you'd like to create ELB for Gerrit usage region = \"<REGION>\" role_arn = \"<ROLE_ARN>\" platform_name = \"<PLATFORM_NAME>\" # the name of the cluster and AWS resources platform_domain_name = \"<PLATFORM_DOMAIN_NAME>\" # must be created as a prerequisite # The following will be created or used existing depending on the create_vpc value subnet_azs = [\"<SUBNET_AZS1>\", \"<SUBNET_AZS2>\"] platform_cidr = \"<PLATFORM_CIDR>\" private_cidrs = [\"<PRIVATE_CIDRS1>\", \"<PRIVATE_CIDRS2>\"] public_cidrs = [\"<PUBLIC_CIDRS1>\", \"<PUBLIC_CIDRS2>\"] infrastructure_public_security_group_ids = [ \"<INFRASTRUCTURE_PUBLIC_SECURITY_GROUP_IDS1>\", \"<INFRASTRUCTURE_PUBLIC_SECURITY_GROUP_IDS2>\", ] ssl_policy = \"<SSL_POLICY>\" # EKS cluster configuration cluster_version = \"1.22\" key_name = \"<AWS_KEY_PAIR_NAME>\" # must be created as a prerequisite enable_irsa = true cluster_iam_role_name = \"<SERVICE_ROLE_FOR_EKS>\" worker_iam_instance_profile_name = \"<SERVICE_ROLE_FOR_EKS_WORKER_NODE\" add_userdata = <<EOF export TOKEN=$(aws ssm get-parameter --name <PARAMETER_NAME> --query 'Parameter.Value' --region <REGION> --output text) cat <<DATA > /var/lib/kubelet/config.json { \"auths\":{ \"https://index.docker.io/v1/\":{ \"auth\":\"$TOKEN\" } } } DATA EOF map_users = [ { \"userarn\" : \"<IAM_USER_ARN1>\", \"username\" : \"<IAM_USER_NAME1>\", \"groups\" : [\"system:masters\"] }, { \"userarn\" : \"<IAM_USER_ARN2>\", \"username\" : \"<IAM_USER_NAME2>\", \"groups\" : [\"system:masters\"] } ] map_roles = [ { \"rolearn\" : \"<IAM_ROLE_ARN1>\", \"username\" : \"<IAM_ROLE_NAME1>\", \"groups\" : [\"system:masters\"] }, ] tags = { \"SysName\" = \"<SYS_NAME>\" \"SysOwner\" = \"<SYSTEM_OWNER>\" \"Environment\" = \"<ENVIRONMENT>\" \"CostCenter\" = \"<COST_CENTER>\" \"BusinessUnit\" = \"<BUSINESS_UNIT>\" \"Department\" = \"<DEPARTMENT>\" \"user:tag\" = \"<PLATFORM_NAME>\" } # Variables for demand pool demand_instance_types = [\"r5.large\"] demand_max_nodes_count = 0 demand_min_nodes_count = 0 demand_desired_nodes_count = 0 // Variables for spot pool spot_instance_types = [\"r5.xlarge\", \"r5.large\", \"r4.large\"] # need to ensure we use nodes with more memory spot_max_nodes_count = 2 spot_desired_nodes_count = 2 spot_min_nodes_count = 2 Note The file above is an example. Please find the latest version in the project repo in the terraform.tfvars file. There are the following possible scenarios to deploy the EKS cluster: Case 1: Create new VPC and deploy the EKS cluster, terraform.tfvars file example create_elb = true # set to true if you'd like to create ELB for Gerrit usage region = \"eu-central-1\" role_arn = \"arn:aws:iam::012345678910:role/EKSDeployerRole\" platform_name = \"test-eks\" platform_domain_name = \"example.com\" # must be created as a prerequisite # The following will be created or used existing depending on the create_vpc value subnet_azs = [\"eu-central-1a\", \"eu-central-1b\"] platform_cidr = \"172.31.0.0/16\" private_cidrs = [\"172.31.0.0/20\", \"172.31.16.0/20\"] public_cidrs = [\"172.31.32.0/20\", \"172.31.48.0/20\"] # Use this parameter the second time you apply the code to specify new AWS Security Groups infrastructure_public_security_group_ids = [ # \"sg-00000000000000000\", # \"sg-00000000000000000\", ] # EKS cluster configuration cluster_version = \"1.22\" key_name = \"test-kn\" # must be created as a prerequisite enable_irsa = true # Define if IAM roles should be created during the deployment or used existing ones cluster_iam_role_name = \"ServiceRoleForEKSShared\" worker_iam_instance_profile_name = \"ServiceRoleForEksSharedWorkerNode0000000000000000000000\" add_userdata = <<EOF export TOKEN=$(aws ssm get-parameter --name edprobot --query 'Parameter.Value' --region eu-central-1 --output text) cat <<DATA > /var/lib/kubelet/config.json { \"auths\":{ \"https://index.docker.io/v1/\":{ \"auth\":\"$TOKEN\" } } } DATA EOF map_users = [ { \"userarn\" : \"arn:aws:iam::012345678910:user/user_name1@example.com\", \"username\" : \"user_name1@example.com\", \"groups\" : [\"system:masters\"] }, { \"userarn\" : \"arn:aws:iam::012345678910:user/user_name2@example.com\", \"username\" : \"user_name2@example.com\", \"groups\" : [\"system:masters\"] } ] map_roles = [ { \"rolearn\" : \"arn:aws:iam::012345678910:role/EKSClusterAdminRole\", \"username\" : \"eksadminrole\", \"groups\" : [\"system:masters\"] }, ] tags = { \"SysName\" = \"EKS\" \"SysOwner\" = \"owner@example.com\" \"Environment\" = \"EKS-TEST-CLUSTER\" \"CostCenter\" = \"2020\" \"BusinessUnit\" = \"BU\" \"Department\" = \"DEPARTMENT\" \"user:tag\" = \"test-eks\" } # Variables for spot pool spot_instance_types = [\"r5.large\", \"r4.large\"] # need to ensure we use nodes with more memory spot_max_nodes_count = 1 spot_desired_nodes_count = 1 spot_min_nodes_count = 1 Run Terraform apply. Initialize the backend and apply the changes. apply the changes $ terraform init $ terraform apply ... Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes ... Check EKS cluster deployment \u2693\ufe0e As a result, the \u2039PLATFORM_NAME\u203a EKS cluster is deployed to the specified AWS account. Make sure you have all required tools listed in the Install required tools section. To connect to the cluster find the kubeconfig _ file in the project folder which is output of the last Terraform apply run. Move it to the ~/.kube/ folder. $ mv kubeconfig_<PLATFORM_NAME> ~/.kube/ Run the following commands to ensure the EKS cluster is up and has required nodes count: $ kubectl config get-contexts $ kubectl get nodes Note If the there are any authorisation issues, make sure the users section in the kubeconfig_ file has all required parameters based on you AWS CLI version. Find more details in the create kubeconfig AWS user guide. And pay attention on the kubeconfig_aws_authenticator terraform input variables. Optionally, a Lens tool can be installed and used for further work with Kubernetes cluster. Refer to the original documentation to add and process the cluster.","title":"Deploy AWS EKS Cluster"},{"location":"operator-guide/deploy-aws-eks/#deploy-aws-eks-cluster","text":"This instruction provides detailed information on the Amazon Elastic Kubernetes Service cluster deployment and contains the additional setup necessary for the managed infrastructure.","title":"Deploy AWS EKS Cluster"},{"location":"operator-guide/deploy-aws-eks/#prerequisites","text":"Before the EKS cluster deployment and configuration, make sure to check the prerequisites.","title":"Prerequisites"},{"location":"operator-guide/deploy-aws-eks/#required-tools","text":"Install the required tools listed below: Git tfenv AWS CLI kubectl helm lens (optional) To check the correct tools installation, run the following commands: $ git --version $ tfenv --version $ aws --version $ kubectl version $ helm version","title":"Required Tools"},{"location":"operator-guide/deploy-aws-eks/#aws-account-and-iam-roles","text":"Make sure the AWS account is active. Create the AWS IAM role: EKSDeployerRole to deploy EKS cluster on the project side. The provided resources will allow to use cross-account deployment by assuming the created EKSDeployerRole from the root AWS account. Take the following steps: Clone git repo with the edp-terraform-aws-platform.git ism-deployer project, and rename it according to the project name. clone project $ git clone https://github.com/epmd-edp/edp-terraform-aws-platform.git $ mv edp-terraform-aws-platform edp-terraform-aws-platform-<PROJECT_NAME> $ cd edp-terraform-aws-platform-<PROJECT_NAME>/iam-deployer where: \u2039PROJECT_NAME\u203a - is a project name or a unique platform identifier, for example, shared or test-eks . Fill in the input variables for Terraform run in the \u2039iam-deployer/terraform.tfvars\u203a file. Use the iam-deployer/template.tfvars as an example. Please find the detailed description of the variables in the iam-deployer/variables.tf file. terraform.tfvars file example aws_profile = \"aws_user\" region = \"eu-central-1\" tags = { \"SysName\" = \"EKS\" \"SysOwner\" = \"owner@example.com\" \"Environment\" = \"EKS-TEST-CLUSTER\" \"CostCenter\" = \"0000\" \"BusinessUnit\" = \"BU\" \"Department\" = \"DEPARTMENT\" } Run the terraform apply command. Then initialize the backend and apply the changes. apply the changes $ terraform init $ terraform apply ... Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes aws_iam_role.deployer: Creating... aws_iam_role.deployer: Creation complete after 4s [ id = EKSDeployerRole ] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: deployer_iam_role_arn = \"arn:aws:iam::012345678910:role/EKSDeployerRole\" deployer_iam_role_id = \"EKSDeployerRole\" deployer_iam_role_name = \"EKSDeployerRole\" Commit the local state. At this run, Terraform will use the local backend to store the state on the local filesystem. Terraform locks that state using system APIs and performs operations locally. It is not mandatory to store the resulted state file in Git, but this option can be used since the file data is not sensitive. Optionally, commit the state of the s3-backend project. $ git add iam-deployer/terraform.tfstate iam-deployer/terraform.tfvars $ git commit -m \"Terraform state for IAM deployer role\" Create the AWS IAM role: ServiceRoleForEKS WorkerNode to connect to the EKS cluster. Take the following steps: Use the local state file or the AWS S3 bucket for saving the state file. The AWS S3 bucket creation is described in the Terraform Backend section. Go to the folder with the iam-workernode role edp-terraform-aws-platform.git , and rename it according to the project name. go to the iam-workernode folder $ cd edp-terraform-aws-platform-<PROJECT_NAME>/iam-workernode where: \u2039PROJECT_NAME\u203a - is a project name or a unique platform identifier, for example, shared or test-eks . Fill in the input variables for Terraform run in the \u2039iam-workernode/terraform.tfvars\u203a file, use the iam-workernode/template.tfvars as an example. Please find the detailed description of the variables in the iam-workernode/variables.tf file. terraform.tfvars file example role_arn = \"arn:aws:iam::012345678910:role/EKSDeployerRole\" platform_name = \"<PROJECT_NAME>\" iam_permissions_boundary_policy_arn = \"arn:aws:iam::012345678910:policy/some_role_boundary\" region = \"eu-central-1\" tags = { \"SysName\" = \"EKS\" \"SysOwner\" = \"owner@example.com\" \"Environment\" = \"EKS-TEST-CLUSTER\" \"CostCenter\" = \"0000\" \"BusinessUnit\" = \"BU\" \"Department\" = \"DEPARTMENT\" } Run the terraform apply command. Then initialize the backend and apply the changes. apply the changes $ terraform init $ terraform apply ... Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Create the AWS IAM role: ServiceRoleForEKSShared for the EKS cluster. Take the following steps: Create the AWS IAM role: ServiceRoleForEKSShared Attach the following policies: \"AmazonEKSClusterPolicy\" and \"AmazonEKSServicePolicy\" Configure AWS profile for deployment from the local node. Please, refer to the AWS documentation for detailed guide to configure profiles. Create AWS Key pair for EKS cluster nodes access. Please refer to the AWS documentation for detailed guide to create a Key pair. Create a public Hosted Zone if there is no any to provide for EKS cluster deployment. Please, refer to the AWS documentation for detailed guide to create a Hosted zone.","title":"AWS Account and IAM Roles"},{"location":"operator-guide/deploy-aws-eks/#terraform-backend","text":"The Terraform configuration for EKS cluster deployment has a backend block, which defines where and how the operations are performed, and where the state snapshots are stored. Currently, the best practice is to store the state as a given key in a given bucket on Amazon S3. This backend also supports state locking and consistency checking via Dynamo DB, which can be enabled by setting the dynamodb_table field to an existing DynamoDB table name. In the following configuration a single DynamoDB table can be used to lock multiple remote state files. Terraform generates key names that include the values of the bucket and key variables. In the edp-terraform-aws-platform.git repo an optional project is provided to create initial resources to start using Terraform from the scratch. The provided resources will allow to use the following Terraform options : to store Terraform states remotely in the Amazon S3 bucket; to manage remote state access with S3 bucket policy; to support state locking and consistency checking via DynamoDB. After Terraform run the following AWS resources will be created: S3 bucket: terraform-states-\u2039AWS_ACCOUNT_ID\u203a S3 bucket policy: terraform-states-\u2039AWS_ACCOUNT_ID\u203a DynamoDB lock table: terraform_locks Please, skip this section if you already have the listed resources for further Terraform remote backend usage. To create the required resources, do the following: Clone git repo with s3-backend project edp-terraform-aws-platform.git , rename it in the correspondence with project name. clone project $ git clone https://github.com/epmd-edp/edp-terraform-aws-platform.git $ mv edp-terraform-aws-platform tedp-terraform-aws-platform-<PROJECT_NAME> $ cd edp-terraform-aws-platform-<PROJECT_NAME>/s3-backend where: \u2039PROJECT_NAME\u203a - is a project name, a unique platform identifier, e.g. shared, test-eks etc. Fill the input variables for Terraform run in the \u2039s3-backend/terraform.tfvars\u203a file, refer to the s3-backend/template.tfvars as an example. terraform.tfvars file example region = \"eu-central-1\" s3_states_bucket_name = \"terraform-states\" table_name = \"terraform_locks\" tags = { \"SysName\" = \"EKS\" \"SysOwner\" = \"owner@example.com\" \"Environment\" = \"EKS-TEST-CLUSTER\" \"CostCenter\" = \"0000\" \"BusinessUnit\" = \"BU\" \"Department\" = \"DEPARTMENT\" } Find the detailed description of the variables in the s3-backend/variables.tf file. Run Terraform apply. Initialize the backend and apply the changes. apply the changes $ terraform init $ terraform apply ... Do you want to perform these actions ? Terraform will perform the actions described above . Only ' yes ' will be accepted to approve . Enter a value : yes aws_dynamodb_table . terraform_lock_table : Creating ... aws_s3_bucket . terraform_states : Creating ... aws_dynamodb_table . terraform_lock_table : Creation complete after 27 s [ id = terraform - locks - test ] aws_s3_bucket . terraform_states : Creation complete after 1 m10s [ id = terraform - states - test - 012345678910 ] aws_s3_bucket_policy . terraform_states : Creating ... aws_s3_bucket_policy . terraform_states : Creation complete after 1 s [ id = terraform - states - test - 012345678910 ] Apply complete ! Resources : 3 added , 0 changed , 0 destroyed . Outputs : terraform_lock_table_dynamodb_id = \" terraform_locks \" terraform_states_s3_bucket_name = \" terraform-states-012345678910 \" Commit the local state. At this run Terraform will use the local backend to store state on the local filesystem, locks that state using system APIs, and performs operations locally. There is no strong requirements to store the resulted state file in the git, but it's possible at will since there is no sensitive data. On your choice, commit the state of the s3-backend project or not. $ git add s3 - backend / terraform . tfstate $ git commit - m \" Terraform state for s3-backend \" As a result, the projects that run Terraform can use the following definition for remote state configuration: providers.tf - terraform backend configuration block terraform { backend \"s3\" { bucket = \"terraform-states-<AWS_ACCOUNT_ID>\" key = \"<PROJECT_NAME>/<REGION>/terraform/terraform.tfstate\" region = \"<REGION>\" acl = \"bucket-owner-full-control\" dynamodb_table = \"terraform_locks\" encrypt = true } } where: AWS_ACCOUNT_ID - is AWS account id, e.g. 012345678910, REGION - is AWS region, e.g. eu-central-1, PROJECT_NAME - is a project name, a unique platform identifier, e.g. shared, test-eks etc. View: providers.tf - terraform backend configuration example terraform { backend \"s3\" { bucket = \"terraform-states-012345678910\" key = \"test-eks/eu-central-1/terraform/terraform.tfstate\" region = \"eu-central-1\" acl = \"bucket-owner-full-control\" dynamodb_table = \"terraform_locks\" encrypt = true } } Note At the moment, it is recommended to use common s3 bucket and Dynamo DB in the root EDP account both for Shared and Standalone clusters deployment.","title":"Terraform Backend"},{"location":"operator-guide/deploy-aws-eks/#deploy-eks-cluster","text":"To deploy the EKS cluster, make sure that all the above-mentioned Prerequisites are ready to be used.","title":"Deploy EKS Cluster"},{"location":"operator-guide/deploy-aws-eks/#eks-cluster-deployment-with-terraform","text":"Clone git repo with the Terraform project for EKS infrastructure edp-terraform-aws-platform.git and rename it in the correspondence with project name if not yet. clone project $ git clone https://github.com/epmd-edp/edp-terraform-aws-platform.git $ mv edp-terraform-aws-platform edp-terraform-aws-platform-<PROJECT_NAME> $ cd edp-terraform-aws-platform-<PROJECT_NAME> where: \u2039PROJECT_NAME\u203a - is a project name, a unique platform identifier, e.g. shared, test-eks etc. Configure Terraform backend according to your project needs or use instructions from the Configure Terraform backend section. Fill the input variables for Terraform run in the \u2039terraform.tfvars\u203a file, refer to the template.tfvars file and apply the changes. See details below. Be sure to put the correct values of the variables created in the Prerequisites section. Find the detailed description of the variables in the variables.tf file. Warning Please, do not use upper case in the input variables. It can lead to unexpected issues. template.tfvars file template # Check out all the inputs based on the comments below and fill the gaps instead <...> # More details on each variable can be found in the variables.tf file create_elb = true # set to true if you'd like to create ELB for Gerrit usage region = \"<REGION>\" role_arn = \"<ROLE_ARN>\" platform_name = \"<PLATFORM_NAME>\" # the name of the cluster and AWS resources platform_domain_name = \"<PLATFORM_DOMAIN_NAME>\" # must be created as a prerequisite # The following will be created or used existing depending on the create_vpc value subnet_azs = [\"<SUBNET_AZS1>\", \"<SUBNET_AZS2>\"] platform_cidr = \"<PLATFORM_CIDR>\" private_cidrs = [\"<PRIVATE_CIDRS1>\", \"<PRIVATE_CIDRS2>\"] public_cidrs = [\"<PUBLIC_CIDRS1>\", \"<PUBLIC_CIDRS2>\"] infrastructure_public_security_group_ids = [ \"<INFRASTRUCTURE_PUBLIC_SECURITY_GROUP_IDS1>\", \"<INFRASTRUCTURE_PUBLIC_SECURITY_GROUP_IDS2>\", ] ssl_policy = \"<SSL_POLICY>\" # EKS cluster configuration cluster_version = \"1.22\" key_name = \"<AWS_KEY_PAIR_NAME>\" # must be created as a prerequisite enable_irsa = true cluster_iam_role_name = \"<SERVICE_ROLE_FOR_EKS>\" worker_iam_instance_profile_name = \"<SERVICE_ROLE_FOR_EKS_WORKER_NODE\" add_userdata = <<EOF export TOKEN=$(aws ssm get-parameter --name <PARAMETER_NAME> --query 'Parameter.Value' --region <REGION> --output text) cat <<DATA > /var/lib/kubelet/config.json { \"auths\":{ \"https://index.docker.io/v1/\":{ \"auth\":\"$TOKEN\" } } } DATA EOF map_users = [ { \"userarn\" : \"<IAM_USER_ARN1>\", \"username\" : \"<IAM_USER_NAME1>\", \"groups\" : [\"system:masters\"] }, { \"userarn\" : \"<IAM_USER_ARN2>\", \"username\" : \"<IAM_USER_NAME2>\", \"groups\" : [\"system:masters\"] } ] map_roles = [ { \"rolearn\" : \"<IAM_ROLE_ARN1>\", \"username\" : \"<IAM_ROLE_NAME1>\", \"groups\" : [\"system:masters\"] }, ] tags = { \"SysName\" = \"<SYS_NAME>\" \"SysOwner\" = \"<SYSTEM_OWNER>\" \"Environment\" = \"<ENVIRONMENT>\" \"CostCenter\" = \"<COST_CENTER>\" \"BusinessUnit\" = \"<BUSINESS_UNIT>\" \"Department\" = \"<DEPARTMENT>\" \"user:tag\" = \"<PLATFORM_NAME>\" } # Variables for demand pool demand_instance_types = [\"r5.large\"] demand_max_nodes_count = 0 demand_min_nodes_count = 0 demand_desired_nodes_count = 0 // Variables for spot pool spot_instance_types = [\"r5.xlarge\", \"r5.large\", \"r4.large\"] # need to ensure we use nodes with more memory spot_max_nodes_count = 2 spot_desired_nodes_count = 2 spot_min_nodes_count = 2 Note The file above is an example. Please find the latest version in the project repo in the terraform.tfvars file. There are the following possible scenarios to deploy the EKS cluster: Case 1: Create new VPC and deploy the EKS cluster, terraform.tfvars file example create_elb = true # set to true if you'd like to create ELB for Gerrit usage region = \"eu-central-1\" role_arn = \"arn:aws:iam::012345678910:role/EKSDeployerRole\" platform_name = \"test-eks\" platform_domain_name = \"example.com\" # must be created as a prerequisite # The following will be created or used existing depending on the create_vpc value subnet_azs = [\"eu-central-1a\", \"eu-central-1b\"] platform_cidr = \"172.31.0.0/16\" private_cidrs = [\"172.31.0.0/20\", \"172.31.16.0/20\"] public_cidrs = [\"172.31.32.0/20\", \"172.31.48.0/20\"] # Use this parameter the second time you apply the code to specify new AWS Security Groups infrastructure_public_security_group_ids = [ # \"sg-00000000000000000\", # \"sg-00000000000000000\", ] # EKS cluster configuration cluster_version = \"1.22\" key_name = \"test-kn\" # must be created as a prerequisite enable_irsa = true # Define if IAM roles should be created during the deployment or used existing ones cluster_iam_role_name = \"ServiceRoleForEKSShared\" worker_iam_instance_profile_name = \"ServiceRoleForEksSharedWorkerNode0000000000000000000000\" add_userdata = <<EOF export TOKEN=$(aws ssm get-parameter --name edprobot --query 'Parameter.Value' --region eu-central-1 --output text) cat <<DATA > /var/lib/kubelet/config.json { \"auths\":{ \"https://index.docker.io/v1/\":{ \"auth\":\"$TOKEN\" } } } DATA EOF map_users = [ { \"userarn\" : \"arn:aws:iam::012345678910:user/user_name1@example.com\", \"username\" : \"user_name1@example.com\", \"groups\" : [\"system:masters\"] }, { \"userarn\" : \"arn:aws:iam::012345678910:user/user_name2@example.com\", \"username\" : \"user_name2@example.com\", \"groups\" : [\"system:masters\"] } ] map_roles = [ { \"rolearn\" : \"arn:aws:iam::012345678910:role/EKSClusterAdminRole\", \"username\" : \"eksadminrole\", \"groups\" : [\"system:masters\"] }, ] tags = { \"SysName\" = \"EKS\" \"SysOwner\" = \"owner@example.com\" \"Environment\" = \"EKS-TEST-CLUSTER\" \"CostCenter\" = \"2020\" \"BusinessUnit\" = \"BU\" \"Department\" = \"DEPARTMENT\" \"user:tag\" = \"test-eks\" } # Variables for spot pool spot_instance_types = [\"r5.large\", \"r4.large\"] # need to ensure we use nodes with more memory spot_max_nodes_count = 1 spot_desired_nodes_count = 1 spot_min_nodes_count = 1 Run Terraform apply. Initialize the backend and apply the changes. apply the changes $ terraform init $ terraform apply ... Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes ...","title":"EKS Cluster Deployment with Terraform"},{"location":"operator-guide/deploy-aws-eks/#check-eks-cluster-deployment","text":"As a result, the \u2039PLATFORM_NAME\u203a EKS cluster is deployed to the specified AWS account. Make sure you have all required tools listed in the Install required tools section. To connect to the cluster find the kubeconfig _ file in the project folder which is output of the last Terraform apply run. Move it to the ~/.kube/ folder. $ mv kubeconfig_<PLATFORM_NAME> ~/.kube/ Run the following commands to ensure the EKS cluster is up and has required nodes count: $ kubectl config get-contexts $ kubectl get nodes Note If the there are any authorisation issues, make sure the users section in the kubeconfig_ file has all required parameters based on you AWS CLI version. Find more details in the create kubeconfig AWS user guide. And pay attention on the kubeconfig_aws_authenticator terraform input variables. Optionally, a Lens tool can be installed and used for further work with Kubernetes cluster. Refer to the original documentation to add and process the cluster.","title":"Check EKS cluster deployment"},{"location":"operator-guide/deploy-okd-4.10/","text":"Deploy OKD 4.10 Cluster \u2693\ufe0e This instruction provides detailed information on the OKD 4.10 cluster deployment in the AWS Cloud and contains the additional setup necessary for the managed infrastructure. A full description of the cluster deployment can be found in the official documentation . Prerequisites \u2693\ufe0e Before the OKD cluster deployment and configuration, make sure to check the prerequisites. Required Tools \u2693\ufe0e Install the following tools listed below: AWS CLI OpenShift CLI Lens (optional) Create the AWS IAM user with the required permissions . Make sure the AWS account is active, and the user doesn't have a permission boundary. Remove any Service Control Policy (SCP) restrictions from the AWS account. Generate a key pair for cluster node SSH access. Please perform the steps below: Generate the SSH key. Specify the path and file name, such as ~/.ssh/id_ed25519, of the new SSH key. If there is an existing key pair, ensure that the public key is in the ~/.ssh directory. ssh-keygen -t ed25519 -N '' -f <path>/<file_name> Add the SSH private key identity to the SSH agent for a local user if it has not already been added. eval \" $( ssh-agent -s ) \" Add the SSH private key to the ssh-agent: ssh-add <path>/<file_name> Build the ccoctl tool: Clone the cloud-credential-operator repository. git clone https://github.com/openshift/cloud-credential-operator.git Move to the cloud-credential-operator folder and build the ccoctl tool. cd cloud-credential-operator && git checkout release-4.10 GO_PACKAGE = 'github.com/openshift/cloud-credential-operator' go build -ldflags \"-X $GO_PACKAGE /pkg/version.versionFromGit= $( git describe --long --tags --abbrev = 7 --match 'v[0-9]*' ) \" ./cmd/ccoctl Prepare for the Deployment Process \u2693\ufe0e Before deploying the OKD cluster, please perform the steps below: Create AWS Resources \u2693\ufe0e Create the AWS resources with the Cloud Credential Operator utility (the ccoctl tool): Generate the public and private RSA key files that are used to set up the OpenID Connect identity provider for the cluster: ./ccoctl aws create-key-pair Create an OpenID Connect identity provider and an S3 bucket on AWS: ./ccoctl aws create-identity-provider \\ --name = <NAME> \\ --region = <AWS_REGION> \\ --public-key-file = ./serviceaccount-signer.public where: NAME - is the name used to tag any cloud resources created for tracking, AWS_REGION - is the AWS region in which cloud resources will be created. Create the IAM roles for each component in the cluster: Extract the list of the CredentialsRequest objects from the OpenShift Container Platform release image: oc adm release extract \\ --credentials-requests \\ --cloud = aws \\ --to = ./credrequests \\ --quay.io/openshift-release-dev/ocp-release:4.10.25-x86_64 Note A version of the openshift-release-dev docker image can be found in the Quay registry . Use the ccoctl tool to process all CredentialsRequest objects in the credrequests directory: ccoctl aws create-iam-roles \\ --name = <NAME> \\ --region = <AWS_REGION> \\ --credentials-requests-dir = ./credrequests --identity-provider-arn = arn:aws:iam::<AWS_ACCOUNT_ID>:oidc-provider/<NAME>-oidc.s3.<AWS_REGION>.amazonaws.com Create OKD Manifests \u2693\ufe0e Before deploying the OKD cluster, please perform the steps below: Download the OKD installer . Extract the installation program: tar -xvf openshift-install-linux.tar.gz Download the installation pull secret for any private registry. This pull secret allows to authenticate with the services that are provided by the authorities, including Quay.io , serving the container images for OKD components. For example, here is a pull secret for Docker Hub: The pull secret for the private registry { \"auths\" :{ \"https://index.docker.io/v1/\" :{ \"auth\" : \"$TOKEN\" } } } Create a deployment directory and the install-config.yaml file: mkdir okd-deployment touch okd-deployment/install-config.yaml To specify more details about the OKD cluster platform or to modify the values of the required parameters, customize the install-config.yaml file for the AWS. Please see below an example of the customized file: install-config.yaml - OKD cluster\u2019s platform installation configuration file apiVersion : v1 baseDomain : <YOUR_DOMAIN> credentialsMode : Manual compute : - architecture : amd64 hyperthreading : Enabled name : worker platform : aws : rootVolume : size : 30 zones : - eu-central-1a type : r5.large replicas : 3 controlPlane : architecture : amd64 hyperthreading : Enabled name : master platform : aws : rootVolume : size : 50 zones : - eu-central-1a type : m5.xlarge replicas : 3 metadata : creationTimestamp : null name : 4-10-okd-sandbox networking : clusterNetwork : - cidr : 10.128.0.0/14 hostPrefix : 23 machineNetwork : - cidr : 10.0.0.0/16 networkType : OVNKubernetes serviceNetwork : - 172.30.0.0/16 platform : aws : region : eu-central-1 userTags : user:tag : 4-10-okd-sandbox publish : External pullSecret : <PULL_SECRET> sshKey : | <SSH_KEY> where: YOUR_DOMAIN - is a base domain, PULL_SECRET - is a created pull secret for a private registry, SSH_KEY - is a created SSH key. Create the required OpenShift Container Platform installation manifests: ./openshift-install create manifests --dir okd-deployment Copy the manifests generated by the ccoctl tool to the manifests directory created by the installation program: cp ./manifests/* ./okd-deployment/manifests/ Copy the private key generated in the tls directory by the ccoctl tool to the installation directory: cp -a ./tls ./okd-deployment Deploy the Cluster \u2693\ufe0e To initialize the cluster deployment, run the following command: ./openshift-install create cluster --dir okd-deployment --log-level = info Note If the cloud provider account configured on the host does not have sufficient permissions to deploy the cluster, the installation process stops, and the missing permissions are displayed. When the cluster deployment is completed, directions for accessing the cluster are displayed in the terminal, including a link to the web console and credentials for the kubeadmin user. The kubeconfig for the cluster will be located in okd-deployment/auth/kubeconfig . Example output ... INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/myuser/install_dir/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.mycluster.example.com INFO Login to the console with the user: \"kubeadmin\", and password: \"4vYBz-Ee6gm-ymBZj-Wt5AL\" INFO Time elapsed: 36m22s: Warning The Ignition config files contain certificates that expire after 24 hours, which are then renewed at that time. Do not turn off the cluster for this time, or you will have to update the certificates manually. See OpenShift Container Platform documentation for more information. Log Into the Cluster \u2693\ufe0e To log into the cluster, export the kubeconfig : export KUBECONFIG =< installation_directory >/ auth / kubeconfig Optionally, use the Lens tool for further work with the Kubernetes cluster. Note To install and manage the cluster, refer to Lens documentation . Manage OKD Cluster Without the Inbound Rules \u2693\ufe0e In order to manage the OKD cluster without the 0.0.0.0/0 inbound rules, please perform the steps below: Create a Security Group with a list of your external IPs: aws ec2 create-security-group --group-name <SECURITY_GROUP_NAME> --description \"<DESCRIPTION_OF_SECURITY_GROUP>\" --vpc-id <VPC_ID> aws ec2 authorize-security-group-ingress \\ --group-id '<SECURITY_GROUP_ID>' \\ --ip-permissions 'IpProtocol=all,PrefixListIds=[{PrefixListId=<PREFIX_LIST_ID>}]' Manually attach this new Security Group to all master nodes of the cluster. Create another Security Group with an Elastic IP of the Cluster VPC: aws ec2 create-security-group --group-name custom-okd-4-10 --description \"Cluster Ip to 80, 443\" --vpc-id <VPC_ID> aws ec2 authorize-security-group-ingress \\ --group-id '<SECURITY_GROUP_ID>' \\ --protocol all \\ --port 80 \\ --cidr <ELASTIC_IP_OF_CLUSTER_VPC> aws ec2 authorize-security-group-ingress \\ --group-id '<SECURITY_GROUP_ID>' \\ --protocol all \\ --port 443 \\ --cidr <ELASTIC_IP_OF_CLUSTER_VPC> Modify the cluster load balancer via the router-default svc in the openshift-ingress namespace, paste two Security Groups created on previous steps: The pull secret for the private registry apiVersion: v1 kind: Service metadata: name: router-default namespace: openshift-ingress annotations: service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: \"tag_name=some_value\" service.beta.kubernetes.io/aws-load-balancer-security-groups: \"<SECURITY_GROUP_IDs>\" ... Optimize Spot Instances Usage \u2693\ufe0e In order to optimize the usage of Spot Instances on the AWS, add the following line under the providerSpec field in the MachineSet of Worker Nodes: providerSpec : value : spotMarketOptions : {} Related Articles \u2693\ufe0e Deploy AWS EKS Cluster Manage Jenkins Agent Associate IAM Roles With Service Accounts Deploy OKD 4.9 Cluster","title":"Deploy OKD 4.10 Cluster"},{"location":"operator-guide/deploy-okd-4.10/#deploy-okd-410-cluster","text":"This instruction provides detailed information on the OKD 4.10 cluster deployment in the AWS Cloud and contains the additional setup necessary for the managed infrastructure. A full description of the cluster deployment can be found in the official documentation .","title":"Deploy OKD 4.10 Cluster"},{"location":"operator-guide/deploy-okd-4.10/#prerequisites","text":"Before the OKD cluster deployment and configuration, make sure to check the prerequisites.","title":"Prerequisites"},{"location":"operator-guide/deploy-okd-4.10/#required-tools","text":"Install the following tools listed below: AWS CLI OpenShift CLI Lens (optional) Create the AWS IAM user with the required permissions . Make sure the AWS account is active, and the user doesn't have a permission boundary. Remove any Service Control Policy (SCP) restrictions from the AWS account. Generate a key pair for cluster node SSH access. Please perform the steps below: Generate the SSH key. Specify the path and file name, such as ~/.ssh/id_ed25519, of the new SSH key. If there is an existing key pair, ensure that the public key is in the ~/.ssh directory. ssh-keygen -t ed25519 -N '' -f <path>/<file_name> Add the SSH private key identity to the SSH agent for a local user if it has not already been added. eval \" $( ssh-agent -s ) \" Add the SSH private key to the ssh-agent: ssh-add <path>/<file_name> Build the ccoctl tool: Clone the cloud-credential-operator repository. git clone https://github.com/openshift/cloud-credential-operator.git Move to the cloud-credential-operator folder and build the ccoctl tool. cd cloud-credential-operator && git checkout release-4.10 GO_PACKAGE = 'github.com/openshift/cloud-credential-operator' go build -ldflags \"-X $GO_PACKAGE /pkg/version.versionFromGit= $( git describe --long --tags --abbrev = 7 --match 'v[0-9]*' ) \" ./cmd/ccoctl","title":"Required Tools"},{"location":"operator-guide/deploy-okd-4.10/#prepare-for-the-deployment-process","text":"Before deploying the OKD cluster, please perform the steps below:","title":"Prepare for the Deployment Process"},{"location":"operator-guide/deploy-okd-4.10/#create-aws-resources","text":"Create the AWS resources with the Cloud Credential Operator utility (the ccoctl tool): Generate the public and private RSA key files that are used to set up the OpenID Connect identity provider for the cluster: ./ccoctl aws create-key-pair Create an OpenID Connect identity provider and an S3 bucket on AWS: ./ccoctl aws create-identity-provider \\ --name = <NAME> \\ --region = <AWS_REGION> \\ --public-key-file = ./serviceaccount-signer.public where: NAME - is the name used to tag any cloud resources created for tracking, AWS_REGION - is the AWS region in which cloud resources will be created. Create the IAM roles for each component in the cluster: Extract the list of the CredentialsRequest objects from the OpenShift Container Platform release image: oc adm release extract \\ --credentials-requests \\ --cloud = aws \\ --to = ./credrequests \\ --quay.io/openshift-release-dev/ocp-release:4.10.25-x86_64 Note A version of the openshift-release-dev docker image can be found in the Quay registry . Use the ccoctl tool to process all CredentialsRequest objects in the credrequests directory: ccoctl aws create-iam-roles \\ --name = <NAME> \\ --region = <AWS_REGION> \\ --credentials-requests-dir = ./credrequests --identity-provider-arn = arn:aws:iam::<AWS_ACCOUNT_ID>:oidc-provider/<NAME>-oidc.s3.<AWS_REGION>.amazonaws.com","title":"Create AWS Resources"},{"location":"operator-guide/deploy-okd-4.10/#create-okd-manifests","text":"Before deploying the OKD cluster, please perform the steps below: Download the OKD installer . Extract the installation program: tar -xvf openshift-install-linux.tar.gz Download the installation pull secret for any private registry. This pull secret allows to authenticate with the services that are provided by the authorities, including Quay.io , serving the container images for OKD components. For example, here is a pull secret for Docker Hub: The pull secret for the private registry { \"auths\" :{ \"https://index.docker.io/v1/\" :{ \"auth\" : \"$TOKEN\" } } } Create a deployment directory and the install-config.yaml file: mkdir okd-deployment touch okd-deployment/install-config.yaml To specify more details about the OKD cluster platform or to modify the values of the required parameters, customize the install-config.yaml file for the AWS. Please see below an example of the customized file: install-config.yaml - OKD cluster\u2019s platform installation configuration file apiVersion : v1 baseDomain : <YOUR_DOMAIN> credentialsMode : Manual compute : - architecture : amd64 hyperthreading : Enabled name : worker platform : aws : rootVolume : size : 30 zones : - eu-central-1a type : r5.large replicas : 3 controlPlane : architecture : amd64 hyperthreading : Enabled name : master platform : aws : rootVolume : size : 50 zones : - eu-central-1a type : m5.xlarge replicas : 3 metadata : creationTimestamp : null name : 4-10-okd-sandbox networking : clusterNetwork : - cidr : 10.128.0.0/14 hostPrefix : 23 machineNetwork : - cidr : 10.0.0.0/16 networkType : OVNKubernetes serviceNetwork : - 172.30.0.0/16 platform : aws : region : eu-central-1 userTags : user:tag : 4-10-okd-sandbox publish : External pullSecret : <PULL_SECRET> sshKey : | <SSH_KEY> where: YOUR_DOMAIN - is a base domain, PULL_SECRET - is a created pull secret for a private registry, SSH_KEY - is a created SSH key. Create the required OpenShift Container Platform installation manifests: ./openshift-install create manifests --dir okd-deployment Copy the manifests generated by the ccoctl tool to the manifests directory created by the installation program: cp ./manifests/* ./okd-deployment/manifests/ Copy the private key generated in the tls directory by the ccoctl tool to the installation directory: cp -a ./tls ./okd-deployment","title":"Create OKD Manifests"},{"location":"operator-guide/deploy-okd-4.10/#deploy-the-cluster","text":"To initialize the cluster deployment, run the following command: ./openshift-install create cluster --dir okd-deployment --log-level = info Note If the cloud provider account configured on the host does not have sufficient permissions to deploy the cluster, the installation process stops, and the missing permissions are displayed. When the cluster deployment is completed, directions for accessing the cluster are displayed in the terminal, including a link to the web console and credentials for the kubeadmin user. The kubeconfig for the cluster will be located in okd-deployment/auth/kubeconfig . Example output ... INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/myuser/install_dir/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.mycluster.example.com INFO Login to the console with the user: \"kubeadmin\", and password: \"4vYBz-Ee6gm-ymBZj-Wt5AL\" INFO Time elapsed: 36m22s: Warning The Ignition config files contain certificates that expire after 24 hours, which are then renewed at that time. Do not turn off the cluster for this time, or you will have to update the certificates manually. See OpenShift Container Platform documentation for more information.","title":"Deploy the Cluster"},{"location":"operator-guide/deploy-okd-4.10/#log-into-the-cluster","text":"To log into the cluster, export the kubeconfig : export KUBECONFIG =< installation_directory >/ auth / kubeconfig Optionally, use the Lens tool for further work with the Kubernetes cluster. Note To install and manage the cluster, refer to Lens documentation .","title":"Log Into the Cluster"},{"location":"operator-guide/deploy-okd-4.10/#manage-okd-cluster-without-the-inbound-rules","text":"In order to manage the OKD cluster without the 0.0.0.0/0 inbound rules, please perform the steps below: Create a Security Group with a list of your external IPs: aws ec2 create-security-group --group-name <SECURITY_GROUP_NAME> --description \"<DESCRIPTION_OF_SECURITY_GROUP>\" --vpc-id <VPC_ID> aws ec2 authorize-security-group-ingress \\ --group-id '<SECURITY_GROUP_ID>' \\ --ip-permissions 'IpProtocol=all,PrefixListIds=[{PrefixListId=<PREFIX_LIST_ID>}]' Manually attach this new Security Group to all master nodes of the cluster. Create another Security Group with an Elastic IP of the Cluster VPC: aws ec2 create-security-group --group-name custom-okd-4-10 --description \"Cluster Ip to 80, 443\" --vpc-id <VPC_ID> aws ec2 authorize-security-group-ingress \\ --group-id '<SECURITY_GROUP_ID>' \\ --protocol all \\ --port 80 \\ --cidr <ELASTIC_IP_OF_CLUSTER_VPC> aws ec2 authorize-security-group-ingress \\ --group-id '<SECURITY_GROUP_ID>' \\ --protocol all \\ --port 443 \\ --cidr <ELASTIC_IP_OF_CLUSTER_VPC> Modify the cluster load balancer via the router-default svc in the openshift-ingress namespace, paste two Security Groups created on previous steps: The pull secret for the private registry apiVersion: v1 kind: Service metadata: name: router-default namespace: openshift-ingress annotations: service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: \"tag_name=some_value\" service.beta.kubernetes.io/aws-load-balancer-security-groups: \"<SECURITY_GROUP_IDs>\" ...","title":"Manage OKD Cluster Without the Inbound Rules"},{"location":"operator-guide/deploy-okd-4.10/#optimize-spot-instances-usage","text":"In order to optimize the usage of Spot Instances on the AWS, add the following line under the providerSpec field in the MachineSet of Worker Nodes: providerSpec : value : spotMarketOptions : {}","title":"Optimize Spot Instances Usage"},{"location":"operator-guide/deploy-okd-4.10/#related-articles","text":"Deploy AWS EKS Cluster Manage Jenkins Agent Associate IAM Roles With Service Accounts Deploy OKD 4.9 Cluster","title":"Related Articles"},{"location":"operator-guide/deploy-okd/","text":"Deploy OKD 4.9 Cluster \u2693\ufe0e This instruction provides detailed information on the OKD 4.9 cluster deployment in the AWS Cloud and contains the additional setup necessary for the managed infrastructure. A full description of the cluster deployment can be found in the official documentation . Prerequisites \u2693\ufe0e Before the OKD cluster deployment and configuration, make sure to check the prerequisites. Required Tools \u2693\ufe0e Install the following tools listed below: AWS CLI OpenShift CLI Lens (optional) Create the AWS IAM user with the required permissions . Make sure the AWS account is active, and the user doesn't have a permission boundary. Remove any Service Control Policy (SCP) restrictions from the AWS account. Generate a key pair for cluster node SSH access. Please perform the steps below: Generate the SSH key. Specify the path and file name, such as ~/.ssh/id_ed25519, of the new SSH key. If there is an existing key pair, ensure that the public key is in the ~/.ssh directory. ssh-keygen -t ed25519 -N '' -f <path>/<file_name> Add the SSH private key identity to the SSH agent for a local user if it has not already been added. eval \"$(ssh-agent -s)\" Add the SSH private key to the ssh-agent: ssh-add <path>/<file_name> Prepare for the Deployment Process \u2693\ufe0e Before deploying the OKD cluster, please perform the steps below: Download the OKD installer . Extract the installation program: tar -xvf openshift-install-linux.tar.gz Download the installation pull secret for any private registry. This pull secret allows to authenticate with the services that are provided by the included authorities, including Quay.io serving container images for OKD components. For example, here is a pull secret for Docker Hub: The pull secret for the private registry { \"auths\":{ \"https://index.docker.io/v1/\":{ \"auth\":\"$TOKEN\" } } } Create the deployment directory and the install-config.yaml file: mkdir okd-deployment touch okd-deployment/install-config.yaml To specify more details about the OKD cluster platform or to modify the values of the required parameters, customize the install-config.yaml file for AWS. Please see an example of the customized file below: install-config.yaml - OKD cluster\u2019s platform installation configuration file apiVersion: v1 baseDomain: <YOUR_DOMAIN> compute: - architecture: amd64 hyperthreading: Enabled name: worker platform: aws: zones: - eu-central-1a rootVolume: size: 50 type: r5.large replicas: 3 controlPlane: architecture: amd64 hyperthreading: Enabled name: master platform: aws: rootVolume: size: 50 zones: - eu-central-1a type: m5.xlarge replicas: 3 metadata: creationTimestamp: null name: 4-9-okd-sandbox platform: aws: region: eu-central-1 userTags: user:tag: 4-9-okd-sandbox publish: External pullSecret: <PULL_SECRET> sshKey: | <SSH_KEY> where: YOUR_DOMAIN - is a base domain, PULL_SECRET - is a created pull secret for a private registry, SSH_KEY - is a created SSH key. Deploy the Cluster \u2693\ufe0e To initialize the cluster deployment, run the following command: ./openshift-install create cluster --dir <installation_directory> --log-level=info Note If the cloud provider account configured on the host does not have sufficient permissions to deploy the cluster, the installation process stops, and the missing permissions are displayed. When the cluster deployment is completed, directions for accessing the cluster are displayed in the terminal, including a link to the web console and credentials for the kubeadmin user. The kubeconfig for the cluster will be located in okd-deployment/auth/kubeconfig . Example output ... INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/myuser/install_dir/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.mycluster.example.com INFO Login to the console with the user: \"kubeadmin\", and password: \"4vYBz-Ee6gm-ymBZj-Wt5AL\" INFO Time elapsed: 36m22s: Warning The Ignition config files contain certificates that expire after 24 hours, which are then renewed at that time. Do not turn off the cluster for this time, or you will have to update the certificates manually. See OpenShift Container Platform documentation for more information. Log Into the Cluster \u2693\ufe0e To log into the cluster, export the kubeconfig : export KUBECONFIG =< installation_directory >/ auth / kubeconfig Optionally, use the Lens tool for further work with the Kubernetes cluster. Note To install and manage the cluster, refer to Lens documentation . Related Articles \u2693\ufe0e Deploy AWS EKS Cluster Manage Jenkins Agent Deploy OKD 4.10 Cluster","title":"Deploy OKD 4.9 Cluster"},{"location":"operator-guide/deploy-okd/#deploy-okd-49-cluster","text":"This instruction provides detailed information on the OKD 4.9 cluster deployment in the AWS Cloud and contains the additional setup necessary for the managed infrastructure. A full description of the cluster deployment can be found in the official documentation .","title":"Deploy OKD 4.9 Cluster"},{"location":"operator-guide/deploy-okd/#prerequisites","text":"Before the OKD cluster deployment and configuration, make sure to check the prerequisites.","title":"Prerequisites"},{"location":"operator-guide/deploy-okd/#required-tools","text":"Install the following tools listed below: AWS CLI OpenShift CLI Lens (optional) Create the AWS IAM user with the required permissions . Make sure the AWS account is active, and the user doesn't have a permission boundary. Remove any Service Control Policy (SCP) restrictions from the AWS account. Generate a key pair for cluster node SSH access. Please perform the steps below: Generate the SSH key. Specify the path and file name, such as ~/.ssh/id_ed25519, of the new SSH key. If there is an existing key pair, ensure that the public key is in the ~/.ssh directory. ssh-keygen -t ed25519 -N '' -f <path>/<file_name> Add the SSH private key identity to the SSH agent for a local user if it has not already been added. eval \"$(ssh-agent -s)\" Add the SSH private key to the ssh-agent: ssh-add <path>/<file_name>","title":"Required Tools"},{"location":"operator-guide/deploy-okd/#prepare-for-the-deployment-process","text":"Before deploying the OKD cluster, please perform the steps below: Download the OKD installer . Extract the installation program: tar -xvf openshift-install-linux.tar.gz Download the installation pull secret for any private registry. This pull secret allows to authenticate with the services that are provided by the included authorities, including Quay.io serving container images for OKD components. For example, here is a pull secret for Docker Hub: The pull secret for the private registry { \"auths\":{ \"https://index.docker.io/v1/\":{ \"auth\":\"$TOKEN\" } } } Create the deployment directory and the install-config.yaml file: mkdir okd-deployment touch okd-deployment/install-config.yaml To specify more details about the OKD cluster platform or to modify the values of the required parameters, customize the install-config.yaml file for AWS. Please see an example of the customized file below: install-config.yaml - OKD cluster\u2019s platform installation configuration file apiVersion: v1 baseDomain: <YOUR_DOMAIN> compute: - architecture: amd64 hyperthreading: Enabled name: worker platform: aws: zones: - eu-central-1a rootVolume: size: 50 type: r5.large replicas: 3 controlPlane: architecture: amd64 hyperthreading: Enabled name: master platform: aws: rootVolume: size: 50 zones: - eu-central-1a type: m5.xlarge replicas: 3 metadata: creationTimestamp: null name: 4-9-okd-sandbox platform: aws: region: eu-central-1 userTags: user:tag: 4-9-okd-sandbox publish: External pullSecret: <PULL_SECRET> sshKey: | <SSH_KEY> where: YOUR_DOMAIN - is a base domain, PULL_SECRET - is a created pull secret for a private registry, SSH_KEY - is a created SSH key.","title":"Prepare for the Deployment Process"},{"location":"operator-guide/deploy-okd/#deploy-the-cluster","text":"To initialize the cluster deployment, run the following command: ./openshift-install create cluster --dir <installation_directory> --log-level=info Note If the cloud provider account configured on the host does not have sufficient permissions to deploy the cluster, the installation process stops, and the missing permissions are displayed. When the cluster deployment is completed, directions for accessing the cluster are displayed in the terminal, including a link to the web console and credentials for the kubeadmin user. The kubeconfig for the cluster will be located in okd-deployment/auth/kubeconfig . Example output ... INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/myuser/install_dir/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.mycluster.example.com INFO Login to the console with the user: \"kubeadmin\", and password: \"4vYBz-Ee6gm-ymBZj-Wt5AL\" INFO Time elapsed: 36m22s: Warning The Ignition config files contain certificates that expire after 24 hours, which are then renewed at that time. Do not turn off the cluster for this time, or you will have to update the certificates manually. See OpenShift Container Platform documentation for more information.","title":"Deploy the Cluster"},{"location":"operator-guide/deploy-okd/#log-into-the-cluster","text":"To log into the cluster, export the kubeconfig : export KUBECONFIG =< installation_directory >/ auth / kubeconfig Optionally, use the Lens tool for further work with the Kubernetes cluster. Note To install and manage the cluster, refer to Lens documentation .","title":"Log Into the Cluster"},{"location":"operator-guide/deploy-okd/#related-articles","text":"Deploy AWS EKS Cluster Manage Jenkins Agent Deploy OKD 4.10 Cluster","title":"Related Articles"},{"location":"operator-guide/edp-kiosk-usage/","text":"EDP Kiosk Usage \u2693\ufe0e Explore the way Kiosk, a multi-tenancy extension for Kubernetes , is used in EDP. Prerequisites \u2693\ufe0e Installed Kiosk 0.2.11. Diagram of using Kiosk by EDP \u2693\ufe0e Kiosk usage Agenda blue - created by Helm chart; grey - created manually Usage \u2693\ufe0e EDP installation area on a diagram is described by following link ; Once the above step is executed, edp-cd-pipeline-operator service account will be linked to kiosk-edit ClusterRole to get an ability for leveraging Kiosk specific resources (e.g. Space); Newly created stage in \u2039edp-project\u203a installation of EDP generates new Kiosk Space resource that is linked to Kiosk Account; According to Kiosk doc the Space resource creates namespace with RoleBinding that contains relation between service account which is linked to Kiosk Account and kiosk-space-admin ClusterRole. As cd-pipeline-operator ServiceAccount is linked to Account, it has admin permissions in all generated by him namespaces. Related Articles \u2693\ufe0e Install EDP Set Up Kiosk","title":"EDP Kiosk Usage"},{"location":"operator-guide/edp-kiosk-usage/#edp-kiosk-usage","text":"Explore the way Kiosk, a multi-tenancy extension for Kubernetes , is used in EDP.","title":"EDP Kiosk Usage"},{"location":"operator-guide/edp-kiosk-usage/#prerequisites","text":"Installed Kiosk 0.2.11.","title":"Prerequisites"},{"location":"operator-guide/edp-kiosk-usage/#diagram-of-using-kiosk-by-edp","text":"Kiosk usage Agenda blue - created by Helm chart; grey - created manually","title":"Diagram of using Kiosk by EDP"},{"location":"operator-guide/edp-kiosk-usage/#usage","text":"EDP installation area on a diagram is described by following link ; Once the above step is executed, edp-cd-pipeline-operator service account will be linked to kiosk-edit ClusterRole to get an ability for leveraging Kiosk specific resources (e.g. Space); Newly created stage in \u2039edp-project\u203a installation of EDP generates new Kiosk Space resource that is linked to Kiosk Account; According to Kiosk doc the Space resource creates namespace with RoleBinding that contains relation between service account which is linked to Kiosk Account and kiosk-space-admin ClusterRole. As cd-pipeline-operator ServiceAccount is linked to Account, it has admin permissions in all generated by him namespaces.","title":"Usage"},{"location":"operator-guide/edp-kiosk-usage/#related-articles","text":"Install EDP Set Up Kiosk","title":"Related Articles"},{"location":"operator-guide/enable-irsa/","text":"Associate IAM Roles With Service Accounts \u2693\ufe0e This page contains accurate information on how to associate an IAM role with the service account (IRSA) in EPAM Delivery Platform. Get acquainted with the AWS Official Documentation on the subject before proceeding. Common Configuration of IAM Roles With Service Accounts \u2693\ufe0e To successfully associate the IAM role with the service account, follow the steps below: Create an IAM role that will further be associated with the service account. This role must have the following trust policy: IAM Role { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::<AWS_ACCOUNT_ID>:oidc-provider/<OIDC_PROVIDER>\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"<OIDC_PROVIDER>:sub\": \"system:serviceaccount:<SERVICE_ACCOUNT_NAMESPACE>:<SERVICE_ACCOUNT_NAME>\" } } } ] } View cluster's \u2039OIDC_PROVIDER\u203a URL. aws eks describe-cluster --name <CLUSTER_NAME> --query \"cluster.identity.oidc.issuer\" --output text Example output: https://oidc.eks.us-west-2.amazonaws.com/id/EXAMPLED539D4633E53DE1B716D3041E \u2039OIDC_PROVIDER\u203a in this example will be: oidc.eks.us-west-2.amazonaws.com/id/EXAMPLED539D4633E53DE1B716D3041E Deploy the amazon-eks-pod-identity-webhook v0.2.0. Note The amazon-eks-pod-identity-webhook functionality is provided out of the box in EKS v1.21 and higher. This does not apply if the cluster has been upgraded from older versions. Therefore, skip step 2 and continue from step 3 in this documentation. 2.1. Provide the stable (ed8c41f) version of the Docker image in the deploy/deployment-base.yaml file. 2.2. Provide ${CA_BUNDLE}_in the_deploy/mutatingwebhook.yaml file: secret_name=$(kubectl -n default get sa default -o jsonpath='{.secrets[0].name}') \\ CA_BUNDLE=$(kubectl -n default get secret/$secret_name -o jsonpath='{.data.ca\\.crt}' | tr -d '\\n') 2.3. Deploy the Webhook: kubectl apply -f deploy/ 2.4. Approve the csr : csr_name=$(kubectl get csr -o jsonpath='{.items[?(@.spec.username==\"system:serviceaccount:default:pod-identity-webhook\")].metadata.name}') kubectl certificate approve $csr_name Annotate the created service account with the IAM role: Service Account apiVersion: v1 kind: ServiceAccount metadata: name: <SERVICE_ACCOUNT_NAME> namespace: <NAMESPACE> annotations: eks.amazonaws.com/role-arn: \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/<IAM_ROLE_NAME>\" All newly launched pods with this service account will be modified and then use the associated IAM role. Find below the pod specification template: Pod Template apiVersion: v1 kind: Pod metadata: name: irsa-test namespace: <POD_NAMESPACE> spec: serviceAccountName: <SERVICE_ACCOUNT_NAME> securityContext: fsGroup: 65534 containers: - name: terraform image: epamedp/edp-jenkins-terraform-agent:2.0.4 command: ['sh', '-c', 'aws sts \"get-caller-identity\" && sleep 3600'] Check the logs of the created pod from the template above. Example output: { \"UserId\": \"XXXXXXXXXXXXXXXXXXXXX:botocore-session-XXXXXXXXXX\", \"Account\": \"XXXXXXXXXXXX\", \"Arn\": \"arn:aws:sts::XXXXXXXXXXXX:assumed-role/AWSIRSATestRole/botocore-session-XXXXXXXXXX\" } As a result, it is possible to perform actions in AWS under the AWSIRSATestRole role. Related Articles \u2693\ufe0e Use Terraform Library in EDP","title":"Associate IAM Roles With Service Accounts"},{"location":"operator-guide/enable-irsa/#associate-iam-roles-with-service-accounts","text":"This page contains accurate information on how to associate an IAM role with the service account (IRSA) in EPAM Delivery Platform. Get acquainted with the AWS Official Documentation on the subject before proceeding.","title":"Associate IAM Roles With Service Accounts"},{"location":"operator-guide/enable-irsa/#common-configuration-of-iam-roles-with-service-accounts","text":"To successfully associate the IAM role with the service account, follow the steps below: Create an IAM role that will further be associated with the service account. This role must have the following trust policy: IAM Role { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::<AWS_ACCOUNT_ID>:oidc-provider/<OIDC_PROVIDER>\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"<OIDC_PROVIDER>:sub\": \"system:serviceaccount:<SERVICE_ACCOUNT_NAMESPACE>:<SERVICE_ACCOUNT_NAME>\" } } } ] } View cluster's \u2039OIDC_PROVIDER\u203a URL. aws eks describe-cluster --name <CLUSTER_NAME> --query \"cluster.identity.oidc.issuer\" --output text Example output: https://oidc.eks.us-west-2.amazonaws.com/id/EXAMPLED539D4633E53DE1B716D3041E \u2039OIDC_PROVIDER\u203a in this example will be: oidc.eks.us-west-2.amazonaws.com/id/EXAMPLED539D4633E53DE1B716D3041E Deploy the amazon-eks-pod-identity-webhook v0.2.0. Note The amazon-eks-pod-identity-webhook functionality is provided out of the box in EKS v1.21 and higher. This does not apply if the cluster has been upgraded from older versions. Therefore, skip step 2 and continue from step 3 in this documentation. 2.1. Provide the stable (ed8c41f) version of the Docker image in the deploy/deployment-base.yaml file. 2.2. Provide ${CA_BUNDLE}_in the_deploy/mutatingwebhook.yaml file: secret_name=$(kubectl -n default get sa default -o jsonpath='{.secrets[0].name}') \\ CA_BUNDLE=$(kubectl -n default get secret/$secret_name -o jsonpath='{.data.ca\\.crt}' | tr -d '\\n') 2.3. Deploy the Webhook: kubectl apply -f deploy/ 2.4. Approve the csr : csr_name=$(kubectl get csr -o jsonpath='{.items[?(@.spec.username==\"system:serviceaccount:default:pod-identity-webhook\")].metadata.name}') kubectl certificate approve $csr_name Annotate the created service account with the IAM role: Service Account apiVersion: v1 kind: ServiceAccount metadata: name: <SERVICE_ACCOUNT_NAME> namespace: <NAMESPACE> annotations: eks.amazonaws.com/role-arn: \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/<IAM_ROLE_NAME>\" All newly launched pods with this service account will be modified and then use the associated IAM role. Find below the pod specification template: Pod Template apiVersion: v1 kind: Pod metadata: name: irsa-test namespace: <POD_NAMESPACE> spec: serviceAccountName: <SERVICE_ACCOUNT_NAME> securityContext: fsGroup: 65534 containers: - name: terraform image: epamedp/edp-jenkins-terraform-agent:2.0.4 command: ['sh', '-c', 'aws sts \"get-caller-identity\" && sleep 3600'] Check the logs of the created pod from the template above. Example output: { \"UserId\": \"XXXXXXXXXXXXXXXXXXXXX:botocore-session-XXXXXXXXXX\", \"Account\": \"XXXXXXXXXXXX\", \"Arn\": \"arn:aws:sts::XXXXXXXXXXXX:assumed-role/AWSIRSATestRole/botocore-session-XXXXXXXXXX\" } As a result, it is possible to perform actions in AWS under the AWSIRSATestRole role.","title":"Common Configuration of IAM Roles With Service Accounts"},{"location":"operator-guide/enable-irsa/#related-articles","text":"Use Terraform Library in EDP","title":"Related Articles"},{"location":"operator-guide/external-secrets-operator-integration/","text":"External Secrets Operator Integration \u2693\ufe0e External Secrets Operator (ESO) can be integrated with EDP. There are multiple Secrets Providers that can be used within ESO. EDP is integrated with two major providers: Kubernetes Secrets AWS Systems Manager Parameter Store Kubernetes \u2693\ufe0e All secrets are stored in Kubernetes in pre-defined namespaces. EDP suggests using the following approach for secrets management: EDP_NAMESPACE-vault , where EDP_NAMESPACE is a name of the namespace where EDP is deployed, such as edp-vault . This namespace is used by EDP platform. Access to secrets in the edp-vault is permitted only for EDP Administrators . EDP_NAMESPACE-cicd-vault , where EDP_NAMESPACE is a name of the namespace where EDP is deployed, such as edp-cicd-vault . Development team uses access to secrets in the edp-cicd-vault for microservices development. See a diagram below for more details: EDP Install Scenario \u2693\ufe0e In order to install EDP , a list of passwords must be created: super-admin-db, db-admin-console, and keycloak. Secrets are provided automatically when using ESO. Create a common namespace for secrets and EDP: kubectl create namespace edp-vault kubectl create namespace edp Create secrets in the edp-vault namespace: apiVersion : v1 kind : Secret metadata : name : super-admin-db namespace : edp-vault data : password : cGFzcw== # pass username : dXNlcg== # user type : Opaque In the edp-vault namespace, create a Role with a permission to read secrets: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : edp-vault name : external-secret-store rules : - apiGroups : [ \"\" ] resources : - secrets verbs : - get - list - watch - apiGroups : - authorization.k8s.io resources : - selfsubjectrulesreviews verbs : - create In the edp-vault namespace, create a ServiceAccount used by SecretStore : apiVersion : v1 kind : ServiceAccount metadata : name : secret-manager namespace : edp Connect the Role from the edp-vault namespace with the ServiceAccount in the edp namespace: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : eso-from-edp namespace : edp-vault subjects : - kind : ServiceAccount name : secret-manager namespace : edp roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : external-secret-store Create a SecretStore in the edp namespace, and use ServiceAccount for authentication: apiVersion : external-secrets.io/v1beta1 kind : SecretStore metadata : name : edp-vault namespace : edp spec : provider : kubernetes : remoteNamespace : edp-vault # namespace with secrets auth : serviceAccount : name : secret-manager server : caProvider : type : ConfigMap name : kube-root-ca.crt key : ca.crt Each secret must be defined by the ExternalSecret object. A code example below creates the super-admin-db secret in the edp namespace based on a secret with the same name in the edp-vault namespace: apiVersion : external-secrets.io/v1beta1 kind : ExternalSecret metadata : name : super-admin-db namespace : edp spec : refreshInterval : 1h secretStoreRef : kind : SecretStore name : edp-vault # target: # name: secret-to-be-created # name of the k8s Secret to be created. metadata.name used if not defined data : - secretKey : username # key to be created remoteRef : key : super-admin-db # remote secret name property : username # value will be fetched from this field - secretKey : password # key to be created remoteRef : key : super-admin-db # remote secret name property : password # value will be fetched from this field Apply the same approach for enabling secrets management in the namespaces used for microservices development, such as sit and qa on the diagram above. AWS Systems Manager Parameter Store \u2693\ufe0e AWS SSM Parameter Store can be used as a Secret Provider for ESO . For EDP, it is recommended to use the IAM Roles For Service Accounts approach (see a diagram below). EDP Install Scenario \u2693\ufe0e In order to install EDP , a list of passwords must be created: super-admin-db, db-admin-console, and keycloak. Follow the steps below, to get secrets from the SSM: In the AWS, create an AWS IAM policy and an IAM role used by ServiceAccount in SecretStore . The IAM role must have permissions to get values from the SSM Parameter Store. a. Create an IAM policy that allows to get values from the Parameter Store with the edp/ path. Use your AWS Region and AWS Account Id : { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : \"ssm:GetParameter*\" , \"Resource\" : \"arn:aws:ssm:eu-central-1:012345678910:parameter/edp/*\" } ] } b. Create an AWS IAM role with trust relationships (defined below) and attach the IAM policy. Put your string for Federated value ( see more on IRSA enablement for EKS Cluster) and AWS region. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::012345678910:oidc-provider/oidc.eks.eu-central-1.amazonaws.com/id/XXXXXXXXXXXXXXXXXX\" }, \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringLike\" : { \"oidc.eks.eu-central-1.amazonaws.com/id/XXXXXXXXXXXXXXXXXX:sub\" : \"system:serviceaccount:edp:*\" } } } ] } In the AWS Parameter Store, create a secret with the /edp/my-json-secret name: { \"super-admin-db\" : { \"username\" : \"super-user\" , \"password\" : \"pass\" }, \"db-admin-console\" : { \"username\" : \"edp-user\" , \"password\" : \"pass\" }, \"keycloak\" : { \"username\" : \"realm-user\" , \"password\" : \"pass\" }, } In the edp Kubernetes namespace, create the following objects: --- apiVersion : v1 kind : ServiceAccount metadata : annotations : eks.amazonaws.com/role-arn : arn:aws:iam::012345678910:role/ROLE_NAME name : secret-manager namespace : edp --- apiVersion : external-secrets.io/v1beta1 kind : SecretStore metadata : name : aws-parameter-store namespace : edp spec : provider : aws : service : ParameterStore region : AWS_REGION auth : jwt : serviceAccountRef : name : secret-manager --- apiVersion : external-secrets.io/v1beta1 kind : ExternalSecret metadata : name : super-admin-db namespace : edp spec : refreshInterval : 1h secretStoreRef : kind : SecretStore name : aws-parameter-store data : - secretKey : username remoteRef : key : /edp/my-json-secret property : super-admin-db.username - secretKey : password remoteRef : key : /edp/my-json-secret property : super-admin-db.password where: ROLE_NAME - is a value defined on step 1.b , AWS_REGION - is the AWS region, used on step 1 . As a result, a secret with the super-admin-db name is created in the edp namespace with the content defined in JSON format on step 2 . Related Articles \u2693\ufe0e Install External Secrets Operator","title":"External Secrets Operator Integration"},{"location":"operator-guide/external-secrets-operator-integration/#external-secrets-operator-integration","text":"External Secrets Operator (ESO) can be integrated with EDP. There are multiple Secrets Providers that can be used within ESO. EDP is integrated with two major providers: Kubernetes Secrets AWS Systems Manager Parameter Store","title":"External Secrets Operator Integration"},{"location":"operator-guide/external-secrets-operator-integration/#kubernetes","text":"All secrets are stored in Kubernetes in pre-defined namespaces. EDP suggests using the following approach for secrets management: EDP_NAMESPACE-vault , where EDP_NAMESPACE is a name of the namespace where EDP is deployed, such as edp-vault . This namespace is used by EDP platform. Access to secrets in the edp-vault is permitted only for EDP Administrators . EDP_NAMESPACE-cicd-vault , where EDP_NAMESPACE is a name of the namespace where EDP is deployed, such as edp-cicd-vault . Development team uses access to secrets in the edp-cicd-vault for microservices development. See a diagram below for more details:","title":"Kubernetes"},{"location":"operator-guide/external-secrets-operator-integration/#edp-install-scenario","text":"In order to install EDP , a list of passwords must be created: super-admin-db, db-admin-console, and keycloak. Secrets are provided automatically when using ESO. Create a common namespace for secrets and EDP: kubectl create namespace edp-vault kubectl create namespace edp Create secrets in the edp-vault namespace: apiVersion : v1 kind : Secret metadata : name : super-admin-db namespace : edp-vault data : password : cGFzcw== # pass username : dXNlcg== # user type : Opaque In the edp-vault namespace, create a Role with a permission to read secrets: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : edp-vault name : external-secret-store rules : - apiGroups : [ \"\" ] resources : - secrets verbs : - get - list - watch - apiGroups : - authorization.k8s.io resources : - selfsubjectrulesreviews verbs : - create In the edp-vault namespace, create a ServiceAccount used by SecretStore : apiVersion : v1 kind : ServiceAccount metadata : name : secret-manager namespace : edp Connect the Role from the edp-vault namespace with the ServiceAccount in the edp namespace: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : eso-from-edp namespace : edp-vault subjects : - kind : ServiceAccount name : secret-manager namespace : edp roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : external-secret-store Create a SecretStore in the edp namespace, and use ServiceAccount for authentication: apiVersion : external-secrets.io/v1beta1 kind : SecretStore metadata : name : edp-vault namespace : edp spec : provider : kubernetes : remoteNamespace : edp-vault # namespace with secrets auth : serviceAccount : name : secret-manager server : caProvider : type : ConfigMap name : kube-root-ca.crt key : ca.crt Each secret must be defined by the ExternalSecret object. A code example below creates the super-admin-db secret in the edp namespace based on a secret with the same name in the edp-vault namespace: apiVersion : external-secrets.io/v1beta1 kind : ExternalSecret metadata : name : super-admin-db namespace : edp spec : refreshInterval : 1h secretStoreRef : kind : SecretStore name : edp-vault # target: # name: secret-to-be-created # name of the k8s Secret to be created. metadata.name used if not defined data : - secretKey : username # key to be created remoteRef : key : super-admin-db # remote secret name property : username # value will be fetched from this field - secretKey : password # key to be created remoteRef : key : super-admin-db # remote secret name property : password # value will be fetched from this field Apply the same approach for enabling secrets management in the namespaces used for microservices development, such as sit and qa on the diagram above.","title":"EDP Install Scenario"},{"location":"operator-guide/external-secrets-operator-integration/#aws-systems-manager-parameter-store","text":"AWS SSM Parameter Store can be used as a Secret Provider for ESO . For EDP, it is recommended to use the IAM Roles For Service Accounts approach (see a diagram below).","title":"AWS Systems Manager Parameter Store"},{"location":"operator-guide/external-secrets-operator-integration/#edp-install-scenario_1","text":"In order to install EDP , a list of passwords must be created: super-admin-db, db-admin-console, and keycloak. Follow the steps below, to get secrets from the SSM: In the AWS, create an AWS IAM policy and an IAM role used by ServiceAccount in SecretStore . The IAM role must have permissions to get values from the SSM Parameter Store. a. Create an IAM policy that allows to get values from the Parameter Store with the edp/ path. Use your AWS Region and AWS Account Id : { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : \"ssm:GetParameter*\" , \"Resource\" : \"arn:aws:ssm:eu-central-1:012345678910:parameter/edp/*\" } ] } b. Create an AWS IAM role with trust relationships (defined below) and attach the IAM policy. Put your string for Federated value ( see more on IRSA enablement for EKS Cluster) and AWS region. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::012345678910:oidc-provider/oidc.eks.eu-central-1.amazonaws.com/id/XXXXXXXXXXXXXXXXXX\" }, \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringLike\" : { \"oidc.eks.eu-central-1.amazonaws.com/id/XXXXXXXXXXXXXXXXXX:sub\" : \"system:serviceaccount:edp:*\" } } } ] } In the AWS Parameter Store, create a secret with the /edp/my-json-secret name: { \"super-admin-db\" : { \"username\" : \"super-user\" , \"password\" : \"pass\" }, \"db-admin-console\" : { \"username\" : \"edp-user\" , \"password\" : \"pass\" }, \"keycloak\" : { \"username\" : \"realm-user\" , \"password\" : \"pass\" }, } In the edp Kubernetes namespace, create the following objects: --- apiVersion : v1 kind : ServiceAccount metadata : annotations : eks.amazonaws.com/role-arn : arn:aws:iam::012345678910:role/ROLE_NAME name : secret-manager namespace : edp --- apiVersion : external-secrets.io/v1beta1 kind : SecretStore metadata : name : aws-parameter-store namespace : edp spec : provider : aws : service : ParameterStore region : AWS_REGION auth : jwt : serviceAccountRef : name : secret-manager --- apiVersion : external-secrets.io/v1beta1 kind : ExternalSecret metadata : name : super-admin-db namespace : edp spec : refreshInterval : 1h secretStoreRef : kind : SecretStore name : aws-parameter-store data : - secretKey : username remoteRef : key : /edp/my-json-secret property : super-admin-db.username - secretKey : password remoteRef : key : /edp/my-json-secret property : super-admin-db.password where: ROLE_NAME - is a value defined on step 1.b , AWS_REGION - is the AWS region, used on step 1 . As a result, a secret with the super-admin-db name is created in the edp namespace with the content defined in JSON format on step 2 .","title":"EDP Install Scenario"},{"location":"operator-guide/external-secrets-operator-integration/#related-articles","text":"Install External Secrets Operator","title":"Related Articles"},{"location":"operator-guide/github-debug-webhooks/","text":"Debug GitHub Webhooks in Jenkins \u2693\ufe0e A webhook enables third-party services like GitHub to send real-time updates to an application. Updates are triggered by an event or an action by the webhook provider (for example, a push to a repository, a Pull Request creation), and pushed to the application via HTTP requests (namely, Jenkins). The GitHub Jenkins job provisioner creates a webhook in the GitHub repository during the Create release pipeline once the VCS Import Strategy is enabled and the GitHub Integration is completed. The Jenkins setup in EDP uses two following plugins responsible for listening on GitHub webhooks: GitHub plugin is configured to listen on Push events. GitHub Pull Request Builder is configured to listen on Pull Request events. In case of any sort of issues with webhooks, make sure that: Firewalls are configured to accept incoming traffic from the IP address range that is described here . The job ran at least once before the hook will work (once an application is created in EDP, the build job should be run automatically in Jenkins). Both webhooks ( Push and issue comment and Pull Request ) are created on the GitHub side (GitHub does not need separate webhooks for each branch unlike GitLab): Go to the GitHub repository - > Settings -> Webhooks. Webhooks settings Click each webhook and check if the event delivery is successful. The URL payload should be as follows: https://jenkins-the-host.com/github-webhook/ is for the GitHub plugin and https://jenkins-the-host.com/ghprbhook/ is for the GitHub Pull Request Builder. The content type should be application/json for Push events and application/x-www-form-urlencoded for Pull Request events. The html_url in the Payload request should match the repository URL. The event can also be redelivered by clicking the Redeliver button. Manage webhook Note It may be convenient to debug webhooks with tools like Postman. Make sure to add all the headers to Postman from the webhook Request -> Headers field and send the payload (Request body) using the appropriate content type. Examples for Push and Pull Request events : GitHub plugin push events The response in the Jenkins log: Jan 17, 2022 8:51:14 AM INFO org.jenkinsci.plugins.github.webhook.subscriber.PingGHEventSubscriber onEvent PING webhook received from repo <https://github.com/user-profile/user-repo>! Github pull request builder The response in the Jenkins log: Jan 17, 2022 8:17:53 AM FINE org.jenkinsci.plugins.ghprb.GhprbRootAction Got payload event: ping The repo pushing to Jenkins, the GitHub project URL in the project configuration, and the repo(s) in the pipeline Job have to line up. GitHub hook trigger for GITScm polling is enabled for the Build job. GitHub hook trigger GitHub Pull Request Builder is enabled for the Code Review job. GitHub pull request builder It is convenient to filter through Jenkins log by using Jenkins custom Log Recorder . Go to Manage Jenkins -> System Log -> Add new log recorder The Push events for the GitHub: Logger Log Level org.jenkinsci.plugins.github.webhook.subscriber.DefaultPushGHEventSubscriber ALL com.cloudbees.jenkins.GitHubPushTrigger ALL com.cloudbees.jenkins.GitHubWebHook ALL org.jenkinsci.plugins.github.webhook.WebhookManager ALL org.jenkinsci.plugins.github.webhook.subscriber.PingGHEventSubscriber ALL The Pull Request events for the GitHub Pull Request Builder: Logger Log Level org.jenkinsci.plugins.ghprb.GhprbRootAction ALL org.jenkinsci.plugins.ghprb.GhprbTrigger ALL org.jenkinsci.plugins.ghprb.GhprbPullRequest ALL org.jenkinsci.plugins.ghprb.GhprbRepository ALL Note Here is a small example on how to use Pipeline script with webhooks for the GitHub plugin (it is already implemented in the EDP pipelines): properties([pipelineTriggers([githubPush()])]) node { git credentialsId: 'github-sshkey', url: 'https://github.com/someone/something.git', branch: 'master' } Push events may not work correctly with the Job Pipeline script from SCM option in the current version of the GitHub plugin 1.34.1 Related Articles \u2693\ufe0e Enable VCS Import Strategy GitHub Integration Manage Jenkins CI Pipeline Job Provision GitHub plugin GitHub Pull Request Builder","title":"Debug GitHub Webhooks in Jenkins"},{"location":"operator-guide/github-debug-webhooks/#debug-github-webhooks-in-jenkins","text":"A webhook enables third-party services like GitHub to send real-time updates to an application. Updates are triggered by an event or an action by the webhook provider (for example, a push to a repository, a Pull Request creation), and pushed to the application via HTTP requests (namely, Jenkins). The GitHub Jenkins job provisioner creates a webhook in the GitHub repository during the Create release pipeline once the VCS Import Strategy is enabled and the GitHub Integration is completed. The Jenkins setup in EDP uses two following plugins responsible for listening on GitHub webhooks: GitHub plugin is configured to listen on Push events. GitHub Pull Request Builder is configured to listen on Pull Request events. In case of any sort of issues with webhooks, make sure that: Firewalls are configured to accept incoming traffic from the IP address range that is described here . The job ran at least once before the hook will work (once an application is created in EDP, the build job should be run automatically in Jenkins). Both webhooks ( Push and issue comment and Pull Request ) are created on the GitHub side (GitHub does not need separate webhooks for each branch unlike GitLab): Go to the GitHub repository - > Settings -> Webhooks. Webhooks settings Click each webhook and check if the event delivery is successful. The URL payload should be as follows: https://jenkins-the-host.com/github-webhook/ is for the GitHub plugin and https://jenkins-the-host.com/ghprbhook/ is for the GitHub Pull Request Builder. The content type should be application/json for Push events and application/x-www-form-urlencoded for Pull Request events. The html_url in the Payload request should match the repository URL. The event can also be redelivered by clicking the Redeliver button. Manage webhook Note It may be convenient to debug webhooks with tools like Postman. Make sure to add all the headers to Postman from the webhook Request -> Headers field and send the payload (Request body) using the appropriate content type. Examples for Push and Pull Request events : GitHub plugin push events The response in the Jenkins log: Jan 17, 2022 8:51:14 AM INFO org.jenkinsci.plugins.github.webhook.subscriber.PingGHEventSubscriber onEvent PING webhook received from repo <https://github.com/user-profile/user-repo>! Github pull request builder The response in the Jenkins log: Jan 17, 2022 8:17:53 AM FINE org.jenkinsci.plugins.ghprb.GhprbRootAction Got payload event: ping The repo pushing to Jenkins, the GitHub project URL in the project configuration, and the repo(s) in the pipeline Job have to line up. GitHub hook trigger for GITScm polling is enabled for the Build job. GitHub hook trigger GitHub Pull Request Builder is enabled for the Code Review job. GitHub pull request builder It is convenient to filter through Jenkins log by using Jenkins custom Log Recorder . Go to Manage Jenkins -> System Log -> Add new log recorder The Push events for the GitHub: Logger Log Level org.jenkinsci.plugins.github.webhook.subscriber.DefaultPushGHEventSubscriber ALL com.cloudbees.jenkins.GitHubPushTrigger ALL com.cloudbees.jenkins.GitHubWebHook ALL org.jenkinsci.plugins.github.webhook.WebhookManager ALL org.jenkinsci.plugins.github.webhook.subscriber.PingGHEventSubscriber ALL The Pull Request events for the GitHub Pull Request Builder: Logger Log Level org.jenkinsci.plugins.ghprb.GhprbRootAction ALL org.jenkinsci.plugins.ghprb.GhprbTrigger ALL org.jenkinsci.plugins.ghprb.GhprbPullRequest ALL org.jenkinsci.plugins.ghprb.GhprbRepository ALL Note Here is a small example on how to use Pipeline script with webhooks for the GitHub plugin (it is already implemented in the EDP pipelines): properties([pipelineTriggers([githubPush()])]) node { git credentialsId: 'github-sshkey', url: 'https://github.com/someone/something.git', branch: 'master' } Push events may not work correctly with the Job Pipeline script from SCM option in the current version of the GitHub plugin 1.34.1","title":"Debug GitHub Webhooks in Jenkins"},{"location":"operator-guide/github-debug-webhooks/#related-articles","text":"Enable VCS Import Strategy GitHub Integration Manage Jenkins CI Pipeline Job Provision GitHub plugin GitHub Pull Request Builder","title":"Related Articles"},{"location":"operator-guide/github-integration/","text":"GitHub Integration \u2693\ufe0e Discover the steps below to apply the GitHub integration correctly. Note Before applying the GitHub integration, make sure to enable VCS Import strategy. For details, please refer to the Enable VCS Import Strategy section. Create a new Job Provision by following the instruction . The job provisioner will create a job suite for an application added to EDP. It will also create webhooks for the project in GitHub. Note The steps below are required in order to automatically create and integrate Jenkins with GitHub webhooks. Create access token for GitHub: Click the profile account and navigate to Settings; Go to Developer Settings; Select Personal access token and generate a new one with the following parameters Repo permission Note The access is required for the GitHub Pull Request Builder plugin to get Pull Request commits, their status, and author info. Admin permission User permission Warning Make sure to copy a new personal access token right at this moment because there will not be any ability to see it again. Navigate to Jenkins -> Credentials -> System -> Global credentials -> Add credentials , and create new credentials with the Secret text kind. In the Secret field, provide the GitHub API token, fill in the ID field with the github-access-token value: Jenkins github credentials Navigate to Jenkins -> Manage Jenkins -> Configure system -> GitHub , and configure the GitHub server: Github plugin config Note Keep the Manage hooks check box clear since the Job Provisioner automatically creates webhooks in the repository regardless of the check box selection. Configure the GitHub Pull Request Builder plugin (this plugin is responsible for listening on Pull Request webhook events and triggering Code Review jobs): Note The Secret field is optional, for details, please refer to the official GitHub pull request builder plugin documentation . Github pull plugin config Related Articles \u2693\ufe0e Enable VCS Import Strategy Adjust Jira Integration Manage Jenkins CI Pipeline Job Provision","title":"GitHub Integration"},{"location":"operator-guide/github-integration/#github-integration","text":"Discover the steps below to apply the GitHub integration correctly. Note Before applying the GitHub integration, make sure to enable VCS Import strategy. For details, please refer to the Enable VCS Import Strategy section. Create a new Job Provision by following the instruction . The job provisioner will create a job suite for an application added to EDP. It will also create webhooks for the project in GitHub. Note The steps below are required in order to automatically create and integrate Jenkins with GitHub webhooks. Create access token for GitHub: Click the profile account and navigate to Settings; Go to Developer Settings; Select Personal access token and generate a new one with the following parameters Repo permission Note The access is required for the GitHub Pull Request Builder plugin to get Pull Request commits, their status, and author info. Admin permission User permission Warning Make sure to copy a new personal access token right at this moment because there will not be any ability to see it again. Navigate to Jenkins -> Credentials -> System -> Global credentials -> Add credentials , and create new credentials with the Secret text kind. In the Secret field, provide the GitHub API token, fill in the ID field with the github-access-token value: Jenkins github credentials Navigate to Jenkins -> Manage Jenkins -> Configure system -> GitHub , and configure the GitHub server: Github plugin config Note Keep the Manage hooks check box clear since the Job Provisioner automatically creates webhooks in the repository regardless of the check box selection. Configure the GitHub Pull Request Builder plugin (this plugin is responsible for listening on Pull Request webhook events and triggering Code Review jobs): Note The Secret field is optional, for details, please refer to the official GitHub pull request builder plugin documentation . Github pull plugin config","title":"GitHub Integration"},{"location":"operator-guide/github-integration/#related-articles","text":"Enable VCS Import Strategy Adjust Jira Integration Manage Jenkins CI Pipeline Job Provision","title":"Related Articles"},{"location":"operator-guide/gitlab-debug-webhooks/","text":"Debug GitLab Webhooks in Jenkins \u2693\ufe0e A webhook enables third-party services like GitLab to send real-time updates to the application. Updates are triggered by some event or action by the webhook provider (for example, a push to a repository, a Merge Request creation), and pushed to the application via HTTP requests (namely, Jenkins). The GitLab Jenkins job provisioner creates a webhook in the GitLab repository during the Create release pipeline once the VCS Import Strategy is enabled and the GitLab Integration is completed. The Jenkins setup in EDP uses the GitLab plugin responsible for listening on GitLab webhook Push and Merge Request events. In case of any sort of issues with webhooks, make sure that: Firewalls are configured to accept incoming traffic from the IP address range that is described here . The job ran at least once before the hook will work (once an application is created in EDP, the build job should be run automatically in Jenkins). Both webhooks ( Push Events, Note Events and Merge Requests Events, Note Events ) are created on the GitLab side for each branch (GitLab should have separate webhooks for each branch unlike GitHub). Go to the GitLab repository - > Settings -> Webhooks Webhooks list Click Edit next to each webhook and check if the event delivery is successful. If the webhook is sent, the Recent Deliveries list becomes available. Click View details . Webhooks settings The URL payload should be similar to the job URL on Jenkins. For example: https://jenkins-server.com/project/the-project-name/MAIN-Build-the-job is for the Push events and https://jenkins-server.com/project/the-project-name/MAIN-Code-review-the-job is the Merge Request events. The content type should be application/json for both events. The web_url in the Request body should match the repository URL. The event can also be redelivered by clicking the Resend Request button. Note It may be convenient to debug webhooks with tools like Postman. Make sure to add all the headers to Postman from the webhook Request headers field and send the payload (Request body) using the appropriate content type. Examples for Push and Merge Request events: Push request build pipeline The response in the Jenkins log: Jan 17, 2022 11:26:34 AM INFO com.dabsquared.gitlabjenkins.webhook.GitLabWebHook getDynamic WebHook called with url: /project/the-project-name/MAIN-Build-the-job Jan 17, 2022 11:26:34 AM INFO com.dabsquared.gitlabjenkins.trigger.handler.AbstractWebHookTriggerHandler handle the-project-name/MAIN-Build-the-job triggered for push. Push request code review pipeline The response in the Jenkins log: Jan 17, 2022 11:14:58 AM INFO com.dabsquared.gitlabjenkins.webhook.GitLabWebHook getDynamic WebHook called with url: /project/the-project-name/MAIN-Code-review-the-job The repository pushing to Jenkins and the repository(ies) in the pipeline Job have to line up. GitLab Connection should be present in the job settings. The settings in Build Triggers for the Build job should be as follows: Build triggers build pipeline The settings in Build Triggers for the Code Review job should be as follows: Build triggers code review pipeline It will be convenient to filter through Jenkins log by using Jenkins custom Log Recorder . Go to Manage Jenkins -> System Log -> Add new log recorder. The Push and Merge Request events for the GitLab: Logger Log Level com.dabsquared.gitlabjenkins.webhook.GitLabWebHook ALL com.dabsquared.gitlabjenkins.trigger.handler.AbstractWebHookTriggerHandler ALL com.dabsquared.gitlabjenkins.trigger.handler.merge.MergeRequestHookTriggerHandlerImpl ALL com.dabsquared.gitlabjenkins.util.CommitStatusUpdater ALL Related Articles \u2693\ufe0e Enable VCS Import Strategy Jenkins integration with GitLab GitLab Integration Manage Jenkins CI Pipeline Job Provision GitLab plugin","title":"Debug GitLab Webhooks in Jenkins"},{"location":"operator-guide/gitlab-debug-webhooks/#debug-gitlab-webhooks-in-jenkins","text":"A webhook enables third-party services like GitLab to send real-time updates to the application. Updates are triggered by some event or action by the webhook provider (for example, a push to a repository, a Merge Request creation), and pushed to the application via HTTP requests (namely, Jenkins). The GitLab Jenkins job provisioner creates a webhook in the GitLab repository during the Create release pipeline once the VCS Import Strategy is enabled and the GitLab Integration is completed. The Jenkins setup in EDP uses the GitLab plugin responsible for listening on GitLab webhook Push and Merge Request events. In case of any sort of issues with webhooks, make sure that: Firewalls are configured to accept incoming traffic from the IP address range that is described here . The job ran at least once before the hook will work (once an application is created in EDP, the build job should be run automatically in Jenkins). Both webhooks ( Push Events, Note Events and Merge Requests Events, Note Events ) are created on the GitLab side for each branch (GitLab should have separate webhooks for each branch unlike GitHub). Go to the GitLab repository - > Settings -> Webhooks Webhooks list Click Edit next to each webhook and check if the event delivery is successful. If the webhook is sent, the Recent Deliveries list becomes available. Click View details . Webhooks settings The URL payload should be similar to the job URL on Jenkins. For example: https://jenkins-server.com/project/the-project-name/MAIN-Build-the-job is for the Push events and https://jenkins-server.com/project/the-project-name/MAIN-Code-review-the-job is the Merge Request events. The content type should be application/json for both events. The web_url in the Request body should match the repository URL. The event can also be redelivered by clicking the Resend Request button. Note It may be convenient to debug webhooks with tools like Postman. Make sure to add all the headers to Postman from the webhook Request headers field and send the payload (Request body) using the appropriate content type. Examples for Push and Merge Request events: Push request build pipeline The response in the Jenkins log: Jan 17, 2022 11:26:34 AM INFO com.dabsquared.gitlabjenkins.webhook.GitLabWebHook getDynamic WebHook called with url: /project/the-project-name/MAIN-Build-the-job Jan 17, 2022 11:26:34 AM INFO com.dabsquared.gitlabjenkins.trigger.handler.AbstractWebHookTriggerHandler handle the-project-name/MAIN-Build-the-job triggered for push. Push request code review pipeline The response in the Jenkins log: Jan 17, 2022 11:14:58 AM INFO com.dabsquared.gitlabjenkins.webhook.GitLabWebHook getDynamic WebHook called with url: /project/the-project-name/MAIN-Code-review-the-job The repository pushing to Jenkins and the repository(ies) in the pipeline Job have to line up. GitLab Connection should be present in the job settings. The settings in Build Triggers for the Build job should be as follows: Build triggers build pipeline The settings in Build Triggers for the Code Review job should be as follows: Build triggers code review pipeline It will be convenient to filter through Jenkins log by using Jenkins custom Log Recorder . Go to Manage Jenkins -> System Log -> Add new log recorder. The Push and Merge Request events for the GitLab: Logger Log Level com.dabsquared.gitlabjenkins.webhook.GitLabWebHook ALL com.dabsquared.gitlabjenkins.trigger.handler.AbstractWebHookTriggerHandler ALL com.dabsquared.gitlabjenkins.trigger.handler.merge.MergeRequestHookTriggerHandlerImpl ALL com.dabsquared.gitlabjenkins.util.CommitStatusUpdater ALL","title":"Debug GitLab Webhooks in Jenkins"},{"location":"operator-guide/gitlab-debug-webhooks/#related-articles","text":"Enable VCS Import Strategy Jenkins integration with GitLab GitLab Integration Manage Jenkins CI Pipeline Job Provision GitLab plugin","title":"Related Articles"},{"location":"operator-guide/gitlab-integration/","text":"GitLab Integration \u2693\ufe0e Discover the steps below to apply the GitLab integration correctly. Note Before applying the GitLab integration, make sure to enable VCS Import strategy. For details, please refer to the Enable VCS Import Strategy section. Create a new Job Provision by following the instruction .The job provisioner will create a job suite for an application added to EDP. It will also create webhooks for the project in GitLab. Note The steps below are required in order to automatically create and integrate Jenkins GitLab webhooks. Create access token in Gitlab : Log in to GitLab ; In the top-right corner, click the avatar and select Settings ; On the User Settings menu, select Access Tokens ; Choose a name and an optional expiry date for the token; In the Scopes block, select the api scope for the token; Personal access tokens Click the Create personal access token button. Note Make sure to save the access token as there will not be any ability to access it once again. Create Jenkins Credential ID by navigating to Jenkins -> Credentials -> System -> Global Credentials -> Add Credentials : Select Kind Secret text ; Select Global scope; Secret - the Access Token that was created earlier; ID - the gitlab-access-token ID; Description - the description of the current Credential ID; Jenkins credential Warning Using the GitLab integration, a webhook is automatically created. After the removal of the application, the webhook stops working but not deleted. If necessary, it must be deleted manually. Note The next step is necessary if it is needed to see the status of Jenkins Merge Requests builds in the GitLab CI/CD Pipelines section. Configure Gitlab plugin by navigating to Manage Jenkins -> Configure System and fill in the GitLab plugin settings: Connection name - gitlab; Gitlab host URL - a host URL to GitLab; Credentials - credentials with Access Token to GitLab ( gitlab-access-token ); Gitlab plugin configuration This is how the Merge Requests build statuses look like in the GitLab CI/CD Pipelines section: Gitlab pipelines statuses Related Articles \u2693\ufe0e Adjust Jira Integration Enable VCS Import Strategy Jenkins integration with GitLab Manage Jenkins CI Pipeline Job Provision","title":"GitLab Integration"},{"location":"operator-guide/gitlab-integration/#gitlab-integration","text":"Discover the steps below to apply the GitLab integration correctly. Note Before applying the GitLab integration, make sure to enable VCS Import strategy. For details, please refer to the Enable VCS Import Strategy section. Create a new Job Provision by following the instruction .The job provisioner will create a job suite for an application added to EDP. It will also create webhooks for the project in GitLab. Note The steps below are required in order to automatically create and integrate Jenkins GitLab webhooks. Create access token in Gitlab : Log in to GitLab ; In the top-right corner, click the avatar and select Settings ; On the User Settings menu, select Access Tokens ; Choose a name and an optional expiry date for the token; In the Scopes block, select the api scope for the token; Personal access tokens Click the Create personal access token button. Note Make sure to save the access token as there will not be any ability to access it once again. Create Jenkins Credential ID by navigating to Jenkins -> Credentials -> System -> Global Credentials -> Add Credentials : Select Kind Secret text ; Select Global scope; Secret - the Access Token that was created earlier; ID - the gitlab-access-token ID; Description - the description of the current Credential ID; Jenkins credential Warning Using the GitLab integration, a webhook is automatically created. After the removal of the application, the webhook stops working but not deleted. If necessary, it must be deleted manually. Note The next step is necessary if it is needed to see the status of Jenkins Merge Requests builds in the GitLab CI/CD Pipelines section. Configure Gitlab plugin by navigating to Manage Jenkins -> Configure System and fill in the GitLab plugin settings: Connection name - gitlab; Gitlab host URL - a host URL to GitLab; Credentials - credentials with Access Token to GitLab ( gitlab-access-token ); Gitlab plugin configuration This is how the Merge Requests build statuses look like in the GitLab CI/CD Pipelines section: Gitlab pipelines statuses","title":"GitLab Integration"},{"location":"operator-guide/gitlab-integration/#related-articles","text":"Adjust Jira Integration Enable VCS Import Strategy Jenkins integration with GitLab Manage Jenkins CI Pipeline Job Provision","title":"Related Articles"},{"location":"operator-guide/gitlabci-integration/","text":"Adjust GitLab CI Tool \u2693\ufe0e EDP allows selecting one of two available CI (Continuous Integration) tools, namely: Jenkins or GitLab. The Jenkins tool is available by default. To use the GitLab CI tool, it is required to make it available first. Follow the steps below to adjust the GitLab CI tool: In GitLab, add the environment variables to the project. To add variables, navigate to Settings -> CI/CD -> Expand Variables -> Add Variable : Gitlab ci environment variables Apply the necessary variables as they differ in accordance with the cluster OpenShift / Kubernetes, see below: OpenShift Environment Variables Description DOCKER_REGISTRY_URL URL to OpenShift docker registry DOCKER_REGISTRY_PASSWORD Service Account token that has an access to registry DOCKER_REGISTRY_USER user name OPENSHIFT_SA_TOKEN token that can be used to log in to OpenShift Info In order to get access to the Docker registry and OpenShift, use the gitlab-ci ServiceAccount; pay attention that SA description contains the credentials and secrets: Service account Kubernetes Environment Variables Description DOCKER_REGISTRY_URL URL to Amazon ECR AWS_ACCESS_KEY_ID auto IAM user access key AWS_SECRET_ACCESS_KEY auto IAM user secret access key K8S_SA_TOKEN token that can be used to log in to Kubernetes Note To get the access to ECR , it is required to have an auto IAM user that has rights to push/create a repository. In Admin Console, select the CI tool in the Advanced Settings menu during the codebase creation: Advanced settings Note The selection of the CI tool is available only with the Import strategy. As soon as the codebase is provisioned, the .gitlab-ci.yml file will be created in the repository that describes the pipeline's stages and logic: .gitlab-ci.yml file presented in repository","title":"Adjust GitLab CI Tool"},{"location":"operator-guide/gitlabci-integration/#adjust-gitlab-ci-tool","text":"EDP allows selecting one of two available CI (Continuous Integration) tools, namely: Jenkins or GitLab. The Jenkins tool is available by default. To use the GitLab CI tool, it is required to make it available first. Follow the steps below to adjust the GitLab CI tool: In GitLab, add the environment variables to the project. To add variables, navigate to Settings -> CI/CD -> Expand Variables -> Add Variable : Gitlab ci environment variables Apply the necessary variables as they differ in accordance with the cluster OpenShift / Kubernetes, see below: OpenShift Environment Variables Description DOCKER_REGISTRY_URL URL to OpenShift docker registry DOCKER_REGISTRY_PASSWORD Service Account token that has an access to registry DOCKER_REGISTRY_USER user name OPENSHIFT_SA_TOKEN token that can be used to log in to OpenShift Info In order to get access to the Docker registry and OpenShift, use the gitlab-ci ServiceAccount; pay attention that SA description contains the credentials and secrets: Service account Kubernetes Environment Variables Description DOCKER_REGISTRY_URL URL to Amazon ECR AWS_ACCESS_KEY_ID auto IAM user access key AWS_SECRET_ACCESS_KEY auto IAM user secret access key K8S_SA_TOKEN token that can be used to log in to Kubernetes Note To get the access to ECR , it is required to have an auto IAM user that has rights to push/create a repository. In Admin Console, select the CI tool in the Advanced Settings menu during the codebase creation: Advanced settings Note The selection of the CI tool is available only with the Import strategy. As soon as the codebase is provisioned, the .gitlab-ci.yml file will be created in the repository that describes the pipeline's stages and logic: .gitlab-ci.yml file presented in repository","title":"Adjust GitLab CI Tool"},{"location":"operator-guide/import-strategy/","text":"Enable VCS Import Strategy \u2693\ufe0e Note Enabling the VCS Import strategy is a prerequisite to integrate EDP with GitLab or GitHub. In order to use the Import strategy, it is required to add a Secret with SSH key, GitServer Custom Resource, and Jenkins credentials by taking the steps below. Generate an SSH key pair and add a public key to GitLab or GitHub account. ssh - keygen - t ed25519 - C \"email@example.com\" Create a Secret in the \u2039edp-project\u203a namespace for the Git account with the id_rsa , id_rsa.pub , and username fields. As a sample, it is possible to use the following command (use github-sshkey instead of gitlab-sshkey for GitHub): kubectl create secret generic gitlab - sshkey - n < edp - project > \\ -- from - file = id_rsa = id_rsa \\ -- from - file = id_rsa . pub = id_rsa . pub \\ --from-literal=username=user@example.com Create GitServer Custom Resource in the project namespace with the gitHost , gitUser , httpsPort , sshPort , nameSshKeySecret , and createCodeReviewPipeline fields. As a sample, it is possible to use the following template: apiVersion : v2 . edp . epam . com / v1 kind : GitServer metadata : name : < git - server - name > namespace : < edp - project > spec : createCodeReviewPipeline : false gitHost : git . sample . com gitUser : git httpsPort : 443 nameSshKeySecret : gitlab - sshkey sshPort : 22 Note The value of the nameSshKeySecret property is the name of the Secret that is indicated in the first point above. Create Jenkinsserviceaccount Custom Resource with the credentials field that corresponds to the nameSshKeySecret property above. apiVersion : v2 . edp . epam . com / v1 kind : JenkinsServiceAccount metadata : name : gitlab - sshkey namespace : < edp - project > spec : credentials : gitlab - sshkey ownerName : '' type : ssh Double-check if the credentials are created in Jenkins correctly. Navigate to Jenkins -> Credentials -> System -> Global Credentials -> Add Credentials : Jenkins credential Make sure that the value of INTEGRATION_STRATEGIES variable is set to Import in the edp-admin-console deployment (should be by default). You can check it here: spec : containers : - name : edp - admin - console .... env : - name : INTEGRATION_STRATEGIES value : 'Create,Clone,Import' Note The default values can be found in the deployment templates for edp-admin-console-operator in edp-install umbrella chart Note The Import strategy can be found on the Applications page of the Admin Console. For details, please refer to the Add Applications page. The next step is to integrate Jenkins with GitHub or GitLab . Related Articles \u2693\ufe0e Add Application GitHub Integration GitLab Integration","title":"Enable VCS Import Strategy"},{"location":"operator-guide/import-strategy/#enable-vcs-import-strategy","text":"Note Enabling the VCS Import strategy is a prerequisite to integrate EDP with GitLab or GitHub. In order to use the Import strategy, it is required to add a Secret with SSH key, GitServer Custom Resource, and Jenkins credentials by taking the steps below. Generate an SSH key pair and add a public key to GitLab or GitHub account. ssh - keygen - t ed25519 - C \"email@example.com\" Create a Secret in the \u2039edp-project\u203a namespace for the Git account with the id_rsa , id_rsa.pub , and username fields. As a sample, it is possible to use the following command (use github-sshkey instead of gitlab-sshkey for GitHub): kubectl create secret generic gitlab - sshkey - n < edp - project > \\ -- from - file = id_rsa = id_rsa \\ -- from - file = id_rsa . pub = id_rsa . pub \\ --from-literal=username=user@example.com Create GitServer Custom Resource in the project namespace with the gitHost , gitUser , httpsPort , sshPort , nameSshKeySecret , and createCodeReviewPipeline fields. As a sample, it is possible to use the following template: apiVersion : v2 . edp . epam . com / v1 kind : GitServer metadata : name : < git - server - name > namespace : < edp - project > spec : createCodeReviewPipeline : false gitHost : git . sample . com gitUser : git httpsPort : 443 nameSshKeySecret : gitlab - sshkey sshPort : 22 Note The value of the nameSshKeySecret property is the name of the Secret that is indicated in the first point above. Create Jenkinsserviceaccount Custom Resource with the credentials field that corresponds to the nameSshKeySecret property above. apiVersion : v2 . edp . epam . com / v1 kind : JenkinsServiceAccount metadata : name : gitlab - sshkey namespace : < edp - project > spec : credentials : gitlab - sshkey ownerName : '' type : ssh Double-check if the credentials are created in Jenkins correctly. Navigate to Jenkins -> Credentials -> System -> Global Credentials -> Add Credentials : Jenkins credential Make sure that the value of INTEGRATION_STRATEGIES variable is set to Import in the edp-admin-console deployment (should be by default). You can check it here: spec : containers : - name : edp - admin - console .... env : - name : INTEGRATION_STRATEGIES value : 'Create,Clone,Import' Note The default values can be found in the deployment templates for edp-admin-console-operator in edp-install umbrella chart Note The Import strategy can be found on the Applications page of the Admin Console. For details, please refer to the Add Applications page. The next step is to integrate Jenkins with GitHub or GitLab .","title":"Enable VCS Import Strategy"},{"location":"operator-guide/import-strategy/#related-articles","text":"Add Application GitHub Integration GitLab Integration","title":"Related Articles"},{"location":"operator-guide/install-argocd/","text":"Install Argo CD \u2693\ufe0e Inspect the prerequisites and the main steps to perform for enabling Argo CD in EDP. Prerequisites \u2693\ufe0e Keycloak is installed EDP is installed Kubectl version 1.18.0 is installed. Please refer to the Kubernetes official website for details. Helm version 3.6.0 is installed. Please refer to the Helm page on GitHub for details. Installation \u2693\ufe0e Argo CD enablement for EDP consists of two major steps: Argo CD integration with EDP (SSO enablement, codebase onboarding, etc.) Argo CD installation Info It is also possible to install Argo CD using the Helmfile. For details, please refer to the Install via Helmfile page. Integrate With EDP \u2693\ufe0e To enable Argo CD integration, ensure that the argocd.enabled flag values.yaml is set to true Install With Helm \u2693\ufe0e Argo CD can be installed in many ways, please follow the official documentation for more details. Follow the steps below to install Argo CD using Helm: Note When using the OpenShift platform, apply the SecurityContextConstraints resource. Change the namespace in the users section if required. View: argocd-scc.yaml allowHostDirVolumePlugin : false allowHostIPC : false allowHostNetwork : false allowHostPID : false allowHostPorts : false allowPrivilegeEscalation : true allowPrivilegedContainer : false allowedCapabilities : null apiVersion : security.openshift.io/v1 allowedFlexVolumes : [] defaultAddCapabilities : [] fsGroup : type : MustRunAs ranges : - min : 999 max : 65543 groups : [] kind : SecurityContextConstraints metadata : annotations : \"helm.sh/hook\" : \"pre-install\" name : argo-redis-ha priority : 1 readOnlyRootFilesystem : false requiredDropCapabilities : - KILL - MKNOD - SETUID - SETGID runAsUser : type : MustRunAsRange uidRangeMin : 1 uidRangeMax : 65543 seLinuxContext : type : MustRunAs supplementalGroups : type : RunAsAny users : - system:serviceaccount:argocd:argo-redis-ha - system:serviceaccount:argocd:argo-redis-ha-haproxy volumes : - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret Check out the values.yaml file sample of the Argo CD customization, which is based on HA mode without autoscaling : View: values.yaml redis-ha : enabled : true controller : enableStatefulSet : true server : replicas : 2 extraArgs : - \"--insecure\" env : - name : ARGOCD_API_SERVER_REPLICAS value : '2' ingress : enabled : true hosts : - \"argocd.{{ .Values.global.dnsWildCard }}\" config : # required when SSO is enabled url : \"https://argocd.{{ .Values.global.dnsWildCard }}\" application.instanceLabelKey : argocd.argoproj.io/instance-edp oidc.config : | name: Keycloak issuer: https://{{ .Values.global.keycloakEndpoint }}/auth/realms/{{ .Values.global.edpName }}-main clientID: argocd clientSecret: $oidc.keycloak.clientSecret requestedScopes: - openid - profile - email - groups rbacConfig : # users may be still be able to login, # but will see no apps, projects, etc... policy.default : '' scopes : '[groups]' policy.csv : | # default global admins g, ArgoCDAdmins, role:admin configs : secret : extra : oidc.keycloak.clientSecret : \"REPLACE\" repoServer : replicas : 2 # we use Keycloak so no DEX is required dex : enabled : false # Disabled for multitenancy env with single instance deployment applicationSet : enabled : false Populate Argo CD values with the values from the EDP values.yaml : .Values.global.dnsWildCard - EDP DNS WildCard .Values.global.keycloakEndpoint - Keycloak Hostname .Values.global.edpName - EDP name Run installation kubectl create ns argocd helm repo add argo https://argoproj.github.io/argo-helm helm install argocd argo/argo-cd -f values.yaml Update argocd-secret secret (in argocd namespace) by providing correct keycloak client secret ( oidc.keycloak.clientSecret ) with value from the keycloak-client-argocd-secret secret in EDP namespace and restart the deployment: ARGOCD_CLIENT = $( kubectl -n <EDP_NAMESPACE> get secret keycloak-client-argocd-secret -o jsonpath = '{.data.clientSecret}' ) kubectl -n argocd patch secret argocd-secret -p = \"{\\\"data\\\":{\\\"oidc.keycloak.clientSecret\\\": \\\" ${ ARGOCD_CLIENT } \\\"}}\" -v = 1 kubectl -n argocd rollout restart deployment argocd-server Related Articles \u2693\ufe0e Argo CD Integration","title":"Install Argo CD"},{"location":"operator-guide/install-argocd/#install-argo-cd","text":"Inspect the prerequisites and the main steps to perform for enabling Argo CD in EDP.","title":"Install Argo CD"},{"location":"operator-guide/install-argocd/#prerequisites","text":"Keycloak is installed EDP is installed Kubectl version 1.18.0 is installed. Please refer to the Kubernetes official website for details. Helm version 3.6.0 is installed. Please refer to the Helm page on GitHub for details.","title":"Prerequisites"},{"location":"operator-guide/install-argocd/#installation","text":"Argo CD enablement for EDP consists of two major steps: Argo CD integration with EDP (SSO enablement, codebase onboarding, etc.) Argo CD installation Info It is also possible to install Argo CD using the Helmfile. For details, please refer to the Install via Helmfile page.","title":"Installation"},{"location":"operator-guide/install-argocd/#integrate-with-edp","text":"To enable Argo CD integration, ensure that the argocd.enabled flag values.yaml is set to true","title":"Integrate With EDP"},{"location":"operator-guide/install-argocd/#install-with-helm","text":"Argo CD can be installed in many ways, please follow the official documentation for more details. Follow the steps below to install Argo CD using Helm: Note When using the OpenShift platform, apply the SecurityContextConstraints resource. Change the namespace in the users section if required. View: argocd-scc.yaml allowHostDirVolumePlugin : false allowHostIPC : false allowHostNetwork : false allowHostPID : false allowHostPorts : false allowPrivilegeEscalation : true allowPrivilegedContainer : false allowedCapabilities : null apiVersion : security.openshift.io/v1 allowedFlexVolumes : [] defaultAddCapabilities : [] fsGroup : type : MustRunAs ranges : - min : 999 max : 65543 groups : [] kind : SecurityContextConstraints metadata : annotations : \"helm.sh/hook\" : \"pre-install\" name : argo-redis-ha priority : 1 readOnlyRootFilesystem : false requiredDropCapabilities : - KILL - MKNOD - SETUID - SETGID runAsUser : type : MustRunAsRange uidRangeMin : 1 uidRangeMax : 65543 seLinuxContext : type : MustRunAs supplementalGroups : type : RunAsAny users : - system:serviceaccount:argocd:argo-redis-ha - system:serviceaccount:argocd:argo-redis-ha-haproxy volumes : - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret Check out the values.yaml file sample of the Argo CD customization, which is based on HA mode without autoscaling : View: values.yaml redis-ha : enabled : true controller : enableStatefulSet : true server : replicas : 2 extraArgs : - \"--insecure\" env : - name : ARGOCD_API_SERVER_REPLICAS value : '2' ingress : enabled : true hosts : - \"argocd.{{ .Values.global.dnsWildCard }}\" config : # required when SSO is enabled url : \"https://argocd.{{ .Values.global.dnsWildCard }}\" application.instanceLabelKey : argocd.argoproj.io/instance-edp oidc.config : | name: Keycloak issuer: https://{{ .Values.global.keycloakEndpoint }}/auth/realms/{{ .Values.global.edpName }}-main clientID: argocd clientSecret: $oidc.keycloak.clientSecret requestedScopes: - openid - profile - email - groups rbacConfig : # users may be still be able to login, # but will see no apps, projects, etc... policy.default : '' scopes : '[groups]' policy.csv : | # default global admins g, ArgoCDAdmins, role:admin configs : secret : extra : oidc.keycloak.clientSecret : \"REPLACE\" repoServer : replicas : 2 # we use Keycloak so no DEX is required dex : enabled : false # Disabled for multitenancy env with single instance deployment applicationSet : enabled : false Populate Argo CD values with the values from the EDP values.yaml : .Values.global.dnsWildCard - EDP DNS WildCard .Values.global.keycloakEndpoint - Keycloak Hostname .Values.global.edpName - EDP name Run installation kubectl create ns argocd helm repo add argo https://argoproj.github.io/argo-helm helm install argocd argo/argo-cd -f values.yaml Update argocd-secret secret (in argocd namespace) by providing correct keycloak client secret ( oidc.keycloak.clientSecret ) with value from the keycloak-client-argocd-secret secret in EDP namespace and restart the deployment: ARGOCD_CLIENT = $( kubectl -n <EDP_NAMESPACE> get secret keycloak-client-argocd-secret -o jsonpath = '{.data.clientSecret}' ) kubectl -n argocd patch secret argocd-secret -p = \"{\\\"data\\\":{\\\"oidc.keycloak.clientSecret\\\": \\\" ${ ARGOCD_CLIENT } \\\"}}\" -v = 1 kubectl -n argocd rollout restart deployment argocd-server","title":"Install With Helm"},{"location":"operator-guide/install-argocd/#related-articles","text":"Argo CD Integration","title":"Related Articles"},{"location":"operator-guide/install-defectdojo/","text":"Install DefectDojo \u2693\ufe0e Inspect the main steps to perform for installing DefectDojo via Helm Chart. Info It is also possible to install DefectDojo using the Helmfile. For details, please refer to the Install via Helmfile page. Prerequisites \u2693\ufe0e Kubectl version 1.20.0 is installed. Please refer to the Kubernetes official website for details. Helm version 3.9.2 is installed. Please refer to the Helm page on GitHub for details. Installation \u2693\ufe0e Info Please refer to the DefectDojo Helm Chart and Deploy DefectDojo into the Kubernetes cluster sections for details. To install DefectDojo, follow the steps below: Check that a security namespace is created. If not, run the following command to create it: kubectl create namespace defectdojo Note When using the OpenShift platform, install the SecurityContextConstraints resource. In case of using a custom namespace for defectdojo , change the namespace in the users section. View: defectdojo-scc.yaml allowHostDirVolumePlugin : false allowHostIPC : false allowHostNetwork : false allowHostPID : false allowHostPorts : false allowPrivilegeEscalation : true allowPrivilegedContainer : false allowedCapabilities : null apiVersion : security.openshift.io/v1 allowedFlexVolumes : [] defaultAddCapabilities : [] fsGroup : type : MustRunAs ranges : - min : 999 max : 65543 groups : [] kind : SecurityContextConstraints metadata : annotations : \"helm.sh/hook\" : \"pre-install\" name : defectdojo priority : 1 readOnlyRootFilesystem : false requiredDropCapabilities : - KILL - MKNOD - SETUID - SETGID runAsUser : type : MustRunAsRange uidRangeMin : 1 uidRangeMax : 65543 seLinuxContext : type : MustRunAs supplementalGroups : type : RunAsAny users : - system:serviceaccount:defectdojo:defectdojo - system:serviceaccount:defectdojo:defectdojo-rabbitmq - system:serviceaccount:defectdojo:default volumes : - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret Add a chart repository: helm repo add defectdojo 'https://raw.githubusercontent.com/DefectDojo/django-DefectDojo/helm-charts' helm repo update Create PostgreSQL admin secret: kubectl -n defectdojo create secret generic defectdojo-postgresql-specific \\ --from-literal = postgresql-password = <postgresql_password> \\ --from-literal = postgresql-postgres-password = <postgresql_postgres_password> Note The postgresql_password and postgresql_postgres_password passwords must be 16 characters long. Create Rabbitmq admin secret: kubectl -n defectdojo create secret generic defectdojo-rabbitmq-specific \\ --from-literal = rabbitmq-password = <rabbitmq_password> \\ --from-literal = rabbitmq-erlang-cookie = <rabbitmq_erlang_cookie> Note The rabbitmq_password password must be 10 characters long. The rabbitmq_erlang_cookie password must be 32 characters long. Create DefectDojo admin secret: kubectl -n defectdojo create secret generic defectdojo \\ --from-literal = DD_ADMIN_PASSWORD = <dd_admin_password> \\ --from-literal = DD_SECRET_KEY = <dd_secret_key> \\ --from-literal = DD_CREDENTIAL_AES_256_KEY = <dd_credential_aes_256_key> \\ --from-literal = METRICS_HTTP_AUTH_PASSWORD = <metric_http_auth_password> Note The dd_admin_password password must be 22 characters long. The dd_secret_key password must be 128 characters long. The dd_credential_aes_256_key password must be 128 characters long. The metric_http_auth_password password must be 32 characters long. Install DefectDojo v.2.12.0 using defectdojo/defectdojo Helm chart v.1.6.35: helm upgrade --install \\ defectdojo \\ defectdojo/defectdojo \\ --namespace defectdojo \\ --values values.yaml Check out the values.yaml file sample of the DefectDojo customization: View: values.yaml fullnameOverride : defectdojo host : defectdojo.<ROOT_DOMAIN> site_url : https://defectdojo.<ROOT_DOMAIN> alternativeHosts : - defectdojo-django.defectdojo initializer : # should be false after initial installation was performed run : true django : ingress : enabled : true # change to 'false' for OpenShift uwsgi : livenessProbe : # Enable liveness checks on uwsgi container. Those values are use on nginx readiness checks as well. # default value is 120, so in our case 20 is just fine initialDelaySeconds : 20 For the OpenShift platform, install a Route: View: defectdojo-route.yaml kind : Route apiVersion : route.openshift.io/v1 metadata : name : defectdojo namespace : defectdojo spec : host : defectdojo.<ROOT_DOMAIN> path : / tls : insecureEdgeTerminationPolicy : Redirect termination : edge to : kind : Service name : defectdojo-django port : targetPort : http wildcardPolicy : None Configuration \u2693\ufe0e To prepare DefectDojo for integration with EDP, follow the steps below: Get credentials of the DefectDojo admin: echo \"DefectDojo admin password: $( kubectl \\ get secret defectdojo \\ --namespace = defectdojo \\ --output jsonpath = '{.data.DD_ADMIN_PASSWORD}' \\ | base64 --decode ) \" Get a token of the DefectDojo user: Login to the DefectDojo UI using the credentials. Go to the API v2 key (token). Copy the API key. Create a DefectDojo secret in your edp namespace: kubectl -n <edp_namespace> create secret generic defectdojo-ciuser-token \\ --from-literal = token = <dd_token_of_dd_user> \\ --from-literal = url = \"<defectdojo_url>\" Related Articles \u2693\ufe0e Install via Helmfile","title":"Install DefectDojo"},{"location":"operator-guide/install-defectdojo/#install-defectdojo","text":"Inspect the main steps to perform for installing DefectDojo via Helm Chart. Info It is also possible to install DefectDojo using the Helmfile. For details, please refer to the Install via Helmfile page.","title":"Install DefectDojo"},{"location":"operator-guide/install-defectdojo/#prerequisites","text":"Kubectl version 1.20.0 is installed. Please refer to the Kubernetes official website for details. Helm version 3.9.2 is installed. Please refer to the Helm page on GitHub for details.","title":"Prerequisites"},{"location":"operator-guide/install-defectdojo/#installation","text":"Info Please refer to the DefectDojo Helm Chart and Deploy DefectDojo into the Kubernetes cluster sections for details. To install DefectDojo, follow the steps below: Check that a security namespace is created. If not, run the following command to create it: kubectl create namespace defectdojo Note When using the OpenShift platform, install the SecurityContextConstraints resource. In case of using a custom namespace for defectdojo , change the namespace in the users section. View: defectdojo-scc.yaml allowHostDirVolumePlugin : false allowHostIPC : false allowHostNetwork : false allowHostPID : false allowHostPorts : false allowPrivilegeEscalation : true allowPrivilegedContainer : false allowedCapabilities : null apiVersion : security.openshift.io/v1 allowedFlexVolumes : [] defaultAddCapabilities : [] fsGroup : type : MustRunAs ranges : - min : 999 max : 65543 groups : [] kind : SecurityContextConstraints metadata : annotations : \"helm.sh/hook\" : \"pre-install\" name : defectdojo priority : 1 readOnlyRootFilesystem : false requiredDropCapabilities : - KILL - MKNOD - SETUID - SETGID runAsUser : type : MustRunAsRange uidRangeMin : 1 uidRangeMax : 65543 seLinuxContext : type : MustRunAs supplementalGroups : type : RunAsAny users : - system:serviceaccount:defectdojo:defectdojo - system:serviceaccount:defectdojo:defectdojo-rabbitmq - system:serviceaccount:defectdojo:default volumes : - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret Add a chart repository: helm repo add defectdojo 'https://raw.githubusercontent.com/DefectDojo/django-DefectDojo/helm-charts' helm repo update Create PostgreSQL admin secret: kubectl -n defectdojo create secret generic defectdojo-postgresql-specific \\ --from-literal = postgresql-password = <postgresql_password> \\ --from-literal = postgresql-postgres-password = <postgresql_postgres_password> Note The postgresql_password and postgresql_postgres_password passwords must be 16 characters long. Create Rabbitmq admin secret: kubectl -n defectdojo create secret generic defectdojo-rabbitmq-specific \\ --from-literal = rabbitmq-password = <rabbitmq_password> \\ --from-literal = rabbitmq-erlang-cookie = <rabbitmq_erlang_cookie> Note The rabbitmq_password password must be 10 characters long. The rabbitmq_erlang_cookie password must be 32 characters long. Create DefectDojo admin secret: kubectl -n defectdojo create secret generic defectdojo \\ --from-literal = DD_ADMIN_PASSWORD = <dd_admin_password> \\ --from-literal = DD_SECRET_KEY = <dd_secret_key> \\ --from-literal = DD_CREDENTIAL_AES_256_KEY = <dd_credential_aes_256_key> \\ --from-literal = METRICS_HTTP_AUTH_PASSWORD = <metric_http_auth_password> Note The dd_admin_password password must be 22 characters long. The dd_secret_key password must be 128 characters long. The dd_credential_aes_256_key password must be 128 characters long. The metric_http_auth_password password must be 32 characters long. Install DefectDojo v.2.12.0 using defectdojo/defectdojo Helm chart v.1.6.35: helm upgrade --install \\ defectdojo \\ defectdojo/defectdojo \\ --namespace defectdojo \\ --values values.yaml Check out the values.yaml file sample of the DefectDojo customization: View: values.yaml fullnameOverride : defectdojo host : defectdojo.<ROOT_DOMAIN> site_url : https://defectdojo.<ROOT_DOMAIN> alternativeHosts : - defectdojo-django.defectdojo initializer : # should be false after initial installation was performed run : true django : ingress : enabled : true # change to 'false' for OpenShift uwsgi : livenessProbe : # Enable liveness checks on uwsgi container. Those values are use on nginx readiness checks as well. # default value is 120, so in our case 20 is just fine initialDelaySeconds : 20 For the OpenShift platform, install a Route: View: defectdojo-route.yaml kind : Route apiVersion : route.openshift.io/v1 metadata : name : defectdojo namespace : defectdojo spec : host : defectdojo.<ROOT_DOMAIN> path : / tls : insecureEdgeTerminationPolicy : Redirect termination : edge to : kind : Service name : defectdojo-django port : targetPort : http wildcardPolicy : None","title":"Installation"},{"location":"operator-guide/install-defectdojo/#configuration","text":"To prepare DefectDojo for integration with EDP, follow the steps below: Get credentials of the DefectDojo admin: echo \"DefectDojo admin password: $( kubectl \\ get secret defectdojo \\ --namespace = defectdojo \\ --output jsonpath = '{.data.DD_ADMIN_PASSWORD}' \\ | base64 --decode ) \" Get a token of the DefectDojo user: Login to the DefectDojo UI using the credentials. Go to the API v2 key (token). Copy the API key. Create a DefectDojo secret in your edp namespace: kubectl -n <edp_namespace> create secret generic defectdojo-ciuser-token \\ --from-literal = token = <dd_token_of_dd_user> \\ --from-literal = url = \"<defectdojo_url>\"","title":"Configuration"},{"location":"operator-guide/install-defectdojo/#related-articles","text":"Install via Helmfile","title":"Related Articles"},{"location":"operator-guide/install-edp/","text":"Install EDP \u2693\ufe0e Inspect the main steps to install EPAM Delivery Platform. Please check the prerequisites section before starting the installation. There are two ways to deploy EPAM Delivery Platform: using Helm (see below) and using Helmfile . Note The installation process below is given for a Kubernetes cluster. The steps that differ for an OpenShift cluster are indicated in the notes. Note \u2039edp-project\u203a is the name of the EDP tenant in all the following steps. Create an \u2039edp-project\u203a namespace or a Kiosk space depending on whether Kiosk is used or not. Without Kiosk, create a namespace: kubectl create namespace <edp-project> Note For an OpenShift cluster, run the oc command instead of kubectl one. With Kiosk, create a relevant space: apiVersion : tenancy . kiosk . sh / v1alpha1 kind : Space metadata : name : < edp - project > spec : account : < edp - project >- admin Note Kiosk is mandatory for EDP v.2.8.x. It is not implemented for the previous versions, and is optional for EDP since v.2.9.x. To store EDP data, use any existing Postgres database or create one during the installation. Additionally, create two secrets in the \u2039edp-project\u203a namespace: one with administrative credentials and another with credentials for the EDP tenant (database schema). Create a secret for administrative access to the database: kubectl -n <edp-project> create secret generic super-admin-db \\ --from-literal=username=<super_admin_db_username> \\ --from-literal=password=<super_admin_db_password> Warning Do not use the admin username here since admin is a reserved name. Create a secret for an EDP tenant database user. kubectl -n <edp-project> create secret generic db-admin-console \\ --from-literal=username=<tenant_db_username> \\ --from-literal=password=<tenant_db_password> Warning Do not use the admin username here since admin is a reserved name. For EDP, it is required to have Keycloak access to perform the integration. Create a secret with user and password provisioned in the step 2 of the Keycloak Configuration section. kubectl -n <edp-project> create secret generic keycloak \\ --from-literal=username=<username> \\ --from-literal=password=<password> Add the Helm EPAMEDP Charts for local client. helm repo add epamedp https://epam.github.io/edp-helm-charts/stable Choose the required Helm chart version: helm search repo epamedp / edp - install NAME CHART VERSION APP VERSION DESCRIPTION epamedp / edp - install 2 . 12 . 0 2 . 12 . 0 A Helm chart for EDP Install Note It is highly recommended to use the latest released version. Check the parameters in the EDP installation chart. For details, please refer to the values.yaml file. With the external database, set the global.database.host value to the database DNS name accessible from the \u2039edp-project\u203a namespace. Install EDP in the \u2039edp-project\u203a namespace with the helm tool. helm install edp epamedp / edp - install -- wait -- timeout = 900 s \\ -- version < edp_version > \\ -- values values . yaml \\ -- namespace < edp - project > See the details on parameters below: View: values.yaml global : # Name of the <edp-project> EDP namespace that was previously defined; edpName : <edp-project> # DNS wildcard for routing in the Kubernetes cluster; dnsWildCard : <DNS_wilcdard> # Enable or disable integration with Kiosk (by default the value is true) kioskEnabled : <true/false> # Kubernetes API server; webConsole : url : <kubeconfig.clusters.cluster.server> # set platform type: OpenShift or Kubernetes; platform : <platform_type> # Administrators of the tenant separated by comma (,) e.g. user@example.com; admins : [ user1@example.com , user2@example.com ] # Developers of the tenant separated by comma (,) e.g. user@example.com; developers : [ user1@example.com , user2@example.com ] # AWS Region, e.g. \"eu-central-1\" awsRegion : keycloak-operator : keycloak : # URL to Keycloak; url : <keycloak_endpoint> dockerRegistry : enabled : true # URL to Docker registry e.g. <aws_account_id>.dkr.ecr.<region>.amazonaws.com; url : <aws_account_id>.dkr.ecr.<region>.amazonaws.com gerrit-operator : gerrit : # Gerrit SSH node port; sshPort : <gerrit_ssh_port> kaniko : # AWS IAM role with push access to ECR, e.g. arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_IAM_ROLE_NAME> roleArn : Note Set global.platform=openshift while deploying EDP in OpenShift. Info The full installation with integration between tools will take at least 10 minutes. Next Steps \u2693\ufe0e Consult VCS integration section, if it is necessary to integrate GitLab or GitHub with EDP. Related Articles \u2693\ufe0e Enable VCS Import Strategy GitHub Integration GitLab Integration Install Keycloak Set Up Kubernetes Set Up OpenShift","title":"Install EDP"},{"location":"operator-guide/install-edp/#install-edp","text":"Inspect the main steps to install EPAM Delivery Platform. Please check the prerequisites section before starting the installation. There are two ways to deploy EPAM Delivery Platform: using Helm (see below) and using Helmfile . Note The installation process below is given for a Kubernetes cluster. The steps that differ for an OpenShift cluster are indicated in the notes. Note \u2039edp-project\u203a is the name of the EDP tenant in all the following steps. Create an \u2039edp-project\u203a namespace or a Kiosk space depending on whether Kiosk is used or not. Without Kiosk, create a namespace: kubectl create namespace <edp-project> Note For an OpenShift cluster, run the oc command instead of kubectl one. With Kiosk, create a relevant space: apiVersion : tenancy . kiosk . sh / v1alpha1 kind : Space metadata : name : < edp - project > spec : account : < edp - project >- admin Note Kiosk is mandatory for EDP v.2.8.x. It is not implemented for the previous versions, and is optional for EDP since v.2.9.x. To store EDP data, use any existing Postgres database or create one during the installation. Additionally, create two secrets in the \u2039edp-project\u203a namespace: one with administrative credentials and another with credentials for the EDP tenant (database schema). Create a secret for administrative access to the database: kubectl -n <edp-project> create secret generic super-admin-db \\ --from-literal=username=<super_admin_db_username> \\ --from-literal=password=<super_admin_db_password> Warning Do not use the admin username here since admin is a reserved name. Create a secret for an EDP tenant database user. kubectl -n <edp-project> create secret generic db-admin-console \\ --from-literal=username=<tenant_db_username> \\ --from-literal=password=<tenant_db_password> Warning Do not use the admin username here since admin is a reserved name. For EDP, it is required to have Keycloak access to perform the integration. Create a secret with user and password provisioned in the step 2 of the Keycloak Configuration section. kubectl -n <edp-project> create secret generic keycloak \\ --from-literal=username=<username> \\ --from-literal=password=<password> Add the Helm EPAMEDP Charts for local client. helm repo add epamedp https://epam.github.io/edp-helm-charts/stable Choose the required Helm chart version: helm search repo epamedp / edp - install NAME CHART VERSION APP VERSION DESCRIPTION epamedp / edp - install 2 . 12 . 0 2 . 12 . 0 A Helm chart for EDP Install Note It is highly recommended to use the latest released version. Check the parameters in the EDP installation chart. For details, please refer to the values.yaml file. With the external database, set the global.database.host value to the database DNS name accessible from the \u2039edp-project\u203a namespace. Install EDP in the \u2039edp-project\u203a namespace with the helm tool. helm install edp epamedp / edp - install -- wait -- timeout = 900 s \\ -- version < edp_version > \\ -- values values . yaml \\ -- namespace < edp - project > See the details on parameters below: View: values.yaml global : # Name of the <edp-project> EDP namespace that was previously defined; edpName : <edp-project> # DNS wildcard for routing in the Kubernetes cluster; dnsWildCard : <DNS_wilcdard> # Enable or disable integration with Kiosk (by default the value is true) kioskEnabled : <true/false> # Kubernetes API server; webConsole : url : <kubeconfig.clusters.cluster.server> # set platform type: OpenShift or Kubernetes; platform : <platform_type> # Administrators of the tenant separated by comma (,) e.g. user@example.com; admins : [ user1@example.com , user2@example.com ] # Developers of the tenant separated by comma (,) e.g. user@example.com; developers : [ user1@example.com , user2@example.com ] # AWS Region, e.g. \"eu-central-1\" awsRegion : keycloak-operator : keycloak : # URL to Keycloak; url : <keycloak_endpoint> dockerRegistry : enabled : true # URL to Docker registry e.g. <aws_account_id>.dkr.ecr.<region>.amazonaws.com; url : <aws_account_id>.dkr.ecr.<region>.amazonaws.com gerrit-operator : gerrit : # Gerrit SSH node port; sshPort : <gerrit_ssh_port> kaniko : # AWS IAM role with push access to ECR, e.g. arn:aws:iam::<AWS_ACCOUNT_ID>:role/<AWS_IAM_ROLE_NAME> roleArn : Note Set global.platform=openshift while deploying EDP in OpenShift. Info The full installation with integration between tools will take at least 10 minutes.","title":"Install EDP"},{"location":"operator-guide/install-edp/#next-steps","text":"Consult VCS integration section, if it is necessary to integrate GitLab or GitHub with EDP.","title":"Next Steps"},{"location":"operator-guide/install-edp/#related-articles","text":"Enable VCS Import Strategy GitHub Integration GitLab Integration Install Keycloak Set Up Kubernetes Set Up OpenShift","title":"Related Articles"},{"location":"operator-guide/install-external-secrets-operator/","text":"Install External Secrets Operator \u2693\ufe0e Inspect the prerequisites and the main steps to perform for enabling External Secrets Operator in EDP. Prerequisites \u2693\ufe0e Kubectl version 1.16.0+ is installed. Please refer to the Kubernetes official website for details. Helm version 3.6.0+ is installed. Please refer to the Helm page on GitHub for details. Installation \u2693\ufe0e To install External Secrets Operator with Helm, run the following commands: helm repo add external-secrets https://charts.external-secrets.io helm install external-secrets \\ external-secrets/external-secrets \\ -n external-secrets \\ --create-namespace \\ # --set installCRDs=true Info It is also possible to install External Secrets Operator using the Helmfile or Operator Lifecycle Manager (OLM) . Related Articles \u2693\ufe0e External Secrets Operator Integration","title":"Install External Secrets Operator"},{"location":"operator-guide/install-external-secrets-operator/#install-external-secrets-operator","text":"Inspect the prerequisites and the main steps to perform for enabling External Secrets Operator in EDP.","title":"Install External Secrets Operator"},{"location":"operator-guide/install-external-secrets-operator/#prerequisites","text":"Kubectl version 1.16.0+ is installed. Please refer to the Kubernetes official website for details. Helm version 3.6.0+ is installed. Please refer to the Helm page on GitHub for details.","title":"Prerequisites"},{"location":"operator-guide/install-external-secrets-operator/#installation","text":"To install External Secrets Operator with Helm, run the following commands: helm repo add external-secrets https://charts.external-secrets.io helm install external-secrets \\ external-secrets/external-secrets \\ -n external-secrets \\ --create-namespace \\ # --set installCRDs=true Info It is also possible to install External Secrets Operator using the Helmfile or Operator Lifecycle Manager (OLM) .","title":"Installation"},{"location":"operator-guide/install-external-secrets-operator/#related-articles","text":"External Secrets Operator Integration","title":"Related Articles"},{"location":"operator-guide/install-ingress-nginx/","text":"Install NGINX Ingress Controller \u2693\ufe0e Inspect the prerequisites and the main steps to perform for installing Install NGINX Ingress Controller on Kubernetes. Prerequisites \u2693\ufe0e Kubectl version 1.20.0 is installed. Please refer to the Kubernetes official website for details. Helm version 3.6.0 is installed. Please refer to the Helm page on GitHub for details. Installation \u2693\ufe0e Info It is also possible to install NGINX Ingress Controller using the Helmfile. For details, please refer to the Install via Helmfile page. To install the ingress-nginx chart, follow the steps below: Create an ingress-nginx namespace: kubectl create namespace ingress-nginx Add a chart repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update Install the ingress-nginx chart: helm install ingress ingress-nginx/ingress-nginx \\ --version 4.1.4 \\ --values values.yaml \\ --namespace ingress-nginx Check out the values.yaml file sample of the ingress-nginx chart customization: View: values.yaml controller : addHeaders : X-Content-Type-Options : nosniff X-Frame-Options : SAMEORIGIN resources : limits : memory : \"256Mi\" requests : cpu : \"50m\" memory : \"128M\" config : ssl-redirect : 'true' client-header-buffer-size : '64k' http2-max-field-size : '64k' http2-max-header-size : '64k' large-client-header-buffers : '4 64k' upstream-keepalive-timeout : '120' keep-alive : '10' use-forwarded-headers : 'true' proxy-real-ip-cidr : '172.32.0.0/16' proxy-buffer-size : '8k' # To watch Ingress objects without the ingressClassName field set parameter value to true. # https://kubernetes.github.io/ingress-nginx/#i-have-only-one-ingress-controller-in-my-cluster-what-should-i-do watchIngressWithoutClass : true service : type : NodePort nodePorts : http : 32080 https : 32443 updateStrategy : rollingUpdate : maxUnavailable : 1 type : RollingUpdate metrics : enabled : true defaultBackend : enabled : true serviceAccount : create : true name : nginx-ingress-service-account Note Align value controller.config.proxy-real-ip-cidr with AWS VPC CIDR .","title":"Install NGINX Ingress Controller"},{"location":"operator-guide/install-ingress-nginx/#install-nginx-ingress-controller","text":"Inspect the prerequisites and the main steps to perform for installing Install NGINX Ingress Controller on Kubernetes.","title":"Install NGINX Ingress Controller"},{"location":"operator-guide/install-ingress-nginx/#prerequisites","text":"Kubectl version 1.20.0 is installed. Please refer to the Kubernetes official website for details. Helm version 3.6.0 is installed. Please refer to the Helm page on GitHub for details.","title":"Prerequisites"},{"location":"operator-guide/install-ingress-nginx/#installation","text":"Info It is also possible to install NGINX Ingress Controller using the Helmfile. For details, please refer to the Install via Helmfile page. To install the ingress-nginx chart, follow the steps below: Create an ingress-nginx namespace: kubectl create namespace ingress-nginx Add a chart repository: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update Install the ingress-nginx chart: helm install ingress ingress-nginx/ingress-nginx \\ --version 4.1.4 \\ --values values.yaml \\ --namespace ingress-nginx Check out the values.yaml file sample of the ingress-nginx chart customization: View: values.yaml controller : addHeaders : X-Content-Type-Options : nosniff X-Frame-Options : SAMEORIGIN resources : limits : memory : \"256Mi\" requests : cpu : \"50m\" memory : \"128M\" config : ssl-redirect : 'true' client-header-buffer-size : '64k' http2-max-field-size : '64k' http2-max-header-size : '64k' large-client-header-buffers : '4 64k' upstream-keepalive-timeout : '120' keep-alive : '10' use-forwarded-headers : 'true' proxy-real-ip-cidr : '172.32.0.0/16' proxy-buffer-size : '8k' # To watch Ingress objects without the ingressClassName field set parameter value to true. # https://kubernetes.github.io/ingress-nginx/#i-have-only-one-ingress-controller-in-my-cluster-what-should-i-do watchIngressWithoutClass : true service : type : NodePort nodePorts : http : 32080 https : 32443 updateStrategy : rollingUpdate : maxUnavailable : 1 type : RollingUpdate metrics : enabled : true defaultBackend : enabled : true serviceAccount : create : true name : nginx-ingress-service-account Note Align value controller.config.proxy-real-ip-cidr with AWS VPC CIDR .","title":"Installation"},{"location":"operator-guide/install-keycloak/","text":"Install Keycloak \u2693\ufe0e Inspect the prerequisites and the main steps to perform for installing Keycloak. Note The installation process below is given for a Kubernetes cluster. The steps that differ for an OpenShift cluster are indicated in the notes. Prerequisites \u2693\ufe0e Kubectl version 1.18.0 is installed. Please refer to the Kubernetes official website for details. Helm version 3.6.0 is installed. Please refer to the Helm page on GitHub for details. Note EDP team is using a helm chart from the codecentric repository, but other repositories can be used as well (e.g. Bitnami ). Before installing Keycloak, it is necessary to install a PostgreSQL database . Info It is also possible to install Keycloak using the Helmfile. For details, please refer to the Install via Helmfile page. PostgreSQL Installation \u2693\ufe0e To install PostgreSQL, follow the steps below: Check that a security namespace is created. If not, run the following command to create it: kubectl create namespace security Note On the OpenShift platform, apply the SecurityContextConstraints resource. Change the namespace in the users section if required. View: keycloak-scc.yaml allowHostDirVolumePlugin : false allowHostIPC : false allowHostNetwork : false allowHostPID : false allowHostPorts : false allowPrivilegeEscalation : true allowPrivilegedContainer : false allowedCapabilities : null apiVersion : security.openshift.io/v1 allowedFlexVolumes : [] defaultAddCapabilities : [] fsGroup : type : MustRunAs ranges : - min : 999 max : 65543 groups : [] kind : SecurityContextConstraints metadata : annotations : \"helm.sh/hook\" : \"pre-install\" name : keycloak priority : 1 readOnlyRootFilesystem : false requiredDropCapabilities : - KILL - MKNOD - SETUID - SETGID runAsUser : type : MustRunAsRange uidRangeMin : 1 uidRangeMax : 65543 seLinuxContext : type : MustRunAs supplementalGroups : type : RunAsAny users : - system:serviceaccount:security:keycloakx - system:serviceaccount:security:default volumes : - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret Add a chart repository: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update Create PostgreSQL admin secret: kubectl -n security create secret generic keycloak-postgresql \\ --from-literal=password=<postgresql_password> \\ --from-literal=postgres-password=<postgresql_postgres_password> Install the custom SecurityContextConstraints (only for OpenShift): Note If you use OpenShift as your deployment platform, add customsecuritycontextconstraints.yaml . View: customsecuritycontextconstraints.yaml allowHostDirVolumePlugin : false allowHostIPC : false allowHostNetwork : false allowHostPID : false allowHostPorts : false allowPrivilegeEscalation : true allowPrivilegedContainer : false allowedCapabilities : null apiVersion : security.openshift.io/v1 allowedFlexVolumes : [] defaultAddCapabilities : [] fsGroup : type : MustRunAs ranges : - min : 999 max : 65543 groups : [] kind : SecurityContextConstraints metadata : annotations : \"helm.sh/hook\" : \"pre-install\" name : keycloak priority : 1 readOnlyRootFilesystem : false requiredDropCapabilities : - KILL - MKNOD - SETUID - SETGID runAsUser : type : MustRunAsRange uidRangeMin : 1 uidRangeMax : 65543 seLinuxContext : type : MustRunAs supplementalGroups : type : RunAsAny users : - system:serviceaccount:security:keycloakx - system:serviceaccount:security:default volumes : - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret Install PostgreSQL v.14.4.0 using bitnami/postgresql Helm chart v.11.6.19: Info The PostgreSQL can be deployed in production ready mode. For example, it may include multiple replicas, persistent storage, autoscaling, and monitoring. For details, please refer to the official Chart documentation . helm install postgresql bitnami/postgresql \\ --version 11.6.19 \\ --values values.yaml \\ --namespace security Check out the values.yaml file sample of the PostgreSQL customization: View: values.yaml # PostgreSQL read only replica parameters readReplicas : # Number of PostgreSQL read only replicas replicaCount : 1 global : postgresql : auth : username : admin existingSecret : keycloak-postgresql database : keycloak primary : persistence : enabled : true size : 3Gi # If the StorageClass with reclaimPolicy: Retain is used, install an additional StorageClass before installing PostgreSQL # (the code is given below). # If the default StorageClass will be used - change \"gp2-retain\" to \"gp2\" storageClass : \"gp2-retain\" Install an additional StorageClass (optional): Note If the PostgreSQL installation uses a StorageClass with reclaimPolicy: Retain , install additional StorageClass storageclass.yaml . View: storageclass.yaml kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : gp2-retain provisioner : kubernetes.io/aws-ebs parameters : fsType : ext4 type : gp2 reclaimPolicy : Retain volumeBindingMode : WaitForFirstConsumer Keycloak Installation \u2693\ufe0e To install Keycloak, follow the steps below: Use security namespace from the PostgreSQL installation. Add a chart repository: helm repo add codecentric https://codecentric.github.io/helm-charts helm repo update Create Keycloak admin secret: kubectl -n security create secret generic keycloak-admin-creds \\ --from-literal=username=<keycloak_admin_username> \\ --from-literal=password=<keycloak_admin_password> Install Keycloak 19.0.1 using codecentric/keycloakx Helm chart: Info Keycloak can be deployed in production ready mode. For example, it may include multiple replicas, persistent storage, autoscaling, and monitoring. For details, please refer to the official Chart documentation . helm install keycloakx codecentric/keycloakx \\ --version 1.4.2 \\ --values values.yaml \\ --namespace security Check out the values.yaml file sample of the Keycloak customization: View: values.yaml replicas : 1 # Deploy the latest verion image : tag : \"19.0.1\" # start: create OpenShift realm which is required by EDP extraInitContainers : | - name: realm-provider image: busybox imagePullPolicy: IfNotPresent command: - sh args: - -c - | echo '{\"realm\": \"openshift\",\"enabled\": true}' > /opt/keycloak/data/import/openshift.json volumeMounts: - name: realm mountPath: /opt/keycloak/data/import extraVolumeMounts : | - name: realm mountPath: /opt/keycloak/data/import extraVolumes : | - name: realm emptyDir: {} command : - \"/opt/keycloak/bin/kc.sh\" - \"--verbose\" - \"start\" - \"--auto-build\" - \"--http-enabled=true\" - \"--http-port=8080\" - \"--hostname-strict=false\" - \"--hostname-strict-https=false\" - \"--spi-events-listener-jboss-logging-success-level=info\" - \"--spi-events-listener-jboss-logging-error-level=warn\" - \"--import-realm\" extraEnv : | - name: KC_PROXY value: \"passthrough\" - name: KEYCLOAK_ADMIN valueFrom: secretKeyRef: name: keycloak-admin-creds key: username - name: KEYCLOAK_ADMIN_PASSWORD valueFrom: secretKeyRef: name: keycloak-admin-creds key: password - name: JAVA_OPTS_APPEND value: >- -XX:+UseContainerSupport -XX:MaxRAMPercentage=50.0 -Djava.awt.headless=true -Djgroups.dns.query={{ include \"keycloak.fullname\" . }}-headless # This block should be uncommented if you install Keycloak on Kubernetes ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/affinity : cookie rules : - host : keycloak.<ROOT_DOMAIN> paths : - path : '{{ tpl .Values.http.relativePath $ | trimSuffix \"/\" }}/' pathType : Prefix # This block should be uncommented if you set Keycloak to OpenShift and change the host field # route: # enabled: false # # Path for the Route # path: '/' # # Host name for the Route # host: \"keycloak.<ROOT_DOMAIN>\" # # TLS configuration # tls: # enabled: true resources : limits : memory : \"2048Mi\" requests : cpu : \"50m\" memory : \"512Mi\" # Check database readiness at startup dbchecker : enabled : true database : vendor : postgres existingSecret : keycloak-postgresql hostname : postgresql port : 5432 username : admin database : keycloak Configuration \u2693\ufe0e To prepare Keycloak for integration with EDP, follow the steps below: Ensure that the openshift realm is created. Create a user edp_<EDP_PROJECT> in Master realm. Note This user should be used by EDP to access Keycloak. Please refer to the Install EDP section for details. In the Role Mapping tab, assign the proper roles to user: Realm Roles: create-realm, offline_access, uma_authorization Client Roles openshift-realm : impersonation, manage-authorization, manage-clients, manage-users Role mappings","title":"Install Keycloak"},{"location":"operator-guide/install-keycloak/#install-keycloak","text":"Inspect the prerequisites and the main steps to perform for installing Keycloak. Note The installation process below is given for a Kubernetes cluster. The steps that differ for an OpenShift cluster are indicated in the notes.","title":"Install Keycloak"},{"location":"operator-guide/install-keycloak/#prerequisites","text":"Kubectl version 1.18.0 is installed. Please refer to the Kubernetes official website for details. Helm version 3.6.0 is installed. Please refer to the Helm page on GitHub for details. Note EDP team is using a helm chart from the codecentric repository, but other repositories can be used as well (e.g. Bitnami ). Before installing Keycloak, it is necessary to install a PostgreSQL database . Info It is also possible to install Keycloak using the Helmfile. For details, please refer to the Install via Helmfile page.","title":"Prerequisites"},{"location":"operator-guide/install-keycloak/#postgresql-installation","text":"To install PostgreSQL, follow the steps below: Check that a security namespace is created. If not, run the following command to create it: kubectl create namespace security Note On the OpenShift platform, apply the SecurityContextConstraints resource. Change the namespace in the users section if required. View: keycloak-scc.yaml allowHostDirVolumePlugin : false allowHostIPC : false allowHostNetwork : false allowHostPID : false allowHostPorts : false allowPrivilegeEscalation : true allowPrivilegedContainer : false allowedCapabilities : null apiVersion : security.openshift.io/v1 allowedFlexVolumes : [] defaultAddCapabilities : [] fsGroup : type : MustRunAs ranges : - min : 999 max : 65543 groups : [] kind : SecurityContextConstraints metadata : annotations : \"helm.sh/hook\" : \"pre-install\" name : keycloak priority : 1 readOnlyRootFilesystem : false requiredDropCapabilities : - KILL - MKNOD - SETUID - SETGID runAsUser : type : MustRunAsRange uidRangeMin : 1 uidRangeMax : 65543 seLinuxContext : type : MustRunAs supplementalGroups : type : RunAsAny users : - system:serviceaccount:security:keycloakx - system:serviceaccount:security:default volumes : - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret Add a chart repository: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update Create PostgreSQL admin secret: kubectl -n security create secret generic keycloak-postgresql \\ --from-literal=password=<postgresql_password> \\ --from-literal=postgres-password=<postgresql_postgres_password> Install the custom SecurityContextConstraints (only for OpenShift): Note If you use OpenShift as your deployment platform, add customsecuritycontextconstraints.yaml . View: customsecuritycontextconstraints.yaml allowHostDirVolumePlugin : false allowHostIPC : false allowHostNetwork : false allowHostPID : false allowHostPorts : false allowPrivilegeEscalation : true allowPrivilegedContainer : false allowedCapabilities : null apiVersion : security.openshift.io/v1 allowedFlexVolumes : [] defaultAddCapabilities : [] fsGroup : type : MustRunAs ranges : - min : 999 max : 65543 groups : [] kind : SecurityContextConstraints metadata : annotations : \"helm.sh/hook\" : \"pre-install\" name : keycloak priority : 1 readOnlyRootFilesystem : false requiredDropCapabilities : - KILL - MKNOD - SETUID - SETGID runAsUser : type : MustRunAsRange uidRangeMin : 1 uidRangeMax : 65543 seLinuxContext : type : MustRunAs supplementalGroups : type : RunAsAny users : - system:serviceaccount:security:keycloakx - system:serviceaccount:security:default volumes : - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret Install PostgreSQL v.14.4.0 using bitnami/postgresql Helm chart v.11.6.19: Info The PostgreSQL can be deployed in production ready mode. For example, it may include multiple replicas, persistent storage, autoscaling, and monitoring. For details, please refer to the official Chart documentation . helm install postgresql bitnami/postgresql \\ --version 11.6.19 \\ --values values.yaml \\ --namespace security Check out the values.yaml file sample of the PostgreSQL customization: View: values.yaml # PostgreSQL read only replica parameters readReplicas : # Number of PostgreSQL read only replicas replicaCount : 1 global : postgresql : auth : username : admin existingSecret : keycloak-postgresql database : keycloak primary : persistence : enabled : true size : 3Gi # If the StorageClass with reclaimPolicy: Retain is used, install an additional StorageClass before installing PostgreSQL # (the code is given below). # If the default StorageClass will be used - change \"gp2-retain\" to \"gp2\" storageClass : \"gp2-retain\" Install an additional StorageClass (optional): Note If the PostgreSQL installation uses a StorageClass with reclaimPolicy: Retain , install additional StorageClass storageclass.yaml . View: storageclass.yaml kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : gp2-retain provisioner : kubernetes.io/aws-ebs parameters : fsType : ext4 type : gp2 reclaimPolicy : Retain volumeBindingMode : WaitForFirstConsumer","title":"PostgreSQL Installation"},{"location":"operator-guide/install-keycloak/#keycloak-installation","text":"To install Keycloak, follow the steps below: Use security namespace from the PostgreSQL installation. Add a chart repository: helm repo add codecentric https://codecentric.github.io/helm-charts helm repo update Create Keycloak admin secret: kubectl -n security create secret generic keycloak-admin-creds \\ --from-literal=username=<keycloak_admin_username> \\ --from-literal=password=<keycloak_admin_password> Install Keycloak 19.0.1 using codecentric/keycloakx Helm chart: Info Keycloak can be deployed in production ready mode. For example, it may include multiple replicas, persistent storage, autoscaling, and monitoring. For details, please refer to the official Chart documentation . helm install keycloakx codecentric/keycloakx \\ --version 1.4.2 \\ --values values.yaml \\ --namespace security Check out the values.yaml file sample of the Keycloak customization: View: values.yaml replicas : 1 # Deploy the latest verion image : tag : \"19.0.1\" # start: create OpenShift realm which is required by EDP extraInitContainers : | - name: realm-provider image: busybox imagePullPolicy: IfNotPresent command: - sh args: - -c - | echo '{\"realm\": \"openshift\",\"enabled\": true}' > /opt/keycloak/data/import/openshift.json volumeMounts: - name: realm mountPath: /opt/keycloak/data/import extraVolumeMounts : | - name: realm mountPath: /opt/keycloak/data/import extraVolumes : | - name: realm emptyDir: {} command : - \"/opt/keycloak/bin/kc.sh\" - \"--verbose\" - \"start\" - \"--auto-build\" - \"--http-enabled=true\" - \"--http-port=8080\" - \"--hostname-strict=false\" - \"--hostname-strict-https=false\" - \"--spi-events-listener-jboss-logging-success-level=info\" - \"--spi-events-listener-jboss-logging-error-level=warn\" - \"--import-realm\" extraEnv : | - name: KC_PROXY value: \"passthrough\" - name: KEYCLOAK_ADMIN valueFrom: secretKeyRef: name: keycloak-admin-creds key: username - name: KEYCLOAK_ADMIN_PASSWORD valueFrom: secretKeyRef: name: keycloak-admin-creds key: password - name: JAVA_OPTS_APPEND value: >- -XX:+UseContainerSupport -XX:MaxRAMPercentage=50.0 -Djava.awt.headless=true -Djgroups.dns.query={{ include \"keycloak.fullname\" . }}-headless # This block should be uncommented if you install Keycloak on Kubernetes ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/affinity : cookie rules : - host : keycloak.<ROOT_DOMAIN> paths : - path : '{{ tpl .Values.http.relativePath $ | trimSuffix \"/\" }}/' pathType : Prefix # This block should be uncommented if you set Keycloak to OpenShift and change the host field # route: # enabled: false # # Path for the Route # path: '/' # # Host name for the Route # host: \"keycloak.<ROOT_DOMAIN>\" # # TLS configuration # tls: # enabled: true resources : limits : memory : \"2048Mi\" requests : cpu : \"50m\" memory : \"512Mi\" # Check database readiness at startup dbchecker : enabled : true database : vendor : postgres existingSecret : keycloak-postgresql hostname : postgresql port : 5432 username : admin database : keycloak","title":"Keycloak Installation"},{"location":"operator-guide/install-keycloak/#configuration","text":"To prepare Keycloak for integration with EDP, follow the steps below: Ensure that the openshift realm is created. Create a user edp_<EDP_PROJECT> in Master realm. Note This user should be used by EDP to access Keycloak. Please refer to the Install EDP section for details. In the Role Mapping tab, assign the proper roles to user: Realm Roles: create-realm, offline_access, uma_authorization Client Roles openshift-realm : impersonation, manage-authorization, manage-clients, manage-users Role mappings","title":"Configuration"},{"location":"operator-guide/install-kiosk/","text":"Set Up Kiosk \u2693\ufe0e Kiosk is a multi-tenancy extension for managing tenants and namespaces in a shared Kubernetes cluster. Within EDP, Kiosk is used to separate resources and enables the following options (see more details ): Access to the EDP tenants in a Kubernetes cluster; Multi-tenancy access at the service account level for application deploy. Inspect the main steps to set up Kiosk for the proceeding EDP installation. Note Kiosk deploy is mandatory for EDP v.2.8.. In earlier versions, Kiosk is not implemented. Since EDP v.2.9.0, integration with Kiosk is an optional feature. You may not want to use it, so just skip those steps and disable in Helm parameters during EDP deploy . # global.kioskEnabled: <true/false> Prerequisites \u2693\ufe0e Kubectl version 1.18.0 is installed. Please refer to the Kubernetes official website for details. Helm version 3.6.0 is installed. Please refer to the Helm page on GitHub for details. Installation \u2693\ufe0e Deploy Kiosk version 0.2.11 in the cluster. To install it, run the following command: # Install kiosk with helm v3 helm repo add kiosk https://charts.devspace.sh/ kubectl create namespace kiosk helm install kiosk --version 0.2.11 kiosk/kiosk -n kiosk --atomic For more details, please refer to the Kiosk page on the GitHub. Configuration \u2693\ufe0e To provide access to the EDP tenant, follow the steps below. Check that a security namespace is created. If not, run the following command to create it: kubectl create namespace security Note On an OpenShift cluster, run the oc command instead of kubectl one. Add a service account to the security namespace. kubectl -n security create sa <edp-project> Info \u2039edp-project\u203a is the name of the EDP tenant here and in all the following steps. Apply the Account template to the cluster. Please check the sample below: apiVersion : tenancy.kiosk.sh/v1alpha1 kind : Account metadata : name : <edp-project>-admin spec : space : clusterRole : kiosk-space-admin subjects : - kind : ServiceAccount name : <edp-project> namespace : security Apply the ClusterRoleBinding to the 'kiosk-edit' cluster role (current role is added during installation of Kiosk). Please check the sample below: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : <edp-project>-kiosk-edit subjects : - kind : ServiceAccount name : <edp-project> namespace : security roleRef : kind : ClusterRole name : kiosk-edit apiGroup : rbac.authorization.k8s.io To provide access to the EDP tenant, generate kubeconfig with Service Account \u2039edp-project\u203a permission. The \u2039edp-project\u203a account created earlier is located in the security namespace.","title":"Set Up Kiosk"},{"location":"operator-guide/install-kiosk/#set-up-kiosk","text":"Kiosk is a multi-tenancy extension for managing tenants and namespaces in a shared Kubernetes cluster. Within EDP, Kiosk is used to separate resources and enables the following options (see more details ): Access to the EDP tenants in a Kubernetes cluster; Multi-tenancy access at the service account level for application deploy. Inspect the main steps to set up Kiosk for the proceeding EDP installation. Note Kiosk deploy is mandatory for EDP v.2.8.. In earlier versions, Kiosk is not implemented. Since EDP v.2.9.0, integration with Kiosk is an optional feature. You may not want to use it, so just skip those steps and disable in Helm parameters during EDP deploy . # global.kioskEnabled: <true/false>","title":"Set Up Kiosk"},{"location":"operator-guide/install-kiosk/#prerequisites","text":"Kubectl version 1.18.0 is installed. Please refer to the Kubernetes official website for details. Helm version 3.6.0 is installed. Please refer to the Helm page on GitHub for details.","title":"Prerequisites"},{"location":"operator-guide/install-kiosk/#installation","text":"Deploy Kiosk version 0.2.11 in the cluster. To install it, run the following command: # Install kiosk with helm v3 helm repo add kiosk https://charts.devspace.sh/ kubectl create namespace kiosk helm install kiosk --version 0.2.11 kiosk/kiosk -n kiosk --atomic For more details, please refer to the Kiosk page on the GitHub.","title":"Installation"},{"location":"operator-guide/install-kiosk/#configuration","text":"To provide access to the EDP tenant, follow the steps below. Check that a security namespace is created. If not, run the following command to create it: kubectl create namespace security Note On an OpenShift cluster, run the oc command instead of kubectl one. Add a service account to the security namespace. kubectl -n security create sa <edp-project> Info \u2039edp-project\u203a is the name of the EDP tenant here and in all the following steps. Apply the Account template to the cluster. Please check the sample below: apiVersion : tenancy.kiosk.sh/v1alpha1 kind : Account metadata : name : <edp-project>-admin spec : space : clusterRole : kiosk-space-admin subjects : - kind : ServiceAccount name : <edp-project> namespace : security Apply the ClusterRoleBinding to the 'kiosk-edit' cluster role (current role is added during installation of Kiosk). Please check the sample below: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : <edp-project>-kiosk-edit subjects : - kind : ServiceAccount name : <edp-project> namespace : security roleRef : kind : ClusterRole name : kiosk-edit apiGroup : rbac.authorization.k8s.io To provide access to the EDP tenant, generate kubeconfig with Service Account \u2039edp-project\u203a permission. The \u2039edp-project\u203a account created earlier is located in the security namespace.","title":"Configuration"},{"location":"operator-guide/install-loki/","text":"Install Grafana Loki \u2693\ufe0e EDP configures the logging with the help of Grafana Loki aggregation system. Installation \u2693\ufe0e To install Loki, follow the steps below: Create logging namespace: kubectl create namespace logging Note On the OpenShift cluster, run the oc command instead of the kubectl command. Add a chart repository: helm repo add grafana https://grafana.github.io/helm-charts helm repo update Note It is possible to use Amazon Simple Storage Service Amazon S3 as an object storage for Loki. To configure access, please refer to the IRSA for Loki documentation. Install Loki v.2.6.0 : helm install loki grafana/loki \\ --version 2.6.0 \\ --values values.yaml \\ --namespace logging Check out the values.yaml file sample of the Loki customization: View: values.yaml image : repository : grafana/loki tag : 2.3.0 config : auth_enabled : false schema_config : configs : - from : 2021-06-01 store : boltdb-shipper object_store : s3 schema : v11 index : prefix : loki_index_ period : 24h storage_config : aws : s3 : s3://<AWS_REGION>/loki-<CLUSTER_NAME> boltdb_shipper : active_index_directory : /data/loki/index cache_location : /data/loki/boltdb-cache shared_store : s3 chunk_store_config : max_look_back_period : 24h resources : limits : memory : \"128Mi\" requests : cpu : \"50m\" memory : \"128Mi\" serviceAccount : create : true name : edp-loki annotations : eks.amazonaws.com/role-arn : \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/AWSIRSA\u2039CLUSTER_NAME\u203a\u2039LOKI_NAMESPACE\u203aLoki persistence: enabled: false Note In case of using cluster scheduling and amazon-eks-pod-identity-webhook , it is necessary to restart the Loki pod after the cluster is up and running. Please refer to the Schedule Pods Restart documentation. Configure custom bucket policy to delete the old data.","title":"Install Grafana Loki"},{"location":"operator-guide/install-loki/#install-grafana-loki","text":"EDP configures the logging with the help of Grafana Loki aggregation system.","title":"Install Grafana Loki"},{"location":"operator-guide/install-loki/#installation","text":"To install Loki, follow the steps below: Create logging namespace: kubectl create namespace logging Note On the OpenShift cluster, run the oc command instead of the kubectl command. Add a chart repository: helm repo add grafana https://grafana.github.io/helm-charts helm repo update Note It is possible to use Amazon Simple Storage Service Amazon S3 as an object storage for Loki. To configure access, please refer to the IRSA for Loki documentation. Install Loki v.2.6.0 : helm install loki grafana/loki \\ --version 2.6.0 \\ --values values.yaml \\ --namespace logging Check out the values.yaml file sample of the Loki customization: View: values.yaml image : repository : grafana/loki tag : 2.3.0 config : auth_enabled : false schema_config : configs : - from : 2021-06-01 store : boltdb-shipper object_store : s3 schema : v11 index : prefix : loki_index_ period : 24h storage_config : aws : s3 : s3://<AWS_REGION>/loki-<CLUSTER_NAME> boltdb_shipper : active_index_directory : /data/loki/index cache_location : /data/loki/boltdb-cache shared_store : s3 chunk_store_config : max_look_back_period : 24h resources : limits : memory : \"128Mi\" requests : cpu : \"50m\" memory : \"128Mi\" serviceAccount : create : true name : edp-loki annotations : eks.amazonaws.com/role-arn : \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/AWSIRSA\u2039CLUSTER_NAME\u203a\u2039LOKI_NAMESPACE\u203aLoki persistence: enabled: false Note In case of using cluster scheduling and amazon-eks-pod-identity-webhook , it is necessary to restart the Loki pod after the cluster is up and running. Please refer to the Schedule Pods Restart documentation. Configure custom bucket policy to delete the old data.","title":"Installation"},{"location":"operator-guide/install-reportportal/","text":"Install ReportPortal \u2693\ufe0e Inspect the prerequisites and the main steps to perform for installing ReportPortal. Info It is also possible to install ReportPortal using the Helmfile. For details, please refer to the Install via Helmfile page. Prerequisites \u2693\ufe0e Kubectl version 1.20.0 is installed. Please refer to the Kubernetes official website for details. Helm version 3.9.2 is installed. Please refer to the Helm page on GitHub for details. Helm-git plugin version 0.11.4 is installed. Please refer to the GitHub page for details. Info Please refer to the ReportPortal Helm Chart section for details. MinIO Installation \u2693\ufe0e To install MinIO, follow the steps below: Check that <edp-project> namespace is created. If not, run the following command to create it: kubectl create namespace <edp-project> Add a chart repository: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update Create MinIO admin secret: kubectl -n <edp-project> create secret generic reportportal-minio-creds \\ --from-literal = root-password = <root_password> \\ --from-literal = root-user = <root_user> Install MinIO v.11.10.3 using bitnami/minio Helm chart v.11.10.3: helm install minio bitnami/minio \\ --version 11 .10.3 \\ --values values.yaml \\ --namespace <edp-project> Check out the values.yaml file sample of the MinIO customization: View: values.yaml auth : existingSecret : reportportal-minio-creds persistence : size : 1Gi RabbitMQ Installation \u2693\ufe0e To install RabbitMQ, follow the steps below: Use <edp-project> namespace from the MinIO installation. Use bitnami chart repository from the MinIO installation. Create RabbitMQ admin secret: kubectl -n <edp-project> create secret generic reportportal-rabbitmq-creds \\ --from-literal = rabbitmq-password = <rabbitmq_password> \\ --from-literal = rabbitmq-erlang-cookie = <rabbitmq_erlang_cookie> Warning The rabbitmq_password password must be 10 characters long. The rabbitmq_erlang_cookie password must be 32 characters long. Install RabbitMQ v.10.3.8 using bitnami/rabbitmq Helm chart v.10.3.8: helm install rabbitmq bitnami/rabbitmq \\ --version 10 .3.8 \\ --values values.yaml \\ --namespace <edp-project> Check out the values.yaml file sample of the RabbitMQ customization: View: values.yaml auth : existingPasswordSecret : reportportal-rabbitmq-creds existingErlangSecret : reportportal-rabbitmq-creds persistence : size : 1Gi After the rabbitmq pod gets the status Running, you need to configure the RabbitMQ memory threshold kubectl -n <edp-project> exec -it rabbitmq-0 -- rabbitmqctl set_vm_memory_high_watermark 0 .8 Elasticsearch Installation \u2693\ufe0e To install Elasticsearch, follow the steps below: Use <edp-project> namespace from the MinIO installation. Add a chart repository: helm repo add elastic https://helm.elastic.co helm repo update Install Elasticsearch v.7.17.3 using elastic/elasticsearch Helm chart v.7.17.3: helm install elasticsearch elastic/elasticsearch \\ --version 7 .17.3 \\ --values values.yaml \\ --namespace <edp-project> Check out the values.yaml file sample of the Elasticsearch customization: View: values.yaml replicas : 1 extraEnvs : - name : discovery.type value : single-node - name : cluster.initial_master_nodes value : \"\" resources : requests : cpu : \"100m\" memory : \"2Gi\" volumeClaimTemplate : resources : requests : storage : 3Gi PostgreSQL Installation \u2693\ufe0e To install PostgreSQL, follow the steps below: Use <edp-project> namespace from the MinIO installation. Add a chart repository: helm repo add bitnami-archive https://raw.githubusercontent.com/bitnami/charts/archive-full-index/bitnami helm repo update Create PostgreSQL admin secret: kubectl -n <edp-project> create secret generic reportportal-postgresql-creds \\ --from-literal = postgresql-password = <postgresql_password> \\ --from-literal = postgresql-postgres-password = <postgresql_postgres_password> Warning The postgresql_password and postgresql_postgres_password passwords must be 16 characters long. Install PostgreSQL v.10.9.4 using Helm chart v.10.9.4: helm install postgresql bitnami-archive/postgresql \\ --version 10 .9.4 \\ --values values.yaml \\ --namespace <edp-project> Check out the values.yaml file sample of the PostgreSQL customization: View: values.yaml persistence : size : 1Gi resources : requests : cpu : \"100m\" postgresqlUsername : \"rpuser\" postgresqlDatabase : \"reportportal\" existingSecret : \"reportportal-postgresql-creds\" initdbScripts : init_postgres.sh : | #!/bin/sh /opt/bitnami/postgresql/bin/psql -U postgres -d ${POSTGRES_DB} -c 'CREATE EXTENSION IF NOT EXISTS ltree; CREATE EXTENSION IF NOT EXISTS pgcrypto; CREATE EXTENSION IF NOT EXISTS pg_trgm;' ReportPortal Installation \u2693\ufe0e To install ReportPortal, follow the steps below: Use <edp-project> namespace from the MinIO installation. Add a chart repository: helm repo add report-portal \"git+https://github.com/reportportal/kubernetes@reportportal?ref=master\" helm repo update Install ReportPortal v.5.7.2 using Helm chart v.5.7.2: helm install report-portal report-portal/reportportal \\ --values values.yaml \\ --namespace <edp-project> Check out the values.yaml file sample of the ReportPortal customization: View: values.yaml serviceindex : resources : requests : cpu : 50m uat : resources : requests : cpu : 50m serviceui : resources : requests : cpu : 50m serviceapi : resources : requests : cpu : 50m serviceanalyzer : resources : requests : cpu : 50m serviceanalyzertrain : resources : requests : cpu : 50m rabbitmq : SecretName : \"reportportal-rabbitmq-creds\" endpoint : address : rabbitmq.<EDP_PROJECT>.svc.cluster.local user : user apiuser : user postgresql : SecretName : \"reportportal-postgresql-creds\" endpoint : address : postgresql.<EDP_PROJECT>.svc.cluster.local elasticsearch : endpoint : http://elasticsearch-master.<EDP_PROJECT>.svc.cluster.local:9200 minio : secretName : \"reportportal-minio-creds\" endpoint : http://minio.<EDP_PROJECT>.svc.cluster.local:9000 endpointshort : minio.<EDP_PROJECT>.svc.cluster.local:9000 accesskeyName : \"root-user\" secretkeyName : \"root-password\" ingress : # IF YOU HAVE SOME DOMAIN NAME SET INGRESS.USEDOMAINNAME to true usedomainname : true hosts : - report-portal-<EDP_PROJECT>.<ROOT_DOMAIN> Related Articles \u2693\ufe0e Install via Helmfile","title":"Install ReportPortal"},{"location":"operator-guide/install-reportportal/#install-reportportal","text":"Inspect the prerequisites and the main steps to perform for installing ReportPortal. Info It is also possible to install ReportPortal using the Helmfile. For details, please refer to the Install via Helmfile page.","title":"Install ReportPortal"},{"location":"operator-guide/install-reportportal/#prerequisites","text":"Kubectl version 1.20.0 is installed. Please refer to the Kubernetes official website for details. Helm version 3.9.2 is installed. Please refer to the Helm page on GitHub for details. Helm-git plugin version 0.11.4 is installed. Please refer to the GitHub page for details. Info Please refer to the ReportPortal Helm Chart section for details.","title":"Prerequisites"},{"location":"operator-guide/install-reportportal/#minio-installation","text":"To install MinIO, follow the steps below: Check that <edp-project> namespace is created. If not, run the following command to create it: kubectl create namespace <edp-project> Add a chart repository: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update Create MinIO admin secret: kubectl -n <edp-project> create secret generic reportportal-minio-creds \\ --from-literal = root-password = <root_password> \\ --from-literal = root-user = <root_user> Install MinIO v.11.10.3 using bitnami/minio Helm chart v.11.10.3: helm install minio bitnami/minio \\ --version 11 .10.3 \\ --values values.yaml \\ --namespace <edp-project> Check out the values.yaml file sample of the MinIO customization: View: values.yaml auth : existingSecret : reportportal-minio-creds persistence : size : 1Gi","title":"MinIO Installation"},{"location":"operator-guide/install-reportportal/#rabbitmq-installation","text":"To install RabbitMQ, follow the steps below: Use <edp-project> namespace from the MinIO installation. Use bitnami chart repository from the MinIO installation. Create RabbitMQ admin secret: kubectl -n <edp-project> create secret generic reportportal-rabbitmq-creds \\ --from-literal = rabbitmq-password = <rabbitmq_password> \\ --from-literal = rabbitmq-erlang-cookie = <rabbitmq_erlang_cookie> Warning The rabbitmq_password password must be 10 characters long. The rabbitmq_erlang_cookie password must be 32 characters long. Install RabbitMQ v.10.3.8 using bitnami/rabbitmq Helm chart v.10.3.8: helm install rabbitmq bitnami/rabbitmq \\ --version 10 .3.8 \\ --values values.yaml \\ --namespace <edp-project> Check out the values.yaml file sample of the RabbitMQ customization: View: values.yaml auth : existingPasswordSecret : reportportal-rabbitmq-creds existingErlangSecret : reportportal-rabbitmq-creds persistence : size : 1Gi After the rabbitmq pod gets the status Running, you need to configure the RabbitMQ memory threshold kubectl -n <edp-project> exec -it rabbitmq-0 -- rabbitmqctl set_vm_memory_high_watermark 0 .8","title":"RabbitMQ Installation"},{"location":"operator-guide/install-reportportal/#elasticsearch-installation","text":"To install Elasticsearch, follow the steps below: Use <edp-project> namespace from the MinIO installation. Add a chart repository: helm repo add elastic https://helm.elastic.co helm repo update Install Elasticsearch v.7.17.3 using elastic/elasticsearch Helm chart v.7.17.3: helm install elasticsearch elastic/elasticsearch \\ --version 7 .17.3 \\ --values values.yaml \\ --namespace <edp-project> Check out the values.yaml file sample of the Elasticsearch customization: View: values.yaml replicas : 1 extraEnvs : - name : discovery.type value : single-node - name : cluster.initial_master_nodes value : \"\" resources : requests : cpu : \"100m\" memory : \"2Gi\" volumeClaimTemplate : resources : requests : storage : 3Gi","title":"Elasticsearch Installation"},{"location":"operator-guide/install-reportportal/#postgresql-installation","text":"To install PostgreSQL, follow the steps below: Use <edp-project> namespace from the MinIO installation. Add a chart repository: helm repo add bitnami-archive https://raw.githubusercontent.com/bitnami/charts/archive-full-index/bitnami helm repo update Create PostgreSQL admin secret: kubectl -n <edp-project> create secret generic reportportal-postgresql-creds \\ --from-literal = postgresql-password = <postgresql_password> \\ --from-literal = postgresql-postgres-password = <postgresql_postgres_password> Warning The postgresql_password and postgresql_postgres_password passwords must be 16 characters long. Install PostgreSQL v.10.9.4 using Helm chart v.10.9.4: helm install postgresql bitnami-archive/postgresql \\ --version 10 .9.4 \\ --values values.yaml \\ --namespace <edp-project> Check out the values.yaml file sample of the PostgreSQL customization: View: values.yaml persistence : size : 1Gi resources : requests : cpu : \"100m\" postgresqlUsername : \"rpuser\" postgresqlDatabase : \"reportportal\" existingSecret : \"reportportal-postgresql-creds\" initdbScripts : init_postgres.sh : | #!/bin/sh /opt/bitnami/postgresql/bin/psql -U postgres -d ${POSTGRES_DB} -c 'CREATE EXTENSION IF NOT EXISTS ltree; CREATE EXTENSION IF NOT EXISTS pgcrypto; CREATE EXTENSION IF NOT EXISTS pg_trgm;'","title":"PostgreSQL Installation"},{"location":"operator-guide/install-reportportal/#reportportal-installation","text":"To install ReportPortal, follow the steps below: Use <edp-project> namespace from the MinIO installation. Add a chart repository: helm repo add report-portal \"git+https://github.com/reportportal/kubernetes@reportportal?ref=master\" helm repo update Install ReportPortal v.5.7.2 using Helm chart v.5.7.2: helm install report-portal report-portal/reportportal \\ --values values.yaml \\ --namespace <edp-project> Check out the values.yaml file sample of the ReportPortal customization: View: values.yaml serviceindex : resources : requests : cpu : 50m uat : resources : requests : cpu : 50m serviceui : resources : requests : cpu : 50m serviceapi : resources : requests : cpu : 50m serviceanalyzer : resources : requests : cpu : 50m serviceanalyzertrain : resources : requests : cpu : 50m rabbitmq : SecretName : \"reportportal-rabbitmq-creds\" endpoint : address : rabbitmq.<EDP_PROJECT>.svc.cluster.local user : user apiuser : user postgresql : SecretName : \"reportportal-postgresql-creds\" endpoint : address : postgresql.<EDP_PROJECT>.svc.cluster.local elasticsearch : endpoint : http://elasticsearch-master.<EDP_PROJECT>.svc.cluster.local:9200 minio : secretName : \"reportportal-minio-creds\" endpoint : http://minio.<EDP_PROJECT>.svc.cluster.local:9000 endpointshort : minio.<EDP_PROJECT>.svc.cluster.local:9000 accesskeyName : \"root-user\" secretkeyName : \"root-password\" ingress : # IF YOU HAVE SOME DOMAIN NAME SET INGRESS.USEDOMAINNAME to true usedomainname : true hosts : - report-portal-<EDP_PROJECT>.<ROOT_DOMAIN>","title":"ReportPortal Installation"},{"location":"operator-guide/install-reportportal/#related-articles","text":"Install via Helmfile","title":"Related Articles"},{"location":"operator-guide/install-velero/","text":"Install Velero \u2693\ufe0e Velero is an open source tool to safely back up, recover, and migrate Kubernetes clusters and persistent volumes. It works both on premises and in a public cloud. Velero consists of a server process running as a deployment in your Kubernetes cluster and a command-line interface (CLI) with which DevOps teams and platform operators configure scheduled backups, trigger ad-hoc backups, perform restores, and more. Installation \u2693\ufe0e To install Velero, follow the steps below: Create velero namespace: kubectl create namespace velero Note On an OpenShift cluster, run the oc command instead of kubectl one. Add a chart repository: helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts helm repo update Note Velero AWS Plugin requires access to AWS resources. To configure access, please refer to the IRSA for Velero documentation. Install Velero v.2.14.13 : helm install velero vmware-tanzu/velero \\ --version 2.14.13 \\ --values values.yaml \\ --namespace velero Check out the values.yaml file sample of the Velero customization: View: values.yaml image : repository : velero/velero tag : v1.5.3 securityContext : fsGroup : 65534 restic : securityContext : fsGroup : 65534 serviceAccount : server : create : true name : edp-velero annotations : eks.amazonaws.com/role-arn : \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/AWSIRSA\u2039CLUSTER_NAME\u203a\u2039VELERO_NAMESPACE\u203aVelero\" credentials : useSecret : false configuration : provider : aws backupStorageLocation : name : default bucket : velero-<CLUSTER_NAME> config : region : eu-central-1 volumeSnapshotLocation : name : default config : region : <AWS_REGION> initContainers : - name : velero-plugin-for-aws image : velero/velero-plugin-for-aws:v1.1.0 volumeMounts : - mountPath : /target name : plugins Note In case of using cluster scheduling and amazon-eks-pod-identity-webhook , it is necessary to restart the Velero pod after the cluster is up and running. Please refer to the Schedule Pods Restart documentation. Install the client side (velero cli) according to the official documentation . Configuration \u2693\ufe0e Create backup for all components in the namespace: velero backup create < BACKUP_NAME > -- include - namespaces < NAMESPACE > Create a daily backup of the namespace: velero schedule create < BACKUP_NAME > -- schedule \" 0 10 * * MON-FRI \" -- include - namespaces < NAMESPACE > -- ttl 120 h0m0s To restore from backup, use the following command: velero restore create <RESTORE_NAME> --from-backup <BACKUP_NAME>","title":"Install Velero"},{"location":"operator-guide/install-velero/#install-velero","text":"Velero is an open source tool to safely back up, recover, and migrate Kubernetes clusters and persistent volumes. It works both on premises and in a public cloud. Velero consists of a server process running as a deployment in your Kubernetes cluster and a command-line interface (CLI) with which DevOps teams and platform operators configure scheduled backups, trigger ad-hoc backups, perform restores, and more.","title":"Install Velero"},{"location":"operator-guide/install-velero/#installation","text":"To install Velero, follow the steps below: Create velero namespace: kubectl create namespace velero Note On an OpenShift cluster, run the oc command instead of kubectl one. Add a chart repository: helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts helm repo update Note Velero AWS Plugin requires access to AWS resources. To configure access, please refer to the IRSA for Velero documentation. Install Velero v.2.14.13 : helm install velero vmware-tanzu/velero \\ --version 2.14.13 \\ --values values.yaml \\ --namespace velero Check out the values.yaml file sample of the Velero customization: View: values.yaml image : repository : velero/velero tag : v1.5.3 securityContext : fsGroup : 65534 restic : securityContext : fsGroup : 65534 serviceAccount : server : create : true name : edp-velero annotations : eks.amazonaws.com/role-arn : \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/AWSIRSA\u2039CLUSTER_NAME\u203a\u2039VELERO_NAMESPACE\u203aVelero\" credentials : useSecret : false configuration : provider : aws backupStorageLocation : name : default bucket : velero-<CLUSTER_NAME> config : region : eu-central-1 volumeSnapshotLocation : name : default config : region : <AWS_REGION> initContainers : - name : velero-plugin-for-aws image : velero/velero-plugin-for-aws:v1.1.0 volumeMounts : - mountPath : /target name : plugins Note In case of using cluster scheduling and amazon-eks-pod-identity-webhook , it is necessary to restart the Velero pod after the cluster is up and running. Please refer to the Schedule Pods Restart documentation. Install the client side (velero cli) according to the official documentation .","title":"Installation"},{"location":"operator-guide/install-velero/#configuration","text":"Create backup for all components in the namespace: velero backup create < BACKUP_NAME > -- include - namespaces < NAMESPACE > Create a daily backup of the namespace: velero schedule create < BACKUP_NAME > -- schedule \" 0 10 * * MON-FRI \" -- include - namespaces < NAMESPACE > -- ttl 120 h0m0s To restore from backup, use the following command: velero restore create <RESTORE_NAME> --from-backup <BACKUP_NAME>","title":"Configuration"},{"location":"operator-guide/install-via-helmfile/","text":"Install via Helmfile \u2693\ufe0e This article provides the instruction on how to deploy EDP and components in Kubernetes using Helmfile that is intended for deploying Helm charts. Helmfile templates are available in GitHub repository . Prerequisites \u2693\ufe0e Helm version 3.6.0+ is installed. Please refer to the Helm page on GitHub for details. Helmfile version 0.142.0 is installed. Please refer to the GitHub page for details. Helm diff plugin version 3.5.0 is installed. Please refer to the GitHub page for details. Helm-git plugin version 0.11.4 is installed. Please refer to the GitHub page for details. Helmfile Structure \u2693\ufe0e The envs/common.yaml file contains the specification for environments pattern, list of helm repositories from which it is necessary to fetch the helm charts and additional Helm parameters. The envs/platform.yaml file contains global parameters that are used in various Helmfiles. The releases/envs/ contains symbol links to environments files. The releases/*.yaml file contains description of parameters that is used when deploying a Helm chart. The helmfile.yaml file defines components to be installed by defining a path to Helm releases files. The envs/ci.yaml file contains stub parameters for CI linter. The test/lint-ci.sh script for running CI linter with debug loglevel and stub parameters. The resources/*.yaml file contains additional resources for the OpenShift platform. Operate Helmfile \u2693\ufe0e Before applying the Helmfile, please fill in the global parameters in the envs/platform.yaml (check the examples in the envs/ci.yaml ) and releases/*.yaml files for every Helm deploy. Pay attention to the following recommendations while working with the Helmfile: To launch Lint, run the test/lint-ci.sh script. Display the difference between the deployed and environment state ( helm diff ): helmfile --environment platform -f helmfile.yaml diff Apply the deployment: helmfile --selector component=ingress --environment platform -f helmfile.yaml apply Modify the deployment and apply the changes: helmfile --selector component=ingress --environment platform -f helmfile.yaml sync To deploy the components according to the label, use the selector to target a subset of releases when running the Helmfile. It can be useful for large Helmfiles with the releases that are logically grouped together. For example, to display the difference only for the nginx-ingress file, use the following command: helmfile --selector component=ingress --environment platform -f helmfile.yaml diff To destroy the release, run the following command: helmfile --selector component=ingress --environment platform -f helmfile.yaml destroy Deploy Components \u2693\ufe0e Using the Helmfile, the following components can be installed: NGINX Ingress Controller Keycloak EPAM Delivery Platform Argo CD External Secrets Operator DefectDojo ReportPortal Deploy NGINX Ingress Controller \u2693\ufe0e Info Skip this step for the OpenShift platform, because it has its own Ingress Controller. To install NGINX Ingress controller, follow the steps below: In the releases/nginx-ingress.yaml file, set the proxy-real-ip-cidr parameter according to the value with AWS VPC IPv4 CIDR. Install NGINX Ingress controller: helmfile --selector component=ingress --environment platform -f helmfile.yaml apply Deploy Keycloak \u2693\ufe0e Keycloak requires a database deployment, so it has two charts: releases/keycloak.yaml and releases/postgresql-keycloak.yaml . To install Keycloak, follow the steps below: Create a security namespace: Note For the OpenShift users: This namespace is also indicated as users in the following custom SecurityContextConstraints resources: resources/keycloak-scc.yaml and resources/postgresql-keycloak-scc.yaml . Change the namespace name when using a custom namespace. kubectl create namespace security Create PostgreSQL admin secret: kubectl -n security create secret generic keycloak-postgresql \\ --from-literal=password=<postgresql_password> \\ --from-literal=postgres-password=<postgresql_postgres_password> In the envs/platform.yaml file, set the dnsWildCard parameter. Create Keycloak admin secret: kubectl -n security create secret generic keycloak-admin-creds \\ --from-literal=username=<keycloak_admin_username> \\ --from-literal=password=<keycloak_admin_password> Install Keycloak: helmfile --selector component=sso --environment platform -f helmfile.yaml apply Deploy EPAM Delivery Platform \u2693\ufe0e To install EDP, follow the steps below: Create a platform namespace: kubectl create namespace platform Create a secret for administrative access to the database: kubectl -n platform create secret generic super-admin-db \\ --from-literal=username=<super_admin_db_username> \\ --from-literal=password=<super_admin_db_password> Warning Do not use the admin username here since the admin is a reserved name. Create a secret for an EDP tenant database user: kubectl -n platform create secret generic db-admin-console \\ --from-literal=username=<tenant_db_username> \\ --from-literal=password=<tenant_db_password> Warning Do not use the admin username here since the admin is a reserved name. For EDP, it is required to have Keycloak access to perform the integration. Create a secret with the user and password provisioned in the step 2 of the Keycloak Configuration section. kubectl -n platform create secret generic keycloak \\ --from-literal=username=<username> \\ --from-literal=password=<password> In the envs/platform.yaml file, set the edpName and keycloakEndpoint parameters. In the releases/edp-install.yaml file, check and fill in all values. Install EDP: helmfile --selector component=edp --environment platform -f helmfile.yaml apply Deploy Argo CD \u2693\ufe0e To install Argo CD, follow the steps below: Install Argo CD: Note For the OpenShift users: When using a custom namespace for ArgoCD, the argocd namespace is also indicated as users in the resources/argocd-scc.yaml custom SecurityContextConstraints resource. Change it there as well. helmfile --selector component = argocd --environment platform -f helmfile.yaml apply Update the argocd-secret secret (in the Argo CD namespace) by providing the correct Keycloak client secret ( oidc.keycloak.clientSecret ) with the value from the keycloak-client-argocd-secret secret in EDP namespace, and restart the deployment: ARGOCD_CLIENT = $( kubectl -n platform get secret keycloak-client-argocd-secret -o jsonpath = '{.data.clientSecret}' ) kubectl -n argocd patch secret argocd-secret -p = \"{\\\"data\\\":{\\\"oidc.keycloak.clientSecret\\\": \\\" ${ ARGOCD_CLIENT } \\\"}}\" -v = 1 kubectl -n argocd rollout restart deployment argo-argocd-server Deploy External Secrets Operator \u2693\ufe0e To install External Secrets Operator, follow the steps below: helmfile --selector component = secrets --environment platform -f helmfile.yaml apply Deploy DefectDojo \u2693\ufe0e Info It is also possible to install DefectDojo via Helm Chart. For details, please refer to the Install DefectDojo page. To install DefectDojo via Helmfile, follow the steps below: Create a DefectDojo namespace: Note For the OpenShift users: This namespace is also indicated as users in the resources/defectdojo-route.yaml custom SecurityContextConstraints resource. Change it when using a custom namespace. Also, change the namespace in the resources/defectdojo-route.yaml file. kubectl create namespace defectdojo Modify the host in resources/defectdojo-route.yaml (only for OpenShift). Create a PostgreSQL admin secret: kubectl -n defectdojo create secret generic defectdojo-postgresql-specific \\ --from-literal = postgresql-password = <postgresql_password> \\ --from-literal = postgresql-postgres-password = <postgresql_postgres_password> Note The postgresql_password and postgresql_postgres_password passwords must be 16 characters long. Create a RabbitMQ admin secret: kubectl -n defectdojo create secret generic defectdojo-rabbitmq-specific \\ --from-literal = rabbitmq-password = <rabbitmq_password> \\ --from-literal = rabbitmq-erlang-cookie = <rabbitmq_erlang_cookie> Note The rabbitmq_password password must be 10 characters long. The rabbitmq_erlang_cookie password must be 32 characters long. Create a DefectDojo admin secret: kubectl -n defectdojo create secret generic defectdojo \\ --from-literal = DD_ADMIN_PASSWORD = <dd_admin_password> \\ --from-literal = DD_SECRET_KEY = <dd_secret_key> \\ --from-literal = DD_CREDENTIAL_AES_256_KEY = <dd_credential_aes_256_key> \\ --from-literal = METRICS_HTTP_AUTH_PASSWORD = <metric_http_auth_password> Note The dd_admin_password password must be 22 characters long. The dd_secret_key password must be 128 characters long. The dd_credential_aes_256_key password must be 128 characters long. The metric_http_auth_password password must be 32 characters long. In the envs/platform.yaml file, set the dnsWildCard parameter. Install DefectDojo: helmfile --selector component = defectdojo --environment platform -f helmfile.yaml apply Deploy ReportPortal \u2693\ufe0e Info It is also possible to install ReportPortal via Helm Chart. For details, please refer to the Install ReportPortal page. ReportPortal requires third-party deployments: RabbitMQ, ElasticSearch, PostgreSQL, MinIO. To install third-party resources, follow the steps below: Create a RabbitMQ admin secret: kubectl -n platform create secret generic reportportal-rabbitmq-creds \\ --from-literal = rabbitmq-password = <rabbitmq_password> \\ --from-literal = rabbitmq-erlang-cookie = <rabbitmq_erlang_cookie> Warning The rabbitmq_password password must be 10 characters long. The rabbitmq_erlang_cookie password must be 32 characters long. Create a PostgreSQL admin secret: kubectl -n platform create secret generic reportportal-postgresql-creds \\ --from-literal = postgresql-password = <postgresql_password> \\ --from-literal = postgresql-postgres-password = <postgresql_postgres_password> Warning The postgresql_password and postgresql_postgres_password passwords must be 16 characters long. Create a MinIO admin secret: kubectl -n platform create secret generic reportportal-minio-creds \\ --from-literal = root-password = <root_password> \\ --from-literal = root-user = <root_user> In the envs/platform.yaml file, set the dnsWildCard and edpName parameters. Install third-party resources: helmfile --selector component = report-portal-third-party-resources --environment platform -f helmfile.yaml apply After the rabbitmq pod gets the status Running, you need to configure the RabbitMQ memory threshold kubectl -n platform exec -it rabbitmq-0 -- rabbitmqctl set_vm_memory_high_watermark 0 .8 To install ReportPortal via Helmfile, follow the steps below: helmfile --selector component = report-portal --environment platform -f helmfile.yaml apply Note For user access: default/1q2w3e For admin access: superadmin/erebus Please refer to the ReportPortal.io page for details. Related Articles \u2693\ufe0e Install EDP Install NGINX Ingress Controller Install Keycloak Install DefectDojo Install ReportPortal Install Argo CD","title":"Install via Helmfile"},{"location":"operator-guide/install-via-helmfile/#install-via-helmfile","text":"This article provides the instruction on how to deploy EDP and components in Kubernetes using Helmfile that is intended for deploying Helm charts. Helmfile templates are available in GitHub repository .","title":"Install via Helmfile"},{"location":"operator-guide/install-via-helmfile/#prerequisites","text":"Helm version 3.6.0+ is installed. Please refer to the Helm page on GitHub for details. Helmfile version 0.142.0 is installed. Please refer to the GitHub page for details. Helm diff plugin version 3.5.0 is installed. Please refer to the GitHub page for details. Helm-git plugin version 0.11.4 is installed. Please refer to the GitHub page for details.","title":"Prerequisites"},{"location":"operator-guide/install-via-helmfile/#helmfile-structure","text":"The envs/common.yaml file contains the specification for environments pattern, list of helm repositories from which it is necessary to fetch the helm charts and additional Helm parameters. The envs/platform.yaml file contains global parameters that are used in various Helmfiles. The releases/envs/ contains symbol links to environments files. The releases/*.yaml file contains description of parameters that is used when deploying a Helm chart. The helmfile.yaml file defines components to be installed by defining a path to Helm releases files. The envs/ci.yaml file contains stub parameters for CI linter. The test/lint-ci.sh script for running CI linter with debug loglevel and stub parameters. The resources/*.yaml file contains additional resources for the OpenShift platform.","title":"Helmfile Structure"},{"location":"operator-guide/install-via-helmfile/#operate-helmfile","text":"Before applying the Helmfile, please fill in the global parameters in the envs/platform.yaml (check the examples in the envs/ci.yaml ) and releases/*.yaml files for every Helm deploy. Pay attention to the following recommendations while working with the Helmfile: To launch Lint, run the test/lint-ci.sh script. Display the difference between the deployed and environment state ( helm diff ): helmfile --environment platform -f helmfile.yaml diff Apply the deployment: helmfile --selector component=ingress --environment platform -f helmfile.yaml apply Modify the deployment and apply the changes: helmfile --selector component=ingress --environment platform -f helmfile.yaml sync To deploy the components according to the label, use the selector to target a subset of releases when running the Helmfile. It can be useful for large Helmfiles with the releases that are logically grouped together. For example, to display the difference only for the nginx-ingress file, use the following command: helmfile --selector component=ingress --environment platform -f helmfile.yaml diff To destroy the release, run the following command: helmfile --selector component=ingress --environment platform -f helmfile.yaml destroy","title":"Operate Helmfile"},{"location":"operator-guide/install-via-helmfile/#deploy-components","text":"Using the Helmfile, the following components can be installed: NGINX Ingress Controller Keycloak EPAM Delivery Platform Argo CD External Secrets Operator DefectDojo ReportPortal","title":"Deploy Components"},{"location":"operator-guide/install-via-helmfile/#deploy-nginx-ingress-controller","text":"Info Skip this step for the OpenShift platform, because it has its own Ingress Controller. To install NGINX Ingress controller, follow the steps below: In the releases/nginx-ingress.yaml file, set the proxy-real-ip-cidr parameter according to the value with AWS VPC IPv4 CIDR. Install NGINX Ingress controller: helmfile --selector component=ingress --environment platform -f helmfile.yaml apply","title":"Deploy NGINX Ingress Controller"},{"location":"operator-guide/install-via-helmfile/#deploy-keycloak","text":"Keycloak requires a database deployment, so it has two charts: releases/keycloak.yaml and releases/postgresql-keycloak.yaml . To install Keycloak, follow the steps below: Create a security namespace: Note For the OpenShift users: This namespace is also indicated as users in the following custom SecurityContextConstraints resources: resources/keycloak-scc.yaml and resources/postgresql-keycloak-scc.yaml . Change the namespace name when using a custom namespace. kubectl create namespace security Create PostgreSQL admin secret: kubectl -n security create secret generic keycloak-postgresql \\ --from-literal=password=<postgresql_password> \\ --from-literal=postgres-password=<postgresql_postgres_password> In the envs/platform.yaml file, set the dnsWildCard parameter. Create Keycloak admin secret: kubectl -n security create secret generic keycloak-admin-creds \\ --from-literal=username=<keycloak_admin_username> \\ --from-literal=password=<keycloak_admin_password> Install Keycloak: helmfile --selector component=sso --environment platform -f helmfile.yaml apply","title":"Deploy Keycloak"},{"location":"operator-guide/install-via-helmfile/#deploy-epam-delivery-platform","text":"To install EDP, follow the steps below: Create a platform namespace: kubectl create namespace platform Create a secret for administrative access to the database: kubectl -n platform create secret generic super-admin-db \\ --from-literal=username=<super_admin_db_username> \\ --from-literal=password=<super_admin_db_password> Warning Do not use the admin username here since the admin is a reserved name. Create a secret for an EDP tenant database user: kubectl -n platform create secret generic db-admin-console \\ --from-literal=username=<tenant_db_username> \\ --from-literal=password=<tenant_db_password> Warning Do not use the admin username here since the admin is a reserved name. For EDP, it is required to have Keycloak access to perform the integration. Create a secret with the user and password provisioned in the step 2 of the Keycloak Configuration section. kubectl -n platform create secret generic keycloak \\ --from-literal=username=<username> \\ --from-literal=password=<password> In the envs/platform.yaml file, set the edpName and keycloakEndpoint parameters. In the releases/edp-install.yaml file, check and fill in all values. Install EDP: helmfile --selector component=edp --environment platform -f helmfile.yaml apply","title":"Deploy EPAM Delivery Platform"},{"location":"operator-guide/install-via-helmfile/#deploy-argo-cd","text":"To install Argo CD, follow the steps below: Install Argo CD: Note For the OpenShift users: When using a custom namespace for ArgoCD, the argocd namespace is also indicated as users in the resources/argocd-scc.yaml custom SecurityContextConstraints resource. Change it there as well. helmfile --selector component = argocd --environment platform -f helmfile.yaml apply Update the argocd-secret secret (in the Argo CD namespace) by providing the correct Keycloak client secret ( oidc.keycloak.clientSecret ) with the value from the keycloak-client-argocd-secret secret in EDP namespace, and restart the deployment: ARGOCD_CLIENT = $( kubectl -n platform get secret keycloak-client-argocd-secret -o jsonpath = '{.data.clientSecret}' ) kubectl -n argocd patch secret argocd-secret -p = \"{\\\"data\\\":{\\\"oidc.keycloak.clientSecret\\\": \\\" ${ ARGOCD_CLIENT } \\\"}}\" -v = 1 kubectl -n argocd rollout restart deployment argo-argocd-server","title":"Deploy Argo CD"},{"location":"operator-guide/install-via-helmfile/#deploy-external-secrets-operator","text":"To install External Secrets Operator, follow the steps below: helmfile --selector component = secrets --environment platform -f helmfile.yaml apply","title":"Deploy External Secrets Operator"},{"location":"operator-guide/install-via-helmfile/#deploy-defectdojo","text":"Info It is also possible to install DefectDojo via Helm Chart. For details, please refer to the Install DefectDojo page. To install DefectDojo via Helmfile, follow the steps below: Create a DefectDojo namespace: Note For the OpenShift users: This namespace is also indicated as users in the resources/defectdojo-route.yaml custom SecurityContextConstraints resource. Change it when using a custom namespace. Also, change the namespace in the resources/defectdojo-route.yaml file. kubectl create namespace defectdojo Modify the host in resources/defectdojo-route.yaml (only for OpenShift). Create a PostgreSQL admin secret: kubectl -n defectdojo create secret generic defectdojo-postgresql-specific \\ --from-literal = postgresql-password = <postgresql_password> \\ --from-literal = postgresql-postgres-password = <postgresql_postgres_password> Note The postgresql_password and postgresql_postgres_password passwords must be 16 characters long. Create a RabbitMQ admin secret: kubectl -n defectdojo create secret generic defectdojo-rabbitmq-specific \\ --from-literal = rabbitmq-password = <rabbitmq_password> \\ --from-literal = rabbitmq-erlang-cookie = <rabbitmq_erlang_cookie> Note The rabbitmq_password password must be 10 characters long. The rabbitmq_erlang_cookie password must be 32 characters long. Create a DefectDojo admin secret: kubectl -n defectdojo create secret generic defectdojo \\ --from-literal = DD_ADMIN_PASSWORD = <dd_admin_password> \\ --from-literal = DD_SECRET_KEY = <dd_secret_key> \\ --from-literal = DD_CREDENTIAL_AES_256_KEY = <dd_credential_aes_256_key> \\ --from-literal = METRICS_HTTP_AUTH_PASSWORD = <metric_http_auth_password> Note The dd_admin_password password must be 22 characters long. The dd_secret_key password must be 128 characters long. The dd_credential_aes_256_key password must be 128 characters long. The metric_http_auth_password password must be 32 characters long. In the envs/platform.yaml file, set the dnsWildCard parameter. Install DefectDojo: helmfile --selector component = defectdojo --environment platform -f helmfile.yaml apply","title":"Deploy DefectDojo"},{"location":"operator-guide/install-via-helmfile/#deploy-reportportal","text":"Info It is also possible to install ReportPortal via Helm Chart. For details, please refer to the Install ReportPortal page. ReportPortal requires third-party deployments: RabbitMQ, ElasticSearch, PostgreSQL, MinIO. To install third-party resources, follow the steps below: Create a RabbitMQ admin secret: kubectl -n platform create secret generic reportportal-rabbitmq-creds \\ --from-literal = rabbitmq-password = <rabbitmq_password> \\ --from-literal = rabbitmq-erlang-cookie = <rabbitmq_erlang_cookie> Warning The rabbitmq_password password must be 10 characters long. The rabbitmq_erlang_cookie password must be 32 characters long. Create a PostgreSQL admin secret: kubectl -n platform create secret generic reportportal-postgresql-creds \\ --from-literal = postgresql-password = <postgresql_password> \\ --from-literal = postgresql-postgres-password = <postgresql_postgres_password> Warning The postgresql_password and postgresql_postgres_password passwords must be 16 characters long. Create a MinIO admin secret: kubectl -n platform create secret generic reportportal-minio-creds \\ --from-literal = root-password = <root_password> \\ --from-literal = root-user = <root_user> In the envs/platform.yaml file, set the dnsWildCard and edpName parameters. Install third-party resources: helmfile --selector component = report-portal-third-party-resources --environment platform -f helmfile.yaml apply After the rabbitmq pod gets the status Running, you need to configure the RabbitMQ memory threshold kubectl -n platform exec -it rabbitmq-0 -- rabbitmqctl set_vm_memory_high_watermark 0 .8 To install ReportPortal via Helmfile, follow the steps below: helmfile --selector component = report-portal --environment platform -f helmfile.yaml apply Note For user access: default/1q2w3e For admin access: superadmin/erebus Please refer to the ReportPortal.io page for details.","title":"Deploy ReportPortal"},{"location":"operator-guide/install-via-helmfile/#related-articles","text":"Install EDP Install NGINX Ingress Controller Install Keycloak Install DefectDojo Install ReportPortal Install Argo CD","title":"Related Articles"},{"location":"operator-guide/jira-gerrit-integration/","text":"Adjust VCS Integration With Jira \u2693\ufe0e In order to adjust the Version Control System integration with Jira Server, first make sure you have the following prerequisites: VCS Server Jira Crucible When checked the prerequisites, follow the steps below to proceed with the integration: Integrate every project in VCS Server with every project in Crucible by creating a corresponding request in EPAM Support Portal . Add the repositories links and fill in the Keep Informed field as this request must be approved. Request example Provide additional details to the support team. If the VCS is Gerrit, inspect the sample below of its integration: 2.1 Create a new \"crucible- \" user in Gerrit with SSH key and add a new user to the \"Non-Interactive Users\" Gerrit group; 2.2 Create a new group in Gerrit \"crucible-watcher-group\" and add the \"crucible- \" user; 2.3 Provide access to All-Projects for the \"crucible-watcher-group\" group: Gerrit All-Projects configuration Gerrit All-Projects configuration To link commits with Jira ticket, being in Gerrit, enter a Jira ticket ID in a commit message using the specific format: [PROJECT-CODE-1234]: commit message where PROJECT-CODE is a specific code of a project, 1234 is an ID number, and a commit message. As a result, all Gerrit commits will be displayed on Crucible : Crucible project Related Articles \u2693\ufe0e Adjust Jira Integration","title":"Adjust VCS Integration With Jira"},{"location":"operator-guide/jira-gerrit-integration/#adjust-vcs-integration-with-jira","text":"In order to adjust the Version Control System integration with Jira Server, first make sure you have the following prerequisites: VCS Server Jira Crucible When checked the prerequisites, follow the steps below to proceed with the integration: Integrate every project in VCS Server with every project in Crucible by creating a corresponding request in EPAM Support Portal . Add the repositories links and fill in the Keep Informed field as this request must be approved. Request example Provide additional details to the support team. If the VCS is Gerrit, inspect the sample below of its integration: 2.1 Create a new \"crucible- \" user in Gerrit with SSH key and add a new user to the \"Non-Interactive Users\" Gerrit group; 2.2 Create a new group in Gerrit \"crucible-watcher-group\" and add the \"crucible- \" user; 2.3 Provide access to All-Projects for the \"crucible-watcher-group\" group: Gerrit All-Projects configuration Gerrit All-Projects configuration To link commits with Jira ticket, being in Gerrit, enter a Jira ticket ID in a commit message using the specific format: [PROJECT-CODE-1234]: commit message where PROJECT-CODE is a specific code of a project, 1234 is an ID number, and a commit message. As a result, all Gerrit commits will be displayed on Crucible : Crucible project","title":"Adjust VCS Integration With Jira"},{"location":"operator-guide/jira-gerrit-integration/#related-articles","text":"Adjust Jira Integration","title":"Related Articles"},{"location":"operator-guide/jira-integration/","text":"Adjust Jira Integration \u2693\ufe0e In order to adjust the Jira server integration, first add JiraServer CR by performing the following: Create Secret in the OpenShift/Kubernetes namespace for Jira Server account with the username and password fields: apiVersion : v1 data : password : passwordInBase64 username : usernameInBase64 kind : Secret metadata : name : epam - jira - user type : kubernetes . io / basic - auth Create JiraServer CR in the OpenShift/Kubernetes namespace with the apiUrl , credentialName and rootUrl fields: apiVersion : v2 . edp . epam . com / v1 kind : JiraServer metadata : name : epam - jira spec : apiUrl : 'https://jira-api.example.com' credentialName : jira - user rootUrl : 'https://jira.example.com' status : available : true last_time_updated : '2021-04-05T10:51:07.042048633Z' Note The value of the credentialName property is the name of the Secret, which is indicated in the first point above. Being in Admin Console, navigate to the Advanced Settings menu to check that the Integrate with Jira Server check box appeared: Advanced settings","title":"Adjust Jira Integration"},{"location":"operator-guide/jira-integration/#adjust-jira-integration","text":"In order to adjust the Jira server integration, first add JiraServer CR by performing the following: Create Secret in the OpenShift/Kubernetes namespace for Jira Server account with the username and password fields: apiVersion : v1 data : password : passwordInBase64 username : usernameInBase64 kind : Secret metadata : name : epam - jira - user type : kubernetes . io / basic - auth Create JiraServer CR in the OpenShift/Kubernetes namespace with the apiUrl , credentialName and rootUrl fields: apiVersion : v2 . edp . epam . com / v1 kind : JiraServer metadata : name : epam - jira spec : apiUrl : 'https://jira-api.example.com' credentialName : jira - user rootUrl : 'https://jira.example.com' status : available : true last_time_updated : '2021-04-05T10:51:07.042048633Z' Note The value of the credentialName property is the name of the Secret, which is indicated in the first point above. Being in Admin Console, navigate to the Advanced Settings menu to check that the Integrate with Jira Server check box appeared: Advanced settings","title":"Adjust Jira Integration"},{"location":"operator-guide/kaniko-irsa/","text":"IAM Roles for Kaniko Service Accounts \u2693\ufe0e Note The information below is relevant in case ECR is used as Docker container registry. Make sure that IRSA is enabled and amazon-eks-pod-identity-webhook is deployed according to the Associate IAM Roles With Service Accounts documentation. The \"build-image-kaniko\" stage manages ECR through IRSA that should be available on the cluster. Follow the steps below to create a required role: Create AWS IAM Policy \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039EDP_NAMESPACE\u203aKaniko_policy\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ecr:*\", \"cloudtrail:LookupEvents\" ], \"Resource\": \"arn:aws:ecr:<AWS_REGION>:<AWS_ACCOUNT_ID>:repository/<EDP_NAMESPACE>/*\" }, { \"Effect\": \"Allow\", \"Action\": \"ecr:GetAuthorizationToken\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"ecr:DescribeRepositories\", \"ecr:CreateRepository\" ], \"Resource\": \"arn:aws:ecr:<AWS_REGION>:<AWS_ACCOUNT_ID>:repository/*\" } ] } Create AWS IAM Role \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039EDP_NAMESPACE\u203aKaniko\" with trust relationships: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::<AWS_ACCOUNT_ID>:oidc-provider/<OIDC_PROVIDER>\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"<OIDC_PROVIDER>:sub\": \"system:serviceaccount:<EDP_NAMESPACE>:edp-kaniko\" } } } ] } Attach the \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039EDP_NAMESPACE\u203aKaniko_policy\" policy to the \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039EDP_NAMESPACE\u203aKaniko\" role. Define the resulted arn role value into the kaniko.roleArn parameter in values.yaml during the EDP installation . Related Articles \u2693\ufe0e Associate IAM Roles With Service Accounts Install EDP","title":"IAM Roles for Kaniko Service Accounts"},{"location":"operator-guide/kaniko-irsa/#iam-roles-for-kaniko-service-accounts","text":"Note The information below is relevant in case ECR is used as Docker container registry. Make sure that IRSA is enabled and amazon-eks-pod-identity-webhook is deployed according to the Associate IAM Roles With Service Accounts documentation. The \"build-image-kaniko\" stage manages ECR through IRSA that should be available on the cluster. Follow the steps below to create a required role: Create AWS IAM Policy \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039EDP_NAMESPACE\u203aKaniko_policy\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ecr:*\", \"cloudtrail:LookupEvents\" ], \"Resource\": \"arn:aws:ecr:<AWS_REGION>:<AWS_ACCOUNT_ID>:repository/<EDP_NAMESPACE>/*\" }, { \"Effect\": \"Allow\", \"Action\": \"ecr:GetAuthorizationToken\", \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"ecr:DescribeRepositories\", \"ecr:CreateRepository\" ], \"Resource\": \"arn:aws:ecr:<AWS_REGION>:<AWS_ACCOUNT_ID>:repository/*\" } ] } Create AWS IAM Role \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039EDP_NAMESPACE\u203aKaniko\" with trust relationships: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::<AWS_ACCOUNT_ID>:oidc-provider/<OIDC_PROVIDER>\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"<OIDC_PROVIDER>:sub\": \"system:serviceaccount:<EDP_NAMESPACE>:edp-kaniko\" } } } ] } Attach the \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039EDP_NAMESPACE\u203aKaniko_policy\" policy to the \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039EDP_NAMESPACE\u203aKaniko\" role. Define the resulted arn role value into the kaniko.roleArn parameter in values.yaml during the EDP installation .","title":"IAM Roles for Kaniko Service Accounts"},{"location":"operator-guide/kaniko-irsa/#related-articles","text":"Associate IAM Roles With Service Accounts Install EDP","title":"Related Articles"},{"location":"operator-guide/kubernetes-cluster-settings/","text":"Set Up Kubernetes \u2693\ufe0e Make sure the cluster meets the following conditions: Kubernetes cluster is installed with minimum 2 worker nodes with total capacity 32 Cores and 8Gb RAM; Machine with kubectl is installed with a cluster-admin access to the Kubernetes cluster; Ingress controller is installed in a cluster, for example ingress-nginx ; Ingress controller is configured with the disabled HTTP/2 protocol and header size of 64k support; Example of Config Map for Nginx ingress controller: kind : ConfigMap apiVersion : v1 metadata : name : nginx - configuration namespace : ingress - nginx labels : app . kubernetes . io / name : ingress - nginx app . kubernetes . io / part - of : ingress - nginx data : client - header - buffer - size : 64 k large - client - header - buffers : 4 64 k use - http2 : \"false\" Load balancer (if any exists in front of ingress controller) is configured with session stickiness, disabled HTTP/2 protocol and header size of 32k support; Cluster nodes and pods have access to the cluster via external URLs. For instance, add in AWS the VPC NAT gateway elastic IP to the cluster external load balancers security group); Keycloak instance is installed. To get accurate information on how to install Keycloak, please refer to the Install Keycloak instruction; Helm 3.1 or higher is installed on the installation machine with the help of the Installing Helm instruction; A storage class is used with the Retain Reclaim Policy . See the example below. Storage class template with the Retain Reclaim Policy: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp2-retain provisioner: kubernetes.io/aws-ebs parameters: fsType: ext4 type: gp2 reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer Related Articles \u2693\ufe0e Install Ingress-nginx Install Keycloak","title":"Set Up Kubernetes"},{"location":"operator-guide/kubernetes-cluster-settings/#set-up-kubernetes","text":"Make sure the cluster meets the following conditions: Kubernetes cluster is installed with minimum 2 worker nodes with total capacity 32 Cores and 8Gb RAM; Machine with kubectl is installed with a cluster-admin access to the Kubernetes cluster; Ingress controller is installed in a cluster, for example ingress-nginx ; Ingress controller is configured with the disabled HTTP/2 protocol and header size of 64k support; Example of Config Map for Nginx ingress controller: kind : ConfigMap apiVersion : v1 metadata : name : nginx - configuration namespace : ingress - nginx labels : app . kubernetes . io / name : ingress - nginx app . kubernetes . io / part - of : ingress - nginx data : client - header - buffer - size : 64 k large - client - header - buffers : 4 64 k use - http2 : \"false\" Load balancer (if any exists in front of ingress controller) is configured with session stickiness, disabled HTTP/2 protocol and header size of 32k support; Cluster nodes and pods have access to the cluster via external URLs. For instance, add in AWS the VPC NAT gateway elastic IP to the cluster external load balancers security group); Keycloak instance is installed. To get accurate information on how to install Keycloak, please refer to the Install Keycloak instruction; Helm 3.1 or higher is installed on the installation machine with the help of the Installing Helm instruction; A storage class is used with the Retain Reclaim Policy . See the example below. Storage class template with the Retain Reclaim Policy: kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp2-retain provisioner: kubernetes.io/aws-ebs parameters: fsType: ext4 type: gp2 reclaimPolicy: Retain volumeBindingMode: WaitForFirstConsumer","title":"Set Up Kubernetes"},{"location":"operator-guide/kubernetes-cluster-settings/#related-articles","text":"Install Ingress-nginx Install Keycloak","title":"Related Articles"},{"location":"operator-guide/logsight-integration/","text":"Logsight Integration \u2693\ufe0e Logsight can be integrated with the CI/CD pipeline. It connects to log data sources, analyses collected logs, and evaluates deployment risk scores. Overview \u2693\ufe0e In order to understand if a microservice or a component is ready for the deployment, EDP suggests analysing logs via Logsight to decide if the deployment is risky or not. Please find more about Logsight in the official documentation: Logsight key features and workflow Log analysis Stage verification Logsight as a Quality Gate \u2693\ufe0e Integration with Logsight allows enhancing and optimizing software releases by creating an additional quality gate . Logsight can be configured in two ways: SAAS - online system; for this solution a connection string is required. Self-deployment - local installation. To work with Logsight, a new Deployment Risk stage must be added to the pipeline. On this stage, the logs are analysed with the help of Logsight mechanisms. On the verification screen of Logsight, continuous verification of the application deployment can be monitored, and tests can be compared for detecting test flakiness. For example, two versions of a microservice can be compared in order to detect critical differences. Risk score will be calculated for the state reached by version A and version B. Afterwards, the deployment risk will be calculated based on individual risk scores. If the deployment failure risk is greater than a predefined threshold, the verification gate blocks the deployment from going to the target environment. In such case, the Deployment Risk stage of the pipeline is not passed, and additional attention is required. The exact log messages can be displayed in the Logsight verification screen, to help debug the problem. Use Logsight for EDP Development \u2693\ufe0e Please find below the detailed description of Logsight integration with EDP. Deployment Approach \u2693\ufe0e EDP uses Logsight in a self-deploying mode. Logsight provides a deployment approach using Helm charts . Please find below the stack of components that must be deployed: logsight - the core component. logsight-backend - the backend that provides all necessary APIs and user management. logsight-frontend - the frontend that provides the user interface. logsight-result-api - responsible for obtaining results, for example, during the verification. Below is a diagram of interaction when integrating the components: Logsight Structure Configure FluentBit for Sending Log Data \u2693\ufe0e Logsight is integrated with the EDP logging stack. The integration is based on top of the EFK (ElasticSearch-FluentBit-Kibana) stack . It is necessary to deploy a stack with the security support, namely, enable the certificate support. A FluentBit config indicates the namespace from which the logs will be received for further analysis. Below is an example of the FluentBit config for getting logs from the edp-delivery-edp-delivery-sit namespace: View: fluent-bit.conf [ INPUT ] Name tail Tag kube.sit.* Path /var/log/containers/*edp-delivery-edp-delivery-sit*.log Parser docker Mem_Buf_Limit 5MB Skip_Long_Lines Off Refresh_Interval 10 [FILTER] Name kubernetes Match kube.sit.* Kube_URL https://kubernetes.default.svc:443 Kube_CA_File /var/run/secrets/kubernetes.io/serviceaccount/ca.crt Kube_Token_File /var/run/secrets/kubernetes.io/serviceaccount/token Kube_Tag_Prefix kube.sit.var.log.containers. Merge_Log Off K8S-Logging.Parser On K8S-Logging.Exclude On [FILTER] Name nest Match kube.sit.* Operation lift Nested_under kubernetes Add_prefix kubernetes. [FILTER] Name modify Match kube.sit.* Copy kubernetes.container_name tags.container Copy log message Copy kubernetes.container_image tags.image Copy kubernetes.namespace_name tags.namespace [FILTER] Name nest Match kube.sit.* Operation nest Wildcard kubernetes.* Nested_under kubernetes Remove_prefix kubernetes. [OUTPUT] Name es Match kube.sit.* Host elasticsearch-master Port 9200 HTTP_User elastic HTTP_Passwd ***** Logstash_Format On Logstash_Prefix sit Time_Key @timestamp Type flb_type Replace_Dots On Retry_Limit False [OUTPUT] Match kube.sit.* Name http Host logsight-backend Port 8080 http_User logsight@example.com http_Passwd ***** uri /api/v1/logs/singles Format json json_date_format iso8601 json_date_key timestamp Deployment Risk Analysis \u2693\ufe0e A deployment-risk stage is added to the EDP CD pipeline. Deployment Risk If the deployment risk is above 70%, the red state of the pipeline is expected. EDP consists of a set of containerized components. For the convenience of tracking the risk deployment trend for each component, this data is stored as Jenkins artifacts. If the deployment risk is higher than the threshold of 70%, the EDP promotion of artifacts for the next environments does not pass. The deployment risk report can be analysed in order to avoid the potential problems with updating the components. To study the report in detail, use the link from the Jenkins pipeline to the Logsight verification screen: Logsight Insights Logsight Insights In this example, logs from different versions of the gerrit-operator were analyzed. As can be seen from the report, a large number of new messages appeared in the logs, and the output frequency of other notifications has also changed, which led to the high deployment risk. The environment on which the analysis is performed can exist for different time periods. Logsight only processes the minimum total number of logs since the creating of the environment. Related Articles \u2693\ufe0e Customize CD Pipeline Adjust Jira Integration","title":"Logsight Integration"},{"location":"operator-guide/logsight-integration/#logsight-integration","text":"Logsight can be integrated with the CI/CD pipeline. It connects to log data sources, analyses collected logs, and evaluates deployment risk scores.","title":"Logsight Integration"},{"location":"operator-guide/logsight-integration/#overview","text":"In order to understand if a microservice or a component is ready for the deployment, EDP suggests analysing logs via Logsight to decide if the deployment is risky or not. Please find more about Logsight in the official documentation: Logsight key features and workflow Log analysis Stage verification","title":"Overview"},{"location":"operator-guide/logsight-integration/#logsight-as-a-quality-gate","text":"Integration with Logsight allows enhancing and optimizing software releases by creating an additional quality gate . Logsight can be configured in two ways: SAAS - online system; for this solution a connection string is required. Self-deployment - local installation. To work with Logsight, a new Deployment Risk stage must be added to the pipeline. On this stage, the logs are analysed with the help of Logsight mechanisms. On the verification screen of Logsight, continuous verification of the application deployment can be monitored, and tests can be compared for detecting test flakiness. For example, two versions of a microservice can be compared in order to detect critical differences. Risk score will be calculated for the state reached by version A and version B. Afterwards, the deployment risk will be calculated based on individual risk scores. If the deployment failure risk is greater than a predefined threshold, the verification gate blocks the deployment from going to the target environment. In such case, the Deployment Risk stage of the pipeline is not passed, and additional attention is required. The exact log messages can be displayed in the Logsight verification screen, to help debug the problem.","title":"Logsight as a Quality Gate"},{"location":"operator-guide/logsight-integration/#use-logsight-for-edp-development","text":"Please find below the detailed description of Logsight integration with EDP.","title":"Use Logsight for EDP Development"},{"location":"operator-guide/logsight-integration/#deployment-approach","text":"EDP uses Logsight in a self-deploying mode. Logsight provides a deployment approach using Helm charts . Please find below the stack of components that must be deployed: logsight - the core component. logsight-backend - the backend that provides all necessary APIs and user management. logsight-frontend - the frontend that provides the user interface. logsight-result-api - responsible for obtaining results, for example, during the verification. Below is a diagram of interaction when integrating the components: Logsight Structure","title":"Deployment Approach"},{"location":"operator-guide/logsight-integration/#configure-fluentbit-for-sending-log-data","text":"Logsight is integrated with the EDP logging stack. The integration is based on top of the EFK (ElasticSearch-FluentBit-Kibana) stack . It is necessary to deploy a stack with the security support, namely, enable the certificate support. A FluentBit config indicates the namespace from which the logs will be received for further analysis. Below is an example of the FluentBit config for getting logs from the edp-delivery-edp-delivery-sit namespace: View: fluent-bit.conf [ INPUT ] Name tail Tag kube.sit.* Path /var/log/containers/*edp-delivery-edp-delivery-sit*.log Parser docker Mem_Buf_Limit 5MB Skip_Long_Lines Off Refresh_Interval 10 [FILTER] Name kubernetes Match kube.sit.* Kube_URL https://kubernetes.default.svc:443 Kube_CA_File /var/run/secrets/kubernetes.io/serviceaccount/ca.crt Kube_Token_File /var/run/secrets/kubernetes.io/serviceaccount/token Kube_Tag_Prefix kube.sit.var.log.containers. Merge_Log Off K8S-Logging.Parser On K8S-Logging.Exclude On [FILTER] Name nest Match kube.sit.* Operation lift Nested_under kubernetes Add_prefix kubernetes. [FILTER] Name modify Match kube.sit.* Copy kubernetes.container_name tags.container Copy log message Copy kubernetes.container_image tags.image Copy kubernetes.namespace_name tags.namespace [FILTER] Name nest Match kube.sit.* Operation nest Wildcard kubernetes.* Nested_under kubernetes Remove_prefix kubernetes. [OUTPUT] Name es Match kube.sit.* Host elasticsearch-master Port 9200 HTTP_User elastic HTTP_Passwd ***** Logstash_Format On Logstash_Prefix sit Time_Key @timestamp Type flb_type Replace_Dots On Retry_Limit False [OUTPUT] Match kube.sit.* Name http Host logsight-backend Port 8080 http_User logsight@example.com http_Passwd ***** uri /api/v1/logs/singles Format json json_date_format iso8601 json_date_key timestamp","title":"Configure FluentBit for Sending Log Data"},{"location":"operator-guide/logsight-integration/#deployment-risk-analysis","text":"A deployment-risk stage is added to the EDP CD pipeline. Deployment Risk If the deployment risk is above 70%, the red state of the pipeline is expected. EDP consists of a set of containerized components. For the convenience of tracking the risk deployment trend for each component, this data is stored as Jenkins artifacts. If the deployment risk is higher than the threshold of 70%, the EDP promotion of artifacts for the next environments does not pass. The deployment risk report can be analysed in order to avoid the potential problems with updating the components. To study the report in detail, use the link from the Jenkins pipeline to the Logsight verification screen: Logsight Insights Logsight Insights In this example, logs from different versions of the gerrit-operator were analyzed. As can be seen from the report, a large number of new messages appeared in the logs, and the output frequency of other notifications has also changed, which led to the high deployment risk. The environment on which the analysis is performed can exist for different time periods. Logsight only processes the minimum total number of logs since the creating of the environment.","title":"Deployment Risk Analysis"},{"location":"operator-guide/logsight-integration/#related-articles","text":"Customize CD Pipeline Adjust Jira Integration","title":"Related Articles"},{"location":"operator-guide/loki-irsa/","text":"IAM Roles for Loki Service Accounts \u2693\ufe0e Note Make sure that IRSA is enabled and amazon-eks-pod-identity-webhook is deployed according to the Associate IAM Roles With Service Accounts documentation. It is possible to use Amazon Simple Storage Service Amazon S3 as object storage for Loki. In this case Loki requires access to AWS resources. Follow the steps below to create a required role: Create AWS IAM Policy \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039LOKI_NAMESPACE\u203aLoki_policy\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:ListObjects\", \"s3:ListBucket\", \"s3:PutObject\", \"s3:GetObject\", \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::loki-*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\" ], \"Resource\": [ \"arn:aws:s3:::loki-*\" ] } ] } Create AWS IAM Role \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039LOKI_NAMESPACE\u203aLoki\" with trust relationships: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::<AWS_ACCOUNT_ID>:oidc-provider/<OIDC_PROVIDER>\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"<OIDC_PROVIDER>:sub\": \"system:serviceaccount:<LOKI_NAMESPACE>:edp-loki\" } } } ] } Attach the \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039LOKI_NAMESPACE\u203aLoki_policy\" policy to the \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039LOKI_NAMESPACE\u203aLoki\" role. Make sure that Amazon S3 bucket with name loki-\u2039CLUSTER_NAME\u203a exists. Provide key value eks.amazonaws.com/role-arn: \"arn:aws:iam:: :role/AWSIRSA\u2039CLUSTER_NAME\u203a\u2039LOKI_NAMESPACE\u203aLoki\" into the serviceAccount.annotations parameter in values.yaml during the Loki Installation . Related Articles \u2693\ufe0e Associate IAM Roles With Service Accounts Install Grafana Loki","title":"IAM Roles for Loki Service Accounts"},{"location":"operator-guide/loki-irsa/#iam-roles-for-loki-service-accounts","text":"Note Make sure that IRSA is enabled and amazon-eks-pod-identity-webhook is deployed according to the Associate IAM Roles With Service Accounts documentation. It is possible to use Amazon Simple Storage Service Amazon S3 as object storage for Loki. In this case Loki requires access to AWS resources. Follow the steps below to create a required role: Create AWS IAM Policy \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039LOKI_NAMESPACE\u203aLoki_policy\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:ListObjects\", \"s3:ListBucket\", \"s3:PutObject\", \"s3:GetObject\", \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::loki-*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\" ], \"Resource\": [ \"arn:aws:s3:::loki-*\" ] } ] } Create AWS IAM Role \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039LOKI_NAMESPACE\u203aLoki\" with trust relationships: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::<AWS_ACCOUNT_ID>:oidc-provider/<OIDC_PROVIDER>\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"<OIDC_PROVIDER>:sub\": \"system:serviceaccount:<LOKI_NAMESPACE>:edp-loki\" } } } ] } Attach the \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039LOKI_NAMESPACE\u203aLoki_policy\" policy to the \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039LOKI_NAMESPACE\u203aLoki\" role. Make sure that Amazon S3 bucket with name loki-\u2039CLUSTER_NAME\u203a exists. Provide key value eks.amazonaws.com/role-arn: \"arn:aws:iam:: :role/AWSIRSA\u2039CLUSTER_NAME\u203a\u2039LOKI_NAMESPACE\u203aLoki\" into the serviceAccount.annotations parameter in values.yaml during the Loki Installation .","title":"IAM Roles for Loki Service Accounts"},{"location":"operator-guide/loki-irsa/#related-articles","text":"Associate IAM Roles With Service Accounts Install Grafana Loki","title":"Related Articles"},{"location":"operator-guide/manage-jenkins-cd-job-provision/","text":"Manage Jenkins CD Pipeline Job Provisioner \u2693\ufe0e The Jenkins CD job provisioner (or seed-job) is used to create and manage the cd-pipeline folder, and its Deploy pipelines . There is a special job-provisions/cd folder in Jenkins for these provisioners. Explore the steps for managing different provisioner types below. Default \u2693\ufe0e During the EDP deployment, a default provisioner is created to deploy application with container and custom deployment type. Find the configuration in job-provisions/cd/default . Default template is presented below: View: Default template /* Copyright 2022 EPAM Systems. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ import groovy.json.* import jenkins.model.Jenkins Jenkins jenkins = Jenkins . instance def pipelineName = \"${PIPELINE_NAME}-cd-pipeline\" def stageName = \"${STAGE_NAME}\" def qgStages = \"${QG_STAGES}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID}\" def sourceType = \"${SOURCE_TYPE}\" def libraryURL = \"${LIBRARY_URL}\" def libraryBranch = \"${LIBRARY_BRANCH}\" def isAutoDeploy = \"${AUTODEPLOY}\" def scriptPath = \"Jenkinsfile\" def containerDeploymentType = \"container\" def deploymentType = \"${DEPLOYMENT_TYPE}\" def codebaseFolder = jenkins . getItem ( pipelineName ) def autoDeploy = ' { \"name\" : \"auto-deploy-input\" , \"step_name\" : \"auto-deploy-input\" } ' def manualDeploy = ' { \"name\" : \"manual-deploy-input\" , \"step_name\" : \"manual-deploy-input\" } ' def runType = isAutoDeploy . toBoolean () ? autoDeploy : manualDeploy def stages = buildStages ( deploymentType , containerDeploymentType , qgStages , runType ) if ( codebaseFolder == null ) { folder ( pipelineName ) } if ( deploymentType == containerDeploymentType ) { createContainerizedCdPipeline ( pipelineName , stageName , stages , scriptPath , sourceType , libraryURL , libraryBranch , gitCredentialsId , gitServerCrVersion , isAutoDeploy ) } else { createCustomCdPipeline ( pipelineName , stageName ) } def buildStages ( deploymentType , containerDeploymentType , qgStages , runType ) { return deploymentType == containerDeploymentType ? ' [ { \"name\" : \"init\" , \"step_name\" : \"init\" }, ' + runType + ' ,{ \"name\" : \"deploy\" , \"step_name\" : \"deploy\" }, ' + qgStages + ' ,{ \"name\" : \"promote-images\" , \"step_name\" : \"promote-images\" } ] ' : '' } def createContainerizedCdPipeline ( pipelineName , stageName , stages , pipelineScript , sourceType , libraryURL , libraryBranch , libraryCredId , gitServerCrVersion , isAutoDeploy ) { pipelineJob ( \"${pipelineName}/${stageName}\" ) { if ( sourceType == \"library\" ) { definition { cpsScm { scm { git { remote { url ( libraryURL ) credentials ( libraryCredId ) } branches ( \"${libraryBranch}\" ) scriptPath ( \"${pipelineScript}\" ) } } } } } else { definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\nDeploy()\" ) sandbox ( true ) } } } properties { disableConcurrentBuilds () logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } } parameters { stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${stages}\" , \"Consequence of stages in JSON format to be run during execution\" ) if ( isAutoDeploy ? . trim () && isAutoDeploy . toBoolean ()) { stringParam ( \"CODEBASE_VERSION\" , null , \"Codebase versions to deploy.\" ) } } } } def createCustomCdPipeline ( pipelineName , stageName ) { pipelineJob ( \"${pipelineName}/${stageName}\" ) { properties { disableConcurrentBuilds () logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } } } } Custom \u2693\ufe0e In some cases, it is necessary to modify or update the job provisioner logic. For example, when adding a new stage requires a custom job provisioner created on the basis of an existing one out of the box. Take the steps below to add a custom job provision. Navigate to the Jenkins main page and open the job-provisions/cd folder, click New Item and type the name of job provisions, for example - custom. CD provisioner name Scroll down to the Copy from field, enter \"/job-provisions/cd/default\", and click OK: Copy CD provisioner Update the required parameters in the new provisioner. For example, if it is necessary to implement a new stage clean , add the following code to the provisioner: def buildStages ( deploymentType , containerDeploymentType , qgStages ) { return deploymentType == containerDeploymentType ? ' [{\"name\":\"init\",\"step_name\":\"init\"},{\"name\":\"clean\",\"step_name\":\"clean\"},{\"name\":\"deploy\",\"step_name\":\"deploy\"}, ' + qgStages + ' ,{\"name\":\"promote-images-ecr\",\"step_name\":\"promote-images\"}] ' : '' } Note Make sure the support for the above mentioned logic is implemented. Please refer to the How to Redefine or Extend the EDP Pipeline Stages Library section of the guide. After the steps above are performed, the new custom job-provision will be available in Adding Stage during the CD pipeline creation in Admin Console. Custom CD provision","title":"Manage Jenkins CD Pipeline Job Provisioner"},{"location":"operator-guide/manage-jenkins-cd-job-provision/#manage-jenkins-cd-pipeline-job-provisioner","text":"The Jenkins CD job provisioner (or seed-job) is used to create and manage the cd-pipeline folder, and its Deploy pipelines . There is a special job-provisions/cd folder in Jenkins for these provisioners. Explore the steps for managing different provisioner types below.","title":"Manage Jenkins CD Pipeline Job Provisioner"},{"location":"operator-guide/manage-jenkins-cd-job-provision/#default","text":"During the EDP deployment, a default provisioner is created to deploy application with container and custom deployment type. Find the configuration in job-provisions/cd/default . Default template is presented below: View: Default template /* Copyright 2022 EPAM Systems. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ import groovy.json.* import jenkins.model.Jenkins Jenkins jenkins = Jenkins . instance def pipelineName = \"${PIPELINE_NAME}-cd-pipeline\" def stageName = \"${STAGE_NAME}\" def qgStages = \"${QG_STAGES}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID}\" def sourceType = \"${SOURCE_TYPE}\" def libraryURL = \"${LIBRARY_URL}\" def libraryBranch = \"${LIBRARY_BRANCH}\" def isAutoDeploy = \"${AUTODEPLOY}\" def scriptPath = \"Jenkinsfile\" def containerDeploymentType = \"container\" def deploymentType = \"${DEPLOYMENT_TYPE}\" def codebaseFolder = jenkins . getItem ( pipelineName ) def autoDeploy = ' { \"name\" : \"auto-deploy-input\" , \"step_name\" : \"auto-deploy-input\" } ' def manualDeploy = ' { \"name\" : \"manual-deploy-input\" , \"step_name\" : \"manual-deploy-input\" } ' def runType = isAutoDeploy . toBoolean () ? autoDeploy : manualDeploy def stages = buildStages ( deploymentType , containerDeploymentType , qgStages , runType ) if ( codebaseFolder == null ) { folder ( pipelineName ) } if ( deploymentType == containerDeploymentType ) { createContainerizedCdPipeline ( pipelineName , stageName , stages , scriptPath , sourceType , libraryURL , libraryBranch , gitCredentialsId , gitServerCrVersion , isAutoDeploy ) } else { createCustomCdPipeline ( pipelineName , stageName ) } def buildStages ( deploymentType , containerDeploymentType , qgStages , runType ) { return deploymentType == containerDeploymentType ? ' [ { \"name\" : \"init\" , \"step_name\" : \"init\" }, ' + runType + ' ,{ \"name\" : \"deploy\" , \"step_name\" : \"deploy\" }, ' + qgStages + ' ,{ \"name\" : \"promote-images\" , \"step_name\" : \"promote-images\" } ] ' : '' } def createContainerizedCdPipeline ( pipelineName , stageName , stages , pipelineScript , sourceType , libraryURL , libraryBranch , libraryCredId , gitServerCrVersion , isAutoDeploy ) { pipelineJob ( \"${pipelineName}/${stageName}\" ) { if ( sourceType == \"library\" ) { definition { cpsScm { scm { git { remote { url ( libraryURL ) credentials ( libraryCredId ) } branches ( \"${libraryBranch}\" ) scriptPath ( \"${pipelineScript}\" ) } } } } } else { definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\nDeploy()\" ) sandbox ( true ) } } } properties { disableConcurrentBuilds () logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } } parameters { stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${stages}\" , \"Consequence of stages in JSON format to be run during execution\" ) if ( isAutoDeploy ? . trim () && isAutoDeploy . toBoolean ()) { stringParam ( \"CODEBASE_VERSION\" , null , \"Codebase versions to deploy.\" ) } } } } def createCustomCdPipeline ( pipelineName , stageName ) { pipelineJob ( \"${pipelineName}/${stageName}\" ) { properties { disableConcurrentBuilds () logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } } } }","title":"Default"},{"location":"operator-guide/manage-jenkins-cd-job-provision/#custom","text":"In some cases, it is necessary to modify or update the job provisioner logic. For example, when adding a new stage requires a custom job provisioner created on the basis of an existing one out of the box. Take the steps below to add a custom job provision. Navigate to the Jenkins main page and open the job-provisions/cd folder, click New Item and type the name of job provisions, for example - custom. CD provisioner name Scroll down to the Copy from field, enter \"/job-provisions/cd/default\", and click OK: Copy CD provisioner Update the required parameters in the new provisioner. For example, if it is necessary to implement a new stage clean , add the following code to the provisioner: def buildStages ( deploymentType , containerDeploymentType , qgStages ) { return deploymentType == containerDeploymentType ? ' [{\"name\":\"init\",\"step_name\":\"init\"},{\"name\":\"clean\",\"step_name\":\"clean\"},{\"name\":\"deploy\",\"step_name\":\"deploy\"}, ' + qgStages + ' ,{\"name\":\"promote-images-ecr\",\"step_name\":\"promote-images\"}] ' : '' } Note Make sure the support for the above mentioned logic is implemented. Please refer to the How to Redefine or Extend the EDP Pipeline Stages Library section of the guide. After the steps above are performed, the new custom job-provision will be available in Adding Stage during the CD pipeline creation in Admin Console. Custom CD provision","title":"Custom"},{"location":"operator-guide/manage-jenkins-ci-job-provision/","text":"Manage Jenkins CI Pipeline Job Provisioner \u2693\ufe0e The Jenkins CI job provisioner (or seed-job) is used to create and manage the application folder, and its Code Review, Build and Create Release pipelines . Depending on the version control system, different job provisioners are used. EDP supports integration with the following version control systems: Gerrit (default) GitHub (github) GitLab (gitlab) By default, the Jenkins operator creates a pipeline for several types of application and libraries . There is a special job-provisions/ci folder in Jenkins for these provisioners. During the EDP deployment, a default provisioner is created for integration with Gerrit version control system. To configure integration with other version control systems, you need to add the required job provisioners to job-provisions/ci folder in Jenkins. Custom (custom-default/github/gitlab) \u2693\ufe0e In some cases it is necessary to modify or update the job provisioner logic, for example when an added other code language needs to create a custom job provisioner on the basis of an existing one out of the box. Take the steps below to add a custom job provision: Navigate to the Jenkins main page and open the job-provisions/ci folder, click New Item and type the name of job-provisions, for example - custom-github. CI provisioner name Scroll down to the Copy from field and enter \"/job-provisions/ci/github\", and click OK: Copy ci provisioner Update the required parameters in the new provisioner. For example, if it is necessary to implement a new build tool docker , several parameters are to be updated. Add the following stages to the docker Code Review and Build pipelines for docker application: stages [ ' Code-review-application-docker ' ] = ' [{\"name\": \"checkout\"},{\"name\": \"lint\"},{\"name\": \"build\"}] ' ... stages [ ' Build-application-docker ' ] = ' [{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"lint\"},{\"name\": \"build\"},{\"name\": \"push\"},{\"name\": \"git-tag\"}] ' ... def getStageKeyName ( buildTool ) { ... if ( buildTool . toString () . equalsIgnoreCase ( ' docker ' )) { return \" Code-review-application-docker \" } ... } Note Make sure the support for the above mentioned logic is implemented. Please refer to the How to Redefine or Extend the EDP Pipeline Stages Library section of the guide. Note The default template should be changed if there is another creation logic for the Code Review, Build and Create Release pipelines. Furthermore, all pipeline types should have the necessary stages as well. After the steps above are performed, the new custom job provision will be available in Advanced Settings during the application creation in Admin Console. Custom ci provision Gerrit (default) \u2693\ufe0e During the EDP deployment, a default provisioner is created for integration with Gerrit version control system. Find the configuration in job-provisions/ci/default . Default template is presented below: View: Default template /* Copyright 2022 EPAM Systems. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ import groovy.json.* import jenkins.model.Jenkins Jenkins jenkins = Jenkins . instance def stages = [:] def jiraIntegrationEnabled = Boolean . parseBoolean ( \"${JIRA_INTEGRATION_ENABLED}\" as String ) def commitValidateStage = jiraIntegrationEnabled ? ',{\"name\": \"commit-validate\"}' : '' def createJIMStage = jiraIntegrationEnabled ? ',{\"name\": \"create-jira-issue-metadata\"}' : '' def platformType = \"${PLATFORM_TYPE}\" def buildStage = platformType . toString () == \"kubernetes\" ? ',{\"name\": \"build-image-kaniko\"}' : ',{\"name\": \"build-image-from-dockerfile\"}' def buildTool = \"${BUILD_TOOL}\" def goBuildStage = buildTool . toString () == \"go\" ? ',{\"name\": \"build\"}' : ',{\"name\": \"compile\"}' stages [ 'Code-review-application' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + goBuildStage + ',{\"name\": \"tests\"},[{\"name\": \"sonar\"},{\"name\": \"dockerfile-lint\"},{\"name\": \"helm-lint\"}]]' stages [ 'Code-review-library' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"compile\"},{\"name\": \"tests\"},{\"name\": \"sonar\"}]' stages [ 'Code-review-autotests' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"tests\"},{\"name\": \"sonar\"}' + ']' stages [ 'Code-review-default' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ']' stages [ 'Code-review-library-terraform' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"terraform-lint\"}]' stages [ 'Code-review-library-opa' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"tests\"}]' stages [ 'Code-review-library-codenarc' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"sonar\"},{\"name\": \"build\"}]' stages [ 'Code-review-library-kaniko' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"dockerbuild-verify\"}]' stages [ 'Build-library-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-npm' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-gradle' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-terraform' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"terraform-lint\"}' + ',{\"name\": \"terraform-plan\"},{\"name\": \"terraform-apply\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-opa' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"tests\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-codenarc' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-kaniko' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"build-image-kaniko\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-autotests-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-autotests-gradle' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sast\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-npm' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-gradle' ] = stages [ 'Build-application-maven' ] stages [ 'Build-application-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-go' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sast\"},{\"name\": \"tests\"}' + ',{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildStage}\" + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Create-release' ] = '[{\"name\": \"checkout\"},{\"name\": \"create-branch\"},{\"name\": \"trigger-job\"}]' def defaultBuild = '[{\"name\": \"checkout\"}' + \"${createJIMStage}\" + ']' def codebaseName = \"${NAME}\" def gitServerCrName = \"${GIT_SERVER_CR_NAME}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID ? GIT_CREDENTIALS_ID : 'gerrit-ciuser-sshkey'}\" def repositoryPath = \"${REPOSITORY_PATH}\" def defaultBranch = \"${DEFAULT_BRANCH}\" def codebaseFolder = jenkins . getItem ( codebaseName ) if ( codebaseFolder == null ) { folder ( codebaseName ) } createListView ( codebaseName , \"Releases\" ) createReleasePipeline ( \"Create-release-${codebaseName}\" , codebaseName , stages [ \"Create-release\" ], \"CreateRelease\" , repositoryPath , gitCredentialsId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType , defaultBranch ) if ( buildTool . toString (). equalsIgnoreCase ( 'none' )) { return true } if ( BRANCH ) { def branch = \"${BRANCH}\" def formattedBranch = \"${branch.toUpperCase().replaceAll(/\\//, \" - \")}\" createListView ( codebaseName , formattedBranch ) def type = \"${TYPE}\" def crKey = getStageKeyName ( buildTool ) createCiPipeline ( \"Code-review-${codebaseName}\" , codebaseName , stages [ crKey ], \"CodeReview\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) def buildKey = \"Build-${type}-${buildTool.toLowerCase()}\" . toString () if ( type . equalsIgnoreCase ( 'application' ) || type . equalsIgnoreCase ( 'library' ) || type . equalsIgnoreCase ( 'autotests' )) { def jobExists = false if ( \"${formattedBranch}-Build-${codebaseName}\" . toString () in Jenkins . instance . getAllItems (). collect { it . name }) jobExists = true createCiPipeline ( \"Build-${codebaseName}\" , codebaseName , stages . get ( buildKey , defaultBuild ), \"Build\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) if (! jobExists ) queue ( \"${codebaseName}/${formattedBranch}-Build-${codebaseName}\" ) } } def createCiPipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , watchBranch , gitServerCrName , gitServerCrVersion ) { pipelineJob ( \"${codebaseName}/${watchBranch.toUpperCase().replaceAll(/\\//, \" - \")}-${pipelineName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } triggers { gerrit { events { if ( pipelineName . contains ( \"Build\" )) changeMerged () else patchsetCreated () } project ( \"plain:${codebaseName}\" , [ \"plain:${watchBranch}\" ]) } } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) stringParam ( \"BRANCH\" , \"${watchBranch}\" , \"Branch to build artifact from\" ) } } } } def getStageKeyName ( buildTool ) { if ( buildTool . toString (). equalsIgnoreCase ( 'terraform' )) { return \"Code-review-library-terraform\" } if ( buildTool . toString (). equalsIgnoreCase ( 'opa' )) { return \"Code-review-library-opa\" } if ( buildTool . toString (). equalsIgnoreCase ( 'codenarc' )) { return \"Code-review-library-codenarc\" } if ( buildTool . toString (). equalsIgnoreCase ( 'kaniko' )) { return \"Code-review-library-kaniko\" } def buildToolsOutOfTheBox = [ \"maven\" , \"npm\" , \"gradle\" , \"dotnet\" , \"none\" , \"go\" , \"python\" ] def supBuildTool = buildToolsOutOfTheBox . contains ( buildTool . toString ()) return supBuildTool ? \"Code-review-${TYPE}\" : \"Code-review-default\" } def createReleasePipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType , defaultBranch ) { pipelineJob ( \"${codebaseName}/${pipelineName}\" ) { logRotator { numToKeep ( 14 ) daysToKeep ( 30 ) } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"\" ) if ( pipelineName . contains ( \"Create-release\" )) { stringParam ( \"JIRA_INTEGRATION_ENABLED\" , \"${jiraIntegrationEnabled}\" , \"Is Jira integration enabled\" ) stringParam ( \"PLATFORM_TYPE\" , \"${platformType}\" , \"Platform type\" ) stringParam ( \"GERRIT_PROJECT\" , \"${codebaseName}\" , \"\" ) stringParam ( \"RELEASE_NAME\" , \"\" , \"Name of the release(branch to be created)\" ) stringParam ( \"COMMIT_ID\" , \"\" , \"Commit ID that will be used to create branch from for new release. If empty, DEFAULT_BRANCH will be used\" ) stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"REPOSITORY_PATH\" , \"${repository}\" , \"Full repository path\" ) stringParam ( \"DEFAULT_BRANCH\" , \"${defaultBranch}\" , \"Default repository branch\" ) } } } } } def createListView ( codebaseName , branchName ) { listView ( \"${codebaseName}/${branchName}\" ) { if ( branchName . toLowerCase () == \"releases\" ) { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^Create-release.*\" ) } } } else { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^${branchName}-(Code-review|Build).*\" ) } } } columns { status () weather () name () lastSuccess () lastFailure () lastDuration () buildButton () } } } Job Provision Pipeline Parameters The job-provisions pipeline consists of the following parameters: NAME - the application name; TYPE - the codebase type (the application / library / autotest); BUILD_TOOL - a tool that is used to build the application; BRANCH - a branch name; GIT_SERVER_CR_NAME - the name of the application Git server custom resource; GIT_SERVER_CR_VERSION - the version of the application Git server custom resource; GIT_CREDENTIALS_ID - the secret name where Git server credentials are stored (default 'gerrit-ciuser-sshkey'); REPOSITORY_PATH - the full repository path; JIRA_INTEGRATION_ENABLED - the Jira integration is enabled or not; PLATFORM_TYPE - the type of platform (kubernetes or openshift); DEFAULT_BRANCH - the default repository branch. GitHub (github) \u2693\ufe0e To create a new job provision for work with GitHub, take the following steps: Navigate to the Jenkins main page and open the job-provisions/ci folder. Click New Item and type the name of job-provisions - github . Select the Freestyle project option and click OK. Select the Discard old builds check box and configure a few parameters: Strategy: Log Rotation Days to keep builds: 10 Max # of builds to keep: 10 Select the This project is parameterized check box and add a few input parameters: NAME; TYPE; BUILD_TOOL; BRANCH; GIT_SERVER_CR_NAME; GIT_SERVER_CR_VERSION; GIT_CREDENTIALS_ID; REPOSITORY_PATH; JIRA_INTEGRATION_ENABLED; PLATFORM_TYPE; DEFAULT_BRANCH. Check the Execute concurrent builds if necessary option. Check the Restrict where this project can be run option. Fill in the Label Expression field by typing master to ensure job runs on Jenkins Master. In the Build section, perform the following: Select DSL Script ; Select the Use the provided DSL script check box: DSL script check box As soon as all the steps above are performed, insert the code: View: Template import groovy.json.* import jenkins.model.Jenkins import javaposse.jobdsl.plugin.* import com.cloudbees.hudson.plugins.folder.* Jenkins jenkins = Jenkins . instance def stages = [:] def jiraIntegrationEnabled = Boolean . parseBoolean ( \"${JIRA_INTEGRATION_ENABLED}\" as String ) def commitValidateStage = jiraIntegrationEnabled ? ',{\"name\": \"commit-validate\"}' : '' def createJIMStage = jiraIntegrationEnabled ? ',{\"name\": \"create-jira-issue-metadata\"}' : '' def platformType = \"${PLATFORM_TYPE}\" def buildStage = platformType == \"kubernetes\" ? ',{\"name\": \"build-image-kaniko\"}' : ',{\"name\": \"build-image-from-dockerfile\"}' def buildTool = \"${BUILD_TOOL}\" def goBuildStage = buildTool . toString () == \"go\" ? ',{\"name\": \"build\"}' : ',{\"name\": \"compile\"}' stages [ 'Code-review-application' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + goBuildStage + ',{\"name\": \"tests\"},[{\"name\": \"sonar\"},{\"name\": \"dockerfile-lint\"},{\"name\": \"helm-lint\"}]]' stages [ 'Code-review-library' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"compile\"},{\"name\": \"tests\"},' + '{\"name\": \"sonar\"}]' stages [ 'Code-review-autotests' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"tests\"},{\"name\": \"sonar\"}' + ']' stages [ 'Code-review-default' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ']' stages [ 'Code-review-library-terraform' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"terraform-lint\"}]' stages [ 'Code-review-library-opa' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"tests\"}]' stages [ 'Code-review-library-codenarc' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"sonar\"},{\"name\": \"build\"}]' stages [ 'Code-review-library-kaniko' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"dockerbuild-verify\"}]' stages [ 'Build-library-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-npm' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-gradle' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-terraform' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"terraform-lint\"}' + ',{\"name\": \"terraform-plan\"},{\"name\": \"terraform-apply\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-opa' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"tests\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-codenarc' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-kaniko' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"build-image-kaniko\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-autotests-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-autotests-gradle' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sast\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-npm' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-gradle' ] = stages [ 'Build-application-maven' ] stages [ 'Build-application-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-go' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sast\"},{\"name\": \"tests\"},{\"name\": \"sonar\"},' + '{\"name\": \"build\"}' + \"${buildStage}\" + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"${buildStage}\" + ',{\"name\":\"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Create-release' ] = '[{\"name\": \"checkout\"},{\"name\": \"create-branch\"},{\"name\": \"trigger-job\"}]' def buildToolsOutOfTheBox = [ \"maven\" , \"npm\" , \"gradle\" , \"dotnet\" , \"none\" , \"go\" , \"python\" ] def defaultStages = '[{\"name\": \"checkout\"}' + \"${createJIMStage}\" + ']' def codebaseName = \"${NAME}\" def gitServerCrName = \"${GIT_SERVER_CR_NAME}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID ? GIT_CREDENTIALS_ID : 'gerrit-ciuser-sshkey'}\" def repositoryPath = \"${REPOSITORY_PATH.replaceAll(~/:\\d+\\\\//,\" /\")}\" def githubRepository = \"https://${repositoryPath.split(\"@\")[1]}\" def defaultBranch = \"${DEFAULT_BRANCH}\" def codebaseFolder = jenkins.getItem(codebaseName) if (codebaseFolder == null) { folder(codebaseName) } createListView(codebaseName, \"Releases\") createReleasePipeline(\"Create-release-${codebaseName}\", codebaseName, stages[\"Create-release\"], \"CreateRelease\", repositoryPath, gitCredentialsId, gitServerCrName, gitServerCrVersion, jiraIntegrationEnabled, platformType, defaultBranch) if (buildTool.toString().equalsIgnoreCase('none')) { return true } if (BRANCH) { def branch = \"${BRANCH}\" def formattedBranch = \"${branch.toUpperCase().replaceAll(/ \\\\ //, \"-\")}\" createListView ( codebaseName , formattedBranch ) def type = \"${TYPE}\" def supBuildTool = buildToolsOutOfTheBox . contains ( buildTool . toString ()) def crKey = getStageKeyName ( buildTool ). toString () createCodeReviewPipeline ( \"Code-review-${codebaseName}\" , codebaseName , stages . get ( crKey , defaultStages ), \"CodeReview\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion , githubRepository ) registerWebHook ( repositoryPath ) def buildKey = \"Build-${type}-${buildTool.toLowerCase()}\" . toString () if ( type . equalsIgnoreCase ( 'application' ) || type . equalsIgnoreCase ( 'library' ) || type . equalsIgnoreCase ( 'autotests' )) { def jobExists = false if ( \"${formattedBranch}-Build-${codebaseName}\" . toString () in Jenkins . instance . getAllItems (). collect { it . name }) jobExists = true createBuildPipeline ( \"Build-${codebaseName}\" , codebaseName , stages . get ( buildKey , defaultStages ), \"Build\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion , githubRepository ) registerWebHook ( repositoryPath , 'build' ) if (! jobExists ) queue ( \"${codebaseName}/${formattedBranch}-Build-${codebaseName}\" ) } } def getStageKeyName ( buildTool ) { if ( buildTool . toString (). equalsIgnoreCase ( 'terraform' )) { return \"Code-review-library-terraform\" } if ( buildTool . toString (). equalsIgnoreCase ( 'opa' )) { return \"Code-review-library-opa\" } if ( buildTool . toString (). equalsIgnoreCase ( 'codenarc' )) { return \"Code-review-library-codenarc\" } if ( buildTool . toString (). equalsIgnoreCase ( 'kaniko' )) { return \"Code-review-library-kaniko\" } def buildToolsOutOfTheBox = [ \"maven\" , \"npm\" , \"gradle\" , \"dotnet\" , \"none\" , \"go\" , \"python\" ] def supBuildTool = buildToolsOutOfTheBox . contains ( buildTool . toString ()) return supBuildTool ? \"Code-review-${TYPE}\" : \"Code-review-default\" } def createCodeReviewPipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , defaultBranch , gitServerCrName , gitServerCrVersion , githubRepository ) { pipelineJob ( \"${codebaseName}/${defaultBranch.toUpperCase().replaceAll(/\\\\//, \" - \")}-${pipelineName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) if ( pipelineName . contains ( \"Build\" )) stringParam ( \"BRANCH\" , \"${defaultBranch}\" , \"Branch to build artifact from\" ) else stringParam ( \"BRANCH\" , \"\\${ghprbActualCommit}\" , \"Branch to build artifact from\" ) } } triggers { githubPullRequest { cron ( '' ) onlyTriggerPhrase ( false ) useGitHubHooks ( true ) permitAll ( true ) autoCloseFailedPullRequests ( false ) displayBuildErrorsOnDownstreamBuilds ( false ) whiteListTargetBranches ([ defaultBranch . toString ()]) extensions { commitStatus { context ( 'Jenkins Code-Review' ) triggeredStatus ( 'Build is Triggered' ) startedStatus ( 'Build is Started' ) addTestResults ( true ) completedStatus ( 'SUCCESS' , 'Verified' ) completedStatus ( 'FAILURE' , 'Failed' ) completedStatus ( 'PENDING' , 'Penging' ) completedStatus ( 'ERROR' , 'Error' ) } } } } properties { githubProjectProperty { projectUrlStr ( \"${githubRepository}\" ) } } } } def createBuildPipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , defaultBranch , gitServerCrName , gitServerCrVersion , githubRepository ) { pipelineJob ( \"${codebaseName}/${defaultBranch.toUpperCase().replaceAll(/\\\\//, \" - \")}-${pipelineName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\nnode {\\n git credentialsId: \\'${credId}\\', url: \\'${repository}\\', branch: \\'${BRANCH}\\'\\n}\\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) stringParam ( \"BRANCH\" , \"${defaultBranch}\" , \"Branch to run from\" ) } } triggers { gitHubPushTrigger () } properties { githubProjectProperty { projectUrlStr ( \"${githubRepository}\" ) } } } } def createListView ( codebaseName , branchName ) { listView ( \"${codebaseName}/${branchName}\" ) { if ( branchName . toLowerCase () == \"releases\" ) { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^Create-release.*\" ) } } } else { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^${branchName}-(Code-review|Build).*\" ) } } } columns { status () weather () name () lastSuccess () lastFailure () lastDuration () buildButton () } } } def createReleasePipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType , defaultBranch ) { pipelineJob ( \"${codebaseName}/${pipelineName}\" ) { logRotator { numToKeep ( 14 ) daysToKeep ( 30 ) } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"\" ) if ( pipelineName . contains ( \"Create-release\" )) { stringParam ( \"JIRA_INTEGRATION_ENABLED\" , \"${jiraIntegrationEnabled}\" , \"Is Jira integration enabled\" ) stringParam ( \"PLATFORM_TYPE\" , \"${platformType}\" , \"Platform type\" ) stringParam ( \"GERRIT_PROJECT\" , \"${codebaseName}\" , \"\" ) stringParam ( \"RELEASE_NAME\" , \"\" , \"Name of the release(branch to be created)\" ) stringParam ( \"COMMIT_ID\" , \"\" , \"Commit ID that will be used to create branch from for new release. If empty, DEFAULT_BRANCH will be used\" ) stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"REPOSITORY_PATH\" , \"${repository}\" , \"Full repository path\" ) stringParam ( \"DEFAULT_BRANCH\" , \"${defaultBranch}\" , \"Default repository branch\" ) } } } } } def registerWebHook ( repositoryPath , type = 'code-review' ) { def url = repositoryPath . split ( '@' )[ 1 ]. split ( '/' )[ 0 ] def owner = repositoryPath . split ( '@' )[ 1 ]. split ( '/' )[ 1 ] def repo = repositoryPath . split ( '@' )[ 1 ]. split ( '/' )[ 2 ] def apiUrl = 'https://api.' + url + '/repos/' + owner + '/' + repo + '/hooks' def webhookUrl = '' def webhookConfig = [:] def config = [:] def events = [] if ( type . equalsIgnoreCase ( 'build' )) { webhookUrl = System . getenv ( 'JENKINS_UI_URL' ) + \"/github-webhook/\" events = [ \"push\" ] config [ \"url\" ] = webhookUrl config [ \"content_type\" ] = \"json\" config [ \"insecure_ssl\" ] = 0 webhookConfig [ \"name\" ] = \"web\" webhookConfig [ \"config\" ] = config webhookConfig [ \"events\" ] = events webhookConfig [ \"active\" ] = true } else { webhookUrl = System . getenv ( 'JENKINS_UI_URL' ) + \"/ghprbhook/\" events = [ \"issue_comment\" , \"pull_request\" ] config [ \"url\" ] = webhookUrl config [ \"content_type\" ] = \"form\" config [ \"insecure_ssl\" ] = 0 webhookConfig [ \"name\" ] = \"web\" webhookConfig [ \"config\" ] = config webhookConfig [ \"events\" ] = events webhookConfig [ \"active\" ] = true } def requestBody = JsonOutput . toJson ( webhookConfig ) def http = new URL ( apiUrl ). openConnection () as HttpURLConnection http . setRequestMethod ( 'POST' ) http . setDoOutput ( true ) println ( apiUrl ) http . setRequestProperty ( \"Accept\" , 'application/json' ) http . setRequestProperty ( \"Content-Type\" , 'application/json' ) http . setRequestProperty ( \"Authorization\" , \"token ${getSecretValue('github-access-token')}\" ) http . outputStream . write ( requestBody . getBytes ( \"UTF-8\" )) http . connect () println ( http . responseCode ) if ( http . responseCode == 201 ) { response = new JsonSlurper (). parseText ( http . inputStream . getText ( 'UTF-8' )) } else { response = new JsonSlurper (). parseText ( http . errorStream . getText ( 'UTF-8' )) } println \"response: ${response}\" } def getSecretValue ( name ) { def creds = com . cloudbees . plugins . credentials . CredentialsProvider . lookupCredentials ( com . cloudbees . plugins . credentials . common . StandardCredentials . class , Jenkins . instance , null , null ) def secret = creds . find { it . properties [ 'id' ] == name } return secret != null ? secret [ 'secret' ] : null } After the steps above are performed, the new custom job-provision will be available in Advanced Settings during the application creation in Admin Console. Github job provision GitLab (gitlab) \u2693\ufe0e To create a new job provision for work with GitLab, take the following steps: Navigate to the Jenkins main page and open the job-provisions/ci folder. Click New Item and type the name of job-provisions - gitlab . Select the Freestyle project option and click OK. Select the Discard old builds check box and configure a few parameters: Strategy: Log Rotation Days to keep builds: 10 Max # of builds to keep: 10 Select the This project is parameterized check box and add a few input parameters as the following strings: NAME; TYPE; BUILD_TOOL; BRANCH; GIT_SERVER_CR_NAME; GIT_SERVER_CR_VERSION; GIT_SERVER; GIT_SSH_PORT; GIT_USERNAME; GIT_CREDENTIALS_ID; REPOSITORY_PATH; JIRA_INTEGRATION_ENABLED; PLATFORM_TYPE; DEFAULT_BRANCH; Check the Execute concurrent builds if necessary option. Check the Restrict where this project can be run option. Fill in the Label Expression field by typing master to ensure job runs on Jenkins Master. In the Build section, perform the following: Select DSL Script ; Select the Use the provided DSL script check box: DSL script check box As soon as all the steps above are performed, insert the code: View: Template import groovy.json.* import jenkins.model.Jenkins Jenkins jenkins = Jenkins . instance def stages = [:] def jiraIntegrationEnabled = Boolean . parseBoolean ( \"${JIRA_INTEGRATION_ENABLED}\" as String ) def commitValidateStage = jiraIntegrationEnabled ? ',{\"name\": \"commit-validate\"}' : '' def createJIMStage = jiraIntegrationEnabled ? ',{\"name\": \"create-jira-issue-metadata\"}' : '' def platformType = \"${PLATFORM_TYPE}\" def buildTool = \"${BUILD_TOOL}\" def buildImageStage = platformType == \"kubernetes\" ? ',{\"name\": \"build-image-kaniko\"},' : ',{\"name\": \"build-image-from-dockerfile\"},' def goBuildImageStage = platformType == \"kubernetes\" ? ',{\"name\": \"build-image-kaniko\"}' : ',{\"name\": \"build-image-from-dockerfile\"}' def goBuildStage = buildTool . toString () == \"go\" ? ',{\"name\": \"build\"}' : ',{\"name\": \"compile\"}' stages [ 'Code-review-application' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + goBuildStage + ',{\"name\": \"tests\"},[{\"name\": \"sonar\"},{\"name\": \"dockerfile-lint\"},{\"name\": \"helm-lint\"}]]' stages [ 'Code-review-library' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"compile\"},{\"name\": \"tests\"},' + '{\"name\": \"sonar\"}]' stages [ 'Code-review-autotests' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"tests\"},{\"name\": \"sonar\"}' + ']' stages [ 'Code-review-default' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ']' stages [ 'Code-review-library-terraform' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"terraform-lint\"}]' stages [ 'Code-review-library-opa' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"tests\"}]' stages [ 'Code-review-library-codenarc' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"sonar\"},{\"name\": \"build\"}]' stages [ 'Code-review-library-kaniko' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"dockerbuild-verify\"}]' stages [ 'Build-library-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-npm' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-gradle' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-terraform' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"terraform-lint\"}' + ',{\"name\": \"terraform-plan\"},{\"name\": \"terraform-apply\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-opa' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"tests\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-codenarc' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-kaniko' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"build-image-kaniko\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-autotests-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-autotests-gradle' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sast\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildImageStage}\" + '{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"${buildImageStage}\" + '{\"name\":\"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-npm' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildImageStage}\" + '{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-gradle' ] = stages [ 'Build-application-maven' ] stages [ 'Build-application-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"${buildImageStage}\" + '{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-terraform' ] = '[{\"name\": \"checkout\"},{\"name\": \"tool-init\"},' + '{\"name\": \"lint\"},{\"name\": \"git-tag\"}]' stages [ 'Build-application-helm' ] = '[{\"name\": \"checkout\"},{\"name\": \"lint\"}]' stages [ 'Build-application-docker' ] = '[{\"name\": \"checkout\"},{\"name\": \"lint\"}]' stages [ 'Build-application-go' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sast\"},{\"name\": \"tests\"},{\"name\": \"sonar\"},' + '{\"name\": \"build\"}' + \"${goBuildImageStage}\" + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Create-release' ] = '[{\"name\": \"checkout\"},{\"name\": \"create-branch\"},{\"name\": \"trigger-job\"}]' def defaultStages = '[{\"name\": \"checkout\"}' + \"${createJIMStage}\" + ']' def codebaseName = \"${NAME}\" def gitServerCrName = \"${GIT_SERVER_CR_NAME}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitServer = \"${GIT_SERVER ? GIT_SERVER : 'gerrit'}\" def gitSshPort = \"${GIT_SSH_PORT ? GIT_SSH_PORT : '29418'}\" def gitUsername = \"${GIT_USERNAME ? GIT_USERNAME : 'jenkins'}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID ? GIT_CREDENTIALS_ID : 'gerrit-ciuser-sshkey'}\" def defaultRepoPath = \"ssh://${gitUsername}@${gitServer}:${gitSshPort}/${codebaseName}\" def repositoryPath = \"${REPOSITORY_PATH ? REPOSITORY_PATH : defaultRepoPath}\" def defaultBranch = \"${DEFAULT_BRANCH}\" def codebaseFolder = jenkins . getItem ( codebaseName ) if ( codebaseFolder == null ) { folder ( codebaseName ) } createListView ( codebaseName , \"Releases\" ) createReleasePipeline ( \"Create-release-${codebaseName}\" , codebaseName , stages [ \"Create-release\" ], \"CreateRelease\" , repositoryPath , gitCredentialsId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType , defaultBranch ) if ( BRANCH ) { def branch = \"${BRANCH}\" def formattedBranch = \"${branch.toUpperCase().replaceAll(/\\\\//, \" - \")}\" createListView ( codebaseName , formattedBranch ) def type = \"${TYPE}\" def crKey = getStageKeyName ( buildTool ). toString () createCiPipeline ( \"Code-review-${codebaseName}\" , codebaseName , stages . get ( crKey , defaultStages ), \"CodeReview\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) def buildKey = \"Build-${type}-${buildTool.toLowerCase()}\" . toString () if ( type . equalsIgnoreCase ( 'application' ) || type . equalsIgnoreCase ( 'library' ) || type . equalsIgnoreCase ( 'autotests' )) { def jobExists = false if ( \"${formattedBranch}-Build-${codebaseName}\" . toString () in Jenkins . instance . getAllItems (). collect { it . name }) { jobExists = true } createCiPipeline ( \"Build-${codebaseName}\" , codebaseName , stages . get ( buildKey , defaultStages ), \"Build\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) if (! jobExists ) { queue ( \"${codebaseName}/${formattedBranch}-Build-${codebaseName}\" ) } } } def createCiPipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , defaultBranch , gitServerCrName , gitServerCrVersion ) { def jobName = \"${defaultBranch.toUpperCase().replaceAll(/\\\\//, \" - \")}-${pipelineName}\" def existingJob = Jenkins . getInstance (). getItemByFullName ( \"${codebaseName}/${jobName}\" ) def webhookToken = null if ( existingJob ) { def triggersMap = existingJob . getTriggers () triggersMap . each { key , value -> webhookToken = value . getSecretToken () } } else { def random = new byte [ 16 ] new java . security . SecureRandom (). nextBytes ( random ) webhookToken = random . encodeHex (). toString () } pipelineJob ( \"${codebaseName}/${jobName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } properties { gitLabConnection { gitLabConnection ( 'gitlab' ) } } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) if ( pipelineName . contains ( \"Build\" )) stringParam ( \"BRANCH\" , \"${defaultBranch}\" , \"Branch to build artifact from\" ) else stringParam ( \"BRANCH\" , \"\\${gitlabMergeRequestLastCommit}\" , \"Branch to build artifact from\" ) } } triggers { gitlabPush { buildOnMergeRequestEvents ( pipelineName . contains ( \"Build\" ) ? false : true ) buildOnPushEvents ( pipelineName . contains ( \"Build\" ) ? true : false ) enableCiSkip ( false ) setBuildDescription ( true ) rebuildOpenMergeRequest ( pipelineName . contains ( \"Build\" ) ? 'never' : 'source' ) commentTrigger ( \"Build it please\" ) skipWorkInProgressMergeRequest ( true ) targetBranchRegex ( \"${defaultBranch}\" ) } } configure { it / triggers / 'com.dabsquared.gitlabjenkins.GitLabPushTrigger' << secretToken ( webhookToken ) it / triggers / 'com.dabsquared.gitlabjenkins.GitLabPushTrigger' << triggerOnApprovedMergeRequest ( pipelineName . contains ( \"Build\" ) ? false : true ) it / triggers / 'com.dabsquared.gitlabjenkins.GitLabPushTrigger' << pendingBuildName ( pipelineName . contains ( \"Build\" ) ? \"\" : \"Jenkins\" ) } } registerWebHook ( repository , codebaseName , jobName , webhookToken ) } def getStageKeyName ( buildTool ) { if ( buildTool . toString (). equalsIgnoreCase ( 'terraform' )) { return \"Code-review-library-terraform\" } if ( buildTool . toString (). equalsIgnoreCase ( 'opa' )) { return \"Code-review-library-opa\" } if ( buildTool . toString (). equalsIgnoreCase ( 'codenarc' )) { return \"Code-review-library-codenarc\" } if ( buildTool . toString (). equalsIgnoreCase ( 'kaniko' )) { return \"Code-review-library-kaniko\" } def buildToolsOutOfTheBox = [ \"maven\" , \"npm\" , \"gradle\" , \"dotnet\" , \"none\" , \"go\" , \"python\" ] def supBuildTool = buildToolsOutOfTheBox . contains ( buildTool . toString ()) return supBuildTool ? \"Code-review-${TYPE}\" : \"Code-review-default\" } def createReleasePipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType , defaultBranch ) { pipelineJob ( \"${codebaseName}/${pipelineName}\" ) { logRotator { numToKeep ( 14 ) daysToKeep ( 30 ) } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"\" ) if ( pipelineName . contains ( \"Create-release\" )) { stringParam ( \"JIRA_INTEGRATION_ENABLED\" , \"${jiraIntegrationEnabled}\" , \"Is Jira integration enabled\" ) stringParam ( \"PLATFORM_TYPE\" , \"${platformType}\" , \"Platform type\" ) stringParam ( \"GERRIT_PROJECT\" , \"${codebaseName}\" , \"\" ) stringParam ( \"RELEASE_NAME\" , \"\" , \"Name of the release(branch to be created)\" ) stringParam ( \"COMMIT_ID\" , \"\" , \"Commit ID that will be used to create branch from for new release. If empty, DEFAULT_BRANCH will be used\" ) stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"REPOSITORY_PATH\" , \"${repository}\" , \"Full repository path\" ) stringParam ( \"DEFAULT_BRANCH\" , \"${defaultBranch}\" , \"Default repository branch\" ) } } } } } def createListView ( codebaseName , branchName ) { listView ( \"${codebaseName}/${branchName}\" ) { if ( branchName . toLowerCase () == \"releases\" ) { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^Create-release.*\" ) } } } else { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^${branchName}-(Code-review|Build).*\" ) } } } columns { status () weather () name () lastSuccess () lastFailure () lastDuration () buildButton () } } } def registerWebHook ( repositoryPath , codebaseName , jobName , webhookToken ) { def apiUrl = 'https://' + repositoryPath . replaceAll ( \"ssh://\" , \"\" ). split ( '@' )[ 1 ]. replace ( '/' , \"%2F\" ). replaceAll (~ /:\\d+%2F/ , '/api/v4/projects/' ) + '/hooks' def jobWebhookUrl = \"${System.getenv('JENKINS_UI_URL')}/project/${codebaseName}/${jobName}\" def gitlabToken = getSecretValue ( 'gitlab-access-token' ) if ( checkWebHookExist ( apiUrl , jobWebhookUrl , gitlabToken )) { println ( \"[JENKINS][DEBUG] Webhook for job ${jobName} is already exist\\r\\n\" ) return } println ( \"[JENKINS][DEBUG] Creating webhook for job ${jobName}\" ) def webhookConfig = [:] webhookConfig [ \"url\" ] = jobWebhookUrl webhookConfig [ \"push_events\" ] = jobName . contains ( \"Build\" ) ? \"true\" : \"false\" webhookConfig [ \"merge_requests_events\" ] = jobName . contains ( \"Build\" ) ? \"false\" : \"true\" webhookConfig [ \"issues_events\" ] = \"false\" webhookConfig [ \"confidential_issues_events\" ] = \"false\" webhookConfig [ \"tag_push_events\" ] = \"false\" webhookConfig [ \"note_events\" ] = \"true\" webhookConfig [ \"job_events\" ] = \"false\" webhookConfig [ \"pipeline_events\" ] = \"false\" webhookConfig [ \"wiki_page_events\" ] = \"false\" webhookConfig [ \"enable_ssl_verification\" ] = \"true\" webhookConfig [ \"token\" ] = webhookToken def requestBody = JsonOutput . toJson ( webhookConfig ) def httpConnector = new URL ( apiUrl ). openConnection () as HttpURLConnection httpConnector . setRequestMethod ( 'POST' ) httpConnector . setDoOutput ( true ) httpConnector . setRequestProperty ( \"Accept\" , 'application/json' ) httpConnector . setRequestProperty ( \"Content-Type\" , 'application/json' ) httpConnector . setRequestProperty ( \"PRIVATE-TOKEN\" , \"${gitlabToken}\" ) httpConnector . outputStream . write ( requestBody . getBytes ( \"UTF-8\" )) httpConnector . connect () if ( httpConnector . responseCode == 201 ) println ( \"[JENKINS][DEBUG] Webhook for job ${jobName} has been created\\r\\n\" ) else { println ( \"[JENKINS][ERROR] Responce code - ${httpConnector.responseCode}\" ) def response = new JsonSlurper (). parseText ( httpConnector . errorStream . getText ( 'UTF-8' )) println ( \"[JENKINS][ERROR] Failed to create webhook for job ${jobName}. Response - ${response}\" ) } } def checkWebHookExist ( apiUrl , jobWebhookUrl , gitlabToken ) { println ( \"[JENKINS][DEBUG] Checking if webhook ${jobWebhookUrl} exists\" ) def httpConnector = new URL ( apiUrl ). openConnection () as HttpURLConnection httpConnector . setRequestMethod ( 'GET' ) httpConnector . setDoOutput ( true ) httpConnector . setRequestProperty ( \"Accept\" , 'application/json' ) httpConnector . setRequestProperty ( \"Content-Type\" , 'application/json' ) httpConnector . setRequestProperty ( \"PRIVATE-TOKEN\" , \"${gitlabToken}\" ) httpConnector . connect () if ( httpConnector . responseCode == 200 ) { def response = new JsonSlurper (). parseText ( httpConnector . inputStream . getText ( 'UTF-8' )) return response . find { it . url == jobWebhookUrl } ? true : false } } def getSecretValue ( name ) { def creds = com . cloudbees . plugins . credentials . CredentialsProvider . lookupCredentials ( com . cloudbees . plugins . credentials . common . StandardCredentials . class , Jenkins . instance , null , null ) def secret = creds . find { it . properties [ 'id' ] == name } return secret != null ? secret [ 'secret' ] : null } Create Secret, GitServer CR and Jenkins credentials with the \"gitlab\" ID by following the instruction: Adjust Import Strategy . After the steps above are performed, the new custom job-provision will be available in Advanced Settings during the application creation in Admin Console. Gitlab job provision Related Articles \u2693\ufe0e CI Pipeline for Container","title":"Manage Jenkins CI Pipeline Job Provisioner"},{"location":"operator-guide/manage-jenkins-ci-job-provision/#manage-jenkins-ci-pipeline-job-provisioner","text":"The Jenkins CI job provisioner (or seed-job) is used to create and manage the application folder, and its Code Review, Build and Create Release pipelines . Depending on the version control system, different job provisioners are used. EDP supports integration with the following version control systems: Gerrit (default) GitHub (github) GitLab (gitlab) By default, the Jenkins operator creates a pipeline for several types of application and libraries . There is a special job-provisions/ci folder in Jenkins for these provisioners. During the EDP deployment, a default provisioner is created for integration with Gerrit version control system. To configure integration with other version control systems, you need to add the required job provisioners to job-provisions/ci folder in Jenkins.","title":"Manage Jenkins CI Pipeline Job Provisioner"},{"location":"operator-guide/manage-jenkins-ci-job-provision/#custom-custom-defaultgithubgitlab","text":"In some cases it is necessary to modify or update the job provisioner logic, for example when an added other code language needs to create a custom job provisioner on the basis of an existing one out of the box. Take the steps below to add a custom job provision: Navigate to the Jenkins main page and open the job-provisions/ci folder, click New Item and type the name of job-provisions, for example - custom-github. CI provisioner name Scroll down to the Copy from field and enter \"/job-provisions/ci/github\", and click OK: Copy ci provisioner Update the required parameters in the new provisioner. For example, if it is necessary to implement a new build tool docker , several parameters are to be updated. Add the following stages to the docker Code Review and Build pipelines for docker application: stages [ ' Code-review-application-docker ' ] = ' [{\"name\": \"checkout\"},{\"name\": \"lint\"},{\"name\": \"build\"}] ' ... stages [ ' Build-application-docker ' ] = ' [{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"lint\"},{\"name\": \"build\"},{\"name\": \"push\"},{\"name\": \"git-tag\"}] ' ... def getStageKeyName ( buildTool ) { ... if ( buildTool . toString () . equalsIgnoreCase ( ' docker ' )) { return \" Code-review-application-docker \" } ... } Note Make sure the support for the above mentioned logic is implemented. Please refer to the How to Redefine or Extend the EDP Pipeline Stages Library section of the guide. Note The default template should be changed if there is another creation logic for the Code Review, Build and Create Release pipelines. Furthermore, all pipeline types should have the necessary stages as well. After the steps above are performed, the new custom job provision will be available in Advanced Settings during the application creation in Admin Console. Custom ci provision","title":"Custom (custom-default/github/gitlab)"},{"location":"operator-guide/manage-jenkins-ci-job-provision/#gerrit-default","text":"During the EDP deployment, a default provisioner is created for integration with Gerrit version control system. Find the configuration in job-provisions/ci/default . Default template is presented below: View: Default template /* Copyright 2022 EPAM Systems. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ import groovy.json.* import jenkins.model.Jenkins Jenkins jenkins = Jenkins . instance def stages = [:] def jiraIntegrationEnabled = Boolean . parseBoolean ( \"${JIRA_INTEGRATION_ENABLED}\" as String ) def commitValidateStage = jiraIntegrationEnabled ? ',{\"name\": \"commit-validate\"}' : '' def createJIMStage = jiraIntegrationEnabled ? ',{\"name\": \"create-jira-issue-metadata\"}' : '' def platformType = \"${PLATFORM_TYPE}\" def buildStage = platformType . toString () == \"kubernetes\" ? ',{\"name\": \"build-image-kaniko\"}' : ',{\"name\": \"build-image-from-dockerfile\"}' def buildTool = \"${BUILD_TOOL}\" def goBuildStage = buildTool . toString () == \"go\" ? ',{\"name\": \"build\"}' : ',{\"name\": \"compile\"}' stages [ 'Code-review-application' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + goBuildStage + ',{\"name\": \"tests\"},[{\"name\": \"sonar\"},{\"name\": \"dockerfile-lint\"},{\"name\": \"helm-lint\"}]]' stages [ 'Code-review-library' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"compile\"},{\"name\": \"tests\"},{\"name\": \"sonar\"}]' stages [ 'Code-review-autotests' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"tests\"},{\"name\": \"sonar\"}' + ']' stages [ 'Code-review-default' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ']' stages [ 'Code-review-library-terraform' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"terraform-lint\"}]' stages [ 'Code-review-library-opa' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"tests\"}]' stages [ 'Code-review-library-codenarc' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"sonar\"},{\"name\": \"build\"}]' stages [ 'Code-review-library-kaniko' ] = '[{\"name\": \"gerrit-checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"dockerbuild-verify\"}]' stages [ 'Build-library-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-npm' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-gradle' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-terraform' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"terraform-lint\"}' + ',{\"name\": \"terraform-plan\"},{\"name\": \"terraform-apply\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-opa' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"tests\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-codenarc' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-kaniko' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"build-image-kaniko\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-autotests-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-autotests-gradle' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sast\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-npm' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-gradle' ] = stages [ 'Build-application-maven' ] stages [ 'Build-application-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-go' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sast\"},{\"name\": \"tests\"}' + ',{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildStage}\" + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Create-release' ] = '[{\"name\": \"checkout\"},{\"name\": \"create-branch\"},{\"name\": \"trigger-job\"}]' def defaultBuild = '[{\"name\": \"checkout\"}' + \"${createJIMStage}\" + ']' def codebaseName = \"${NAME}\" def gitServerCrName = \"${GIT_SERVER_CR_NAME}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID ? GIT_CREDENTIALS_ID : 'gerrit-ciuser-sshkey'}\" def repositoryPath = \"${REPOSITORY_PATH}\" def defaultBranch = \"${DEFAULT_BRANCH}\" def codebaseFolder = jenkins . getItem ( codebaseName ) if ( codebaseFolder == null ) { folder ( codebaseName ) } createListView ( codebaseName , \"Releases\" ) createReleasePipeline ( \"Create-release-${codebaseName}\" , codebaseName , stages [ \"Create-release\" ], \"CreateRelease\" , repositoryPath , gitCredentialsId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType , defaultBranch ) if ( buildTool . toString (). equalsIgnoreCase ( 'none' )) { return true } if ( BRANCH ) { def branch = \"${BRANCH}\" def formattedBranch = \"${branch.toUpperCase().replaceAll(/\\//, \" - \")}\" createListView ( codebaseName , formattedBranch ) def type = \"${TYPE}\" def crKey = getStageKeyName ( buildTool ) createCiPipeline ( \"Code-review-${codebaseName}\" , codebaseName , stages [ crKey ], \"CodeReview\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) def buildKey = \"Build-${type}-${buildTool.toLowerCase()}\" . toString () if ( type . equalsIgnoreCase ( 'application' ) || type . equalsIgnoreCase ( 'library' ) || type . equalsIgnoreCase ( 'autotests' )) { def jobExists = false if ( \"${formattedBranch}-Build-${codebaseName}\" . toString () in Jenkins . instance . getAllItems (). collect { it . name }) jobExists = true createCiPipeline ( \"Build-${codebaseName}\" , codebaseName , stages . get ( buildKey , defaultBuild ), \"Build\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) if (! jobExists ) queue ( \"${codebaseName}/${formattedBranch}-Build-${codebaseName}\" ) } } def createCiPipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , watchBranch , gitServerCrName , gitServerCrVersion ) { pipelineJob ( \"${codebaseName}/${watchBranch.toUpperCase().replaceAll(/\\//, \" - \")}-${pipelineName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } triggers { gerrit { events { if ( pipelineName . contains ( \"Build\" )) changeMerged () else patchsetCreated () } project ( \"plain:${codebaseName}\" , [ \"plain:${watchBranch}\" ]) } } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) stringParam ( \"BRANCH\" , \"${watchBranch}\" , \"Branch to build artifact from\" ) } } } } def getStageKeyName ( buildTool ) { if ( buildTool . toString (). equalsIgnoreCase ( 'terraform' )) { return \"Code-review-library-terraform\" } if ( buildTool . toString (). equalsIgnoreCase ( 'opa' )) { return \"Code-review-library-opa\" } if ( buildTool . toString (). equalsIgnoreCase ( 'codenarc' )) { return \"Code-review-library-codenarc\" } if ( buildTool . toString (). equalsIgnoreCase ( 'kaniko' )) { return \"Code-review-library-kaniko\" } def buildToolsOutOfTheBox = [ \"maven\" , \"npm\" , \"gradle\" , \"dotnet\" , \"none\" , \"go\" , \"python\" ] def supBuildTool = buildToolsOutOfTheBox . contains ( buildTool . toString ()) return supBuildTool ? \"Code-review-${TYPE}\" : \"Code-review-default\" } def createReleasePipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType , defaultBranch ) { pipelineJob ( \"${codebaseName}/${pipelineName}\" ) { logRotator { numToKeep ( 14 ) daysToKeep ( 30 ) } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"\" ) if ( pipelineName . contains ( \"Create-release\" )) { stringParam ( \"JIRA_INTEGRATION_ENABLED\" , \"${jiraIntegrationEnabled}\" , \"Is Jira integration enabled\" ) stringParam ( \"PLATFORM_TYPE\" , \"${platformType}\" , \"Platform type\" ) stringParam ( \"GERRIT_PROJECT\" , \"${codebaseName}\" , \"\" ) stringParam ( \"RELEASE_NAME\" , \"\" , \"Name of the release(branch to be created)\" ) stringParam ( \"COMMIT_ID\" , \"\" , \"Commit ID that will be used to create branch from for new release. If empty, DEFAULT_BRANCH will be used\" ) stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"REPOSITORY_PATH\" , \"${repository}\" , \"Full repository path\" ) stringParam ( \"DEFAULT_BRANCH\" , \"${defaultBranch}\" , \"Default repository branch\" ) } } } } } def createListView ( codebaseName , branchName ) { listView ( \"${codebaseName}/${branchName}\" ) { if ( branchName . toLowerCase () == \"releases\" ) { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^Create-release.*\" ) } } } else { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^${branchName}-(Code-review|Build).*\" ) } } } columns { status () weather () name () lastSuccess () lastFailure () lastDuration () buildButton () } } } Job Provision Pipeline Parameters The job-provisions pipeline consists of the following parameters: NAME - the application name; TYPE - the codebase type (the application / library / autotest); BUILD_TOOL - a tool that is used to build the application; BRANCH - a branch name; GIT_SERVER_CR_NAME - the name of the application Git server custom resource; GIT_SERVER_CR_VERSION - the version of the application Git server custom resource; GIT_CREDENTIALS_ID - the secret name where Git server credentials are stored (default 'gerrit-ciuser-sshkey'); REPOSITORY_PATH - the full repository path; JIRA_INTEGRATION_ENABLED - the Jira integration is enabled or not; PLATFORM_TYPE - the type of platform (kubernetes or openshift); DEFAULT_BRANCH - the default repository branch.","title":"Gerrit (default)"},{"location":"operator-guide/manage-jenkins-ci-job-provision/#github-github","text":"To create a new job provision for work with GitHub, take the following steps: Navigate to the Jenkins main page and open the job-provisions/ci folder. Click New Item and type the name of job-provisions - github . Select the Freestyle project option and click OK. Select the Discard old builds check box and configure a few parameters: Strategy: Log Rotation Days to keep builds: 10 Max # of builds to keep: 10 Select the This project is parameterized check box and add a few input parameters: NAME; TYPE; BUILD_TOOL; BRANCH; GIT_SERVER_CR_NAME; GIT_SERVER_CR_VERSION; GIT_CREDENTIALS_ID; REPOSITORY_PATH; JIRA_INTEGRATION_ENABLED; PLATFORM_TYPE; DEFAULT_BRANCH. Check the Execute concurrent builds if necessary option. Check the Restrict where this project can be run option. Fill in the Label Expression field by typing master to ensure job runs on Jenkins Master. In the Build section, perform the following: Select DSL Script ; Select the Use the provided DSL script check box: DSL script check box As soon as all the steps above are performed, insert the code: View: Template import groovy.json.* import jenkins.model.Jenkins import javaposse.jobdsl.plugin.* import com.cloudbees.hudson.plugins.folder.* Jenkins jenkins = Jenkins . instance def stages = [:] def jiraIntegrationEnabled = Boolean . parseBoolean ( \"${JIRA_INTEGRATION_ENABLED}\" as String ) def commitValidateStage = jiraIntegrationEnabled ? ',{\"name\": \"commit-validate\"}' : '' def createJIMStage = jiraIntegrationEnabled ? ',{\"name\": \"create-jira-issue-metadata\"}' : '' def platformType = \"${PLATFORM_TYPE}\" def buildStage = platformType == \"kubernetes\" ? ',{\"name\": \"build-image-kaniko\"}' : ',{\"name\": \"build-image-from-dockerfile\"}' def buildTool = \"${BUILD_TOOL}\" def goBuildStage = buildTool . toString () == \"go\" ? ',{\"name\": \"build\"}' : ',{\"name\": \"compile\"}' stages [ 'Code-review-application' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + goBuildStage + ',{\"name\": \"tests\"},[{\"name\": \"sonar\"},{\"name\": \"dockerfile-lint\"},{\"name\": \"helm-lint\"}]]' stages [ 'Code-review-library' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"compile\"},{\"name\": \"tests\"},' + '{\"name\": \"sonar\"}]' stages [ 'Code-review-autotests' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"tests\"},{\"name\": \"sonar\"}' + ']' stages [ 'Code-review-default' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ']' stages [ 'Code-review-library-terraform' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"terraform-lint\"}]' stages [ 'Code-review-library-opa' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"tests\"}]' stages [ 'Code-review-library-codenarc' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"sonar\"},{\"name\": \"build\"}]' stages [ 'Code-review-library-kaniko' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"dockerbuild-verify\"}]' stages [ 'Build-library-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-npm' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-gradle' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-terraform' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"terraform-lint\"}' + ',{\"name\": \"terraform-plan\"},{\"name\": \"terraform-apply\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-opa' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"tests\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-codenarc' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-kaniko' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"build-image-kaniko\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-autotests-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-autotests-gradle' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sast\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-npm' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-gradle' ] = stages [ 'Build-application-maven' ] stages [ 'Build-application-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"${buildStage}\" + ',{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-go' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sast\"},{\"name\": \"tests\"},{\"name\": \"sonar\"},' + '{\"name\": \"build\"}' + \"${buildStage}\" + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"${buildStage}\" + ',{\"name\":\"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Create-release' ] = '[{\"name\": \"checkout\"},{\"name\": \"create-branch\"},{\"name\": \"trigger-job\"}]' def buildToolsOutOfTheBox = [ \"maven\" , \"npm\" , \"gradle\" , \"dotnet\" , \"none\" , \"go\" , \"python\" ] def defaultStages = '[{\"name\": \"checkout\"}' + \"${createJIMStage}\" + ']' def codebaseName = \"${NAME}\" def gitServerCrName = \"${GIT_SERVER_CR_NAME}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID ? GIT_CREDENTIALS_ID : 'gerrit-ciuser-sshkey'}\" def repositoryPath = \"${REPOSITORY_PATH.replaceAll(~/:\\d+\\\\//,\" /\")}\" def githubRepository = \"https://${repositoryPath.split(\"@\")[1]}\" def defaultBranch = \"${DEFAULT_BRANCH}\" def codebaseFolder = jenkins.getItem(codebaseName) if (codebaseFolder == null) { folder(codebaseName) } createListView(codebaseName, \"Releases\") createReleasePipeline(\"Create-release-${codebaseName}\", codebaseName, stages[\"Create-release\"], \"CreateRelease\", repositoryPath, gitCredentialsId, gitServerCrName, gitServerCrVersion, jiraIntegrationEnabled, platformType, defaultBranch) if (buildTool.toString().equalsIgnoreCase('none')) { return true } if (BRANCH) { def branch = \"${BRANCH}\" def formattedBranch = \"${branch.toUpperCase().replaceAll(/ \\\\ //, \"-\")}\" createListView ( codebaseName , formattedBranch ) def type = \"${TYPE}\" def supBuildTool = buildToolsOutOfTheBox . contains ( buildTool . toString ()) def crKey = getStageKeyName ( buildTool ). toString () createCodeReviewPipeline ( \"Code-review-${codebaseName}\" , codebaseName , stages . get ( crKey , defaultStages ), \"CodeReview\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion , githubRepository ) registerWebHook ( repositoryPath ) def buildKey = \"Build-${type}-${buildTool.toLowerCase()}\" . toString () if ( type . equalsIgnoreCase ( 'application' ) || type . equalsIgnoreCase ( 'library' ) || type . equalsIgnoreCase ( 'autotests' )) { def jobExists = false if ( \"${formattedBranch}-Build-${codebaseName}\" . toString () in Jenkins . instance . getAllItems (). collect { it . name }) jobExists = true createBuildPipeline ( \"Build-${codebaseName}\" , codebaseName , stages . get ( buildKey , defaultStages ), \"Build\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion , githubRepository ) registerWebHook ( repositoryPath , 'build' ) if (! jobExists ) queue ( \"${codebaseName}/${formattedBranch}-Build-${codebaseName}\" ) } } def getStageKeyName ( buildTool ) { if ( buildTool . toString (). equalsIgnoreCase ( 'terraform' )) { return \"Code-review-library-terraform\" } if ( buildTool . toString (). equalsIgnoreCase ( 'opa' )) { return \"Code-review-library-opa\" } if ( buildTool . toString (). equalsIgnoreCase ( 'codenarc' )) { return \"Code-review-library-codenarc\" } if ( buildTool . toString (). equalsIgnoreCase ( 'kaniko' )) { return \"Code-review-library-kaniko\" } def buildToolsOutOfTheBox = [ \"maven\" , \"npm\" , \"gradle\" , \"dotnet\" , \"none\" , \"go\" , \"python\" ] def supBuildTool = buildToolsOutOfTheBox . contains ( buildTool . toString ()) return supBuildTool ? \"Code-review-${TYPE}\" : \"Code-review-default\" } def createCodeReviewPipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , defaultBranch , gitServerCrName , gitServerCrVersion , githubRepository ) { pipelineJob ( \"${codebaseName}/${defaultBranch.toUpperCase().replaceAll(/\\\\//, \" - \")}-${pipelineName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) if ( pipelineName . contains ( \"Build\" )) stringParam ( \"BRANCH\" , \"${defaultBranch}\" , \"Branch to build artifact from\" ) else stringParam ( \"BRANCH\" , \"\\${ghprbActualCommit}\" , \"Branch to build artifact from\" ) } } triggers { githubPullRequest { cron ( '' ) onlyTriggerPhrase ( false ) useGitHubHooks ( true ) permitAll ( true ) autoCloseFailedPullRequests ( false ) displayBuildErrorsOnDownstreamBuilds ( false ) whiteListTargetBranches ([ defaultBranch . toString ()]) extensions { commitStatus { context ( 'Jenkins Code-Review' ) triggeredStatus ( 'Build is Triggered' ) startedStatus ( 'Build is Started' ) addTestResults ( true ) completedStatus ( 'SUCCESS' , 'Verified' ) completedStatus ( 'FAILURE' , 'Failed' ) completedStatus ( 'PENDING' , 'Penging' ) completedStatus ( 'ERROR' , 'Error' ) } } } } properties { githubProjectProperty { projectUrlStr ( \"${githubRepository}\" ) } } } } def createBuildPipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , defaultBranch , gitServerCrName , gitServerCrVersion , githubRepository ) { pipelineJob ( \"${codebaseName}/${defaultBranch.toUpperCase().replaceAll(/\\\\//, \" - \")}-${pipelineName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\nnode {\\n git credentialsId: \\'${credId}\\', url: \\'${repository}\\', branch: \\'${BRANCH}\\'\\n}\\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) stringParam ( \"BRANCH\" , \"${defaultBranch}\" , \"Branch to run from\" ) } } triggers { gitHubPushTrigger () } properties { githubProjectProperty { projectUrlStr ( \"${githubRepository}\" ) } } } } def createListView ( codebaseName , branchName ) { listView ( \"${codebaseName}/${branchName}\" ) { if ( branchName . toLowerCase () == \"releases\" ) { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^Create-release.*\" ) } } } else { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^${branchName}-(Code-review|Build).*\" ) } } } columns { status () weather () name () lastSuccess () lastFailure () lastDuration () buildButton () } } } def createReleasePipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType , defaultBranch ) { pipelineJob ( \"${codebaseName}/${pipelineName}\" ) { logRotator { numToKeep ( 14 ) daysToKeep ( 30 ) } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"\" ) if ( pipelineName . contains ( \"Create-release\" )) { stringParam ( \"JIRA_INTEGRATION_ENABLED\" , \"${jiraIntegrationEnabled}\" , \"Is Jira integration enabled\" ) stringParam ( \"PLATFORM_TYPE\" , \"${platformType}\" , \"Platform type\" ) stringParam ( \"GERRIT_PROJECT\" , \"${codebaseName}\" , \"\" ) stringParam ( \"RELEASE_NAME\" , \"\" , \"Name of the release(branch to be created)\" ) stringParam ( \"COMMIT_ID\" , \"\" , \"Commit ID that will be used to create branch from for new release. If empty, DEFAULT_BRANCH will be used\" ) stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"REPOSITORY_PATH\" , \"${repository}\" , \"Full repository path\" ) stringParam ( \"DEFAULT_BRANCH\" , \"${defaultBranch}\" , \"Default repository branch\" ) } } } } } def registerWebHook ( repositoryPath , type = 'code-review' ) { def url = repositoryPath . split ( '@' )[ 1 ]. split ( '/' )[ 0 ] def owner = repositoryPath . split ( '@' )[ 1 ]. split ( '/' )[ 1 ] def repo = repositoryPath . split ( '@' )[ 1 ]. split ( '/' )[ 2 ] def apiUrl = 'https://api.' + url + '/repos/' + owner + '/' + repo + '/hooks' def webhookUrl = '' def webhookConfig = [:] def config = [:] def events = [] if ( type . equalsIgnoreCase ( 'build' )) { webhookUrl = System . getenv ( 'JENKINS_UI_URL' ) + \"/github-webhook/\" events = [ \"push\" ] config [ \"url\" ] = webhookUrl config [ \"content_type\" ] = \"json\" config [ \"insecure_ssl\" ] = 0 webhookConfig [ \"name\" ] = \"web\" webhookConfig [ \"config\" ] = config webhookConfig [ \"events\" ] = events webhookConfig [ \"active\" ] = true } else { webhookUrl = System . getenv ( 'JENKINS_UI_URL' ) + \"/ghprbhook/\" events = [ \"issue_comment\" , \"pull_request\" ] config [ \"url\" ] = webhookUrl config [ \"content_type\" ] = \"form\" config [ \"insecure_ssl\" ] = 0 webhookConfig [ \"name\" ] = \"web\" webhookConfig [ \"config\" ] = config webhookConfig [ \"events\" ] = events webhookConfig [ \"active\" ] = true } def requestBody = JsonOutput . toJson ( webhookConfig ) def http = new URL ( apiUrl ). openConnection () as HttpURLConnection http . setRequestMethod ( 'POST' ) http . setDoOutput ( true ) println ( apiUrl ) http . setRequestProperty ( \"Accept\" , 'application/json' ) http . setRequestProperty ( \"Content-Type\" , 'application/json' ) http . setRequestProperty ( \"Authorization\" , \"token ${getSecretValue('github-access-token')}\" ) http . outputStream . write ( requestBody . getBytes ( \"UTF-8\" )) http . connect () println ( http . responseCode ) if ( http . responseCode == 201 ) { response = new JsonSlurper (). parseText ( http . inputStream . getText ( 'UTF-8' )) } else { response = new JsonSlurper (). parseText ( http . errorStream . getText ( 'UTF-8' )) } println \"response: ${response}\" } def getSecretValue ( name ) { def creds = com . cloudbees . plugins . credentials . CredentialsProvider . lookupCredentials ( com . cloudbees . plugins . credentials . common . StandardCredentials . class , Jenkins . instance , null , null ) def secret = creds . find { it . properties [ 'id' ] == name } return secret != null ? secret [ 'secret' ] : null } After the steps above are performed, the new custom job-provision will be available in Advanced Settings during the application creation in Admin Console. Github job provision","title":"GitHub (github)"},{"location":"operator-guide/manage-jenkins-ci-job-provision/#gitlab-gitlab","text":"To create a new job provision for work with GitLab, take the following steps: Navigate to the Jenkins main page and open the job-provisions/ci folder. Click New Item and type the name of job-provisions - gitlab . Select the Freestyle project option and click OK. Select the Discard old builds check box and configure a few parameters: Strategy: Log Rotation Days to keep builds: 10 Max # of builds to keep: 10 Select the This project is parameterized check box and add a few input parameters as the following strings: NAME; TYPE; BUILD_TOOL; BRANCH; GIT_SERVER_CR_NAME; GIT_SERVER_CR_VERSION; GIT_SERVER; GIT_SSH_PORT; GIT_USERNAME; GIT_CREDENTIALS_ID; REPOSITORY_PATH; JIRA_INTEGRATION_ENABLED; PLATFORM_TYPE; DEFAULT_BRANCH; Check the Execute concurrent builds if necessary option. Check the Restrict where this project can be run option. Fill in the Label Expression field by typing master to ensure job runs on Jenkins Master. In the Build section, perform the following: Select DSL Script ; Select the Use the provided DSL script check box: DSL script check box As soon as all the steps above are performed, insert the code: View: Template import groovy.json.* import jenkins.model.Jenkins Jenkins jenkins = Jenkins . instance def stages = [:] def jiraIntegrationEnabled = Boolean . parseBoolean ( \"${JIRA_INTEGRATION_ENABLED}\" as String ) def commitValidateStage = jiraIntegrationEnabled ? ',{\"name\": \"commit-validate\"}' : '' def createJIMStage = jiraIntegrationEnabled ? ',{\"name\": \"create-jira-issue-metadata\"}' : '' def platformType = \"${PLATFORM_TYPE}\" def buildTool = \"${BUILD_TOOL}\" def buildImageStage = platformType == \"kubernetes\" ? ',{\"name\": \"build-image-kaniko\"},' : ',{\"name\": \"build-image-from-dockerfile\"},' def goBuildImageStage = platformType == \"kubernetes\" ? ',{\"name\": \"build-image-kaniko\"}' : ',{\"name\": \"build-image-from-dockerfile\"}' def goBuildStage = buildTool . toString () == \"go\" ? ',{\"name\": \"build\"}' : ',{\"name\": \"compile\"}' stages [ 'Code-review-application' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + goBuildStage + ',{\"name\": \"tests\"},[{\"name\": \"sonar\"},{\"name\": \"dockerfile-lint\"},{\"name\": \"helm-lint\"}]]' stages [ 'Code-review-library' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"compile\"},{\"name\": \"tests\"},' + '{\"name\": \"sonar\"}]' stages [ 'Code-review-autotests' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"tests\"},{\"name\": \"sonar\"}' + ']' stages [ 'Code-review-default' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ']' stages [ 'Code-review-library-terraform' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"terraform-lint\"}]' stages [ 'Code-review-library-opa' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"tests\"}]' stages [ 'Code-review-library-codenarc' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"sonar\"},{\"name\": \"build\"}]' stages [ 'Code-review-library-kaniko' ] = '[{\"name\": \"checkout\"}' + \"${commitValidateStage}\" + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"dockerbuild-verify\"}]' stages [ 'Build-library-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-npm' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-gradle' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-terraform' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"terraform-lint\"}' + ',{\"name\": \"terraform-plan\"},{\"name\": \"terraform-apply\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-opa' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"tests\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-codenarc' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-kaniko' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"build-image-kaniko\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-autotests-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-autotests-gradle' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sast\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildImageStage}\" + '{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"${buildImageStage}\" + '{\"name\":\"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-npm' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"${buildImageStage}\" + '{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-gradle' ] = stages [ 'Build-application-maven' ] stages [ 'Build-application-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"${buildImageStage}\" + '{\"name\": \"push\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-terraform' ] = '[{\"name\": \"checkout\"},{\"name\": \"tool-init\"},' + '{\"name\": \"lint\"},{\"name\": \"git-tag\"}]' stages [ 'Build-application-helm' ] = '[{\"name\": \"checkout\"},{\"name\": \"lint\"}]' stages [ 'Build-application-docker' ] = '[{\"name\": \"checkout\"},{\"name\": \"lint\"}]' stages [ 'Build-application-go' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"sast\"},{\"name\": \"tests\"},{\"name\": \"sonar\"},' + '{\"name\": \"build\"}' + \"${goBuildImageStage}\" + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Create-release' ] = '[{\"name\": \"checkout\"},{\"name\": \"create-branch\"},{\"name\": \"trigger-job\"}]' def defaultStages = '[{\"name\": \"checkout\"}' + \"${createJIMStage}\" + ']' def codebaseName = \"${NAME}\" def gitServerCrName = \"${GIT_SERVER_CR_NAME}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitServer = \"${GIT_SERVER ? GIT_SERVER : 'gerrit'}\" def gitSshPort = \"${GIT_SSH_PORT ? GIT_SSH_PORT : '29418'}\" def gitUsername = \"${GIT_USERNAME ? GIT_USERNAME : 'jenkins'}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID ? GIT_CREDENTIALS_ID : 'gerrit-ciuser-sshkey'}\" def defaultRepoPath = \"ssh://${gitUsername}@${gitServer}:${gitSshPort}/${codebaseName}\" def repositoryPath = \"${REPOSITORY_PATH ? REPOSITORY_PATH : defaultRepoPath}\" def defaultBranch = \"${DEFAULT_BRANCH}\" def codebaseFolder = jenkins . getItem ( codebaseName ) if ( codebaseFolder == null ) { folder ( codebaseName ) } createListView ( codebaseName , \"Releases\" ) createReleasePipeline ( \"Create-release-${codebaseName}\" , codebaseName , stages [ \"Create-release\" ], \"CreateRelease\" , repositoryPath , gitCredentialsId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType , defaultBranch ) if ( BRANCH ) { def branch = \"${BRANCH}\" def formattedBranch = \"${branch.toUpperCase().replaceAll(/\\\\//, \" - \")}\" createListView ( codebaseName , formattedBranch ) def type = \"${TYPE}\" def crKey = getStageKeyName ( buildTool ). toString () createCiPipeline ( \"Code-review-${codebaseName}\" , codebaseName , stages . get ( crKey , defaultStages ), \"CodeReview\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) def buildKey = \"Build-${type}-${buildTool.toLowerCase()}\" . toString () if ( type . equalsIgnoreCase ( 'application' ) || type . equalsIgnoreCase ( 'library' ) || type . equalsIgnoreCase ( 'autotests' )) { def jobExists = false if ( \"${formattedBranch}-Build-${codebaseName}\" . toString () in Jenkins . instance . getAllItems (). collect { it . name }) { jobExists = true } createCiPipeline ( \"Build-${codebaseName}\" , codebaseName , stages . get ( buildKey , defaultStages ), \"Build\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) if (! jobExists ) { queue ( \"${codebaseName}/${formattedBranch}-Build-${codebaseName}\" ) } } } def createCiPipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , defaultBranch , gitServerCrName , gitServerCrVersion ) { def jobName = \"${defaultBranch.toUpperCase().replaceAll(/\\\\//, \" - \")}-${pipelineName}\" def existingJob = Jenkins . getInstance (). getItemByFullName ( \"${codebaseName}/${jobName}\" ) def webhookToken = null if ( existingJob ) { def triggersMap = existingJob . getTriggers () triggersMap . each { key , value -> webhookToken = value . getSecretToken () } } else { def random = new byte [ 16 ] new java . security . SecureRandom (). nextBytes ( random ) webhookToken = random . encodeHex (). toString () } pipelineJob ( \"${codebaseName}/${jobName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } properties { gitLabConnection { gitLabConnection ( 'gitlab' ) } } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) if ( pipelineName . contains ( \"Build\" )) stringParam ( \"BRANCH\" , \"${defaultBranch}\" , \"Branch to build artifact from\" ) else stringParam ( \"BRANCH\" , \"\\${gitlabMergeRequestLastCommit}\" , \"Branch to build artifact from\" ) } } triggers { gitlabPush { buildOnMergeRequestEvents ( pipelineName . contains ( \"Build\" ) ? false : true ) buildOnPushEvents ( pipelineName . contains ( \"Build\" ) ? true : false ) enableCiSkip ( false ) setBuildDescription ( true ) rebuildOpenMergeRequest ( pipelineName . contains ( \"Build\" ) ? 'never' : 'source' ) commentTrigger ( \"Build it please\" ) skipWorkInProgressMergeRequest ( true ) targetBranchRegex ( \"${defaultBranch}\" ) } } configure { it / triggers / 'com.dabsquared.gitlabjenkins.GitLabPushTrigger' << secretToken ( webhookToken ) it / triggers / 'com.dabsquared.gitlabjenkins.GitLabPushTrigger' << triggerOnApprovedMergeRequest ( pipelineName . contains ( \"Build\" ) ? false : true ) it / triggers / 'com.dabsquared.gitlabjenkins.GitLabPushTrigger' << pendingBuildName ( pipelineName . contains ( \"Build\" ) ? \"\" : \"Jenkins\" ) } } registerWebHook ( repository , codebaseName , jobName , webhookToken ) } def getStageKeyName ( buildTool ) { if ( buildTool . toString (). equalsIgnoreCase ( 'terraform' )) { return \"Code-review-library-terraform\" } if ( buildTool . toString (). equalsIgnoreCase ( 'opa' )) { return \"Code-review-library-opa\" } if ( buildTool . toString (). equalsIgnoreCase ( 'codenarc' )) { return \"Code-review-library-codenarc\" } if ( buildTool . toString (). equalsIgnoreCase ( 'kaniko' )) { return \"Code-review-library-kaniko\" } def buildToolsOutOfTheBox = [ \"maven\" , \"npm\" , \"gradle\" , \"dotnet\" , \"none\" , \"go\" , \"python\" ] def supBuildTool = buildToolsOutOfTheBox . contains ( buildTool . toString ()) return supBuildTool ? \"Code-review-${TYPE}\" : \"Code-review-default\" } def createReleasePipeline ( pipelineName , codebaseName , codebaseStages , pipelineType , repository , credId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType , defaultBranch ) { pipelineJob ( \"${codebaseName}/${pipelineName}\" ) { logRotator { numToKeep ( 14 ) daysToKeep ( 30 ) } definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\n${pipelineType}()\" ) sandbox ( true ) } parameters { stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"\" ) if ( pipelineName . contains ( \"Create-release\" )) { stringParam ( \"JIRA_INTEGRATION_ENABLED\" , \"${jiraIntegrationEnabled}\" , \"Is Jira integration enabled\" ) stringParam ( \"PLATFORM_TYPE\" , \"${platformType}\" , \"Platform type\" ) stringParam ( \"GERRIT_PROJECT\" , \"${codebaseName}\" , \"\" ) stringParam ( \"RELEASE_NAME\" , \"\" , \"Name of the release(branch to be created)\" ) stringParam ( \"COMMIT_ID\" , \"\" , \"Commit ID that will be used to create branch from for new release. If empty, DEFAULT_BRANCH will be used\" ) stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"REPOSITORY_PATH\" , \"${repository}\" , \"Full repository path\" ) stringParam ( \"DEFAULT_BRANCH\" , \"${defaultBranch}\" , \"Default repository branch\" ) } } } } } def createListView ( codebaseName , branchName ) { listView ( \"${codebaseName}/${branchName}\" ) { if ( branchName . toLowerCase () == \"releases\" ) { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^Create-release.*\" ) } } } else { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^${branchName}-(Code-review|Build).*\" ) } } } columns { status () weather () name () lastSuccess () lastFailure () lastDuration () buildButton () } } } def registerWebHook ( repositoryPath , codebaseName , jobName , webhookToken ) { def apiUrl = 'https://' + repositoryPath . replaceAll ( \"ssh://\" , \"\" ). split ( '@' )[ 1 ]. replace ( '/' , \"%2F\" ). replaceAll (~ /:\\d+%2F/ , '/api/v4/projects/' ) + '/hooks' def jobWebhookUrl = \"${System.getenv('JENKINS_UI_URL')}/project/${codebaseName}/${jobName}\" def gitlabToken = getSecretValue ( 'gitlab-access-token' ) if ( checkWebHookExist ( apiUrl , jobWebhookUrl , gitlabToken )) { println ( \"[JENKINS][DEBUG] Webhook for job ${jobName} is already exist\\r\\n\" ) return } println ( \"[JENKINS][DEBUG] Creating webhook for job ${jobName}\" ) def webhookConfig = [:] webhookConfig [ \"url\" ] = jobWebhookUrl webhookConfig [ \"push_events\" ] = jobName . contains ( \"Build\" ) ? \"true\" : \"false\" webhookConfig [ \"merge_requests_events\" ] = jobName . contains ( \"Build\" ) ? \"false\" : \"true\" webhookConfig [ \"issues_events\" ] = \"false\" webhookConfig [ \"confidential_issues_events\" ] = \"false\" webhookConfig [ \"tag_push_events\" ] = \"false\" webhookConfig [ \"note_events\" ] = \"true\" webhookConfig [ \"job_events\" ] = \"false\" webhookConfig [ \"pipeline_events\" ] = \"false\" webhookConfig [ \"wiki_page_events\" ] = \"false\" webhookConfig [ \"enable_ssl_verification\" ] = \"true\" webhookConfig [ \"token\" ] = webhookToken def requestBody = JsonOutput . toJson ( webhookConfig ) def httpConnector = new URL ( apiUrl ). openConnection () as HttpURLConnection httpConnector . setRequestMethod ( 'POST' ) httpConnector . setDoOutput ( true ) httpConnector . setRequestProperty ( \"Accept\" , 'application/json' ) httpConnector . setRequestProperty ( \"Content-Type\" , 'application/json' ) httpConnector . setRequestProperty ( \"PRIVATE-TOKEN\" , \"${gitlabToken}\" ) httpConnector . outputStream . write ( requestBody . getBytes ( \"UTF-8\" )) httpConnector . connect () if ( httpConnector . responseCode == 201 ) println ( \"[JENKINS][DEBUG] Webhook for job ${jobName} has been created\\r\\n\" ) else { println ( \"[JENKINS][ERROR] Responce code - ${httpConnector.responseCode}\" ) def response = new JsonSlurper (). parseText ( httpConnector . errorStream . getText ( 'UTF-8' )) println ( \"[JENKINS][ERROR] Failed to create webhook for job ${jobName}. Response - ${response}\" ) } } def checkWebHookExist ( apiUrl , jobWebhookUrl , gitlabToken ) { println ( \"[JENKINS][DEBUG] Checking if webhook ${jobWebhookUrl} exists\" ) def httpConnector = new URL ( apiUrl ). openConnection () as HttpURLConnection httpConnector . setRequestMethod ( 'GET' ) httpConnector . setDoOutput ( true ) httpConnector . setRequestProperty ( \"Accept\" , 'application/json' ) httpConnector . setRequestProperty ( \"Content-Type\" , 'application/json' ) httpConnector . setRequestProperty ( \"PRIVATE-TOKEN\" , \"${gitlabToken}\" ) httpConnector . connect () if ( httpConnector . responseCode == 200 ) { def response = new JsonSlurper (). parseText ( httpConnector . inputStream . getText ( 'UTF-8' )) return response . find { it . url == jobWebhookUrl } ? true : false } } def getSecretValue ( name ) { def creds = com . cloudbees . plugins . credentials . CredentialsProvider . lookupCredentials ( com . cloudbees . plugins . credentials . common . StandardCredentials . class , Jenkins . instance , null , null ) def secret = creds . find { it . properties [ 'id' ] == name } return secret != null ? secret [ 'secret' ] : null } Create Secret, GitServer CR and Jenkins credentials with the \"gitlab\" ID by following the instruction: Adjust Import Strategy . After the steps above are performed, the new custom job-provision will be available in Advanced Settings during the application creation in Admin Console. Gitlab job provision","title":"GitLab (gitlab)"},{"location":"operator-guide/manage-jenkins-ci-job-provision/#related-articles","text":"CI Pipeline for Container","title":"Related Articles"},{"location":"operator-guide/multitenant-logging/","text":"Multitenant Logging \u2693\ufe0e Get acquainted with the multitenant logging components and the project logs location in the Shared cluster. Logging Components \u2693\ufe0e To configure the multitenant logging, it is necessary to deploy the following components: Grafana Loki Logging-operator Logging-operator stack-fluentbit In Grafana, every tenant represents an organization, i.e. it is necessary to create an organization for every namespace in the cluster. To get more details regarding the architecture of the Logging Operator, please review the Diagram 1. Logging operator scheme Note It is necessary to deploy Loki with the auth_enabled: true flag with the aim to ensure that the logs are separated for each tenant. For the authentication, Loki requires the HTTP header X-Scope-OrgID. Review Project Logs in Grafana \u2693\ufe0e To find the project logs, navigate to Grafana and follow the steps below: Note Grafana is a common service for different customers where each customer works in its own separated Grafana Organization and doesn't have any access to another project. Choose the organization by clicking the Current Organization drop-down list. If a user is assigned to several organizations, switch easily by using the Switch button. Current organization Navigate to the left-side menu and click the Explore button to see the Log Browser: Grafana explore Click the Log Browser button to see the labels that can be used to filter logs (e.g., hostname, namespace, application name, pod, etc.): Note Enable the correct data source, select the relevant logging data from the top left-side corner, and pay attention that the data source name always follows the \u2039project_name\u203a-logging pattern. Log browser Filter out logs by clicking the Show logs button or write the query and click the Run query button. Review the results with the quantity of logs per time, see the example below: Logs example Expand the logs to get detailed information about the object entry: Expand logs Use the following buttons to include or remove the labels from the query: Addition button See the ad-hoc statistics for a particular label: Ad-hoc stat example Related Articles \u2693\ufe0e Grafana Documentation","title":"Multitenant Logging"},{"location":"operator-guide/multitenant-logging/#multitenant-logging","text":"Get acquainted with the multitenant logging components and the project logs location in the Shared cluster.","title":"Multitenant Logging"},{"location":"operator-guide/multitenant-logging/#logging-components","text":"To configure the multitenant logging, it is necessary to deploy the following components: Grafana Loki Logging-operator Logging-operator stack-fluentbit In Grafana, every tenant represents an organization, i.e. it is necessary to create an organization for every namespace in the cluster. To get more details regarding the architecture of the Logging Operator, please review the Diagram 1. Logging operator scheme Note It is necessary to deploy Loki with the auth_enabled: true flag with the aim to ensure that the logs are separated for each tenant. For the authentication, Loki requires the HTTP header X-Scope-OrgID.","title":"Logging Components"},{"location":"operator-guide/multitenant-logging/#review-project-logs-in-grafana","text":"To find the project logs, navigate to Grafana and follow the steps below: Note Grafana is a common service for different customers where each customer works in its own separated Grafana Organization and doesn't have any access to another project. Choose the organization by clicking the Current Organization drop-down list. If a user is assigned to several organizations, switch easily by using the Switch button. Current organization Navigate to the left-side menu and click the Explore button to see the Log Browser: Grafana explore Click the Log Browser button to see the labels that can be used to filter logs (e.g., hostname, namespace, application name, pod, etc.): Note Enable the correct data source, select the relevant logging data from the top left-side corner, and pay attention that the data source name always follows the \u2039project_name\u203a-logging pattern. Log browser Filter out logs by clicking the Show logs button or write the query and click the Run query button. Review the results with the quantity of logs per time, see the example below: Logs example Expand the logs to get detailed information about the object entry: Expand logs Use the following buttons to include or remove the labels from the query: Addition button See the ad-hoc statistics for a particular label: Ad-hoc stat example","title":"Review Project Logs in Grafana"},{"location":"operator-guide/multitenant-logging/#related-articles","text":"Grafana Documentation","title":"Related Articles"},{"location":"operator-guide/openshift-cluster-settings/","text":"Set Up OpenShift \u2693\ufe0e Make sure the cluster meets the following conditions: OpenShift cluster is installed with minimum 2 worker nodes with total capacity 32 Cores and 8Gb RAM; Load balancer (if any exists in front of OpenShift router or ingress controller) is configured with session stickiness, disabled HTTP/2 protocol and header size of 64k support; Example of Config Map for Nginx ingress controller: kind : ConfigMap apiVersion : v1 metadata : name : nginx - configuration namespace : ingress - nginx labels : app . kubernetes . io / name : ingress - nginx app . kubernetes . io / part - of : ingress - nginx data : client - header - buffer - size : 64 k large - client - header - buffers : 4 64 k use - http2 : \"false\" Cluster nodes and pods have access to the cluster via external URLs. For instance, add in AWS the VPC NAT gateway elastic IP to the cluster external load balancers security group); Keycloak instance is installed. To get accurate information on how to install Keycloak, please refer to the Install Keycloak instruction; The installation machine with oc is installed with the cluster-admin access to the OpenShift cluster; Helm 3.1 is installed on the installation machine with the help of the Installing Helm instruction. A storage class is used with the Retain Reclaim Policy : Storage class template with Retain Reclaim Policy: kind : StorageClass apiVersion : storage . k8s . io / v1 metadata : name : gp2 - retain provisioner : kubernetes . io / aws - ebs parameters : fsType : ext4 type : gp2 reclaimPolicy : Retain volumeBindingMode : WaitForFirstConsumer Related Articles \u2693\ufe0e Install Keycloak","title":"Set Up OpenShift"},{"location":"operator-guide/openshift-cluster-settings/#set-up-openshift","text":"Make sure the cluster meets the following conditions: OpenShift cluster is installed with minimum 2 worker nodes with total capacity 32 Cores and 8Gb RAM; Load balancer (if any exists in front of OpenShift router or ingress controller) is configured with session stickiness, disabled HTTP/2 protocol and header size of 64k support; Example of Config Map for Nginx ingress controller: kind : ConfigMap apiVersion : v1 metadata : name : nginx - configuration namespace : ingress - nginx labels : app . kubernetes . io / name : ingress - nginx app . kubernetes . io / part - of : ingress - nginx data : client - header - buffer - size : 64 k large - client - header - buffers : 4 64 k use - http2 : \"false\" Cluster nodes and pods have access to the cluster via external URLs. For instance, add in AWS the VPC NAT gateway elastic IP to the cluster external load balancers security group); Keycloak instance is installed. To get accurate information on how to install Keycloak, please refer to the Install Keycloak instruction; The installation machine with oc is installed with the cluster-admin access to the OpenShift cluster; Helm 3.1 is installed on the installation machine with the help of the Installing Helm instruction. A storage class is used with the Retain Reclaim Policy : Storage class template with Retain Reclaim Policy: kind : StorageClass apiVersion : storage . k8s . io / v1 metadata : name : gp2 - retain provisioner : kubernetes . io / aws - ebs parameters : fsType : ext4 type : gp2 reclaimPolicy : Retain volumeBindingMode : WaitForFirstConsumer","title":"Set Up OpenShift"},{"location":"operator-guide/openshift-cluster-settings/#related-articles","text":"Install Keycloak","title":"Related Articles"},{"location":"operator-guide/overview-manage-jenkins-pipelines/","text":"Overview \u2693\ufe0e Jenkins job provisioners are responsible for creating and managing pipelines in Jenkins. In other words, provisioners configure all Jenkins pipelines and bring them to the state described in the provisioners code. Two types of provisioners are available in EDP: CI-provisioner - manages the application folder, and its Code Review, Build and Create Release pipelines. CD-provisioner - manages the Deploy pipelines. The subsections describe the creation/update process of provisioners and their content depending on EDP customization.","title":"Overview"},{"location":"operator-guide/overview-manage-jenkins-pipelines/#overview","text":"Jenkins job provisioners are responsible for creating and managing pipelines in Jenkins. In other words, provisioners configure all Jenkins pipelines and bring them to the state described in the provisioners code. Two types of provisioners are available in EDP: CI-provisioner - manages the application folder, and its Code Review, Build and Create Release pipelines. CD-provisioner - manages the Deploy pipelines. The subsections describe the creation/update process of provisioners and their content depending on EDP customization.","title":"Overview"},{"location":"operator-guide/overview-sast/","text":"Static Application Security Testing Overview \u2693\ufe0e EPAM Delivery Platform provides the implemented Static Application Security Testing support allowing to work with the Semgrep security scanner and the DefectDojo vulnerability management system to check the source code for known vulnerabilities. Supported Languages \u2693\ufe0e EDP SAST supports a number of languages and package managers. Language (Package Managers) Scan Tool Build Tool Java Semgrep Maven, Gradle Go Semgrep Go React Semgrep Npm Supported Vulnerability Management System \u2693\ufe0e To get and then manage a SAST report after scanning, it is necessary to deploy the vulnerability management system, for instance, DefectDojo. DefectDojo \u2693\ufe0e DefectDojo is a vulnerability management and security orchestration platform that allows managing the uploaded security reports. Inspect the prerequisites and the main steps for installing DefectDojo on Kubernetes or OpenShift platforms. Related Articles \u2693\ufe0e Add Security Scanner Semgrep","title":"Static Application Security Testing Overview"},{"location":"operator-guide/overview-sast/#static-application-security-testing-overview","text":"EPAM Delivery Platform provides the implemented Static Application Security Testing support allowing to work with the Semgrep security scanner and the DefectDojo vulnerability management system to check the source code for known vulnerabilities.","title":"Static Application Security Testing Overview"},{"location":"operator-guide/overview-sast/#supported-languages","text":"EDP SAST supports a number of languages and package managers. Language (Package Managers) Scan Tool Build Tool Java Semgrep Maven, Gradle Go Semgrep Go React Semgrep Npm","title":"Supported Languages"},{"location":"operator-guide/overview-sast/#supported-vulnerability-management-system","text":"To get and then manage a SAST report after scanning, it is necessary to deploy the vulnerability management system, for instance, DefectDojo.","title":"Supported Vulnerability Management System"},{"location":"operator-guide/overview-sast/#defectdojo","text":"DefectDojo is a vulnerability management and security orchestration platform that allows managing the uploaded security reports. Inspect the prerequisites and the main steps for installing DefectDojo on Kubernetes or OpenShift platforms.","title":"DefectDojo"},{"location":"operator-guide/overview-sast/#related-articles","text":"Add Security Scanner Semgrep","title":"Related Articles"},{"location":"operator-guide/perf-integration/","text":"Perf Server Integration \u2693\ufe0e Integration with Perf Server allows connecting to the PERF Board (Project Performance Board) and monitoring the overall team performance as well as setting up necessary metrics. Note To adjust the PERF Server integration, make sure that PERF Operator is deployed. To get more information about the PERF Operator installation and architecture, please refer to the PERF Operator page. For integration, take the following steps: Create Secret in the OpenShift/Kubernetes namespace for Perf Server account with the username and password fields: apiVersion : v1 data : password : passwordInBase64 username : usernameInBase64 kind : Secret metadata : name : epam - perf - user type : kubernetes . io / basic - auth In the edp-config config map, enable the perf_integration flag and click Save : perf_integration_enabled: 'true' Being in Admin Console, navigate to the Advanced Settings menu to check that the Integrate with Perf Server check box appeared: Advanced settings Related Articles \u2693\ufe0e Add Application Add Autotest Add Library","title":"Perf Server Integration"},{"location":"operator-guide/perf-integration/#perf-server-integration","text":"Integration with Perf Server allows connecting to the PERF Board (Project Performance Board) and monitoring the overall team performance as well as setting up necessary metrics. Note To adjust the PERF Server integration, make sure that PERF Operator is deployed. To get more information about the PERF Operator installation and architecture, please refer to the PERF Operator page. For integration, take the following steps: Create Secret in the OpenShift/Kubernetes namespace for Perf Server account with the username and password fields: apiVersion : v1 data : password : passwordInBase64 username : usernameInBase64 kind : Secret metadata : name : epam - perf - user type : kubernetes . io / basic - auth In the edp-config config map, enable the perf_integration flag and click Save : perf_integration_enabled: 'true' Being in Admin Console, navigate to the Advanced Settings menu to check that the Integrate with Perf Server check box appeared: Advanced settings","title":"Perf Server Integration"},{"location":"operator-guide/perf-integration/#related-articles","text":"Add Application Add Autotest Add Library","title":"Related Articles"},{"location":"operator-guide/restore-edp-with-velero/","text":"Restore EDP Tenant With Velero \u2693\ufe0e You can use the Velero tool to restore a EDP tenant. Explore the main steps for backup and restoring below. Delete all related entities in Keycloak : realm and clients from master/openshift realms. Navigate to the entitities list in the Keycloak, select the necessary ones, and click the deletion icon on the entitiy overview page. If there are customized configs in Keycloak, save them before making backup. Remove keycloak realm To restore EDP, install and configure the Velero tool. Please refer to the Install Velero documentation for details. Remove all locks for operators. Delete all config maps that have \u2039OPERATOR_NAME\u203a-operator-lock names. Then restart all pods with operators, or simply run the following command: kubectl -n <EDP_NAMESPACE> delete cm $(kubectl -n <EDP_NAMESPACE> get cm | grep 'operator-lock' | awk '{print $1}') Recreate the admin password and delete the Jenkins pod. Or change the script to update the admin password in Jenkins every time when the pod is updated.","title":"Restore EDP Tenant With Velero"},{"location":"operator-guide/restore-edp-with-velero/#restore-edp-tenant-with-velero","text":"You can use the Velero tool to restore a EDP tenant. Explore the main steps for backup and restoring below. Delete all related entities in Keycloak : realm and clients from master/openshift realms. Navigate to the entitities list in the Keycloak, select the necessary ones, and click the deletion icon on the entitiy overview page. If there are customized configs in Keycloak, save them before making backup. Remove keycloak realm To restore EDP, install and configure the Velero tool. Please refer to the Install Velero documentation for details. Remove all locks for operators. Delete all config maps that have \u2039OPERATOR_NAME\u203a-operator-lock names. Then restart all pods with operators, or simply run the following command: kubectl -n <EDP_NAMESPACE> delete cm $(kubectl -n <EDP_NAMESPACE> get cm | grep 'operator-lock' | awk '{print $1}') Recreate the admin password and delete the Jenkins pod. Or change the script to update the admin password in Jenkins every time when the pod is updated.","title":"Restore EDP Tenant With Velero"},{"location":"operator-guide/sast-scaner-semgrep/","text":"Semgrep \u2693\ufe0e Semgrep is an open-source static source code analyzer for finding bugs and enforcing code standards. Semgrep scanner is installed on the EDP Jenkins SAST agent and runs on the sast pipeline stage. For details, please refer to the edp-library-stages repository . Supported Languages \u2693\ufe0e Semgrep supports more than 20 languages, see the full list in the official documentation . EDP uses Semgrep to scan Java, JavaScript and Go languages. Related Articles \u2693\ufe0e Add Security Scanner","title":"Semgrep"},{"location":"operator-guide/sast-scaner-semgrep/#semgrep","text":"Semgrep is an open-source static source code analyzer for finding bugs and enforcing code standards. Semgrep scanner is installed on the EDP Jenkins SAST agent and runs on the sast pipeline stage. For details, please refer to the edp-library-stages repository .","title":"Semgrep"},{"location":"operator-guide/sast-scaner-semgrep/#supported-languages","text":"Semgrep supports more than 20 languages, see the full list in the official documentation . EDP uses Semgrep to scan Java, JavaScript and Go languages.","title":"Supported Languages"},{"location":"operator-guide/sast-scaner-semgrep/#related-articles","text":"Add Security Scanner","title":"Related Articles"},{"location":"operator-guide/schedule-pods-restart/","text":"Schedule Pods Restart \u2693\ufe0e In case it is necessary to restart pods, use a CronJob according to the following template: View: template --- kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : namespace : <NAMESPACE> name : apps-restart rules : - apiGroups : [ \"apps\" ] resources : - deployments - statefulsets verbs : - 'get' - 'list' - 'patch' --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : apps-restart namespace : <NAMESPACE> subjects : - kind : ServiceAccount name : apps-restart-sa namespace : <NAMESPACE> roleRef : kind : Role name : apps-restart apiGroup : \"\" --- apiVersion : v1 kind : ServiceAccount metadata : name : apps-restart-sa namespace : <NAMESPACE> --- apiVersion : batch/v1beta1 kind : CronJob metadata : name : apps-rollout-restart namespace : <NAMESPACE> spec : schedule : \"0 9 * * MON-FRI\" jobTemplate : spec : template : spec : serviceAccountName : apps-restart-sa containers : - name : kubectl-runner image : bitnami/kubectl command : - /bin/sh - -c - kubectl get -n <NAMESPACE> -o name deployment,statefulset | grep <NAME_PATTERN>| xargs kubectl -n <NAMESPACE> rollout restart restartPolicy : Never Modify the Cron expression in the CronJob manifest if needed.","title":"Schedule Pods Restart"},{"location":"operator-guide/schedule-pods-restart/#schedule-pods-restart","text":"In case it is necessary to restart pods, use a CronJob according to the following template: View: template --- kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : namespace : <NAMESPACE> name : apps-restart rules : - apiGroups : [ \"apps\" ] resources : - deployments - statefulsets verbs : - 'get' - 'list' - 'patch' --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : apps-restart namespace : <NAMESPACE> subjects : - kind : ServiceAccount name : apps-restart-sa namespace : <NAMESPACE> roleRef : kind : Role name : apps-restart apiGroup : \"\" --- apiVersion : v1 kind : ServiceAccount metadata : name : apps-restart-sa namespace : <NAMESPACE> --- apiVersion : batch/v1beta1 kind : CronJob metadata : name : apps-rollout-restart namespace : <NAMESPACE> spec : schedule : \"0 9 * * MON-FRI\" jobTemplate : spec : template : spec : serviceAccountName : apps-restart-sa containers : - name : kubectl-runner image : bitnami/kubectl command : - /bin/sh - -c - kubectl get -n <NAMESPACE> -o name deployment,statefulset | grep <NAME_PATTERN>| xargs kubectl -n <NAMESPACE> rollout restart restartPolicy : Never Modify the Cron expression in the CronJob manifest if needed.","title":"Schedule Pods Restart"},{"location":"operator-guide/ssl-automation-okd/","text":"Use Cert-Manager in OpenShift \u2693\ufe0e The following material covers Let's Encrypt certificate automation with cert-manager using AWS Route53 . The cert-manager is a Kubernetes/OpenShift operator that allows to issue and automatically renew SSL certificates. In this tutorial, the steps to secure DNS Name will be demonstrated. Below is an instruction on how to automatically issue and install wildcard certificates on OpenShift Ingress Controller and API Server covering all cluster Routes. To secure separate OpenShift Routes, please refer to the OpenShift Route Support project for cert-manager . Prerequisites \u2693\ufe0e The cert-manager; OpenShift v4.7 - v4.11; Connection to the OpenShift Cluster; Enabled AWS IRSA; The latest oc utility . The kubectl tool can also be used for most of the commands. Install Cert-Manager Operator \u2693\ufe0e Install the cert-manager operator via OpenShift OperatorHub that uses Operator Lifecycle Manager (OLM) : Go to the OpenShift Admin Console \u2192 OperatorHub , search for the cert-manager , and click Install : Cert-Manager Installation Modify the ClusterServiceVersion OLM resource, by selecting the Update approval \u2192 Manual . If selecting Update approval \u2192 Automatic after the automatic operator update, the parameters in the ClusterServiceVersion will be reset to default. Note Installing an operator with Manual approval causes all operators installed in namespace openshift-operators to function as manual approval strategy . In case the Manual approval is chosen, review the manual installation plan and approve it. Cert-Manager Installation Navigate to Operators \u2192 Installed Operators and check the operator status to be Succeeded : Cert-Manager Installation In case of errors, troubleshoot the Operator issues: oc describe operator cert-manager -n openshift-operators oc describe sub cert-manager -n openshift-operators Create AWS Role for Route53 \u2693\ufe0e The cert-manager should be configured to validate Wildcard certificates using the DNS-based method. Check the DNS Hosted zone ID in AWS Route53 for your domain. Hosted Zone ID Create Route53 Permissions policy in AWS for cert-manager to be able to create DNS TXT records for the certificate validation. In this example, cert-manager permissions are given for a particular DNS zone only. Replace Hosted zone ID XXXXXXXX in the \"Resource\": \"arn:aws:route53:::hostedzone/XXXXXXXXXXXX\" . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"route53:GetChange\" , \"Resource\" : \"arn:aws:route53:::change/*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : \"arn:aws:route53:::hostedzone/XXXXXXXXXXXX\" } ] } Create an AWS Role with Custom trust policy for the cert-manager service account to use the AWS IRSA feature and then attach the created policy. Replace the following: ${aws-account-id} with the AWS account ID of the EKS cluster. ${aws-region} with the region where the EKS cluster is located. ${eks-hash} with the hash in the EKS API URL; this will be a random 32 character hex string, for example, 45DABD88EEE3A227AF0FA468BE4EF0B5. ${namespace} with the namespace where cert-manager is running. ${service-account-name} with the name of the ServiceAccount object created by cert-manager. By default, it is \"system:serviceaccount:openshift-operators:cert-manager\" if cert-manager is installed via OperatorHub. Attach the created Permission policy for Route53 to the Role. Optionally, add Permissions boundary to the Role. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::* ${aws-account-id}:oidc-provider/oidc.eks.${aws-region}.amazonaws.com/id/${eks-hash}\" }, \"Condition\" : { \"StringEquals\" : { \"oidc.eks.${aws-region}.amazonaws.com/id/${eks-hash}:sub\" : \"system:serviceaccount:${namespace}:${service-account-name}\" } } } ] } Copy the created Role ARN . Configure Cert-Manager Integration With AWS Route53 \u2693\ufe0e Annotate the ServiceAccount created by cert-manager (required for AWS IRSA ), and restart the cert-manager pod. Replace the eks.amazonaws.com/role-arn annotation value with your own Role ARN. oc edit sa cert-manager -n openshift-operators apiVersion : v1 kind : ServiceAccount metadata : annotations : eks.amazonaws.com/role-arn : arn:aws:iam::XXXXXXXXXXXX:role/cert-manager Modify the cert-manager Deployment with the correct file system permissions fsGroup: 1001 , so that the ServiceAccount token can be read. Note In case the ServiceAccount token cannot be read and the operator is installed using the OperatorHub, add fsGroup: 1001 via OpenShift ClusterServiceVersion OLM resource. It should be a cert-manager controller spec. These actions are not required for OpenShift v4.10. oc get csv oc edit csv cert-manager. ${ VERSION } spec : template : spec : securityContext : fsGroup : 1001 serviceAccountName : cert-manager Cert-Manager System Permissions Info A mutating admission controller will automatically modify all pods running with the service account: cert-manager controller pod apiVersion : apps/v1 kind : Pod # ... spec : # ... serviceAccountName : cert-manager serviceAccount : cert-manager containers : - name : ... # ... env : - name : AWS_ROLE_ARN value : >- arn:aws:iam::XXXXXXXXXXX:role/cert-manager - name : AWS_WEB_IDENTITY_TOKEN_FILE value : /var/run/secrets/eks.amazonaws.com/serviceaccount/token volumeMounts : - name : aws-iam-token readOnly : true mountPath : /var/run/secrets/eks.amazonaws.com/serviceaccount volumes : - name : aws-iam-token projected : sources : - serviceAccountToken : audience : sts.amazonaws.com expirationSeconds : 86400 path : token defaultMode : 420 If you have separate public and private DNS zones for the same domain (split-horizon DNS), modify the cert-manager Deployment in order to validate DNS TXT records via public recursive nameservers . Note Otherwise, you will be getting an error during a record validation: Waiting for DNS-01 challenge propagation: NS ns-123.awsdns-00.net.:53 returned REFUSED for _acme-challenge. To avoid the error, add --dns01-recursive-nameservers-only --dns01-recursive-nameservers=8.8.8.8:53,1.1.1.1:53 as ARGs to the cert-manager controller Deployment . oc get csv oc edit csv cert-manager. ${ VERSION } labels : app : cert-manager app.kubernetes.io/component : controller app.kubernetes.io/instance : cert-manager app.kubernetes.io/name : cert-manager app.kubernetes.io/version : v1.9.1 spec : containers : - args : - '--v=2' - '--cluster-resource-namespace=$(POD_NAMESPACE)' - '--leader-election-namespace=kube-system' - '--dns01-recursive-nameservers-only' - '--dns01-recursive-nameservers=8.8.8.8:53,1.1.1.1:53' Note The Deployment must be modified via OpenShift ClusterServiceVersion OLM resource if the operator was installed using the OperatorHub. The OpenShift ClusterServiceVersion OLM resource includes several Deployments, and the ARGs must be modified only for the cert-manager controller. Save the resource. After that, OLM will try to reload the resource automatically and save it to the YAML file. If OLM resets the config file, double-check the entered values. Cert-Manager Nameservers Configure ClusterIssuers \u2693\ufe0e ClusterIssuer is available on the whole cluster. Create the ClusterIssuer resource for Let's Encrypt Staging and Prod environments that signs a Certificate using cert-manager . Note Let's Encrypt has a limit of duplicate certificates in the Prod environment. Therefore, a ClusterIssuer has been created for Let's Encrypt Staging environment. By default, Let's Encrypt Staging certificates will not be trusted in your browser. The certificate validation cannot be tested in the Let's Encrypt Staging environment. Change user@example.com with your contact email. Replace hostedZoneID XXXXXXXXXXX with the DNS Hosted zone ID in AWS for your domain. Replace the region value ${region} . The secret under privateKeySecretRef will be created automatically by the cert-manager operator. apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : email : user@example.com server : https://acme-staging-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-staging-issuer-account-key solvers : - dns01 : route53 : region : ${region} hostedZoneID : XXXXXXXXXXX apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : email : user@example.com server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-prod-issuer-account-key solvers : - dns01 : route53 : region : ${region} hostedZoneID : XXXXXXXXXXX Cert-Manager ClusterIssuer Check the ClusterIssuer status: Cert-Manager ClusterIssuer oc describe clusterissuer letsencrypt-prod oc describe clusterissuer letsencrypt-staging If the ClusterIssuer state is not ready, investigate cert-manager controller pod logs: oc get pod -n openshift-operators | grep 'cert-manager' oc logs -f cert-manager- ${ replica_set } - ${ random_string } -n openshift-operators Configure Certificates \u2693\ufe0e In two different namespaces, create a Certificate resource for the OpenShift Router (Ingress controller for OpenShift) and for the OpenShift APIServer. OpenShift Router supports a single wildcard certificate for Ingress/Route resources in different namespaces (so called, default SSL certificate ). The Ingress controller expects the certificates in a Secret to be created in the openshift-ingress namespace; the API Server, in the openshift-config namespace. The cert-manager operator will automatically create these secrets from the Certificate resource. Replace ${DOMAIN} with your domain name. It can be checked with oc whoami --show-server . Put domain names in quotes. The certificate for OpenShift Router in the `openshift-ingress` namespace apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : router-certs namespace : openshift-ingress labels : app : cert-manager spec : secretName : router-certs secretTemplate : labels : app : cert-manager duration : 2160h # 90d renewBefore : 360h # 15d subject : organizations : - Org Name commonName : '*.${DOMAIN}' privateKey : algorithm : RSA encoding : PKCS1 size : 2048 rotationPolicy : Always usages : - server auth - client auth dnsNames : - '*.${DOMAIN}' - '*.apps.${DOMAIN}' issuerRef : name : letsencrypt-staging kind : ClusterIssuer The certificate for OpenShift APIServer in the `openshift-config` namespace apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : api-certs namespace : openshift-config labels : app : cert-manager spec : secretName : api-certs secretTemplate : labels : app : cert-manager duration : 2160h # 90d renewBefore : 360h # 15d subject : organizations : - Org Name commonName : '*.${DOMAIN}' privateKey : algorithm : RSA encoding : PKCS1 size : 2048 rotationPolicy : Always usages : - server auth - client auth dnsNames : - '*.${DOMAIN}' - '*.apps.${DOMAIN}' issuerRef : name : letsencrypt-staging kind : ClusterIssuer Info cert-manager supports ECDSA key pairs in the Certificate resource. To use it, change RSA privateKey to ECDSA: privateKey : algorithm : ECDSA encoding : PKCS1 size : 256 rotationPolicy : Always rotationPolicy: Always is highly recommended since cert-manager does not rotate private keys by default. Full Certificate spec is described in the cert-manager API documentation . Check that the certificates in the namespaces are ready: Cert-Manager Certificate Status Cert-Manager Certificate Status Check the details of the certificates via CLI: oc describe certificate api-certs -n openshift-config oc describe certificate router-certs -n openshift-ingress Check the cert-manager controller pod logs if the Staging Certificate condition is not ready for more than 7 minutes: oc get pod -n openshift-operators | grep 'cert-manager' oc logs -f cert-manager- ${ replica_set } - ${ random_string } -n openshift-operators When the certificate is ready, its private key will be put into the OpenShift Secret in the namespace indicated in the Certificate resource: oc describe secret api-certs -n openshift-config oc describe secret router-certs -n openshift-ingress Modify OpenShift Router and API Server Custom Resources \u2693\ufe0e Update the Custom Resource of your Router (Ingress controller). Patch the defaultCertificate object value with { \"name\": \"router-certs\" } : oc patch ingresscontroller default -n openshift-ingress-operator --type = merge --patch = '{\"spec\": { \"defaultCertificate\": { \"name\": \"router-certs\" }}}' --insecure-skip-tls-verify Info After updating the IngressController object, the OpenShift Ingress operator redeploys the router. Update the Custom Resource for the OpenShift API Server: Export the name of APIServer : export OKD_API = $( oc whoami --show-server --insecure-skip-tls-verify | cut -f 2 -d ':' | cut -f 3 -d '/' | sed 's/-api././' ) Patch the servingCertificate object value with { \"name\": \"api-certs\" } : oc patch apiserver cluster --type merge --patch = \"{\\\"spec\\\": {\\\"servingCerts\\\": {\\\"namedCertificates\\\": [ { \\\"names\\\": [ \\\" $OKD_API \\\" ], \\\"servingCertificate\\\": {\\\"name\\\": \\\"api-certs\\\" }}]}}}\" --insecure-skip-tls-verify Move From Let's Encrypt Staging Environment to Prod \u2693\ufe0e Test the Staging certificate on the OpenShift Admin Console. The --insecure flag is used because Let's Encrypt Staging certificates are not trusted in browsers by default: curl -v --insecure https://console-openshift-console.apps. ${ DOMAIN } Change issuerRef to letsencrypt-prod in both Certificate resources: oc edit certificate api-certs -n openshift-config oc edit certificate router-certs -n openshift-ingress issuerRef : name : letsencrypt-prod kind : ClusterIssuer Note In case the certificate reissue is not triggered after that, try to force the certificate renewal with cmctl : cmctl renew router-certs -n openshift-ingress cmctl renew api-certs -n openshift-config If this won't work, delete the api-certs and router-certs secrets. It should trigger the Prod certificates issuance: oc delete secret router-certs -n openshift-ingress oc delete secret api-certs -n openshift-config Please note that these actions will lead to logging your account out of the OpenShift Admin Console, since certificates will be deleted. Accept the certificate warning in the browser and log in again after that. Check the status of the Prod certificates: oc describe certificate api-certs -n openshift-config oc describe certificate router-certs -n openshift-ingress cmctl status certificate api-certs -n openshift-config cmctl status certificate router-certs -n openshift-ingress Check the web console and make sure it has secure connection: curl -v https://console-openshift-console.apps. ${ DOMAIN } Troubleshoot Certificates \u2693\ufe0e Below is an example of the DNS TXT challenge record created by the cert-manager operator: DNS Validation Use nslookup or dig tools to check if the DNS propagation for the TXT record is complete: nslookup -type = txt _acme-challenge. ${ DOMAIN } dig txt _acme-challenge. ${ DOMAIN } Otherwise, use web tools like Google Admin Toolbox : DNS Validation If the correct TXT value is shown (the value corresponds to the current TXT value in the DNS zone), it means that the DNS propagation is complete and Let's Encrypt is able to access the record in order to validate it and issue a trusted certificate. Note If the DNS validation challenge self check fails, cert-manager will retry the self check with a fixed 10-second retry interval. Challenges that do not ever complete the self check will continue retrying until the user intervenes by either retrying the Order (by deleting the Order resource) or amending the associated Certificate resource to resolve any configuration errors. As soon as the domain ownership has been verified, any cert-manager affected validation TXT records in the AWS Route53 DNS zone will be cleaned up. Please find below the issues that may occur and their troubleshooting: When certificates are not issued for a long time, or a cert-manager resource is not in a Ready state, describing a resource may show the reason for the error. Basically, the cert-manager creates the following resources during a Certificate issuance: CertificateRequest , Order , and Challenge . Investigate each of them in case of errors. Use the cmctl tool to show the state of a Certificate and its associated resources. Check the cert-manager controller pod logs: oc get pod -n openshift-operators | grep 'cert-manager' oc logs -f cert-manager- ${ replica_set } - ${ random_string } -n openshift-operators Certificate error debugging: a. Decode certificate chain located in the secrets: oc get secret router-certs -n openshift-ingress -o 'go-template={{index .data \"tls.crt\"}}' | base64 -d | while openssl x509 -noout -text ; do : ; done 2 >/dev/null oc get secret api-certs -n openshift-config -o 'go-template={{index .data \"tls.crt\"}}' | base64 -d | while openssl x509 -noout -text ; do : ; done 2 >/dev/null cmctl inspect secret router-certs -n openshift-ingress cmctl inspect secret api-certs -n openshift-config b. Check the SSL RSA private key consistency: oc get secret router-certs -n openshift-ingress -o 'go-template={{index .data \"tls.key\"}}' | base64 -d | openssl rsa -check -noout oc get secret api-certs -n openshift-config -o 'go-template={{index .data \"tls.key\"}}' | base64 -d | openssl rsa -check -noout c. Match the SSL certificate public key against its RSA private key. Their modulus must be identical: diff < ( oc get secret api-certs -n openshift-config -o 'go-template={{index .data \"tls.crt\"}}' | base64 -d | openssl x509 -noout -modulus | openssl md5 ) < ( oc get secret api-certs -n openshift-config -o 'go-template={{index .data \"tls.key\"}}' | base64 -d | openssl rsa -noout -modulus | openssl md5 ) diff < ( oc get secret router-certs -n openshift-ingress -o 'go-template={{index .data \"tls.crt\"}}' | base64 -d | openssl x509 -noout -modulus | openssl md5 ) < ( oc get secret router-certs -n openshift-ingress -o 'go-template={{index .data \"tls.key\"}}' | base64 -d | openssl rsa -noout -modulus | openssl md5 ) Remove Obsolete Certificate Authority Data From Kubeconfig \u2693\ufe0e After updating the certificates, the access to the cluster via Lens or CLI will be denied because of the untrusted certificate errors: $ oc whoami Unable to connect to the server: x509: certificate signed by unknown authority Such behavior appears because the oc tool references an old CA data in the kubeconfig file. Note Examine the Certificate Authority data using the following command: oc config view --minify --raw -o jsonpath = '{.clusters[].cluster.certificate-authority-data}' | base64 -d | openssl x509 -text This certificate has the CA:TRUE parameter, which means that this is a self-signed root CA certificate. To fix the error, remove the old CA data from your OpenShift kubeconfig file: sed -i \"/certificate-authority-data/d\" $KUBECONFIG Since this field will be absent in the kubeconfig file, system root SSL certificate will be used to validate the cluster certificate trust chain. On Ubuntu, Let's Encrypt OpenShift cluster certificates will be validated against Internet Security Research Group root in /etc/ssl/certs/ca-certificates.crt . Certificate Renewals \u2693\ufe0e The cert-manager automatically renews the certificates based on the X.509 certificate's duration and the renewBefore value. The minimum value for the spec.duration is 1 hour; for spec.renewBefore , 5 minutes. It is also required that spec.duration > spec.renewBefore . Use the cmctl tool to manually trigger a single instant certificate renewal: cmctl renew router-certs -n openshift-ingress cmctl renew api-certs -n openshift-config Otherwise, manually renew all certificates in all namespaces with the app=cert-manager label: cmctl renew --all-namespaces -l app = cert-manager Run the cmctl renew --help command to get more details. Related Articles \u2693\ufe0e Cert-Manager Official Documentation Installing the Cert-Manager Operator for Red Hat OpenShift Checking Issued Certificate Details","title":"Use Cert-Manager in OpenShift"},{"location":"operator-guide/ssl-automation-okd/#use-cert-manager-in-openshift","text":"The following material covers Let's Encrypt certificate automation with cert-manager using AWS Route53 . The cert-manager is a Kubernetes/OpenShift operator that allows to issue and automatically renew SSL certificates. In this tutorial, the steps to secure DNS Name will be demonstrated. Below is an instruction on how to automatically issue and install wildcard certificates on OpenShift Ingress Controller and API Server covering all cluster Routes. To secure separate OpenShift Routes, please refer to the OpenShift Route Support project for cert-manager .","title":"Use Cert-Manager in OpenShift"},{"location":"operator-guide/ssl-automation-okd/#prerequisites","text":"The cert-manager; OpenShift v4.7 - v4.11; Connection to the OpenShift Cluster; Enabled AWS IRSA; The latest oc utility . The kubectl tool can also be used for most of the commands.","title":"Prerequisites"},{"location":"operator-guide/ssl-automation-okd/#install-cert-manager-operator","text":"Install the cert-manager operator via OpenShift OperatorHub that uses Operator Lifecycle Manager (OLM) : Go to the OpenShift Admin Console \u2192 OperatorHub , search for the cert-manager , and click Install : Cert-Manager Installation Modify the ClusterServiceVersion OLM resource, by selecting the Update approval \u2192 Manual . If selecting Update approval \u2192 Automatic after the automatic operator update, the parameters in the ClusterServiceVersion will be reset to default. Note Installing an operator with Manual approval causes all operators installed in namespace openshift-operators to function as manual approval strategy . In case the Manual approval is chosen, review the manual installation plan and approve it. Cert-Manager Installation Navigate to Operators \u2192 Installed Operators and check the operator status to be Succeeded : Cert-Manager Installation In case of errors, troubleshoot the Operator issues: oc describe operator cert-manager -n openshift-operators oc describe sub cert-manager -n openshift-operators","title":"Install Cert-Manager Operator"},{"location":"operator-guide/ssl-automation-okd/#create-aws-role-for-route53","text":"The cert-manager should be configured to validate Wildcard certificates using the DNS-based method. Check the DNS Hosted zone ID in AWS Route53 for your domain. Hosted Zone ID Create Route53 Permissions policy in AWS for cert-manager to be able to create DNS TXT records for the certificate validation. In this example, cert-manager permissions are given for a particular DNS zone only. Replace Hosted zone ID XXXXXXXX in the \"Resource\": \"arn:aws:route53:::hostedzone/XXXXXXXXXXXX\" . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"route53:GetChange\" , \"Resource\" : \"arn:aws:route53:::change/*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : \"arn:aws:route53:::hostedzone/XXXXXXXXXXXX\" } ] } Create an AWS Role with Custom trust policy for the cert-manager service account to use the AWS IRSA feature and then attach the created policy. Replace the following: ${aws-account-id} with the AWS account ID of the EKS cluster. ${aws-region} with the region where the EKS cluster is located. ${eks-hash} with the hash in the EKS API URL; this will be a random 32 character hex string, for example, 45DABD88EEE3A227AF0FA468BE4EF0B5. ${namespace} with the namespace where cert-manager is running. ${service-account-name} with the name of the ServiceAccount object created by cert-manager. By default, it is \"system:serviceaccount:openshift-operators:cert-manager\" if cert-manager is installed via OperatorHub. Attach the created Permission policy for Route53 to the Role. Optionally, add Permissions boundary to the Role. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::* ${aws-account-id}:oidc-provider/oidc.eks.${aws-region}.amazonaws.com/id/${eks-hash}\" }, \"Condition\" : { \"StringEquals\" : { \"oidc.eks.${aws-region}.amazonaws.com/id/${eks-hash}:sub\" : \"system:serviceaccount:${namespace}:${service-account-name}\" } } } ] } Copy the created Role ARN .","title":"Create AWS Role for Route53"},{"location":"operator-guide/ssl-automation-okd/#configure-cert-manager-integration-with-aws-route53","text":"Annotate the ServiceAccount created by cert-manager (required for AWS IRSA ), and restart the cert-manager pod. Replace the eks.amazonaws.com/role-arn annotation value with your own Role ARN. oc edit sa cert-manager -n openshift-operators apiVersion : v1 kind : ServiceAccount metadata : annotations : eks.amazonaws.com/role-arn : arn:aws:iam::XXXXXXXXXXXX:role/cert-manager Modify the cert-manager Deployment with the correct file system permissions fsGroup: 1001 , so that the ServiceAccount token can be read. Note In case the ServiceAccount token cannot be read and the operator is installed using the OperatorHub, add fsGroup: 1001 via OpenShift ClusterServiceVersion OLM resource. It should be a cert-manager controller spec. These actions are not required for OpenShift v4.10. oc get csv oc edit csv cert-manager. ${ VERSION } spec : template : spec : securityContext : fsGroup : 1001 serviceAccountName : cert-manager Cert-Manager System Permissions Info A mutating admission controller will automatically modify all pods running with the service account: cert-manager controller pod apiVersion : apps/v1 kind : Pod # ... spec : # ... serviceAccountName : cert-manager serviceAccount : cert-manager containers : - name : ... # ... env : - name : AWS_ROLE_ARN value : >- arn:aws:iam::XXXXXXXXXXX:role/cert-manager - name : AWS_WEB_IDENTITY_TOKEN_FILE value : /var/run/secrets/eks.amazonaws.com/serviceaccount/token volumeMounts : - name : aws-iam-token readOnly : true mountPath : /var/run/secrets/eks.amazonaws.com/serviceaccount volumes : - name : aws-iam-token projected : sources : - serviceAccountToken : audience : sts.amazonaws.com expirationSeconds : 86400 path : token defaultMode : 420 If you have separate public and private DNS zones for the same domain (split-horizon DNS), modify the cert-manager Deployment in order to validate DNS TXT records via public recursive nameservers . Note Otherwise, you will be getting an error during a record validation: Waiting for DNS-01 challenge propagation: NS ns-123.awsdns-00.net.:53 returned REFUSED for _acme-challenge. To avoid the error, add --dns01-recursive-nameservers-only --dns01-recursive-nameservers=8.8.8.8:53,1.1.1.1:53 as ARGs to the cert-manager controller Deployment . oc get csv oc edit csv cert-manager. ${ VERSION } labels : app : cert-manager app.kubernetes.io/component : controller app.kubernetes.io/instance : cert-manager app.kubernetes.io/name : cert-manager app.kubernetes.io/version : v1.9.1 spec : containers : - args : - '--v=2' - '--cluster-resource-namespace=$(POD_NAMESPACE)' - '--leader-election-namespace=kube-system' - '--dns01-recursive-nameservers-only' - '--dns01-recursive-nameservers=8.8.8.8:53,1.1.1.1:53' Note The Deployment must be modified via OpenShift ClusterServiceVersion OLM resource if the operator was installed using the OperatorHub. The OpenShift ClusterServiceVersion OLM resource includes several Deployments, and the ARGs must be modified only for the cert-manager controller. Save the resource. After that, OLM will try to reload the resource automatically and save it to the YAML file. If OLM resets the config file, double-check the entered values. Cert-Manager Nameservers","title":"Configure Cert-Manager Integration With AWS Route53"},{"location":"operator-guide/ssl-automation-okd/#configure-clusterissuers","text":"ClusterIssuer is available on the whole cluster. Create the ClusterIssuer resource for Let's Encrypt Staging and Prod environments that signs a Certificate using cert-manager . Note Let's Encrypt has a limit of duplicate certificates in the Prod environment. Therefore, a ClusterIssuer has been created for Let's Encrypt Staging environment. By default, Let's Encrypt Staging certificates will not be trusted in your browser. The certificate validation cannot be tested in the Let's Encrypt Staging environment. Change user@example.com with your contact email. Replace hostedZoneID XXXXXXXXXXX with the DNS Hosted zone ID in AWS for your domain. Replace the region value ${region} . The secret under privateKeySecretRef will be created automatically by the cert-manager operator. apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-staging spec : acme : email : user@example.com server : https://acme-staging-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-staging-issuer-account-key solvers : - dns01 : route53 : region : ${region} hostedZoneID : XXXXXXXXXXX apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : email : user@example.com server : https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef : name : letsencrypt-prod-issuer-account-key solvers : - dns01 : route53 : region : ${region} hostedZoneID : XXXXXXXXXXX Cert-Manager ClusterIssuer Check the ClusterIssuer status: Cert-Manager ClusterIssuer oc describe clusterissuer letsencrypt-prod oc describe clusterissuer letsencrypt-staging If the ClusterIssuer state is not ready, investigate cert-manager controller pod logs: oc get pod -n openshift-operators | grep 'cert-manager' oc logs -f cert-manager- ${ replica_set } - ${ random_string } -n openshift-operators","title":"Configure ClusterIssuers"},{"location":"operator-guide/ssl-automation-okd/#configure-certificates","text":"In two different namespaces, create a Certificate resource for the OpenShift Router (Ingress controller for OpenShift) and for the OpenShift APIServer. OpenShift Router supports a single wildcard certificate for Ingress/Route resources in different namespaces (so called, default SSL certificate ). The Ingress controller expects the certificates in a Secret to be created in the openshift-ingress namespace; the API Server, in the openshift-config namespace. The cert-manager operator will automatically create these secrets from the Certificate resource. Replace ${DOMAIN} with your domain name. It can be checked with oc whoami --show-server . Put domain names in quotes. The certificate for OpenShift Router in the `openshift-ingress` namespace apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : router-certs namespace : openshift-ingress labels : app : cert-manager spec : secretName : router-certs secretTemplate : labels : app : cert-manager duration : 2160h # 90d renewBefore : 360h # 15d subject : organizations : - Org Name commonName : '*.${DOMAIN}' privateKey : algorithm : RSA encoding : PKCS1 size : 2048 rotationPolicy : Always usages : - server auth - client auth dnsNames : - '*.${DOMAIN}' - '*.apps.${DOMAIN}' issuerRef : name : letsencrypt-staging kind : ClusterIssuer The certificate for OpenShift APIServer in the `openshift-config` namespace apiVersion : cert-manager.io/v1 kind : Certificate metadata : name : api-certs namespace : openshift-config labels : app : cert-manager spec : secretName : api-certs secretTemplate : labels : app : cert-manager duration : 2160h # 90d renewBefore : 360h # 15d subject : organizations : - Org Name commonName : '*.${DOMAIN}' privateKey : algorithm : RSA encoding : PKCS1 size : 2048 rotationPolicy : Always usages : - server auth - client auth dnsNames : - '*.${DOMAIN}' - '*.apps.${DOMAIN}' issuerRef : name : letsencrypt-staging kind : ClusterIssuer Info cert-manager supports ECDSA key pairs in the Certificate resource. To use it, change RSA privateKey to ECDSA: privateKey : algorithm : ECDSA encoding : PKCS1 size : 256 rotationPolicy : Always rotationPolicy: Always is highly recommended since cert-manager does not rotate private keys by default. Full Certificate spec is described in the cert-manager API documentation . Check that the certificates in the namespaces are ready: Cert-Manager Certificate Status Cert-Manager Certificate Status Check the details of the certificates via CLI: oc describe certificate api-certs -n openshift-config oc describe certificate router-certs -n openshift-ingress Check the cert-manager controller pod logs if the Staging Certificate condition is not ready for more than 7 minutes: oc get pod -n openshift-operators | grep 'cert-manager' oc logs -f cert-manager- ${ replica_set } - ${ random_string } -n openshift-operators When the certificate is ready, its private key will be put into the OpenShift Secret in the namespace indicated in the Certificate resource: oc describe secret api-certs -n openshift-config oc describe secret router-certs -n openshift-ingress","title":"Configure Certificates"},{"location":"operator-guide/ssl-automation-okd/#modify-openshift-router-and-api-server-custom-resources","text":"Update the Custom Resource of your Router (Ingress controller). Patch the defaultCertificate object value with { \"name\": \"router-certs\" } : oc patch ingresscontroller default -n openshift-ingress-operator --type = merge --patch = '{\"spec\": { \"defaultCertificate\": { \"name\": \"router-certs\" }}}' --insecure-skip-tls-verify Info After updating the IngressController object, the OpenShift Ingress operator redeploys the router. Update the Custom Resource for the OpenShift API Server: Export the name of APIServer : export OKD_API = $( oc whoami --show-server --insecure-skip-tls-verify | cut -f 2 -d ':' | cut -f 3 -d '/' | sed 's/-api././' ) Patch the servingCertificate object value with { \"name\": \"api-certs\" } : oc patch apiserver cluster --type merge --patch = \"{\\\"spec\\\": {\\\"servingCerts\\\": {\\\"namedCertificates\\\": [ { \\\"names\\\": [ \\\" $OKD_API \\\" ], \\\"servingCertificate\\\": {\\\"name\\\": \\\"api-certs\\\" }}]}}}\" --insecure-skip-tls-verify","title":"Modify OpenShift Router and API Server Custom Resources"},{"location":"operator-guide/ssl-automation-okd/#move-from-lets-encrypt-staging-environment-to-prod","text":"Test the Staging certificate on the OpenShift Admin Console. The --insecure flag is used because Let's Encrypt Staging certificates are not trusted in browsers by default: curl -v --insecure https://console-openshift-console.apps. ${ DOMAIN } Change issuerRef to letsencrypt-prod in both Certificate resources: oc edit certificate api-certs -n openshift-config oc edit certificate router-certs -n openshift-ingress issuerRef : name : letsencrypt-prod kind : ClusterIssuer Note In case the certificate reissue is not triggered after that, try to force the certificate renewal with cmctl : cmctl renew router-certs -n openshift-ingress cmctl renew api-certs -n openshift-config If this won't work, delete the api-certs and router-certs secrets. It should trigger the Prod certificates issuance: oc delete secret router-certs -n openshift-ingress oc delete secret api-certs -n openshift-config Please note that these actions will lead to logging your account out of the OpenShift Admin Console, since certificates will be deleted. Accept the certificate warning in the browser and log in again after that. Check the status of the Prod certificates: oc describe certificate api-certs -n openshift-config oc describe certificate router-certs -n openshift-ingress cmctl status certificate api-certs -n openshift-config cmctl status certificate router-certs -n openshift-ingress Check the web console and make sure it has secure connection: curl -v https://console-openshift-console.apps. ${ DOMAIN }","title":"Move From Let's Encrypt Staging Environment to Prod"},{"location":"operator-guide/ssl-automation-okd/#troubleshoot-certificates","text":"Below is an example of the DNS TXT challenge record created by the cert-manager operator: DNS Validation Use nslookup or dig tools to check if the DNS propagation for the TXT record is complete: nslookup -type = txt _acme-challenge. ${ DOMAIN } dig txt _acme-challenge. ${ DOMAIN } Otherwise, use web tools like Google Admin Toolbox : DNS Validation If the correct TXT value is shown (the value corresponds to the current TXT value in the DNS zone), it means that the DNS propagation is complete and Let's Encrypt is able to access the record in order to validate it and issue a trusted certificate. Note If the DNS validation challenge self check fails, cert-manager will retry the self check with a fixed 10-second retry interval. Challenges that do not ever complete the self check will continue retrying until the user intervenes by either retrying the Order (by deleting the Order resource) or amending the associated Certificate resource to resolve any configuration errors. As soon as the domain ownership has been verified, any cert-manager affected validation TXT records in the AWS Route53 DNS zone will be cleaned up. Please find below the issues that may occur and their troubleshooting: When certificates are not issued for a long time, or a cert-manager resource is not in a Ready state, describing a resource may show the reason for the error. Basically, the cert-manager creates the following resources during a Certificate issuance: CertificateRequest , Order , and Challenge . Investigate each of them in case of errors. Use the cmctl tool to show the state of a Certificate and its associated resources. Check the cert-manager controller pod logs: oc get pod -n openshift-operators | grep 'cert-manager' oc logs -f cert-manager- ${ replica_set } - ${ random_string } -n openshift-operators Certificate error debugging: a. Decode certificate chain located in the secrets: oc get secret router-certs -n openshift-ingress -o 'go-template={{index .data \"tls.crt\"}}' | base64 -d | while openssl x509 -noout -text ; do : ; done 2 >/dev/null oc get secret api-certs -n openshift-config -o 'go-template={{index .data \"tls.crt\"}}' | base64 -d | while openssl x509 -noout -text ; do : ; done 2 >/dev/null cmctl inspect secret router-certs -n openshift-ingress cmctl inspect secret api-certs -n openshift-config b. Check the SSL RSA private key consistency: oc get secret router-certs -n openshift-ingress -o 'go-template={{index .data \"tls.key\"}}' | base64 -d | openssl rsa -check -noout oc get secret api-certs -n openshift-config -o 'go-template={{index .data \"tls.key\"}}' | base64 -d | openssl rsa -check -noout c. Match the SSL certificate public key against its RSA private key. Their modulus must be identical: diff < ( oc get secret api-certs -n openshift-config -o 'go-template={{index .data \"tls.crt\"}}' | base64 -d | openssl x509 -noout -modulus | openssl md5 ) < ( oc get secret api-certs -n openshift-config -o 'go-template={{index .data \"tls.key\"}}' | base64 -d | openssl rsa -noout -modulus | openssl md5 ) diff < ( oc get secret router-certs -n openshift-ingress -o 'go-template={{index .data \"tls.crt\"}}' | base64 -d | openssl x509 -noout -modulus | openssl md5 ) < ( oc get secret router-certs -n openshift-ingress -o 'go-template={{index .data \"tls.key\"}}' | base64 -d | openssl rsa -noout -modulus | openssl md5 )","title":"Troubleshoot Certificates"},{"location":"operator-guide/ssl-automation-okd/#remove-obsolete-certificate-authority-data-from-kubeconfig","text":"After updating the certificates, the access to the cluster via Lens or CLI will be denied because of the untrusted certificate errors: $ oc whoami Unable to connect to the server: x509: certificate signed by unknown authority Such behavior appears because the oc tool references an old CA data in the kubeconfig file. Note Examine the Certificate Authority data using the following command: oc config view --minify --raw -o jsonpath = '{.clusters[].cluster.certificate-authority-data}' | base64 -d | openssl x509 -text This certificate has the CA:TRUE parameter, which means that this is a self-signed root CA certificate. To fix the error, remove the old CA data from your OpenShift kubeconfig file: sed -i \"/certificate-authority-data/d\" $KUBECONFIG Since this field will be absent in the kubeconfig file, system root SSL certificate will be used to validate the cluster certificate trust chain. On Ubuntu, Let's Encrypt OpenShift cluster certificates will be validated against Internet Security Research Group root in /etc/ssl/certs/ca-certificates.crt .","title":"Remove Obsolete Certificate Authority Data From Kubeconfig"},{"location":"operator-guide/ssl-automation-okd/#certificate-renewals","text":"The cert-manager automatically renews the certificates based on the X.509 certificate's duration and the renewBefore value. The minimum value for the spec.duration is 1 hour; for spec.renewBefore , 5 minutes. It is also required that spec.duration > spec.renewBefore . Use the cmctl tool to manually trigger a single instant certificate renewal: cmctl renew router-certs -n openshift-ingress cmctl renew api-certs -n openshift-config Otherwise, manually renew all certificates in all namespaces with the app=cert-manager label: cmctl renew --all-namespaces -l app = cert-manager Run the cmctl renew --help command to get more details.","title":"Certificate Renewals"},{"location":"operator-guide/ssl-automation-okd/#related-articles","text":"Cert-Manager Official Documentation Installing the Cert-Manager Operator for Red Hat OpenShift Checking Issued Certificate Details","title":"Related Articles"},{"location":"operator-guide/upgrade-edp-2.10.x-to-2.11.x/","text":"Upgrade EDP v.2.10.x to v.2.11.x \u2693\ufe0e This section provides the details on the EDP upgrade from the v.2.10.x to the v.2.11.x. Explore the actions and requirements below. Update Custom Resource Definitions. Run the following command to apply all the necessary CRDs to the cluster: kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.12/deploy-templates/crds/edp_v1alpha1_cd_stage_deploy_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.11/deploy-templates/crds/v2_v1alpha1_merge_request_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-nexus-operator/release/2.11/deploy-templates/crds/edp_v1alpha1_user_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-cd-pipeline-operator/release/2.11/deploy-templates/crds/edp_v1alpha1_cdpipeline_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.11/deploy-templates/crds/v2_v1alpha1_jenkinssharedlibrary_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.11/deploy-templates/crds/v2_v1alpha1_cdstagejenkinsdeployment_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.11/deploy-templates/crds/v1_v1alpha1_keycloakauthflow_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.11/deploy-templates/crds/v1_v1alpha1_keycloakrealmuser_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.12/deploy-templates/crds/edp_v1alpha1_codebaseimagestream_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.12/deploy-templates/crds/edp_v1alpha1_codebase_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-sonar-operator/release/2.11/deploy-templates/crds/edp_v1alpha1_sonar_group_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-sonar-operator/release/2.11/deploy-templates/crds/edp_v1alpha1_permission_template_crd.yaml Backup kaniko-template config-map and then remove it. This component will be delivered during upgrade. Set required awsRegion parameter. Pay attention that the nesting of the kanikoRoleArn parameter has been changed to the kaniko.roleArn parameter. Check the parameters in the EDP installation chart. For details, please refer to the values.yaml file. To upgrade EDP to the v.2.11.x, run the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.11.x Note To verify the installation, it is possible to test the deployment before applying it to the cluster with: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.11.x --dry-run Update Sonar Project Key: Note Avoid using special characters when creating projects in SonarQube. Allowed characters are: letters , numbers , - , _ , . and : , with at least one non-digit. For details, please refer to the SonarQube documentation . As the result, the project name will be: project-name-release-0.0 or project-name-branchName . Such actions are required to be followed with the aim to store the SonarQube statistics from the previous EDP version: Warning Do not run any pipeline with the updated sonar stage on any existing application before the completion of the first step. 4.1. Update the project key in SonarQube from old to new format by adding the default branch name. - Navigate to Project Settings -> Update Key: Update SonarQube project key - Enter the default branch name and click Update: Update SonarQube project key 4.2. As the result, after the first run, the project name will be changed to a new format containing all previous statistics: SonarQube project history activity Update image versions for the Jenkins agents in the ConfigMap : kubectl edit configmap jenkins-slaves -n <edp-namespace> The versions of the images should be: epamedp / edp - jenkins - codenarc - agent : 3.0.4 epamedp / edp - jenkins - dotnet - 21 - agent : 3.0.4 epamedp / edp - jenkins - dotnet - 31 - agent : 3.0.3 epamedp / edp - jenkins - go - agent : 3.0.5 epamedp / edp - jenkins - gradle - java11 - agent : 3.0.2 epamedp / edp - jenkins - gradle - java8 - agent : 3.0.2 epamedp / edp - jenkins - helm - agent : 3.0.3 epamedp / edp - jenkins - maven - java11 - agent : 3.0.3 epamedp / edp - jenkins - maven - java8 - agent : 3.0.3 epamedp / edp - jenkins - npm - agent : 3.0.4 epamedp / edp - jenkins - opa - agent : 3.0.2 epamedp / edp - jenkins - python - 38 - agent : 3.0.2 epamedp / edp - jenkins - terraform - agent : 3.0.3 Add Jenkins agent by following the template: View: values.yaml kaniko-docker-template : |- <org.csanchez.jenkins.plugins.kubernetes.PodTemplate> <inheritFrom></inheritFrom> <name>kaniko-docker</name> <namespace></namespace> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <instanceCap>2147483647</instanceCap> <slaveConnectTimeout>100</slaveConnectTimeout> <idleMinutes>5</idleMinutes> <activeDeadlineSeconds>0</activeDeadlineSeconds> <label>kaniko-docker</label> <serviceAccount>jenkins</serviceAccount> <nodeSelector>beta.kubernetes.io/os=linux</nodeSelector> <nodeUsageMode>NORMAL</nodeUsageMode> <workspaceVolume class=\"org.csanchez.jenkins.plugins.kubernetes.volumes.workspace.EmptyDirWorkspaceVolume\"> <memory>false</memory> </workspaceVolume> <volumes/> <containers> <org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> <name>jnlp</name> <image>epamedp/edp-jenkins-kaniko-docker-agent:1.0.4</image> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <workingDir>/tmp</workingDir> <command></command> <args>${computer.jnlpmac} ${computer.name}</args> <ttyEnabled>false</ttyEnabled> <resourceRequestCpu></resourceRequestCpu> <resourceRequestMemory></resourceRequestMemory> <resourceLimitCpu></resourceLimitCpu> <resourceLimitMemory></resourceLimitMemory> <envVars> <org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> <key>JAVA_TOOL_OPTIONS</key> <value>-XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true</value> </org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> </envVars> <ports/> </org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> </containers> <envVars/> <annotations/> <imagePullSecrets/> <podRetention class=\"org.csanchez.jenkins.plugins.kubernetes.pod.retention.Default\"/> </org.csanchez.jenkins.plugins.kubernetes.PodTemplate> Restart the Jenkins pod. Update the Jenkins plugins with the 'pipeline' name and 'HTTP Request Plugin'. Update Jenkins provisioners according to the Manage Jenkins CI Pipeline Job Provisioner and Manage Jenkins CD Pipeline Job Provisioner documentation. Restart the codebase-operator to recreate the Code-review and Build pipelines for codebases. Run the CD job-provisioners for every CD pipeline to align the CD stages.","title":"Upgrade EDP v.2.10.x to v.2.11.x"},{"location":"operator-guide/upgrade-edp-2.10.x-to-2.11.x/#upgrade-edp-v210x-to-v211x","text":"This section provides the details on the EDP upgrade from the v.2.10.x to the v.2.11.x. Explore the actions and requirements below. Update Custom Resource Definitions. Run the following command to apply all the necessary CRDs to the cluster: kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.12/deploy-templates/crds/edp_v1alpha1_cd_stage_deploy_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.11/deploy-templates/crds/v2_v1alpha1_merge_request_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-nexus-operator/release/2.11/deploy-templates/crds/edp_v1alpha1_user_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-cd-pipeline-operator/release/2.11/deploy-templates/crds/edp_v1alpha1_cdpipeline_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.11/deploy-templates/crds/v2_v1alpha1_jenkinssharedlibrary_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.11/deploy-templates/crds/v2_v1alpha1_cdstagejenkinsdeployment_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.11/deploy-templates/crds/v1_v1alpha1_keycloakauthflow_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.11/deploy-templates/crds/v1_v1alpha1_keycloakrealmuser_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.12/deploy-templates/crds/edp_v1alpha1_codebaseimagestream_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.12/deploy-templates/crds/edp_v1alpha1_codebase_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-sonar-operator/release/2.11/deploy-templates/crds/edp_v1alpha1_sonar_group_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-sonar-operator/release/2.11/deploy-templates/crds/edp_v1alpha1_permission_template_crd.yaml Backup kaniko-template config-map and then remove it. This component will be delivered during upgrade. Set required awsRegion parameter. Pay attention that the nesting of the kanikoRoleArn parameter has been changed to the kaniko.roleArn parameter. Check the parameters in the EDP installation chart. For details, please refer to the values.yaml file. To upgrade EDP to the v.2.11.x, run the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.11.x Note To verify the installation, it is possible to test the deployment before applying it to the cluster with: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.11.x --dry-run Update Sonar Project Key: Note Avoid using special characters when creating projects in SonarQube. Allowed characters are: letters , numbers , - , _ , . and : , with at least one non-digit. For details, please refer to the SonarQube documentation . As the result, the project name will be: project-name-release-0.0 or project-name-branchName . Such actions are required to be followed with the aim to store the SonarQube statistics from the previous EDP version: Warning Do not run any pipeline with the updated sonar stage on any existing application before the completion of the first step. 4.1. Update the project key in SonarQube from old to new format by adding the default branch name. - Navigate to Project Settings -> Update Key: Update SonarQube project key - Enter the default branch name and click Update: Update SonarQube project key 4.2. As the result, after the first run, the project name will be changed to a new format containing all previous statistics: SonarQube project history activity Update image versions for the Jenkins agents in the ConfigMap : kubectl edit configmap jenkins-slaves -n <edp-namespace> The versions of the images should be: epamedp / edp - jenkins - codenarc - agent : 3.0.4 epamedp / edp - jenkins - dotnet - 21 - agent : 3.0.4 epamedp / edp - jenkins - dotnet - 31 - agent : 3.0.3 epamedp / edp - jenkins - go - agent : 3.0.5 epamedp / edp - jenkins - gradle - java11 - agent : 3.0.2 epamedp / edp - jenkins - gradle - java8 - agent : 3.0.2 epamedp / edp - jenkins - helm - agent : 3.0.3 epamedp / edp - jenkins - maven - java11 - agent : 3.0.3 epamedp / edp - jenkins - maven - java8 - agent : 3.0.3 epamedp / edp - jenkins - npm - agent : 3.0.4 epamedp / edp - jenkins - opa - agent : 3.0.2 epamedp / edp - jenkins - python - 38 - agent : 3.0.2 epamedp / edp - jenkins - terraform - agent : 3.0.3 Add Jenkins agent by following the template: View: values.yaml kaniko-docker-template : |- <org.csanchez.jenkins.plugins.kubernetes.PodTemplate> <inheritFrom></inheritFrom> <name>kaniko-docker</name> <namespace></namespace> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <instanceCap>2147483647</instanceCap> <slaveConnectTimeout>100</slaveConnectTimeout> <idleMinutes>5</idleMinutes> <activeDeadlineSeconds>0</activeDeadlineSeconds> <label>kaniko-docker</label> <serviceAccount>jenkins</serviceAccount> <nodeSelector>beta.kubernetes.io/os=linux</nodeSelector> <nodeUsageMode>NORMAL</nodeUsageMode> <workspaceVolume class=\"org.csanchez.jenkins.plugins.kubernetes.volumes.workspace.EmptyDirWorkspaceVolume\"> <memory>false</memory> </workspaceVolume> <volumes/> <containers> <org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> <name>jnlp</name> <image>epamedp/edp-jenkins-kaniko-docker-agent:1.0.4</image> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <workingDir>/tmp</workingDir> <command></command> <args>${computer.jnlpmac} ${computer.name}</args> <ttyEnabled>false</ttyEnabled> <resourceRequestCpu></resourceRequestCpu> <resourceRequestMemory></resourceRequestMemory> <resourceLimitCpu></resourceLimitCpu> <resourceLimitMemory></resourceLimitMemory> <envVars> <org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> <key>JAVA_TOOL_OPTIONS</key> <value>-XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true</value> </org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> </envVars> <ports/> </org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> </containers> <envVars/> <annotations/> <imagePullSecrets/> <podRetention class=\"org.csanchez.jenkins.plugins.kubernetes.pod.retention.Default\"/> </org.csanchez.jenkins.plugins.kubernetes.PodTemplate> Restart the Jenkins pod. Update the Jenkins plugins with the 'pipeline' name and 'HTTP Request Plugin'. Update Jenkins provisioners according to the Manage Jenkins CI Pipeline Job Provisioner and Manage Jenkins CD Pipeline Job Provisioner documentation. Restart the codebase-operator to recreate the Code-review and Build pipelines for codebases. Run the CD job-provisioners for every CD pipeline to align the CD stages.","title":"Upgrade EDP v.2.10.x to v.2.11.x"},{"location":"operator-guide/upgrade-edp-2.11.x-to-2.12.x/","text":"Upgrade EDP v.2.11.x to v.2.12.x \u2693\ufe0e Warning Please make a backup of your EDP environment before the upgrade procedure. This section provides the details on the EDP upgrade from the v.2.11.x to the v.2.12.x. Explore the actions and requirements below. Notes EDP now supports Kubernetes 1.22: Ingress Resources use networking.k8s.io/v1 , and Ingress Operators use CustomResourceDefinition apiextensions.k8s.io/v1 . EDP Team now delivers its own Gerrit Docker image: epamedp/edp-gerrit . It is based on the openfrontier Gerrit Docker image . EDP now uses DefectDojo as a SAST tool. It is mandatory to deploy DefectDojo before updating EDP to v.2.12.x. Update Custom Resource Definitions (CRDs). Run the following command, to apply all necessary CRDs to the cluster: kubectl apply -f https://raw.githubusercontent.com/epam/edp-admin-console-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_adminconsoles.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-cd-pipeline-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_cdpipelines.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_cdstagedeployments.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_cdstagejenkinsdeployments.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_codebasebranches.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_codebaseimagestreams.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_codebases.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-component-operator/release/0.12/deploy-templates/crds/v1.edp.epam.com_edpcomponents.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerritgroupmembers.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerritgroups.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerritmergerequests.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerritprojectaccesses.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerritprojects.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerritreplicationconfigs.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerrits.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_gitservers.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_gittags.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_imagestreamtags.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkins.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsagents.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsauthorizationrolemappings.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsauthorizationroles.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsfolders.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsjobbuildruns.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsjobs.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsscripts.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsserviceaccounts.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinssharedlibraries.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_jiraissuemetadatas.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_jiraservers.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakauthflows.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakclients.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakclientscopes.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealmcomponents.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealmgroups.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealmidentityproviders.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealmrolebatches.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealmroles.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealms.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealmusers.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloaks.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-nexus-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_nexuses.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-nexus-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_nexususers.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-perf-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_perfdatasourcegitlabs.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-perf-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_perfdatasourcejenkinses.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-perf-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_perfdatasourcesonars.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-perf-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_perfservers.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-sonar-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_sonargroups.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-sonar-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_sonarpermissiontemplates.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-sonar-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_sonars.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-cd-pipeline-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_stages.yaml Set the required parameters. For details, please refer to the values.yaml file. In version v.2.12.x, EDP contains Gerrit v3.6.1 . According to the Official Gerrit Upgrade flow , a user must initially upgrade to Gerrit v3.5.2 , and then upgrade to v3.6.1 . Therefore, define the gerrit-operator.gerrit.version=3.5.2 value in the edp-install values.yaml file. Two more components are available with the new functionality: edp-argocd-operator external-secrets If there is no need to use these new operators, define false values for them in the existing value.yaml file: View: values.yaml gerrit-operator : gerrit : version : \"3.5.2\" externalSecrets : enabled : false argocd : enabled : false The edp-jenkins-role is renamed to the jenkins-resources-role . Delete the edp-jenkins-role with the following command: kubectl delete role edp-jenkins-role -n <edp-namespace> The jenkins-resources-role role will be created automatically while EDP upgrade. Recreate the edp-jenkins-resources-permissions RoleBinding according to the following template: View: jenkins-resources-role apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : edp-jenkins-resources-permissions namespace : <edp-namespace> roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : jenkins-resources-role To upgrade EDP to the v.2.12.x, run the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.12.x Note To verify the installation, it is possible to test the deployment before applying it to the cluster with the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.12.x --dry-run After the update, please remove the gerrit-operator.gerrit.version value. In this case, the default value will be used, and Gerrit will be updated to the v3.6.1 version. Run the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.12.x Note To verify the installation, it is possible to test the deployment before applying it to the cluster with the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.12.x --dry-run Update image versions for the Jenkins agents in the ConfigMap : kubectl edit configmap jenkins-slaves -n <edp-namespace> The versions of the images must be the following: epamedp / edp - jenkins - codenarc - agent : 3.0.8 epamedp / edp - jenkins - dotnet - 21 - agent : 3.0.7 epamedp / edp - jenkins - dotnet - 31 - agent : 3.0.7 epamedp / edp - jenkins - go - agent : 3.0.11 epamedp / edp - jenkins - gradle - java11 - agent : 3.0.5 epamedp / edp - jenkins - gradle - java8 - agent : 3.0.5 epamedp / edp - jenkins - helm - agent : 3.0.8 epamedp / edp - jenkins - maven - java11 - agent : 3.0.6 epamedp / edp - jenkins - maven - java8 - agent : 3.0.6 epamedp / edp - jenkins - npm - agent : 3.0.7 epamedp / edp - jenkins - opa - agent : 3.0.5 epamedp / edp - jenkins - python - 38 - agent : 3.0.5 epamedp / edp - jenkins - terraform - agent : 3.0.6 Add Jenkins agents by following the template: View: jenkins-slaves sast-template : | <org.csanchez.jenkins.plugins.kubernetes.PodTemplate> <inheritFrom></inheritFrom> <name>sast</name> <namespace></namespace> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <instanceCap>2147483647</instanceCap> <slaveConnectTimeout>100</slaveConnectTimeout> <idleMinutes>5</idleMinutes> <activeDeadlineSeconds>0</activeDeadlineSeconds> <label>sast</label> <serviceAccount>jenkins</serviceAccount> <nodeSelector>beta.kubernetes.io/os=linux</nodeSelector> <nodeUsageMode>NORMAL</nodeUsageMode> <workspaceVolume class=\"org.csanchez.jenkins.plugins.kubernetes.volumes.workspace.EmptyDirWorkspaceVolume\"> <memory>false</memory> </workspaceVolume> <volumes/> <containers> <org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> <name>jnlp</name> <image>epamedp/edp-jenkins-sast-agent:0.1.3</image> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <workingDir>/tmp</workingDir> <command></command> <args>${computer.jnlpmac} ${computer.name}</args> <ttyEnabled>false</ttyEnabled> <resourceRequestCpu></resourceRequestCpu> <resourceRequestMemory></resourceRequestMemory> <resourceLimitCpu></resourceLimitCpu> <resourceLimitMemory></resourceLimitMemory> <envVars> <org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> <key>JAVA_TOOL_OPTIONS</key> <value>-XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true</value> </org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> </envVars> <ports/> </org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> </containers> <envVars/> <annotations/> <imagePullSecrets/> <podRetention class=\"org.csanchez.jenkins.plugins.kubernetes.pod.retention.Default\"/> </org.csanchez.jenkins.plugins.kubernetes.PodTemplate> If required, update the requests and limits for the following Jenkins agents: edp-jenkins-codenarc-agent edp-jenkins-go-agent edp-jenkins-gradle-java11-agent edp-jenkins-gradle-java8-agent edp-jenkins-maven-java11-agent edp-jenkins-maven-java8-agent edp-jenkins-npm-agent edp-jenkins-dotnet-21-agent edp-jenkins-dotnet-31-agent EDP requires to start with the following values: View: jenkins-slaves <resourceRequestCpu>500m</resourceRequestCpu> <resourceRequestMemory>1Gi</resourceRequestMemory> <resourceLimitCpu>2</resourceLimitCpu> <resourceLimitMemory>5Gi</resourceLimitMemory> Restart the Jenkins pod. Update Jenkins provisioners according to the Manage Jenkins CI Pipeline Job Provisioner instruction. Restart the codebase-operator , to recreate the Code Review and Build pipelines for the codebases. Warning In case there are different EDP versions on one cluster, the following error may occur on the init stage of Jenkins Groovy pipeline: java.lang.NumberFormatException: For input string: \"\" . To fix this issue, please run the following command using kubectl v1.24.4+ : kubectl patch codebasebranches.v2.edp.epam.com <codebase-branch-name> -n <edp-namespace> '--subresource=status' '--type=merge' -p '{\"status\": {\"build\": \"0\"}}'","title":"Upgrade EDP v.2.11.x to v.2.12.x"},{"location":"operator-guide/upgrade-edp-2.11.x-to-2.12.x/#upgrade-edp-v211x-to-v212x","text":"Warning Please make a backup of your EDP environment before the upgrade procedure. This section provides the details on the EDP upgrade from the v.2.11.x to the v.2.12.x. Explore the actions and requirements below. Notes EDP now supports Kubernetes 1.22: Ingress Resources use networking.k8s.io/v1 , and Ingress Operators use CustomResourceDefinition apiextensions.k8s.io/v1 . EDP Team now delivers its own Gerrit Docker image: epamedp/edp-gerrit . It is based on the openfrontier Gerrit Docker image . EDP now uses DefectDojo as a SAST tool. It is mandatory to deploy DefectDojo before updating EDP to v.2.12.x. Update Custom Resource Definitions (CRDs). Run the following command, to apply all necessary CRDs to the cluster: kubectl apply -f https://raw.githubusercontent.com/epam/edp-admin-console-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_adminconsoles.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-cd-pipeline-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_cdpipelines.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_cdstagedeployments.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_cdstagejenkinsdeployments.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_codebasebranches.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_codebaseimagestreams.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_codebases.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-component-operator/release/0.12/deploy-templates/crds/v1.edp.epam.com_edpcomponents.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerritgroupmembers.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerritgroups.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerritmergerequests.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerritprojectaccesses.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerritprojects.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerritreplicationconfigs.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_gerrits.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_gitservers.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_gittags.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_imagestreamtags.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkins.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsagents.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsauthorizationrolemappings.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsauthorizationroles.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsfolders.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsjobbuildruns.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsjobs.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsscripts.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinsserviceaccounts.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_jenkinssharedlibraries.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_jiraissuemetadatas.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.13/deploy-templates/crds/v2.edp.epam.com_jiraservers.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakauthflows.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakclients.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakclientscopes.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealmcomponents.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealmgroups.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealmidentityproviders.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealmrolebatches.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealmroles.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealms.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloakrealmusers.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.12/deploy-templates/crds/v1.edp.epam.com_keycloaks.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-nexus-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_nexuses.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-nexus-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_nexususers.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-perf-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_perfdatasourcegitlabs.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-perf-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_perfdatasourcejenkinses.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-perf-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_perfdatasourcesonars.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-perf-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_perfservers.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-sonar-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_sonargroups.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-sonar-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_sonarpermissiontemplates.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-sonar-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_sonars.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-cd-pipeline-operator/release/2.12/deploy-templates/crds/v2.edp.epam.com_stages.yaml Set the required parameters. For details, please refer to the values.yaml file. In version v.2.12.x, EDP contains Gerrit v3.6.1 . According to the Official Gerrit Upgrade flow , a user must initially upgrade to Gerrit v3.5.2 , and then upgrade to v3.6.1 . Therefore, define the gerrit-operator.gerrit.version=3.5.2 value in the edp-install values.yaml file. Two more components are available with the new functionality: edp-argocd-operator external-secrets If there is no need to use these new operators, define false values for them in the existing value.yaml file: View: values.yaml gerrit-operator : gerrit : version : \"3.5.2\" externalSecrets : enabled : false argocd : enabled : false The edp-jenkins-role is renamed to the jenkins-resources-role . Delete the edp-jenkins-role with the following command: kubectl delete role edp-jenkins-role -n <edp-namespace> The jenkins-resources-role role will be created automatically while EDP upgrade. Recreate the edp-jenkins-resources-permissions RoleBinding according to the following template: View: jenkins-resources-role apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : edp-jenkins-resources-permissions namespace : <edp-namespace> roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : jenkins-resources-role To upgrade EDP to the v.2.12.x, run the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.12.x Note To verify the installation, it is possible to test the deployment before applying it to the cluster with the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.12.x --dry-run After the update, please remove the gerrit-operator.gerrit.version value. In this case, the default value will be used, and Gerrit will be updated to the v3.6.1 version. Run the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.12.x Note To verify the installation, it is possible to test the deployment before applying it to the cluster with the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.12.x --dry-run Update image versions for the Jenkins agents in the ConfigMap : kubectl edit configmap jenkins-slaves -n <edp-namespace> The versions of the images must be the following: epamedp / edp - jenkins - codenarc - agent : 3.0.8 epamedp / edp - jenkins - dotnet - 21 - agent : 3.0.7 epamedp / edp - jenkins - dotnet - 31 - agent : 3.0.7 epamedp / edp - jenkins - go - agent : 3.0.11 epamedp / edp - jenkins - gradle - java11 - agent : 3.0.5 epamedp / edp - jenkins - gradle - java8 - agent : 3.0.5 epamedp / edp - jenkins - helm - agent : 3.0.8 epamedp / edp - jenkins - maven - java11 - agent : 3.0.6 epamedp / edp - jenkins - maven - java8 - agent : 3.0.6 epamedp / edp - jenkins - npm - agent : 3.0.7 epamedp / edp - jenkins - opa - agent : 3.0.5 epamedp / edp - jenkins - python - 38 - agent : 3.0.5 epamedp / edp - jenkins - terraform - agent : 3.0.6 Add Jenkins agents by following the template: View: jenkins-slaves sast-template : | <org.csanchez.jenkins.plugins.kubernetes.PodTemplate> <inheritFrom></inheritFrom> <name>sast</name> <namespace></namespace> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <instanceCap>2147483647</instanceCap> <slaveConnectTimeout>100</slaveConnectTimeout> <idleMinutes>5</idleMinutes> <activeDeadlineSeconds>0</activeDeadlineSeconds> <label>sast</label> <serviceAccount>jenkins</serviceAccount> <nodeSelector>beta.kubernetes.io/os=linux</nodeSelector> <nodeUsageMode>NORMAL</nodeUsageMode> <workspaceVolume class=\"org.csanchez.jenkins.plugins.kubernetes.volumes.workspace.EmptyDirWorkspaceVolume\"> <memory>false</memory> </workspaceVolume> <volumes/> <containers> <org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> <name>jnlp</name> <image>epamedp/edp-jenkins-sast-agent:0.1.3</image> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <workingDir>/tmp</workingDir> <command></command> <args>${computer.jnlpmac} ${computer.name}</args> <ttyEnabled>false</ttyEnabled> <resourceRequestCpu></resourceRequestCpu> <resourceRequestMemory></resourceRequestMemory> <resourceLimitCpu></resourceLimitCpu> <resourceLimitMemory></resourceLimitMemory> <envVars> <org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> <key>JAVA_TOOL_OPTIONS</key> <value>-XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true</value> </org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> </envVars> <ports/> </org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> </containers> <envVars/> <annotations/> <imagePullSecrets/> <podRetention class=\"org.csanchez.jenkins.plugins.kubernetes.pod.retention.Default\"/> </org.csanchez.jenkins.plugins.kubernetes.PodTemplate> If required, update the requests and limits for the following Jenkins agents: edp-jenkins-codenarc-agent edp-jenkins-go-agent edp-jenkins-gradle-java11-agent edp-jenkins-gradle-java8-agent edp-jenkins-maven-java11-agent edp-jenkins-maven-java8-agent edp-jenkins-npm-agent edp-jenkins-dotnet-21-agent edp-jenkins-dotnet-31-agent EDP requires to start with the following values: View: jenkins-slaves <resourceRequestCpu>500m</resourceRequestCpu> <resourceRequestMemory>1Gi</resourceRequestMemory> <resourceLimitCpu>2</resourceLimitCpu> <resourceLimitMemory>5Gi</resourceLimitMemory> Restart the Jenkins pod. Update Jenkins provisioners according to the Manage Jenkins CI Pipeline Job Provisioner instruction. Restart the codebase-operator , to recreate the Code Review and Build pipelines for the codebases. Warning In case there are different EDP versions on one cluster, the following error may occur on the init stage of Jenkins Groovy pipeline: java.lang.NumberFormatException: For input string: \"\" . To fix this issue, please run the following command using kubectl v1.24.4+ : kubectl patch codebasebranches.v2.edp.epam.com <codebase-branch-name> -n <edp-namespace> '--subresource=status' '--type=merge' -p '{\"status\": {\"build\": \"0\"}}'","title":"Upgrade EDP v.2.11.x to v.2.12.x"},{"location":"operator-guide/upgrade-edp-2.7.8-to-2.8.4/","text":"Upgrade EDP v.2.7.8 to v.2.8.4 \u2693\ufe0e This section provides the details on the EDP upgrade from the v.2.7.8 to the v.2.8.4. Explore the actions and requirements below. Note Kiosk is implemented and mandatory for EDP v.2.8.4 and is optional for EDP v.2.9.0 and higher. To upgrade EDP v.2.7.8 to the v.2.8.4, take the following steps: Deploy and configure Kiosk (create a Service Account, Account, and ClusterRoleBinging) according to the Set Up Kiosk documentation. Update the spec field in the Kiosk space: apiVersion : tenancy . kiosk . sh / v1alpha1 kind : Space metadata : name : < edp - project > spec : account : < edp - project >- admin Create RoleBinding (required for namespaces created before using Kiosk): Note In the uid field under the ownerReferences in the Kubernetes manifest, indicate the Account Custom Resource ID from accounts.config.kiosk.sh kubectl get account <edp-project>-admin -o=custom-columns=NAME:.metadata.uid --no-headers=true View: rolebinding-kiosk.yaml apiVersion : rbac . authorization . k8s . io / v1 kind : RoleBinding metadata : generateName : < edp - project >- admin - namespace : < edp - project > ownerReferences : - apiVersion : config . kiosk . sh / v1alpha1 blockOwnerDeletion : true controller : true kind : Account name : < edp - project >- admin uid : '' roleRef : apiGroup : rbac . authorization . k8s . io kind : ClusterRole name : kiosk - space - admin subjects : - kind : ServiceAccount name : < edp - project > namespace : security kubectl create -f rolebinding-kiosk.yaml With Amazon Elastic Container Registry to store the images, there are two options: Enable IRSA and create AWS IAM Role for Kaniko image builder. Please refer to the IAM Roles for Kaniko Service Accounts section for the details. The Amazon Elastic Container Registry Roles can be stored in an instance profile . Update Custom Resource Definitions by applying all the necessary CRD to the cluster with the command below: kubectl apply -f https://raw.githubusercontent.com/epam/edp-cd-pipeline-operator/release/2.8/deploy-templates/crds/edp_v1alpha1_cdpipeline_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.8/deploy-templates/crds/edp_v1alpha1_codebase_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.8/deploy-templates/crds/edp_v1alpha1_cd_stage_deploy_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.8/deploy-templates/crds/v2_v1alpha1_jenkinsjobbuildrun_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.8/deploy-templates/crds/v2_v1alpha1_cdstagejenkinsdeployment_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.8/deploy-templates/crds/v2_v1alpha1_jenkinsjob_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-nexus-operator/release/2.8/deploy-templates/crds/edp_v1alpha1_nexus_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.8/deploy-templates/crds/v1_v1alpha1_keycloakauthflow_crd.yaml With Amazon Elastic Container Registry to store and Kaniko to build the images, add the kanikoRoleArn parameter to the values before starting the update process. This parameter is indicated in AWS Roles once IRSA is enabled and AWS IAM Role is created for Kaniko. The value should look as follows: kanikoRoleArn : arn : aws : iam ::< AWS_ACCOUNT_ID >: role / AWSIRSA \u2039 CLUSTER_NAME \u203a\u2039 EDP_NAMESPACE \u203a Kaniko To upgrade EDP to the v.2.8.4, run the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.8.4 Note To verify the installation, it is possible to test the deployment before applying it to the cluster with: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.8.4 --dry-run Remove the following Kubernetes resources left from the previous EDP installation (it is optional): kubectl delete cm luminatesec-conf -n <edp-namespace> kubectl delete sa edp edp-perf-operator -n <edp-namespace> kubectl delete deployment perf-operator -n <edp-namespace> kubectl delete clusterrole edp-<edp-namespace> edp-perf-operator-<edp-namespace> kubectl delete clusterrolebinding edp-<edp-namespace> edp-perf-operator-<edp-namespace> kubectl delete rolebinding edp-<edp-namespace> edp-perf-operator-<edp-namespace>-admin -n <edp-namespace> kubectl delete perfserver epam-perf -n <edp-namespace> kubectl delete services.v2.edp.epam.com postgres rabbit-mq -n <edp-namespace> Update the CI and CD Jenkins job provisioners: Note Please refer to the Manage Jenkins CI Pipeline Job Provisioner section for the details. View: Default CI provisioner template for EDP 2.8.4 /* Copyright 2021 EPAM Systems. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ import groovy.json.* import jenkins.model.Jenkins import hudson.model.* Jenkins jenkins = Jenkins . instance def stages = [ : ] def jiraIntegrationEnabled = Boolean . parseBoolean ( \"${JIRA_INTEGRATION_ENABLED}\" as String ) def commitValidateStage = jiraIntegrationEnabled ? ' ,{ \"name\" : \"commit-validate\" } ' : '' def createJIMStage = jiraIntegrationEnabled ? ' ,{ \"name\" : \"create-jira-issue-metadata\" } ' : '' def buildTool = \"${BUILD_TOOL}\" def goBuildStage = buildTool . toString () == \"go\" ? ' ,{ \"name\" : \"build\" } ' : ' ,{ \"name\" : \"compile\" } ' stages [ ' Code - review - application ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + goBuildStage + ' ,{ \"name\" : \"tests\" }, [ { \"name\" : \"sonar\" },{ \"name\" : \"dockerfile-lint\" },{ \"name\" : \"helm-lint\" } ]] ' stages [ ' Code - review - library ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"compile\" },{ \"name\" : \"tests\" }, ' + ' { \"name\" : \"sonar\" } ] ' stages [ ' Code - review - autotests ' ] = ' [ { \"name\" : \"gerrit-checkout\" },{ \"name\" : \"get-version\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"tests\" },{ \"name\" : \"sonar\" } ' + \"${createJIMStage}\" + ']' stages [ ' Code - review - default ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ']' stages [ ' Code - review - library - terraform ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"terraform-lint\" } ] ' stages [ ' Code - review - library - opa ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"tests\" } ] ' stages [ ' Code - review - library - codenarc ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"sonar\" },{ \"name\" : \"build\" } ] ' stages [ ' Build - library - maven ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" },{ \"name\" : \"build\" },{ \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - library - npm ' ] = stages [ ' Build - library - maven ' ] stages [ ' Build - library - gradle ' ] = stages [ ' Build - library - maven ' ] stages [ ' Build - library - dotnet ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" },{ \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - library - python ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" },{ \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - library - terraform ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"terraform-lint\" } ' + ' ,{ \"name\" : \"terraform-plan\" },{ \"name\" : \"terraform-apply\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - library - opa ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" } ' + ' ,{ \"name\" : \"tests\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - library - codenarc ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"sonar\" },{ \"name\" : \"build\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - maven ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" }, [ { \"name\" : \"sonar\" } ] ,{ \"name\" : \"build\" },{ \"name\" : \"build-image-kaniko\" }, ' + ' { \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - npm ' ] = stages [ ' Build - application - maven ' ] stages [ ' Build - application - gradle ' ] = stages [ ' Build - application - maven ' ] stages [ ' Build - application - dotnet ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" }, [ { \"name\" : \"sonar\" } ] ,{ \"name\" : \"build-image-kaniko\" }, ' + ' { \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - go ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"tests\" },{ \"name\" : \"sonar\" }, ' + ' { \"name\" : \"build\" },{ \"name\" : \"build-image-kaniko\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - python ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" }, ' + ' { \"name\" : \"build-image-kaniko\" },{ \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Create - release ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"create-branch\" },{ \"name\" : \"trigger-job\" } ] ' def defaultBuild = ' [ { \"name\" : \"checkout\" } ' + \"${createJIMStage}\" + ']' def codebaseName = \"${NAME}\" def gitServerCrName = \"${GIT_SERVER_CR_NAME}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID ? GIT_CREDENTIALS_ID : 'gerrit-ciuser-sshkey'}\" def repositoryPath = \"${REPOSITORY_PATH}\" def defaultBranch = \"${DEFAULT_BRANCH}\" def codebaseFolder = jenkins . getItem ( codebaseName ) if ( codebaseFolder == null ) { folder ( codebaseName ) } createListView ( codebaseName , \"Releases\" ) createReleasePipeline ( \"Create-release-${codebaseName}\" , codebaseName , stages [ \"Create-release\" ] , \"create-release.groovy\" , repositoryPath , gitCredentialsId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , defaultBranch ) if ( buildTool . toString (). equalsIgnoreCase ( ' none ' )) { return true } if ( BRANCH ) { def branch = \"${BRANCH}\" def formattedBranch = \"${branch.toUpperCase().replaceAll(/\\\\//, \" - \")}\" createListView ( codebaseName , formattedBranch ) def type = \"${TYPE}\" def crKey = getStageKeyName ( buildTool ) createCiPipeline ( \"Code-review-${codebaseName}\" , codebaseName , stages [ crKey ] , \"code-review.groovy\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) def buildKey = \"Build-${type}-${buildTool.toLowerCase()}\" . toString () if ( type . equalsIgnoreCase ( ' application ' ) || type . equalsIgnoreCase ( ' library ' )) { def jobExists = false if ( \"${formattedBranch}-Build-${codebaseName}\" . toString () in Jenkins . instance . getAllItems (). collect { it . name }) jobExists = true createCiPipeline ( \"Build-${codebaseName}\" , codebaseName , stages . get ( buildKey , defaultBuild ), \"build.groovy\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) if ( ! jobExists ) queue ( \"${codebaseName}/${formattedBranch}-Build-${codebaseName}\" ) } } def createCiPipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , watchBranch , gitServerCrName , gitServerCrVersion ) { pipelineJob ( \"${codebaseName}/${watchBranch.toUpperCase().replaceAll(/\\\\//, \" - \")}-${pipelineName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } triggers { gerrit { events { if ( pipelineName . contains ( \"Build\" )) changeMerged () else patchsetCreated () } project ( \"plain:${codebaseName}\" , [ \"plain:${watchBranch}\" ] ) } } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) } branches ( \"${watchBranch}\" ) scriptPath ( \"${pipelineScript}\" ) } } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) stringParam ( \"BRANCH\" , \"${watchBranch}\" , \"Branch to build artifact from\" ) } } } } } def getStageKeyName ( buildTool ) { if ( buildTool . toString (). equalsIgnoreCase ( ' terraform ' )) { return \"Code-review-library-terraform\" } if ( buildTool . toString (). equalsIgnoreCase ( ' opa ' )) { return \"Code-review-library-opa\" } if ( buildTool . toString (). equalsIgnoreCase ( ' codenarc ' )) { return \"Code-review-library-codenarc\" } def buildToolsOutOfTheBox = [ \"maven\" , \"npm\" , \"gradle\" , \"dotnet\" , \"none\" , \"go\" , \"python\" ] def supBuildTool = buildToolsOutOfTheBox . contains ( buildTool . toString ()) return supBuildTool ? \"Code-review-${TYPE}\" : \"Code-review-default\" } def createReleasePipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , defaultBranch ) { pipelineJob ( \"${codebaseName}/${pipelineName}\" ) { logRotator { numToKeep ( 14 ) daysToKeep ( 30 ) } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) } branches ( \"${defaultBranch}\" ) scriptPath ( \"${pipelineScript}\" ) } } parameters { stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"\" ) if ( pipelineName . contains ( \"Create-release\" )) { stringParam ( \"JIRA_INTEGRATION_ENABLED\" , \"${jiraIntegrationEnabled}\" , \"Is Jira integration enabled\" ) stringParam ( \"GERRIT_PROJECT\" , \"${codebaseName}\" , \"\" ) stringParam ( \"RELEASE_NAME\" , \"\" , \"Name of the release(branch to be created)\" ) stringParam ( \"COMMIT_ID\" , \"\" , \"Commit ID that will be used to create branch from for new release. If empty, HEAD of master will be used\" ) stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"REPOSITORY_PATH\" , \"${repository}\" , \"Full repository path\" ) stringParam ( \"DEFAULT_BRANCH\" , \"${defaultBranch}\" , \"Default repository branch\" ) } } } } } } def createListView ( codebaseName , branchName ) { listView ( \"${codebaseName}/${branchName}\" ) { if ( branchName . toLowerCase () == \"releases\" ) { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^Create-release.*\" ) } } } else { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^${branchName}-(Code-review|Build).*\" ) } } } columns { status () weather () name () lastSuccess () lastFailure () lastDuration () buildButton () } } } Note Please refer to the Manage Jenkins CD Pipeline Job Provisioner page for the details. View: Default CD provisioner template for EDP 2.8.4 /* Copyright 2021 EPAM Systems. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ import groovy.json.* import jenkins.model.Jenkins Jenkins jenkins = Jenkins . instance def pipelineName = \"${PIPELINE_NAME}-cd-pipeline\" def stageName = \"${STAGE_NAME}\" def qgStages = \"${QG_STAGES}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID}\" def sourceType = \"${SOURCE_TYPE}\" def libraryURL = \"${LIBRARY_URL}\" def libraryBranch = \"${LIBRARY_BRANCH}\" def autodeploy = \"${AUTODEPLOY}\" def scriptPath = \"Jenkinsfile\" def containerDeploymentType = \"container\" def deploymentType = \"${DEPLOYMENT_TYPE}\" def stages = buildStages ( deploymentType , containerDeploymentType , qgStages ) def codebaseFolder = jenkins . getItem ( pipelineName ) if ( codebaseFolder == null ) { folder ( pipelineName ) } if ( deploymentType == containerDeploymentType ) { createContainerizedCdPipeline ( pipelineName , stageName , stages , scriptPath , sourceType , libraryURL , libraryBranch , gitCredentialsId , gitServerCrVersion , autodeploy ) } else { createCustomCdPipeline ( pipelineName , stageName ) } def buildStages ( deploymentType , containerDeploymentType , qgStages ) { return deploymentType == containerDeploymentType ? ' [ { \"name\" : \"init\" , \"step_name\" : \"init\" },{ \"name\" : \"deploy\" , \"step_name\" : \"deploy\" }, ' + qgStages + ' ,{ \"name\" : \"promote-images-ecr\" , \"step_name\" : \"promote-images\" } ] ' : '' } def createContainerizedCdPipeline ( pipelineName , stageName , stages , pipelineScript , sourceType , libraryURL , libraryBranch , libraryCredId , gitServerCrVersion , autodeploy ) { pipelineJob ( \"${pipelineName}/${stageName}\" ) { if ( sourceType == \"library\" ) { definition { cpsScm { scm { git { remote { url ( libraryURL ) credentials ( libraryCredId ) } branches ( \"${libraryBranch}\" ) scriptPath ( \"${pipelineScript}\" ) } } } } } else { definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\nDeploy()\" ) sandbox ( true ) } } } properties { disableConcurrentBuilds () } parameters { stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${stages}\" , \"Consequence of stages in JSON format to be run during execution\" ) if ( autodeploy ? . trim () && autodeploy . toBoolean ()) { stringParam ( \"AUTODEPLOY\" , \"${autodeploy}\" , \"Is autodeploy enabled?\" ) stringParam ( \"CODEBASE_VERSION\" , null , \"Codebase versions to deploy.\" ) } } } } def createCustomCdPipeline ( pipelineName , stageName ) { pipelineJob ( \"${pipelineName}/${stageName}\" ) { properties { disableConcurrentBuilds () } } } It is also necessary to add the string parameter DEPLOYMENT_TYPE to the CD provisioner: Go to job-provisions - > cd -> default -> configure ; Add Parameter - > String parameter ; Name -> DEPLOYMENT_TYPE Update Jenkins pipelines and stages to the new release tag: In Jenkins, go to Manage Jenkins -> Configure system -> Find the Global Pipeline Libraries menu. Change the Default version for edp-library-stages from build/2.8.0-RC.6 to build/2.9.0-RC.5 Change the Default version for edp-library-pipelines from build/2.8.0-RC.4 to build/2.9.0-RC.3 Update the edp-admin-console Custom Resource in the KeycloakClient Custom Resource Definition: View: keycloakclient.yaml kind : KeycloakClient apiVersion : v1.edp.epam.com/v1alpha1 metadata : name : edp-admin-console namespace : <edp-namespace> spec : advancedProtocolMappers : false attributes : null audRequired : true clientId : admin-console-client directAccess : true public : false secret : admin-console-client serviceAccount : enabled : true realmRoles : - developer targetRealm : <keycloak-edp-realm> webUrl : >- https://edp-admin-console-example.com kubectl apply -f keycloakclient.yaml Remove the admin-console-client client ID in the edp-namespace-main realm in Keycloak, restart the keycloak-operator pod and check that the new KeycloakClient is created with the confidential access type. Note If \"Internal error\" occurs, regenerate the admin-console-client secret in the Credentials tab in Keycloak and update the admin-console-client secret key \"clientSecret\" and \"password\". Update image versions for the Jenkins agents in the ConfigMap : kubectl edit configmap jenkins-slaves -n <edp-namespace> The versions of the images should be: epamedp / edp - jenkins - dotnet - 21 - agent : 1.0.2 epamedp / edp - jenkins - dotnet - 31 - agent : 1.0.2 epamedp / edp - jenkins - go - agent : 1.0.3 epamedp / edp - jenkins - gradle - java11 - agent : 2.0.2 epamedp / edp - jenkins - gradle - java8 - agent : 1.0.2 epamedp / edp - jenkins - helm - agent : 1.0.6 epamedp / edp - jenkins - maven - java11 - agent : 2.0.3 epamedp / edp - jenkins - maven - java8 - agent : 1.0.2 epamedp / edp - jenkins - npm - agent : 2.0.2 epamedp / edp - jenkins - python - 38 - agent : 2.0.3 epamedp / edp - jenkins - terraform - agent : 2.0.4 Add new Jenkins agents under the data field: View data : codenarc-template : |- <org.csanchez.jenkins.plugins.kubernetes.PodTemplate> <inheritFrom></inheritFrom> <name>codenarc</name> <namespace></namespace> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <instanceCap>2147483647</instanceCap> <slaveConnectTimeout>100</slaveConnectTimeout> <idleMinutes>5</idleMinutes> <activeDeadlineSeconds>0</activeDeadlineSeconds> <label>codenarc</label> <serviceAccount>jenkins</serviceAccount> <nodeSelector>beta.kubernetes.io/os=linux</nodeSelector> <nodeUsageMode>NORMAL</nodeUsageMode> <workspaceVolume class=\"org.csanchez.jenkins.plugins.kubernetes.volumes.workspace.EmptyDirWorkspaceVolume\"> <memory>false</memory> </workspaceVolume> <volumes/> <containers> <org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> <name>jnlp</name> <image>epamedp/edp-jenkins-codenarc-agent:1.0.0</image> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <workingDir>/tmp</workingDir> <command></command> <args>${computer.jnlpmac} ${computer.name}</args> <ttyEnabled>false</ttyEnabled> <resourceRequestCpu></resourceRequestCpu> <resourceRequestMemory></resourceRequestMemory> <resourceLimitCpu></resourceLimitCpu> <resourceLimitMemory></resourceLimitMemory> <envVars> <org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> <key>JAVA_TOOL_OPTIONS</key> <value>-XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true</value> </org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> </envVars> <ports/> </org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> </containers> <envVars/> <annotations/> <imagePullSecrets/> <podRetention class=\"org.csanchez.jenkins.plugins.kubernetes.pod.retention.Default\"/> </org.csanchez.jenkins.plugins.kubernetes.PodTemplate> opa-template : |- <org.csanchez.jenkins.plugins.kubernetes.PodTemplate> <inheritFrom></inheritFrom> <name>opa</name> <namespace></namespace> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <instanceCap>2147483647</instanceCap> <slaveConnectTimeout>100</slaveConnectTimeout> <idleMinutes>5</idleMinutes> <activeDeadlineSeconds>0</activeDeadlineSeconds> <label>opa</label> <serviceAccount>jenkins</serviceAccount> <nodeSelector>beta.kubernetes.io/os=linux</nodeSelector> <nodeUsageMode>NORMAL</nodeUsageMode> <workspaceVolume class=\"org.csanchez.jenkins.plugins.kubernetes.volumes.workspace.EmptyDirWorkspaceVolume\"> <memory>false</memory> </workspaceVolume> <volumes/> <containers> <org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> <name>jnlp</name> <image>epamedp/edp-jenkins-opa-agent:1.0.1</image> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <workingDir>/tmp</workingDir> <command></command> <args>${computer.jnlpmac} ${computer.name}</args> <ttyEnabled>false</ttyEnabled> <resourceRequestCpu></resourceRequestCpu> <resourceRequestMemory></resourceRequestMemory> <resourceLimitCpu></resourceLimitCpu> <resourceLimitMemory></resourceLimitMemory> <envVars> <org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> <key>JAVA_TOOL_OPTIONS</key> <value>-XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true</value> </org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> </envVars> <ports/> </org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> </containers> <envVars/> <annotations/> <imagePullSecrets/> <podRetention class=\"org.csanchez.jenkins.plugins.kubernetes.pod.retention.Default\"/> </org.csanchez.jenkins.plugins.kubernetes.PodTemplate> Restart the Jenkins pod. Update compatible plugins in Jenkins and install additional plugins: Go to Manage Jenkins -> Manage Plugins -> Select Compatible -> Click Download now and install after restart Install the following additional plugins (click the Available plugins tab in Jenkins): Groovy Postbuild CloudBees AWS Credentials Badge Timestamper Add the annotation deploy.edp.epam.com/previous-stage-name: '' (it should be empty if the CD pipeline contains one stage) to each Custom Resource in the Custom Resource Definition Stage , for example: List all Custom Resources in Stage : kubectl get stages.v2.edp.epam.com -n <edp-namespace> Edit resources: kubectl edit stages.v2.edp.epam.com <cd-stage-name> -n <edp-namespace> apiVersion : v2 . edp . epam . com / v1alpha1 kind : Stage metadata : annotations : deploy . edp . epam . com / previous - stage - name : '' Note If a pipeline contains several stages, add a previous stage name indicated in the EDP Admin Console to the annotation, for example: deploy.edp.epam.com/previous-stage-name: 'dev' . Execute script to align CDPipeline resources to the new API ( jq command-line JSON processor is required): pipelines = $ ( kubectl get cdpipelines - n < edp - namespace > - ojson | jq - c ' .items[] ' ) for p in $ pipelines ; do echo \" $p \" | \\ jq ' . | .spec.inputDockerStreams = .spec.input_docker_streams | del(.spec.input_docker_streams) | .spec += { \"deploymentType\": \"container\" } ' | \\ kubectl apply - f - done Update the database in the edp-db pod in the edp-namespace: Log in to the pod: kubectl exec - i - t - n < edp - namespace > edp - db -< pod > - c edp - db \" -- \" sh - c \" (bash || ash || sh) \" Log in to the Postgress DB (where \"admin\" is the user the secret was created for): psql edp-db <admin>; SET search_path to '<edp-namespace>'; UPDATE cd_pipeline SET deployment_type = 'container'; Add \"AUTODEPLOY\":\"true/false\",\"DEPLOYMENT_TYPE\":\"container\" to every Custom Resource in jenkinsjobs.v2.edp.epam.com : Edit Kubernetes resources: kubectl get jenkinsjobs.v2.edp.epam.com -n <edp-namespace> kubectl edit jenkinsjobs.v2.edp.epam.com <cd-pipeline-name> -n <edp-namespace> Alternatively, use this script to update all the necessary jenkinsjobs Custom Resources: edp_namespace =< epd_namespace > for stages in $ ( kubectl get jenkinsjobs - o = name - n $ed p_namespace ) ; do kubectl get $stages -n $edp_namespace -o yaml | grep -q \"container\" && echo -e \"\\n$stages is already updated\" || kubectl get $stages -n $edp_namespace -o yaml | sed 's/\"GIT_SERVER_CR_VERSION\"/\"AUTODEPLOY\":\"false\",\"DEPLOYMENT_TYPE\":\"container\",\"GIT_SERVER_CR_VERSION\"/g' | kubectl apply -f -; done Make sure the edited resource looks as follows: job : config : '{\"AUTODEPLOY\":\"false\",\"DEPLOYMENT_TYPE\":\"container\",\"GIT_SERVER_CR_VERSION\":\"v2\",\"PIPELINE_NAME\":\"your-pipeline-name\",\"QG_STAGES\":\"{\\\"name\\\":\\\"manual\\\",\\\"step_name\\\":\\\"your-step-name\\\"}\",\"SOURCE_TYPE\":\"default\",\"STAGE_NAME\":\"your-stage-name\"}' name : job - provisions /job/cd/job/ default Restart the Jenkins operator pod and wait until the CD job provisioner in Jenkins creates the updated pipelines. Possible Issues \u2693\ufe0e SonarQube fails during the CI pipeline run. The previous builds of SonarQube used the latest version of the OpenID Connect Authentication for SonarQube plugin. Version 2.1.0 of this plugin may have issues with the connection, so it is necessary to downgrade it in order to get rid of errors in the pipeline. Take the following steps: Log in to the Sonar pod: kubectl exec - i - t - n < edp - namespace > sonar -< pod > - c sonar \" -- \" sh - c \" (bash || ash || sh) \" Run the command in the Sonar container: rm extensions/plugins/sonar-auth-oidc-plugin* Install the OpenID Connect Authentication for SonarQube plugin v2.0.0: curl - L https : // github . com / vaulttec / sonar - auth - oidc / releases / download / v2 . 0.0 / sonar - auth - oidc - plugin - 2.0 . 0.j ar -- output extensions / plugins / sonar - auth - oidc - plugin - 2.0 . 0.j ar Restart the SonarQube pod; The Helm lint checker in EDP 2.8.4 has some additional rules. There can be issues with it during the Code Review pipeline in Jenkins for applications that were transferred from previous EDP versions to EDP 2.8.4. To fix this, add the following annotation to the Chart.yaml file: Go to the Git repository -> Choose the application -> Edit the deploy-templates/Chart.yaml file. It is necessary to add the following lines to the bottom of the Chart.yaml file: home : https :// github . com / your - repo . git sources : - https :// github . com / your - repo . git maintainers : - name : DEV Team Add a new line character at the end of the last line. Please be aware it is important. Related Articles \u2693\ufe0e Set Up Kiosk IAM Roles for Kaniko Service Accounts Manage Jenkins CI Pipeline Job Provisioner Manage Jenkins CD Pipeline Job Provisioner","title":"Upgrade EDP v.2.7.8 to v.2.8.4"},{"location":"operator-guide/upgrade-edp-2.7.8-to-2.8.4/#upgrade-edp-v278-to-v284","text":"This section provides the details on the EDP upgrade from the v.2.7.8 to the v.2.8.4. Explore the actions and requirements below. Note Kiosk is implemented and mandatory for EDP v.2.8.4 and is optional for EDP v.2.9.0 and higher. To upgrade EDP v.2.7.8 to the v.2.8.4, take the following steps: Deploy and configure Kiosk (create a Service Account, Account, and ClusterRoleBinging) according to the Set Up Kiosk documentation. Update the spec field in the Kiosk space: apiVersion : tenancy . kiosk . sh / v1alpha1 kind : Space metadata : name : < edp - project > spec : account : < edp - project >- admin Create RoleBinding (required for namespaces created before using Kiosk): Note In the uid field under the ownerReferences in the Kubernetes manifest, indicate the Account Custom Resource ID from accounts.config.kiosk.sh kubectl get account <edp-project>-admin -o=custom-columns=NAME:.metadata.uid --no-headers=true View: rolebinding-kiosk.yaml apiVersion : rbac . authorization . k8s . io / v1 kind : RoleBinding metadata : generateName : < edp - project >- admin - namespace : < edp - project > ownerReferences : - apiVersion : config . kiosk . sh / v1alpha1 blockOwnerDeletion : true controller : true kind : Account name : < edp - project >- admin uid : '' roleRef : apiGroup : rbac . authorization . k8s . io kind : ClusterRole name : kiosk - space - admin subjects : - kind : ServiceAccount name : < edp - project > namespace : security kubectl create -f rolebinding-kiosk.yaml With Amazon Elastic Container Registry to store the images, there are two options: Enable IRSA and create AWS IAM Role for Kaniko image builder. Please refer to the IAM Roles for Kaniko Service Accounts section for the details. The Amazon Elastic Container Registry Roles can be stored in an instance profile . Update Custom Resource Definitions by applying all the necessary CRD to the cluster with the command below: kubectl apply -f https://raw.githubusercontent.com/epam/edp-cd-pipeline-operator/release/2.8/deploy-templates/crds/edp_v1alpha1_cdpipeline_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.8/deploy-templates/crds/edp_v1alpha1_codebase_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-codebase-operator/release/2.8/deploy-templates/crds/edp_v1alpha1_cd_stage_deploy_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.8/deploy-templates/crds/v2_v1alpha1_jenkinsjobbuildrun_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.8/deploy-templates/crds/v2_v1alpha1_cdstagejenkinsdeployment_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.8/deploy-templates/crds/v2_v1alpha1_jenkinsjob_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-nexus-operator/release/2.8/deploy-templates/crds/edp_v1alpha1_nexus_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.8/deploy-templates/crds/v1_v1alpha1_keycloakauthflow_crd.yaml With Amazon Elastic Container Registry to store and Kaniko to build the images, add the kanikoRoleArn parameter to the values before starting the update process. This parameter is indicated in AWS Roles once IRSA is enabled and AWS IAM Role is created for Kaniko. The value should look as follows: kanikoRoleArn : arn : aws : iam ::< AWS_ACCOUNT_ID >: role / AWSIRSA \u2039 CLUSTER_NAME \u203a\u2039 EDP_NAMESPACE \u203a Kaniko To upgrade EDP to the v.2.8.4, run the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.8.4 Note To verify the installation, it is possible to test the deployment before applying it to the cluster with: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.8.4 --dry-run Remove the following Kubernetes resources left from the previous EDP installation (it is optional): kubectl delete cm luminatesec-conf -n <edp-namespace> kubectl delete sa edp edp-perf-operator -n <edp-namespace> kubectl delete deployment perf-operator -n <edp-namespace> kubectl delete clusterrole edp-<edp-namespace> edp-perf-operator-<edp-namespace> kubectl delete clusterrolebinding edp-<edp-namespace> edp-perf-operator-<edp-namespace> kubectl delete rolebinding edp-<edp-namespace> edp-perf-operator-<edp-namespace>-admin -n <edp-namespace> kubectl delete perfserver epam-perf -n <edp-namespace> kubectl delete services.v2.edp.epam.com postgres rabbit-mq -n <edp-namespace> Update the CI and CD Jenkins job provisioners: Note Please refer to the Manage Jenkins CI Pipeline Job Provisioner section for the details. View: Default CI provisioner template for EDP 2.8.4 /* Copyright 2021 EPAM Systems. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ import groovy.json.* import jenkins.model.Jenkins import hudson.model.* Jenkins jenkins = Jenkins . instance def stages = [ : ] def jiraIntegrationEnabled = Boolean . parseBoolean ( \"${JIRA_INTEGRATION_ENABLED}\" as String ) def commitValidateStage = jiraIntegrationEnabled ? ' ,{ \"name\" : \"commit-validate\" } ' : '' def createJIMStage = jiraIntegrationEnabled ? ' ,{ \"name\" : \"create-jira-issue-metadata\" } ' : '' def buildTool = \"${BUILD_TOOL}\" def goBuildStage = buildTool . toString () == \"go\" ? ' ,{ \"name\" : \"build\" } ' : ' ,{ \"name\" : \"compile\" } ' stages [ ' Code - review - application ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + goBuildStage + ' ,{ \"name\" : \"tests\" }, [ { \"name\" : \"sonar\" },{ \"name\" : \"dockerfile-lint\" },{ \"name\" : \"helm-lint\" } ]] ' stages [ ' Code - review - library ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"compile\" },{ \"name\" : \"tests\" }, ' + ' { \"name\" : \"sonar\" } ] ' stages [ ' Code - review - autotests ' ] = ' [ { \"name\" : \"gerrit-checkout\" },{ \"name\" : \"get-version\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"tests\" },{ \"name\" : \"sonar\" } ' + \"${createJIMStage}\" + ']' stages [ ' Code - review - default ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ']' stages [ ' Code - review - library - terraform ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"terraform-lint\" } ] ' stages [ ' Code - review - library - opa ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"tests\" } ] ' stages [ ' Code - review - library - codenarc ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"sonar\" },{ \"name\" : \"build\" } ] ' stages [ ' Build - library - maven ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" },{ \"name\" : \"build\" },{ \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - library - npm ' ] = stages [ ' Build - library - maven ' ] stages [ ' Build - library - gradle ' ] = stages [ ' Build - library - maven ' ] stages [ ' Build - library - dotnet ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" },{ \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - library - python ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" },{ \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - library - terraform ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"terraform-lint\" } ' + ' ,{ \"name\" : \"terraform-plan\" },{ \"name\" : \"terraform-apply\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - library - opa ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" } ' + ' ,{ \"name\" : \"tests\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - library - codenarc ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"sonar\" },{ \"name\" : \"build\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - maven ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" }, [ { \"name\" : \"sonar\" } ] ,{ \"name\" : \"build\" },{ \"name\" : \"build-image-kaniko\" }, ' + ' { \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - npm ' ] = stages [ ' Build - application - maven ' ] stages [ ' Build - application - gradle ' ] = stages [ ' Build - application - maven ' ] stages [ ' Build - application - dotnet ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" }, [ { \"name\" : \"sonar\" } ] ,{ \"name\" : \"build-image-kaniko\" }, ' + ' { \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - go ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"tests\" },{ \"name\" : \"sonar\" }, ' + ' { \"name\" : \"build\" },{ \"name\" : \"build-image-kaniko\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - python ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" }, ' + ' { \"name\" : \"build-image-kaniko\" },{ \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Create - release ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"create-branch\" },{ \"name\" : \"trigger-job\" } ] ' def defaultBuild = ' [ { \"name\" : \"checkout\" } ' + \"${createJIMStage}\" + ']' def codebaseName = \"${NAME}\" def gitServerCrName = \"${GIT_SERVER_CR_NAME}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID ? GIT_CREDENTIALS_ID : 'gerrit-ciuser-sshkey'}\" def repositoryPath = \"${REPOSITORY_PATH}\" def defaultBranch = \"${DEFAULT_BRANCH}\" def codebaseFolder = jenkins . getItem ( codebaseName ) if ( codebaseFolder == null ) { folder ( codebaseName ) } createListView ( codebaseName , \"Releases\" ) createReleasePipeline ( \"Create-release-${codebaseName}\" , codebaseName , stages [ \"Create-release\" ] , \"create-release.groovy\" , repositoryPath , gitCredentialsId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , defaultBranch ) if ( buildTool . toString (). equalsIgnoreCase ( ' none ' )) { return true } if ( BRANCH ) { def branch = \"${BRANCH}\" def formattedBranch = \"${branch.toUpperCase().replaceAll(/\\\\//, \" - \")}\" createListView ( codebaseName , formattedBranch ) def type = \"${TYPE}\" def crKey = getStageKeyName ( buildTool ) createCiPipeline ( \"Code-review-${codebaseName}\" , codebaseName , stages [ crKey ] , \"code-review.groovy\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) def buildKey = \"Build-${type}-${buildTool.toLowerCase()}\" . toString () if ( type . equalsIgnoreCase ( ' application ' ) || type . equalsIgnoreCase ( ' library ' )) { def jobExists = false if ( \"${formattedBranch}-Build-${codebaseName}\" . toString () in Jenkins . instance . getAllItems (). collect { it . name }) jobExists = true createCiPipeline ( \"Build-${codebaseName}\" , codebaseName , stages . get ( buildKey , defaultBuild ), \"build.groovy\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) if ( ! jobExists ) queue ( \"${codebaseName}/${formattedBranch}-Build-${codebaseName}\" ) } } def createCiPipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , watchBranch , gitServerCrName , gitServerCrVersion ) { pipelineJob ( \"${codebaseName}/${watchBranch.toUpperCase().replaceAll(/\\\\//, \" - \")}-${pipelineName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } triggers { gerrit { events { if ( pipelineName . contains ( \"Build\" )) changeMerged () else patchsetCreated () } project ( \"plain:${codebaseName}\" , [ \"plain:${watchBranch}\" ] ) } } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) } branches ( \"${watchBranch}\" ) scriptPath ( \"${pipelineScript}\" ) } } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) stringParam ( \"BRANCH\" , \"${watchBranch}\" , \"Branch to build artifact from\" ) } } } } } def getStageKeyName ( buildTool ) { if ( buildTool . toString (). equalsIgnoreCase ( ' terraform ' )) { return \"Code-review-library-terraform\" } if ( buildTool . toString (). equalsIgnoreCase ( ' opa ' )) { return \"Code-review-library-opa\" } if ( buildTool . toString (). equalsIgnoreCase ( ' codenarc ' )) { return \"Code-review-library-codenarc\" } def buildToolsOutOfTheBox = [ \"maven\" , \"npm\" , \"gradle\" , \"dotnet\" , \"none\" , \"go\" , \"python\" ] def supBuildTool = buildToolsOutOfTheBox . contains ( buildTool . toString ()) return supBuildTool ? \"Code-review-${TYPE}\" : \"Code-review-default\" } def createReleasePipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , defaultBranch ) { pipelineJob ( \"${codebaseName}/${pipelineName}\" ) { logRotator { numToKeep ( 14 ) daysToKeep ( 30 ) } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) } branches ( \"${defaultBranch}\" ) scriptPath ( \"${pipelineScript}\" ) } } parameters { stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"\" ) if ( pipelineName . contains ( \"Create-release\" )) { stringParam ( \"JIRA_INTEGRATION_ENABLED\" , \"${jiraIntegrationEnabled}\" , \"Is Jira integration enabled\" ) stringParam ( \"GERRIT_PROJECT\" , \"${codebaseName}\" , \"\" ) stringParam ( \"RELEASE_NAME\" , \"\" , \"Name of the release(branch to be created)\" ) stringParam ( \"COMMIT_ID\" , \"\" , \"Commit ID that will be used to create branch from for new release. If empty, HEAD of master will be used\" ) stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"REPOSITORY_PATH\" , \"${repository}\" , \"Full repository path\" ) stringParam ( \"DEFAULT_BRANCH\" , \"${defaultBranch}\" , \"Default repository branch\" ) } } } } } } def createListView ( codebaseName , branchName ) { listView ( \"${codebaseName}/${branchName}\" ) { if ( branchName . toLowerCase () == \"releases\" ) { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^Create-release.*\" ) } } } else { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^${branchName}-(Code-review|Build).*\" ) } } } columns { status () weather () name () lastSuccess () lastFailure () lastDuration () buildButton () } } } Note Please refer to the Manage Jenkins CD Pipeline Job Provisioner page for the details. View: Default CD provisioner template for EDP 2.8.4 /* Copyright 2021 EPAM Systems. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ import groovy.json.* import jenkins.model.Jenkins Jenkins jenkins = Jenkins . instance def pipelineName = \"${PIPELINE_NAME}-cd-pipeline\" def stageName = \"${STAGE_NAME}\" def qgStages = \"${QG_STAGES}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID}\" def sourceType = \"${SOURCE_TYPE}\" def libraryURL = \"${LIBRARY_URL}\" def libraryBranch = \"${LIBRARY_BRANCH}\" def autodeploy = \"${AUTODEPLOY}\" def scriptPath = \"Jenkinsfile\" def containerDeploymentType = \"container\" def deploymentType = \"${DEPLOYMENT_TYPE}\" def stages = buildStages ( deploymentType , containerDeploymentType , qgStages ) def codebaseFolder = jenkins . getItem ( pipelineName ) if ( codebaseFolder == null ) { folder ( pipelineName ) } if ( deploymentType == containerDeploymentType ) { createContainerizedCdPipeline ( pipelineName , stageName , stages , scriptPath , sourceType , libraryURL , libraryBranch , gitCredentialsId , gitServerCrVersion , autodeploy ) } else { createCustomCdPipeline ( pipelineName , stageName ) } def buildStages ( deploymentType , containerDeploymentType , qgStages ) { return deploymentType == containerDeploymentType ? ' [ { \"name\" : \"init\" , \"step_name\" : \"init\" },{ \"name\" : \"deploy\" , \"step_name\" : \"deploy\" }, ' + qgStages + ' ,{ \"name\" : \"promote-images-ecr\" , \"step_name\" : \"promote-images\" } ] ' : '' } def createContainerizedCdPipeline ( pipelineName , stageName , stages , pipelineScript , sourceType , libraryURL , libraryBranch , libraryCredId , gitServerCrVersion , autodeploy ) { pipelineJob ( \"${pipelineName}/${stageName}\" ) { if ( sourceType == \"library\" ) { definition { cpsScm { scm { git { remote { url ( libraryURL ) credentials ( libraryCredId ) } branches ( \"${libraryBranch}\" ) scriptPath ( \"${pipelineScript}\" ) } } } } } else { definition { cps { script ( \"@Library(['edp-library-stages', 'edp-library-pipelines']) _ \\n\\nDeploy()\" ) sandbox ( true ) } } } properties { disableConcurrentBuilds () } parameters { stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${stages}\" , \"Consequence of stages in JSON format to be run during execution\" ) if ( autodeploy ? . trim () && autodeploy . toBoolean ()) { stringParam ( \"AUTODEPLOY\" , \"${autodeploy}\" , \"Is autodeploy enabled?\" ) stringParam ( \"CODEBASE_VERSION\" , null , \"Codebase versions to deploy.\" ) } } } } def createCustomCdPipeline ( pipelineName , stageName ) { pipelineJob ( \"${pipelineName}/${stageName}\" ) { properties { disableConcurrentBuilds () } } } It is also necessary to add the string parameter DEPLOYMENT_TYPE to the CD provisioner: Go to job-provisions - > cd -> default -> configure ; Add Parameter - > String parameter ; Name -> DEPLOYMENT_TYPE Update Jenkins pipelines and stages to the new release tag: In Jenkins, go to Manage Jenkins -> Configure system -> Find the Global Pipeline Libraries menu. Change the Default version for edp-library-stages from build/2.8.0-RC.6 to build/2.9.0-RC.5 Change the Default version for edp-library-pipelines from build/2.8.0-RC.4 to build/2.9.0-RC.3 Update the edp-admin-console Custom Resource in the KeycloakClient Custom Resource Definition: View: keycloakclient.yaml kind : KeycloakClient apiVersion : v1.edp.epam.com/v1alpha1 metadata : name : edp-admin-console namespace : <edp-namespace> spec : advancedProtocolMappers : false attributes : null audRequired : true clientId : admin-console-client directAccess : true public : false secret : admin-console-client serviceAccount : enabled : true realmRoles : - developer targetRealm : <keycloak-edp-realm> webUrl : >- https://edp-admin-console-example.com kubectl apply -f keycloakclient.yaml Remove the admin-console-client client ID in the edp-namespace-main realm in Keycloak, restart the keycloak-operator pod and check that the new KeycloakClient is created with the confidential access type. Note If \"Internal error\" occurs, regenerate the admin-console-client secret in the Credentials tab in Keycloak and update the admin-console-client secret key \"clientSecret\" and \"password\". Update image versions for the Jenkins agents in the ConfigMap : kubectl edit configmap jenkins-slaves -n <edp-namespace> The versions of the images should be: epamedp / edp - jenkins - dotnet - 21 - agent : 1.0.2 epamedp / edp - jenkins - dotnet - 31 - agent : 1.0.2 epamedp / edp - jenkins - go - agent : 1.0.3 epamedp / edp - jenkins - gradle - java11 - agent : 2.0.2 epamedp / edp - jenkins - gradle - java8 - agent : 1.0.2 epamedp / edp - jenkins - helm - agent : 1.0.6 epamedp / edp - jenkins - maven - java11 - agent : 2.0.3 epamedp / edp - jenkins - maven - java8 - agent : 1.0.2 epamedp / edp - jenkins - npm - agent : 2.0.2 epamedp / edp - jenkins - python - 38 - agent : 2.0.3 epamedp / edp - jenkins - terraform - agent : 2.0.4 Add new Jenkins agents under the data field: View data : codenarc-template : |- <org.csanchez.jenkins.plugins.kubernetes.PodTemplate> <inheritFrom></inheritFrom> <name>codenarc</name> <namespace></namespace> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <instanceCap>2147483647</instanceCap> <slaveConnectTimeout>100</slaveConnectTimeout> <idleMinutes>5</idleMinutes> <activeDeadlineSeconds>0</activeDeadlineSeconds> <label>codenarc</label> <serviceAccount>jenkins</serviceAccount> <nodeSelector>beta.kubernetes.io/os=linux</nodeSelector> <nodeUsageMode>NORMAL</nodeUsageMode> <workspaceVolume class=\"org.csanchez.jenkins.plugins.kubernetes.volumes.workspace.EmptyDirWorkspaceVolume\"> <memory>false</memory> </workspaceVolume> <volumes/> <containers> <org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> <name>jnlp</name> <image>epamedp/edp-jenkins-codenarc-agent:1.0.0</image> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <workingDir>/tmp</workingDir> <command></command> <args>${computer.jnlpmac} ${computer.name}</args> <ttyEnabled>false</ttyEnabled> <resourceRequestCpu></resourceRequestCpu> <resourceRequestMemory></resourceRequestMemory> <resourceLimitCpu></resourceLimitCpu> <resourceLimitMemory></resourceLimitMemory> <envVars> <org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> <key>JAVA_TOOL_OPTIONS</key> <value>-XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true</value> </org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> </envVars> <ports/> </org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> </containers> <envVars/> <annotations/> <imagePullSecrets/> <podRetention class=\"org.csanchez.jenkins.plugins.kubernetes.pod.retention.Default\"/> </org.csanchez.jenkins.plugins.kubernetes.PodTemplate> opa-template : |- <org.csanchez.jenkins.plugins.kubernetes.PodTemplate> <inheritFrom></inheritFrom> <name>opa</name> <namespace></namespace> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <instanceCap>2147483647</instanceCap> <slaveConnectTimeout>100</slaveConnectTimeout> <idleMinutes>5</idleMinutes> <activeDeadlineSeconds>0</activeDeadlineSeconds> <label>opa</label> <serviceAccount>jenkins</serviceAccount> <nodeSelector>beta.kubernetes.io/os=linux</nodeSelector> <nodeUsageMode>NORMAL</nodeUsageMode> <workspaceVolume class=\"org.csanchez.jenkins.plugins.kubernetes.volumes.workspace.EmptyDirWorkspaceVolume\"> <memory>false</memory> </workspaceVolume> <volumes/> <containers> <org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> <name>jnlp</name> <image>epamedp/edp-jenkins-opa-agent:1.0.1</image> <privileged>false</privileged> <alwaysPullImage>false</alwaysPullImage> <workingDir>/tmp</workingDir> <command></command> <args>${computer.jnlpmac} ${computer.name}</args> <ttyEnabled>false</ttyEnabled> <resourceRequestCpu></resourceRequestCpu> <resourceRequestMemory></resourceRequestMemory> <resourceLimitCpu></resourceLimitCpu> <resourceLimitMemory></resourceLimitMemory> <envVars> <org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> <key>JAVA_TOOL_OPTIONS</key> <value>-XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true</value> </org.csanchez.jenkins.plugins.kubernetes.model.KeyValueEnvVar> </envVars> <ports/> </org.csanchez.jenkins.plugins.kubernetes.ContainerTemplate> </containers> <envVars/> <annotations/> <imagePullSecrets/> <podRetention class=\"org.csanchez.jenkins.plugins.kubernetes.pod.retention.Default\"/> </org.csanchez.jenkins.plugins.kubernetes.PodTemplate> Restart the Jenkins pod. Update compatible plugins in Jenkins and install additional plugins: Go to Manage Jenkins -> Manage Plugins -> Select Compatible -> Click Download now and install after restart Install the following additional plugins (click the Available plugins tab in Jenkins): Groovy Postbuild CloudBees AWS Credentials Badge Timestamper Add the annotation deploy.edp.epam.com/previous-stage-name: '' (it should be empty if the CD pipeline contains one stage) to each Custom Resource in the Custom Resource Definition Stage , for example: List all Custom Resources in Stage : kubectl get stages.v2.edp.epam.com -n <edp-namespace> Edit resources: kubectl edit stages.v2.edp.epam.com <cd-stage-name> -n <edp-namespace> apiVersion : v2 . edp . epam . com / v1alpha1 kind : Stage metadata : annotations : deploy . edp . epam . com / previous - stage - name : '' Note If a pipeline contains several stages, add a previous stage name indicated in the EDP Admin Console to the annotation, for example: deploy.edp.epam.com/previous-stage-name: 'dev' . Execute script to align CDPipeline resources to the new API ( jq command-line JSON processor is required): pipelines = $ ( kubectl get cdpipelines - n < edp - namespace > - ojson | jq - c ' .items[] ' ) for p in $ pipelines ; do echo \" $p \" | \\ jq ' . | .spec.inputDockerStreams = .spec.input_docker_streams | del(.spec.input_docker_streams) | .spec += { \"deploymentType\": \"container\" } ' | \\ kubectl apply - f - done Update the database in the edp-db pod in the edp-namespace: Log in to the pod: kubectl exec - i - t - n < edp - namespace > edp - db -< pod > - c edp - db \" -- \" sh - c \" (bash || ash || sh) \" Log in to the Postgress DB (where \"admin\" is the user the secret was created for): psql edp-db <admin>; SET search_path to '<edp-namespace>'; UPDATE cd_pipeline SET deployment_type = 'container'; Add \"AUTODEPLOY\":\"true/false\",\"DEPLOYMENT_TYPE\":\"container\" to every Custom Resource in jenkinsjobs.v2.edp.epam.com : Edit Kubernetes resources: kubectl get jenkinsjobs.v2.edp.epam.com -n <edp-namespace> kubectl edit jenkinsjobs.v2.edp.epam.com <cd-pipeline-name> -n <edp-namespace> Alternatively, use this script to update all the necessary jenkinsjobs Custom Resources: edp_namespace =< epd_namespace > for stages in $ ( kubectl get jenkinsjobs - o = name - n $ed p_namespace ) ; do kubectl get $stages -n $edp_namespace -o yaml | grep -q \"container\" && echo -e \"\\n$stages is already updated\" || kubectl get $stages -n $edp_namespace -o yaml | sed 's/\"GIT_SERVER_CR_VERSION\"/\"AUTODEPLOY\":\"false\",\"DEPLOYMENT_TYPE\":\"container\",\"GIT_SERVER_CR_VERSION\"/g' | kubectl apply -f -; done Make sure the edited resource looks as follows: job : config : '{\"AUTODEPLOY\":\"false\",\"DEPLOYMENT_TYPE\":\"container\",\"GIT_SERVER_CR_VERSION\":\"v2\",\"PIPELINE_NAME\":\"your-pipeline-name\",\"QG_STAGES\":\"{\\\"name\\\":\\\"manual\\\",\\\"step_name\\\":\\\"your-step-name\\\"}\",\"SOURCE_TYPE\":\"default\",\"STAGE_NAME\":\"your-stage-name\"}' name : job - provisions /job/cd/job/ default Restart the Jenkins operator pod and wait until the CD job provisioner in Jenkins creates the updated pipelines.","title":"Upgrade EDP v.2.7.8 to v.2.8.4"},{"location":"operator-guide/upgrade-edp-2.7.8-to-2.8.4/#possible-issues","text":"SonarQube fails during the CI pipeline run. The previous builds of SonarQube used the latest version of the OpenID Connect Authentication for SonarQube plugin. Version 2.1.0 of this plugin may have issues with the connection, so it is necessary to downgrade it in order to get rid of errors in the pipeline. Take the following steps: Log in to the Sonar pod: kubectl exec - i - t - n < edp - namespace > sonar -< pod > - c sonar \" -- \" sh - c \" (bash || ash || sh) \" Run the command in the Sonar container: rm extensions/plugins/sonar-auth-oidc-plugin* Install the OpenID Connect Authentication for SonarQube plugin v2.0.0: curl - L https : // github . com / vaulttec / sonar - auth - oidc / releases / download / v2 . 0.0 / sonar - auth - oidc - plugin - 2.0 . 0.j ar -- output extensions / plugins / sonar - auth - oidc - plugin - 2.0 . 0.j ar Restart the SonarQube pod; The Helm lint checker in EDP 2.8.4 has some additional rules. There can be issues with it during the Code Review pipeline in Jenkins for applications that were transferred from previous EDP versions to EDP 2.8.4. To fix this, add the following annotation to the Chart.yaml file: Go to the Git repository -> Choose the application -> Edit the deploy-templates/Chart.yaml file. It is necessary to add the following lines to the bottom of the Chart.yaml file: home : https :// github . com / your - repo . git sources : - https :// github . com / your - repo . git maintainers : - name : DEV Team Add a new line character at the end of the last line. Please be aware it is important.","title":"Possible Issues"},{"location":"operator-guide/upgrade-edp-2.7.8-to-2.8.4/#related-articles","text":"Set Up Kiosk IAM Roles for Kaniko Service Accounts Manage Jenkins CI Pipeline Job Provisioner Manage Jenkins CD Pipeline Job Provisioner","title":"Related Articles"},{"location":"operator-guide/upgrade-edp-2.8.4-to-2.9.0/","text":"Upgrade EDP v.2.8.4 to v.2.9.0 \u2693\ufe0e This section provides the details on the EDP upgrade from the v.2.8.4 to the v.2.9.0. Explore the actions and requirements below. Note Kiosk is optional for EDP v.2.9.0 and higher, and enabled by default. To disable it, add the following parameter to the values.yaml file: kioskEnabled: false . Please refer to the Set Up Kiosk documentation for the details. With Amazon Elastic Container Registry to store the images, there are two options: Enable IRSA and create AWS IAM Role for Kaniko image builder. Please refer to the IAM Roles for Kaniko Service Accounts section for the details. The Amazon Elastic Container Registry Roles can be stored in an instance profile . Before updating EDP from v.2.8.4 to v.2.9.0, update the gerrit-is-credentials secret by adding the new clientSecret key with the value from gerrit-is-credentials.client_secret : kubectl edit secret gerrit-is-credentials -n <edp-namespace> Make sure it looks as follows (replace with the necessary key value): data : client_secret : example clientSecret : example Update Custom Resource Definitions. This command will apply all the necessary CRDs to the cluster: kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_gerritgroupmember_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_gerritgroup_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_gerritprojectaccess_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_gerritproject_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_jenkins_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_jenkinsagent_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_jenkinsauthorizationrolemapping_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_jenkinsauthorizationrole_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.9/deploy-templates/crds/v1_v1alpha1_keycloakclientscope_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.9/deploy-templates/crds/v1_v1alpha1_keycloakrealmuser_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-nexus-operator/release/2.9/deploy-templates/crds/edp_v1alpha1_nexus_crd.yaml With Amazon Elastic Container Registry to store and Kaniko to build the images, add the kanikoRoleArn parameter to the values before starting the update process. This parameter is indicated in AWS Roles once IRSA is enabled and AWS IAM Role is created for Kaniko.The value should look as follows: kanikoRoleArn : arn : aws : iam ::< AWS_ACCOUNT_ID >: role / AWSIRSA \u2039 CLUSTER_NAME \u203a\u2039 EDP_NAMESPACE \u203a Kaniko To upgrade EDP to the v.2.9.0, run the following command: helm upgrade --install edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.9.0 Note To verify the installation, it is possible to test the deployment before applying it to the cluster with: helm upgrade --install edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.9.0 --dry-run Remove the following Kubernetes resources left from the previous EDP installation (it is optional): kubectl delete rolebinding edp-cd-pipeline-operator-<edp-namespace>-admin -n <edp-namespace> After EDP update, please restart the 'sonar-operator' pod to address the proper Sonar plugin versioning. After 'sonar-operator' is restarted, check the list of installed plugins in the corresponding SonarQube menu. Update Jenkins pipelines and stages to the new release tag: Restart the Jenkins pod In Jenkins, go to Manage Jenkins -> Configure system -> Find the Global Pipeline Libraries menu Make sure that the Default version for edp-library-stages is build/2.10.0-RC.1 Make sure that the Default version for edp-library-pipelines is build/2.10.0-RC.1 Update image versions for the Jenkins agents in the ConfigMap : kubectl edit configmap jenkins-slaves -n <edp-namespace> The versions of the images should be: epamedp / edp - jenkins - codenarc - agent : 1.0.1 epamedp / edp - jenkins - dotnet - 21 - agent : 1.0.3 epamedp / edp - jenkins - dotnet - 31 - agent : 1.0.3 epamedp / edp - jenkins - go - agent : 1.0.4 epamedp / edp - jenkins - gradle - java8 - agent : 1.0.3 epamedp / edp - jenkins - gradle - java11 - agent : 2.0.3 epamedp / edp - jenkins - helm - agent : 1.0.7 epamedp / edp - jenkins - maven - java8 - agent : 1.0.3 epamedp / edp - jenkins - maven - java11 - agent : 2.0.4 epamedp / edp - jenkins - npm - agent : 2.0.3 epamedp / edp - jenkins - opa - agent : 1.0.2 epamedp / edp - jenkins - python - 38 - agent : 2.0.4 epamedp / edp - jenkins - terraform - agent : 2.0.5 Restart the Jenkins pod. Update the compatible plugins in Jenkins: Go to Manage Jenkins -> Manage Plugins -> Select Compatible -> Click Download now and install after restart Related Articles \u2693\ufe0e Set Up Kiosk IAM Roles for Kaniko Service Accounts","title":"Upgrade EDP v.2.8.4 to v.2.9.0"},{"location":"operator-guide/upgrade-edp-2.8.4-to-2.9.0/#upgrade-edp-v284-to-v290","text":"This section provides the details on the EDP upgrade from the v.2.8.4 to the v.2.9.0. Explore the actions and requirements below. Note Kiosk is optional for EDP v.2.9.0 and higher, and enabled by default. To disable it, add the following parameter to the values.yaml file: kioskEnabled: false . Please refer to the Set Up Kiosk documentation for the details. With Amazon Elastic Container Registry to store the images, there are two options: Enable IRSA and create AWS IAM Role for Kaniko image builder. Please refer to the IAM Roles for Kaniko Service Accounts section for the details. The Amazon Elastic Container Registry Roles can be stored in an instance profile . Before updating EDP from v.2.8.4 to v.2.9.0, update the gerrit-is-credentials secret by adding the new clientSecret key with the value from gerrit-is-credentials.client_secret : kubectl edit secret gerrit-is-credentials -n <edp-namespace> Make sure it looks as follows (replace with the necessary key value): data : client_secret : example clientSecret : example Update Custom Resource Definitions. This command will apply all the necessary CRDs to the cluster: kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_gerritgroupmember_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_gerritgroup_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_gerritprojectaccess_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-gerrit-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_gerritproject_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_jenkins_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_jenkinsagent_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_jenkinsauthorizationrolemapping_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.9/deploy-templates/crds/v2_v1alpha1_jenkinsauthorizationrole_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.9/deploy-templates/crds/v1_v1alpha1_keycloakclientscope_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.9/deploy-templates/crds/v1_v1alpha1_keycloakrealmuser_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-nexus-operator/release/2.9/deploy-templates/crds/edp_v1alpha1_nexus_crd.yaml With Amazon Elastic Container Registry to store and Kaniko to build the images, add the kanikoRoleArn parameter to the values before starting the update process. This parameter is indicated in AWS Roles once IRSA is enabled and AWS IAM Role is created for Kaniko.The value should look as follows: kanikoRoleArn : arn : aws : iam ::< AWS_ACCOUNT_ID >: role / AWSIRSA \u2039 CLUSTER_NAME \u203a\u2039 EDP_NAMESPACE \u203a Kaniko To upgrade EDP to the v.2.9.0, run the following command: helm upgrade --install edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.9.0 Note To verify the installation, it is possible to test the deployment before applying it to the cluster with: helm upgrade --install edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.9.0 --dry-run Remove the following Kubernetes resources left from the previous EDP installation (it is optional): kubectl delete rolebinding edp-cd-pipeline-operator-<edp-namespace>-admin -n <edp-namespace> After EDP update, please restart the 'sonar-operator' pod to address the proper Sonar plugin versioning. After 'sonar-operator' is restarted, check the list of installed plugins in the corresponding SonarQube menu. Update Jenkins pipelines and stages to the new release tag: Restart the Jenkins pod In Jenkins, go to Manage Jenkins -> Configure system -> Find the Global Pipeline Libraries menu Make sure that the Default version for edp-library-stages is build/2.10.0-RC.1 Make sure that the Default version for edp-library-pipelines is build/2.10.0-RC.1 Update image versions for the Jenkins agents in the ConfigMap : kubectl edit configmap jenkins-slaves -n <edp-namespace> The versions of the images should be: epamedp / edp - jenkins - codenarc - agent : 1.0.1 epamedp / edp - jenkins - dotnet - 21 - agent : 1.0.3 epamedp / edp - jenkins - dotnet - 31 - agent : 1.0.3 epamedp / edp - jenkins - go - agent : 1.0.4 epamedp / edp - jenkins - gradle - java8 - agent : 1.0.3 epamedp / edp - jenkins - gradle - java11 - agent : 2.0.3 epamedp / edp - jenkins - helm - agent : 1.0.7 epamedp / edp - jenkins - maven - java8 - agent : 1.0.3 epamedp / edp - jenkins - maven - java11 - agent : 2.0.4 epamedp / edp - jenkins - npm - agent : 2.0.3 epamedp / edp - jenkins - opa - agent : 1.0.2 epamedp / edp - jenkins - python - 38 - agent : 2.0.4 epamedp / edp - jenkins - terraform - agent : 2.0.5 Restart the Jenkins pod. Update the compatible plugins in Jenkins: Go to Manage Jenkins -> Manage Plugins -> Select Compatible -> Click Download now and install after restart","title":"Upgrade EDP v.2.8.4 to v.2.9.0"},{"location":"operator-guide/upgrade-edp-2.8.4-to-2.9.0/#related-articles","text":"Set Up Kiosk IAM Roles for Kaniko Service Accounts","title":"Related Articles"},{"location":"operator-guide/upgrade-edp-2.9.0-to-2.10.2/","text":"Upgrade EDP v.2.9.0 to v.2.10.2 \u2693\ufe0e This section provides the details on the EDP upgrade from the v.2.9.0 to the v.2.10.2. Explore the actions and requirements below. Note Kiosk is optional for EDP v.2.9.0 and higher, and is enabled by default. To disable it, add the following parameter to the values.yaml file: global.kioskEnabled: false . Please refer to the Set Up Kiosk documentation for the details. Note In the process of updating the EDP, it is necessary to migrate the database for SonarQube, before performing the update procedure, please carefully read section 4 of this guide. Before updating EDP from v.2.9.0 to v.2.10.2, delete SonarQube plugins by executing the following command in SonarQube pod: rm -r /opt/sonarqube/extensions/plugins/* Update Custom Resource Definitions. Run the following command to apply all the necessary CRDs to the cluster: kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.10/deploy-templates/crds/v2_v1alpha1_jenkins_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.10/deploy-templates/crds/v1_v1alpha1_keycloakclient_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.10/deploy-templates/crds/v1_v1alpha1_keycloakrealmcomponent_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.10/deploy-templates/crds/v1_v1alpha1_keycloakrealmidentityprovider_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.10/deploy-templates/crds/v1_v1alpha1_keycloakrealmrole_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.10/deploy-templates/crds/v1_v1alpha1_keycloakrealmuser_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.10/deploy-templates/crds/v1_v1alpha1_keycloak_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-nexus-operator/release/2.10/deploy-templates/crds/edp_v1alpha1_nexus_crd.yaml To upgrade EDP to the v.2.10.2, run the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.10.2 Note To verify the installation, it is possible to test the deployment before applying it to the cluster with: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.10.2 --dry-run Migrate the database for SonarQube according to the official documentation . Note Please be aware of possible tables duplication for speeding up the migration process during the upgrade. Due to the duplication, the database disk usage can be temporarily increased to twice as the normal usage. Therefore, the recommended database disk usage is below 50% before the migration start. Navigate to the project http://SonarQubeServerURL/setup link and follow the setup instructions: Migrate SonarQube database Click the Upgrade button and wait for the end of the migration process. Remove the resources related to the deprecated Sonar Gerrit Plugin that is deleted in EDP 2.10.2: Remove Sonar Gerrit Plugin from Jenkins (go to Manage Jenkins -> Manage Plugins -> Installed -> Uninstall Sonar Gerrit Plugin ). In Gerrit, clone the All-Projects repository. Edit the project.config file in the All-Projects repository and remove the Sonar-Verified label declaration: [label \"Sonar-Verified\"] function = MaxWithBlock value = -1 Issues found value = 0 No score value = +1 Verified defaultValue = 0 Also, remove the following permissions for the Sonar-Verified label in the project.config file: label-Sonar-Verified = -1..+1 group Administrators label-Sonar-Verified = -1..+1 group Project Owners label-Sonar-Verified = -1..+1 group Service Users Save the changes, and commit and push the repository to HEAD:refs/meta/config bypassing the Gerrit code review: git push origin HEAD:refs/meta/config Update image versions for the Jenkins agents in the ConfigMap : kubectl edit configmap jenkins-slaves -n <edp-namespace> The versions of the images should be: epamedp / edp - jenkins - codenarc - agent : 1.0.1 epamedp / edp - jenkins - dotnet - 21 - agent : 1.0.5 epamedp / edp - jenkins - dotnet - 31 - agent : 1.0.4 epamedp / edp - jenkins - go - agent : 1.0.6 epamedp / edp - jenkins - gradle - java8 - agent : 1.0.3 epamedp / edp - jenkins - gradle - java11 - agent : 2.0.3 epamedp / edp - jenkins - helm - agent : 1.0.10 epamedp / edp - jenkins - maven - java8 - agent : 1.0.3 epamedp / edp - jenkins - maven - java11 - agent : 2.0.4 epamedp / edp - jenkins - npm - agent : 2.0.3 epamedp / edp - jenkins - opa - agent : 1.0.2 epamedp / edp - jenkins - python - 38 - agent : 2.0.4 epamedp / edp - jenkins - terraform - agent : 2.0.5 Restart the Jenkins pod. Since EDP version v.2.10.x, the create-release.groovy, code-review.groovy, and build.groovy files are deprecated ( pipeline script from SCM is replaced with pipeline script , see below). Pipeline script from SCM: Pipeline script from scm example Pipeline script: Pipeline script example Update the job-provisioner code and restart the codebase-operator pod. Consult the default job-provisioners code section. Related Articles \u2693\ufe0e Manage Jenkins CI Pipeline Job Provisioner Set Up Kiosk SonarQube Upgrade Guide","title":"Upgrade EDP v.2.9.0 to v.2.10.2"},{"location":"operator-guide/upgrade-edp-2.9.0-to-2.10.2/#upgrade-edp-v290-to-v2102","text":"This section provides the details on the EDP upgrade from the v.2.9.0 to the v.2.10.2. Explore the actions and requirements below. Note Kiosk is optional for EDP v.2.9.0 and higher, and is enabled by default. To disable it, add the following parameter to the values.yaml file: global.kioskEnabled: false . Please refer to the Set Up Kiosk documentation for the details. Note In the process of updating the EDP, it is necessary to migrate the database for SonarQube, before performing the update procedure, please carefully read section 4 of this guide. Before updating EDP from v.2.9.0 to v.2.10.2, delete SonarQube plugins by executing the following command in SonarQube pod: rm -r /opt/sonarqube/extensions/plugins/* Update Custom Resource Definitions. Run the following command to apply all the necessary CRDs to the cluster: kubectl apply -f https://raw.githubusercontent.com/epam/edp-jenkins-operator/release/2.10/deploy-templates/crds/v2_v1alpha1_jenkins_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.10/deploy-templates/crds/v1_v1alpha1_keycloakclient_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.10/deploy-templates/crds/v1_v1alpha1_keycloakrealmcomponent_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.10/deploy-templates/crds/v1_v1alpha1_keycloakrealmidentityprovider_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.10/deploy-templates/crds/v1_v1alpha1_keycloakrealmrole_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.10/deploy-templates/crds/v1_v1alpha1_keycloakrealmuser_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-keycloak-operator/release/1.10/deploy-templates/crds/v1_v1alpha1_keycloak_crd.yaml kubectl apply -f https://raw.githubusercontent.com/epam/edp-nexus-operator/release/2.10/deploy-templates/crds/edp_v1alpha1_nexus_crd.yaml To upgrade EDP to the v.2.10.2, run the following command: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.10.2 Note To verify the installation, it is possible to test the deployment before applying it to the cluster with: helm upgrade edp epamedp/edp-install -n <edp-namespace> --values values.yaml --version=2.10.2 --dry-run Migrate the database for SonarQube according to the official documentation . Note Please be aware of possible tables duplication for speeding up the migration process during the upgrade. Due to the duplication, the database disk usage can be temporarily increased to twice as the normal usage. Therefore, the recommended database disk usage is below 50% before the migration start. Navigate to the project http://SonarQubeServerURL/setup link and follow the setup instructions: Migrate SonarQube database Click the Upgrade button and wait for the end of the migration process. Remove the resources related to the deprecated Sonar Gerrit Plugin that is deleted in EDP 2.10.2: Remove Sonar Gerrit Plugin from Jenkins (go to Manage Jenkins -> Manage Plugins -> Installed -> Uninstall Sonar Gerrit Plugin ). In Gerrit, clone the All-Projects repository. Edit the project.config file in the All-Projects repository and remove the Sonar-Verified label declaration: [label \"Sonar-Verified\"] function = MaxWithBlock value = -1 Issues found value = 0 No score value = +1 Verified defaultValue = 0 Also, remove the following permissions for the Sonar-Verified label in the project.config file: label-Sonar-Verified = -1..+1 group Administrators label-Sonar-Verified = -1..+1 group Project Owners label-Sonar-Verified = -1..+1 group Service Users Save the changes, and commit and push the repository to HEAD:refs/meta/config bypassing the Gerrit code review: git push origin HEAD:refs/meta/config Update image versions for the Jenkins agents in the ConfigMap : kubectl edit configmap jenkins-slaves -n <edp-namespace> The versions of the images should be: epamedp / edp - jenkins - codenarc - agent : 1.0.1 epamedp / edp - jenkins - dotnet - 21 - agent : 1.0.5 epamedp / edp - jenkins - dotnet - 31 - agent : 1.0.4 epamedp / edp - jenkins - go - agent : 1.0.6 epamedp / edp - jenkins - gradle - java8 - agent : 1.0.3 epamedp / edp - jenkins - gradle - java11 - agent : 2.0.3 epamedp / edp - jenkins - helm - agent : 1.0.10 epamedp / edp - jenkins - maven - java8 - agent : 1.0.3 epamedp / edp - jenkins - maven - java11 - agent : 2.0.4 epamedp / edp - jenkins - npm - agent : 2.0.3 epamedp / edp - jenkins - opa - agent : 1.0.2 epamedp / edp - jenkins - python - 38 - agent : 2.0.4 epamedp / edp - jenkins - terraform - agent : 2.0.5 Restart the Jenkins pod. Since EDP version v.2.10.x, the create-release.groovy, code-review.groovy, and build.groovy files are deprecated ( pipeline script from SCM is replaced with pipeline script , see below). Pipeline script from SCM: Pipeline script from scm example Pipeline script: Pipeline script example Update the job-provisioner code and restart the codebase-operator pod. Consult the default job-provisioners code section.","title":"Upgrade EDP v.2.9.0 to v.2.10.2"},{"location":"operator-guide/upgrade-edp-2.9.0-to-2.10.2/#related-articles","text":"Manage Jenkins CI Pipeline Job Provisioner Set Up Kiosk SonarQube Upgrade Guide","title":"Related Articles"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/","text":"Upgrade Keycloak v.17.0.x-Legacy to v.19.0.x \u2693\ufe0e Starting from Keycloak v.18.x.x, the Keycloak server has been moved from the Wildfly (JBoss) Application Server to Quarkus framework and is called Keycloak.X . There are two ways to upgrade Keycloak v.17.0.x-legacy to v.19.0.x on Kubernetes, please perform the steps described in the Prerequisites section of this tutorial, and then select a suitable upgrade strategy for your environment: Upgrade Postgres database to a minor release v.11.17 Migrate Postgres database from Postgres v.11.x to v.14.5 Prerequisites \u2693\ufe0e Before upgrading Keycloak, please perform the steps below: Create a backup/snapshot of the Keycloak database volume. Locate the AWS volumeID and then create its snapshot on AWS: Find the PVC name attached to the Postgres pod. It can be similar to data-keycloak-postgresql-0 if the Postgres StatefulSet name is keycloak-postgresql : kubectl get pods keycloak-postgresql-0 -n security -o jsonpath = '{.spec.volumes[*].persistentVolumeClaim.claimName}{\"\\n\"}' Locate the PV volumeName in the data-keycloak-postgresql-0 Persistent Volume Claim: kubectl get pvc data-keycloak-postgresql-0 -n security -o jsonpath = '{.spec.volumeName}{\"\\n\"}' Get volumeID in the Persistent Volume: kubectl get pv ${ pv_name } -n security -o jsonpath = '{.spec.awsElasticBlockStore.volumeID}{\"\\n\"}' Add two additional keys: password and postgres-password , to the keycloak-postgresql secret in the Keycloak namespace. Note The password key must have the same value as the postgresql-password key. The postgres-password key must have the same value as the postgresql-postgres-password key. The latest chart for Keycloak.X does not have an option to override Postgres password and admin password keys in the secret, and it uses the Postgres defaults , therefore, a new secret scheme must be implemented: kubectl -n security edit secret keycloak-postgresql data : postgresql-password : XXXXXX postgresql-postgres-password : YYYYYY password : XXXXXX postgres-password : YYYYYY Save Keycloak StatefulSet names, for example, keycloak and keycloak-postgresql . These names will be used in the new Helm deployments: $ kubectl get statefulset -n security NAME READY AGE keycloak 1 /1 18h keycloak-postgresql 1 /1 18h Upgrade Postgres Database to a Minor Release v.11.17 \u2693\ufe0e To upgrade Keycloak by upgrading Postgres Database to a minor release v.11.17, perform the steps described in the Prerequisites section of this tutorial, and then perform the following steps: Delete Keycloak Resources \u2693\ufe0e Delete Keycloak and Prostgres StatefulSets : kubectl delete statefulset keycloak keycloak-postgresql -n security Delete the Keycloak Ingress object, to prevent hostname duplication issues: kubectl delete ingress keycloak -n security Upgrade Keycloak \u2693\ufe0e Make sure the Keycloak chart repository is added: helm repo add codecentric https://codecentric.github.io/helm-charts helm repo update Create values for Keycloak: Note Since the Keycloak.X release, Keycloak and Postgres database charts are separated. Upgrade Keycloak, and then install the Postgres database. Note nameOverride: \"keycloak\" sets the name of the Keycloak pod. It must be the same Keycloak name as in the previous StatefulSet . Change Ingress host name to the Keycloak host name. hostname: keycloak-postgresql is the hostname of the pod with the Postgres database that is the same as Postgres StatefulSet name, for example, keycloak-postgresql . \"/opt/keycloak/bin/kc.sh start --auto-build\" was used in the legacy Keycloak version. However, it is no longer required in the new Keycloak version since it is deprecated and used by default. Optionally, use the following command for applying the old Keycloak theme: bin/kc.sh start --features-disabled = admin2 View: keycloak-values.yaml nameOverride : \"keycloak\" replicas : 1 # Deploy the latest verion image : tag : \"19.0.1\" # start: create OpenShift realm which is required by EDP extraInitContainers : | - name: realm-provider image: busybox imagePullPolicy: IfNotPresent command: - sh args: - -c - | echo '{\"realm\": \"openshift\",\"enabled\": true}' > /opt/keycloak/data/import/openshift.json volumeMounts: - name: realm mountPath: /opt/keycloak/data/import extraVolumeMounts : | - name: realm mountPath: /opt/keycloak/data/import extraVolumes : | - name: realm emptyDir: {} command : - \"/opt/keycloak/bin/kc.sh\" - \"--verbose\" - \"start\" - \"--http-enabled=true\" - \"--http-port=8080\" - \"--hostname-strict=false\" - \"--hostname-strict-https=false\" - \"--spi-events-listener-jboss-logging-success-level=info\" - \"--spi-events-listener-jboss-logging-error-level=warn\" - \"--import-realm\" extraEnv : | - name: KC_PROXY value: \"passthrough\" - name: KEYCLOAK_ADMIN valueFrom: secretKeyRef: name: keycloak-admin-creds key: username - name: KEYCLOAK_ADMIN_PASSWORD valueFrom: secretKeyRef: name: keycloak-admin-creds key: password - name: JAVA_OPTS_APPEND value: >- -XX:+UseContainerSupport -XX:MaxRAMPercentage=50.0 -Djava.awt.headless=true -Djgroups.dns.query={{ include \"keycloak.fullname\" . }}-headless # This block should be uncommented if you install Keycloak on Kubernetes ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/affinity : cookie rules : - host : keycloak.<ROOT_DOMAIN> paths : - path : '{{ tpl .Values.http.relativePath $ | trimSuffix \"/\" }}/' pathType : Prefix # This block should be uncommented if you set Keycloak to OpenShift and change the host field # route: # enabled: false # # Path for the Route # path: '/' # # Host name for the Route # host: \"keycloak.<ROOT_DOMAIN>\" # # TLS configuration # tls: # enabled: true resources : limits : memory : \"2048Mi\" requests : cpu : \"50m\" memory : \"512Mi\" # Check database readiness at startup dbchecker : enabled : true database : vendor : postgres existingSecret : keycloak-postgresql hostname : keycloak-postgresql port : 5432 username : admin database : keycloak Upgrade the Keycloak Helm chart: Note The Helm chart is substituted with the new Keyacloak.X instance. Change the namespace and the values file name if required. helm upgrade keycloak codecentric/keycloakx --version 1 .6.0 --values keycloak-values.yaml -n security Note If there are error messages when upgrading via Helm, make sure that StatefulSets are removed. If they are removed and the error still persists, try to add the --force flag to the Helm command: helm upgrade keycloak codecentric/keycloakx --version 1 .6.0 --values keycloak-values.yaml -n security --force Install Postgres \u2693\ufe0e Add Bitnami chart repository and update Helm repos: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update Create values for Postgres: Note Postgres v.11 and Postgres v.14.5 are not compatible. Postgres image will be upgraded to a minor release v.11.17. fullnameOverride: \"keycloak-postgresql\" sets the name of the Postgres StatefulSet. It must be the same as in the previous StatefulSet . View: postgres-values.yaml fullnameOverride : \"keycloak-postgresql\" # PostgreSQL read only replica parameters readReplicas : # Number of PostgreSQL read only replicas replicaCount : 1 global : postgresql : auth : username : admin existingSecret : keycloak-postgresql secretKeys : adminPasswordKey : postgres-password userPasswordKey : password database : keycloak image : registry : docker.io repository : bitnami/postgresql tag : 11.17.0-debian-11-r3 auth : existingSecret : keycloak-postgresql secretKeys : adminPasswordKey : postgres-password userPasswordKey : password primary : persistence : enabled : true size : 3Gi # If the StorageClass with reclaimPolicy: Retain is used, install an additional StorageClass before installing PostgreSQL # (the code is given below). # If the default StorageClass will be used - change \"gp2-retain\" to \"gp2\" storageClass : \"gp2-retain\" Install the Postgres database chart: Note Change the namespace and the values file name if required. helm install postgresql bitnami/postgresql \\ --version 11 .7.6 \\ --values postgres-values.yaml \\ --namespace security Log in to Keycloak and check that everything works as expected. Clean and Analyze Database \u2693\ufe0e Optionally, run the vacuumdb application on the database, to recover space occupied by \"dead tuples\" in the tables, analyze the contents of database tables, and collect statistics for PostgreSQL query engine to improve performance: PGPASSWORD = \" ${ postgresql_postgres -password } \" vacuumdb --analyze --verbose -d keycloak -U postgres For all databases, run the following command: PGPASSWORD = \" ${ postgresql_postgres -password } \" vacuumdb --analyze --verbose --all -U postgres Migrate Postgres Database From Postgres v.11.x to v.14.5 \u2693\ufe0e Info There is a Postgres database migration script at the end of this tutorial. Please read the section below before using the script. To upgrade Keycloak by migrating Postgres database from Postgres v.11.x to v.14.5, perform the steps described in the Prerequisites section of this tutorial, and then perform the following steps: Export Postgres Databases \u2693\ufe0e Log in to the current Keycloak Postgres pod and create a logical backup of all roles and databases using the pg_dumpall application. If there is no access to the Postgres Superuser, backup the Keycloak database with the pg_dump application: Note The secret key postgresql-postgres-password is for the postgres Superuser and postgresql-password is for admin user. The admin user is indicated by default in the Postgres Helm chart. The admin user may not have enough permissions to dump all Postgres databases and roles, so the preferred option for exporting all objects is using the pg_dumpall tool with the postgres Superuser. If the PGPASSWORD variable is not specified before using the pg_dumpall tool, you will be prompted to enter a password for each database during the export. If the -l keycloak parameter is specified, pg_dumpall will connect to the keycloak database for dumping global objects and discovering what other databases should be dumped. By default, pg_dumpall will try to connect to postgres or template1 databases. This parameter is optional. The pg_dumpall --clean option adds SQL commands to the dumped file for dropping databases before recreating them during import, as well as DROP commands for roles and tablespaces ( pg_dump also has this option). If the --clean parameter is specified, connect to the postgres database initially during import via psql . The psql script will attempt to drop other databases immediately, and that will fail for the database you are connected to. This flag is optional, and it is not included into this tutorial. PGPASSWORD = \" ${ postgresql_postgres -password } \" pg_dumpall -h localhost -p 5432 -U postgres -l keycloak > /tmp/keycloak_wildfly_db_dump.sql Note If there is no working password for the postgres Superuser, try the admin user using the pg_dump tool to export the keycloak database without global roles: PGPASSWORD = \" ${ postgresql_password } \" pg_dump -h localhost -p 5432 -U admin -d keycloak > /tmp/keycloak_wildfly_db_dump.sql Info Double-check that the contents of the dumped file is not empty. It usually contains more than 4000 lines. Copy the file with the database dump to a local machine. Since tar may not be present in the pod and kubectl cp will not work without tar , use the following command: kubectl exec -n security ${ postgresql_pod } -- cat /tmp/keycloak_wildfly_db_dump.sql > keycloak_wildfly_db_dump.sql Note Please find below the alternative commands for exporting the database to the local machine without copying the file to a pod for Postgres and admin users: kubectl exec -n security ${ postgresql_pod } \"--\" sh -c \"PGPASSWORD='\" ${ postgresql_postgres -password } \"' pg_dumpall -h localhost -p 5432 -U postgres\" > keycloak_wildfly_db_dump.sql kubectl exec -n security ${ postgresql_pod } \"--\" sh -c \"PGPASSWORD='\" ${ postgresql_password } \"' pg_dump -h localhost -p 5432 -U admin -d keycloak\" > keycloak_wildfly_db_dump.sql Delete the dumped file from the pod for security reasons: kubectl exec -n security ${ postgresql_pod } \"--\" sh -c \"rm /tmp/keycloak_wildfly_db_dump.sql\" Delete Keycloak Resources \u2693\ufe0e Delete all previous Keycloak resources along with the Postgres database and keycloak StatefulSets , Ingress , and custom resources via Helm, or via the tool used for their deployment. helm list -n security helm delete keycloak -n security Warning Don't delete the whole namespace. Keep the keycloak-postgresql and keycloak-admin-creds secrets. Delete the volume in AWS, from which a snapshot has been created. Then delete the PVC: kubectl delete pvc data-keycloak-postgresql-0 -n security Install Postgres \u2693\ufe0e Add Bitnami chart repository and update Helm repos: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update Create Postgres values: Note fullnameOverride: \"keycloak-postgresql\" sets the name of the Postgres StatefulSet. It must be same as in the previous StatefulSet . View: postgres-values.yaml nameOverride : \"keycloak-postgresql\" # PostgreSQL read only replica parameters readReplicas : # Number of PostgreSQL read only replicas replicaCount : 1 global : postgresql : auth : username : admin existingSecret : keycloak-postgresql secretKeys : adminPasswordKey : postgres-password userPasswordKey : password database : keycloak auth : existingSecret : keycloak-postgresql secretKeys : adminPasswordKey : postgres-password userPasswordKey : password primary : persistence : enabled : true size : 3Gi # If the StorageClass with reclaimPolicy: Retain is used, install an additional StorageClass before installing PostgreSQL # (the code is given below). # If the default StorageClass will be used - change \"gp2-retain\" to \"gp2\" storageClass : \"gp2-retain\" Install the Postgres database: Note Change the namespace and the values file name if required. helm install postgresql bitnami/postgresql \\ --version 11 .7.6 \\ --values postgres-values.yaml \\ --namespace security Wait for the database to be ready. Import Postgres Databases \u2693\ufe0e Upload the database dump to the new Keycloak Postgres pod: cat keycloak_wildfly_db_dump.sql | kubectl exec -i -n security ${ postgresql_pod } \"--\" sh -c \"cat > /tmp/keycloak_wildfly_db_dump.sql\" Warning Database import must be done before deploying Keycloak, because Keycloak will write its own data to the database during the start, and the import will partially fail. If that happened, scale down the keycloak StatefulSet , and try to drop the Keycloak database in the Postgres pod: dropdb -i -e keycloak -p 5432 -h localhost -U postgres If there still are some conflicting objects like roles, drop them via the DROP ROLE command. If the previous steps do not help, downscale the Keycloak and Postgres StatefulSets and delete the attached PVC (save the volumeID before removing), and delete the volume on AWS if using gp2-retain . In case of using gp2 , the volume will be deleted automatically after removing PVC. After that, redeploy the Postgres database, so that the new PVC is automatically created. Import the SQL dump file to the Postgres database cluster: Info Since the databases were exported in the sql format, the psql tool will be used to restore (reload) them. pg_restore does not support this plain-text format. If the entire Postgres database cluster was migrated with the postgres Superuser using pg_dumpall , use the import command without indicating the database: psql -U postgres -f /tmp/keycloak_wildfly_db_dump.sql If the database was migrated with the admin user using pg_dump , the postgres Superuser still can be used to restore it, but, in this case, a database must be indicated: Warning If the database name was not indicated during the import for the file dumped with pg_dump , the psql tool will import this database to a default Postgres database called postgres . psql -U postgres -d keycloak -f /tmp/keycloak_wildfly_db_dump.sql If the postgres Superuser is not accessible in the Postgres pod, run the command under the admin or any other user that has the database permissions. In this case, indicate the database as well: psql -U admin -d keycloak -f /tmp/keycloak_wildfly_db_dump.sql After a successful import, delete the dump file from the pod for security reasons: kubectl exec -n security ${ postgresql_pod } \"--\" sh -c \"rm /tmp/keycloak_wildfly_db_dump.sql\" Note Please find below the alternative commands for importing the database from the local machine to the pod without storing the backup on a pod for postgres or admin users: cat \"keycloak_wildfly_db_dump.sql\" | kubectl exec -i -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" sh -c \"cat | PGPASSWORD='\" ${ postgresql_superuser_password } \"' psql -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \"\" cat \"keycloak_wildfly_db_dump.sql\" | kubectl exec -i -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" sh -c \"cat | PGPASSWORD='\" ${ postgresql_superuser_password } \"' psql -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \" -d \" ${ database_name } \"\" cat \"keycloak_wildfly_db_dump.sql\" | kubectl exec -i -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" sh -c \"cat | PGPASSWORD='\" ${ postgresql_admin_password } \"' psql -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \" -d \" ${ database_name } \"\" Install Keycloak \u2693\ufe0e Make sure the Keycloak chart repository is added: helm repo add codecentric https://codecentric.github.io/helm-charts helm repo update Create Keycloak values: Note nameOverride: \"keycloak\" sets the name of the Keycloak pod. It must be the same Keycloak name as in the previous StatefulSet . Change Ingress host name to the Keycloak host name. hostname: keycloak-postgresql is the hostname of the pod with the Postgres database that is the same as Postgres StatefulSet name, for example, keycloak-postgresql . \"/opt/keycloak/bin/kc.sh start --auto-build\" was used in the legacy Keycloak version. However, it is no longer required in the new Keycloak version since it is deprecated and used by default. Optionally, use the following command for applying the old Keycloak theme: bin/kc.sh start --features-disabled = admin2 Info Automatic database migration will start after the Keycloak installation. View: keycloak-values.yaml nameOverride : \"keycloak\" replicas : 1 # Deploy the latest verion image : tag : \"19.0.1\" # start: create OpenShift realm which is required by EDP extraInitContainers : | - name: realm-provider image: busybox imagePullPolicy: IfNotPresent command: - sh args: - -c - | echo '{\"realm\": \"openshift\",\"enabled\": true}' > /opt/keycloak/data/import/openshift.json volumeMounts: - name: realm mountPath: /opt/keycloak/data/import extraVolumeMounts : | - name: realm mountPath: /opt/keycloak/data/import extraVolumes : | - name: realm emptyDir: {} command : - \"/opt/keycloak/bin/kc.sh\" - \"--verbose\" - \"start\" - \"--http-enabled=true\" - \"--http-port=8080\" - \"--hostname-strict=false\" - \"--hostname-strict-https=false\" - \"--spi-events-listener-jboss-logging-success-level=info\" - \"--spi-events-listener-jboss-logging-error-level=warn\" - \"--import-realm\" extraEnv : | - name: KC_PROXY value: \"passthrough\" - name: KEYCLOAK_ADMIN valueFrom: secretKeyRef: name: keycloak-admin-creds key: username - name: KEYCLOAK_ADMIN_PASSWORD valueFrom: secretKeyRef: name: keycloak-admin-creds key: password - name: JAVA_OPTS_APPEND value: >- -XX:+UseContainerSupport -XX:MaxRAMPercentage=50.0 -Djava.awt.headless=true -Djgroups.dns.query={{ include \"keycloak.fullname\" . }}-headless # This block should be uncommented if you install Keycloak on Kubernetes ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/affinity : cookie rules : - host : keycloak.<ROOT_DOMAIN> paths : - path : '{{ tpl .Values.http.relativePath $ | trimSuffix \"/\" }}/' pathType : Prefix # This block should be uncommented if you set Keycloak to OpenShift and change the host field # route: # enabled: false # # Path for the Route # path: '/' # # Host name for the Route # host: \"keycloak.<ROOT_DOMAIN>\" # # TLS configuration # tls: # enabled: true resources : limits : memory : \"2048Mi\" requests : cpu : \"50m\" memory : \"512Mi\" # Check database readiness at startup dbchecker : enabled : true database : vendor : postgres existingSecret : keycloak-postgresql hostname : keycloak-postgresql port : 5432 username : admin database : keycloak Deploy Keycloak: Note Change the namespace and the values file name if required. helm install keycloak codecentric/keycloakx --version 1 .6.0 --values keycloak-values.yaml -n security Log in to Keycloak and check if everything has been imported correctly. Clean and Analyze Database \u2693\ufe0e Optionally, run the vacuumdb application on the database, to analyze the contents of database tables and collect statistics for the Postgres query optimizer: PGPASSWORD = \" ${ postgresql_postgres -password } \" vacuumdb --analyze --verbose -d keycloak -U postgres For all databases, run the following command: PGPASSWORD = \" ${ postgresql_postgres -password } \" vacuumdb --analyze --verbose --all -U postgres Postgres Database Migration Script \u2693\ufe0e Info Please read the Migrate Postgres Database From Postgres v.11.x to v.14.5 section of this tutorial before using the script. Note The kubectl tool is required for using this script. This script will likely work for any other Postgres database besides Keycloak after some adjustments. It queries the pg_dump , pg_dumpall , psql , and vacuumdb commands under the hood. The following script can be used for exporting and importing Postgres databases as well as optimizing them with the vacuumdb application. Please examine the code and make the adjustments if required. By default, the following command exports Keycloak Postgres databases from a Kubernetes pod to a local machine: ./script.sh After running the command, please follow the prompt. To import a database backup to a newly created Postgres Kubernetes pod, pass a database dump sql file to the script: ./script.sh path-to/db_dump.sql The -h flag prints help, and -c|-v runs the vacuumdb garbage collector and analyzer. View: keycloak_db_migration.sh #!/bin/bash # set -x db_migration_help (){ echo \"Keycloak Postgres database migration\" echo echo \"Usage:\" echo \"------------------------------------------\" echo \"Export Keycloak Postgres database from pod\" echo \"Run without parameters:\" echo \" $0 \" echo \"------------------------------------------\" echo \"Import Keycloak Postgres database to pod\" echo \"Pass filename to script:\" echo \" $0 path/to/db_dump.sql\" echo \"------------------------------------------\" echo \"Additional options: \" echo \" $0 [OPTIONS...]\" echo \"Options:\" echo \"h Print Help.\" echo \"c|v Run garbage collector and analyzer.\" } keycloak_ns (){ printf '%s\\n' 'Enter keycloak namespace: ' read -r keycloak_namespace if [ -z \" ${ keycloak_namespace } \" ] ; then echo \"Don't skip namespace\" exit 1 fi } postgres_pod (){ printf '%s\\n' 'Enter postgres pod name: ' read -r postgres_pod_name if [ -z \" ${ postgres_pod_name } \" ] ; then echo \"Don't skip pod name\" exit 1 fi } postgres_user (){ printf '%s\\n' 'Enter postgres username: ' printf '%s' \"Skip to use [postgres] superuser: \" read -r postgres_username if [ -z \" ${ postgres_username } \" ] ; then postgres_username = 'postgres' fi } pgdb_host_info (){ database_name = 'keycloak' db_host = 'localhost' db_port = '5432' } postgresql_admin_pass (){ postgresql_password = 'POSTGRES_PASSWORD' postgresql_admin_password = \" $( kubectl exec -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"printenv ${ postgresql_password } \" ) \" } postgresql_su_pass (){ postgresql_postgres_password = 'POSTGRES_POSTGRES_PASSWORD' postgresql_superuser_password = \" $( kubectl exec -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"printenv ${ postgresql_postgres_password } \" ) \" if [ -z \" ${ postgresql_superuser_password } \" ] ; then echo \"SuperUser password variable does not exist. Using user password instead...\" postgresql_admin_pass postgresql_superuser_password = \" ${ postgresql_admin_password } \" fi } keycloak_pgdb_export (){ current_cluster = \" $( kubectl config current-context ) \" exported_db_name = \"keycloak_db_dump_ ${ current_cluster } _ ${ keycloak_namespace } _ ${ postgres_username } _ $( date + \"%Y%m%d%H%M\" ) .sql\" if [ \" ${ postgres_username } \" == 'postgres' ] ; then # call a function to get a pass for postgres user postgresql_su_pass kubectl exec -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"PGPASSWORD='\" ${ postgresql_superuser_password } \"' pg_dumpall -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \"\" > \" ${ exported_db_name } \" else # call a function to get a pass for admin user postgresql_admin_pass kubectl exec -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"PGPASSWORD='\" ${ postgresql_admin_password } \"' pg_dump -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \" -d \" ${ database_name } \"\" > \" ${ exported_db_name } \" fi separate_lines = \"---------------\" if [ ! -s \" ${ exported_db_name } \" ] ; then rm -f \" ${ exported_db_name } \" echo \" ${ separate_lines } \" echo \"Something went wrong. The database dump file is empty and was not saved.\" else echo \" ${ separate_lines } \" grep 'Dumped' \" ${ exported_db_name } \" | sort -u echo \"Database has been exported to $( pwd ) / ${ exported_db_name } \" fi } keycloak_pgdb_import (){ echo \"Preparing Import\" echo \"----------------\" if [ ! -f \" $1 \" ] ; then echo \"The file $1 does not exist.\" exit 1 fi keycloak_ns postgres_pod postgres_user pgdb_host_info if [ \" ${ postgres_username } \" == 'postgres' ] ; then # restore full backup with all databases and roles as superuser or a single database postgresql_su_pass if [ -n \" $( cat \" $1 \" | grep 'CREATE ROLE' ) \" ] ; then cat \" $1 \" | kubectl exec -i -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"cat | PGPASSWORD='\" ${ postgresql_superuser_password } \"' psql -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \"\" else cat \" $1 \" | kubectl exec -i -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"cat | PGPASSWORD='\" ${ postgresql_superuser_password } \"' psql -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \" -d \" ${ database_name } \"\" fi else # restore a single database postgresql_admin_pass cat \" $1 \" | kubectl exec -i -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"cat | PGPASSWORD='\" ${ postgresql_admin_password } \"' psql -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \" -d \" ${ database_name } \"\" fi } vacuum_pgdb (){ echo \"Preparing garbage collector and analyzer\" echo \"----------------------------------------\" keycloak_ns postgres_pod postgres_user pgdb_host_info if [ \" ${ postgres_username } \" == 'postgres' ] ; then postgresql_su_pass kubectl exec -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"PGPASSWORD='\" ${ postgresql_superuser_password } \"' vacuumdb --analyze --all -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \"\" else postgresql_admin_pass kubectl exec -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"PGPASSWORD='\" ${ postgresql_admin_password } \"' vacuumdb --analyze -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \" -d \" ${ database_name } \"\" fi } while [ \" $# \" -eq 1 ] ; do case \" $1 \" in -h | --help ) db_migration_help exit 0 ;; -c | --clean | -v | --vacuum ) vacuum_pgdb exit 0 ;; -- ) break ;; -* ) echo \"Invalid option ' $1 '. Use -h|--help to see the valid options\" > & 2 exit 1 ;; * ) keycloak_pgdb_import \" $1 \" exit 0 ;; esac shift done if [ \" $# \" -gt 1 ] ; then echo \"Please pass a single file to the script\" exit 1 fi echo \"Preparing Export\" echo \"----------------\" keycloak_ns postgres_pod postgres_user pgdb_host_info keycloak_pgdb_export Related Articles \u2693\ufe0e Deploy OKD 4.10 Cluster","title":"Upgrade Keycloak v.17.0.x-legacy to v.19.0.x"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#upgrade-keycloak-v170x-legacy-to-v190x","text":"Starting from Keycloak v.18.x.x, the Keycloak server has been moved from the Wildfly (JBoss) Application Server to Quarkus framework and is called Keycloak.X . There are two ways to upgrade Keycloak v.17.0.x-legacy to v.19.0.x on Kubernetes, please perform the steps described in the Prerequisites section of this tutorial, and then select a suitable upgrade strategy for your environment: Upgrade Postgres database to a minor release v.11.17 Migrate Postgres database from Postgres v.11.x to v.14.5","title":"Upgrade Keycloak v.17.0.x-Legacy to v.19.0.x"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#prerequisites","text":"Before upgrading Keycloak, please perform the steps below: Create a backup/snapshot of the Keycloak database volume. Locate the AWS volumeID and then create its snapshot on AWS: Find the PVC name attached to the Postgres pod. It can be similar to data-keycloak-postgresql-0 if the Postgres StatefulSet name is keycloak-postgresql : kubectl get pods keycloak-postgresql-0 -n security -o jsonpath = '{.spec.volumes[*].persistentVolumeClaim.claimName}{\"\\n\"}' Locate the PV volumeName in the data-keycloak-postgresql-0 Persistent Volume Claim: kubectl get pvc data-keycloak-postgresql-0 -n security -o jsonpath = '{.spec.volumeName}{\"\\n\"}' Get volumeID in the Persistent Volume: kubectl get pv ${ pv_name } -n security -o jsonpath = '{.spec.awsElasticBlockStore.volumeID}{\"\\n\"}' Add two additional keys: password and postgres-password , to the keycloak-postgresql secret in the Keycloak namespace. Note The password key must have the same value as the postgresql-password key. The postgres-password key must have the same value as the postgresql-postgres-password key. The latest chart for Keycloak.X does not have an option to override Postgres password and admin password keys in the secret, and it uses the Postgres defaults , therefore, a new secret scheme must be implemented: kubectl -n security edit secret keycloak-postgresql data : postgresql-password : XXXXXX postgresql-postgres-password : YYYYYY password : XXXXXX postgres-password : YYYYYY Save Keycloak StatefulSet names, for example, keycloak and keycloak-postgresql . These names will be used in the new Helm deployments: $ kubectl get statefulset -n security NAME READY AGE keycloak 1 /1 18h keycloak-postgresql 1 /1 18h","title":"Prerequisites "},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#upgrade-postgres-database-to-a-minor-release-v1117","text":"To upgrade Keycloak by upgrading Postgres Database to a minor release v.11.17, perform the steps described in the Prerequisites section of this tutorial, and then perform the following steps:","title":"Upgrade Postgres Database to a Minor Release v.11.17 "},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#delete-keycloak-resources","text":"Delete Keycloak and Prostgres StatefulSets : kubectl delete statefulset keycloak keycloak-postgresql -n security Delete the Keycloak Ingress object, to prevent hostname duplication issues: kubectl delete ingress keycloak -n security","title":"Delete Keycloak Resources"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#upgrade-keycloak","text":"Make sure the Keycloak chart repository is added: helm repo add codecentric https://codecentric.github.io/helm-charts helm repo update Create values for Keycloak: Note Since the Keycloak.X release, Keycloak and Postgres database charts are separated. Upgrade Keycloak, and then install the Postgres database. Note nameOverride: \"keycloak\" sets the name of the Keycloak pod. It must be the same Keycloak name as in the previous StatefulSet . Change Ingress host name to the Keycloak host name. hostname: keycloak-postgresql is the hostname of the pod with the Postgres database that is the same as Postgres StatefulSet name, for example, keycloak-postgresql . \"/opt/keycloak/bin/kc.sh start --auto-build\" was used in the legacy Keycloak version. However, it is no longer required in the new Keycloak version since it is deprecated and used by default. Optionally, use the following command for applying the old Keycloak theme: bin/kc.sh start --features-disabled = admin2 View: keycloak-values.yaml nameOverride : \"keycloak\" replicas : 1 # Deploy the latest verion image : tag : \"19.0.1\" # start: create OpenShift realm which is required by EDP extraInitContainers : | - name: realm-provider image: busybox imagePullPolicy: IfNotPresent command: - sh args: - -c - | echo '{\"realm\": \"openshift\",\"enabled\": true}' > /opt/keycloak/data/import/openshift.json volumeMounts: - name: realm mountPath: /opt/keycloak/data/import extraVolumeMounts : | - name: realm mountPath: /opt/keycloak/data/import extraVolumes : | - name: realm emptyDir: {} command : - \"/opt/keycloak/bin/kc.sh\" - \"--verbose\" - \"start\" - \"--http-enabled=true\" - \"--http-port=8080\" - \"--hostname-strict=false\" - \"--hostname-strict-https=false\" - \"--spi-events-listener-jboss-logging-success-level=info\" - \"--spi-events-listener-jboss-logging-error-level=warn\" - \"--import-realm\" extraEnv : | - name: KC_PROXY value: \"passthrough\" - name: KEYCLOAK_ADMIN valueFrom: secretKeyRef: name: keycloak-admin-creds key: username - name: KEYCLOAK_ADMIN_PASSWORD valueFrom: secretKeyRef: name: keycloak-admin-creds key: password - name: JAVA_OPTS_APPEND value: >- -XX:+UseContainerSupport -XX:MaxRAMPercentage=50.0 -Djava.awt.headless=true -Djgroups.dns.query={{ include \"keycloak.fullname\" . }}-headless # This block should be uncommented if you install Keycloak on Kubernetes ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/affinity : cookie rules : - host : keycloak.<ROOT_DOMAIN> paths : - path : '{{ tpl .Values.http.relativePath $ | trimSuffix \"/\" }}/' pathType : Prefix # This block should be uncommented if you set Keycloak to OpenShift and change the host field # route: # enabled: false # # Path for the Route # path: '/' # # Host name for the Route # host: \"keycloak.<ROOT_DOMAIN>\" # # TLS configuration # tls: # enabled: true resources : limits : memory : \"2048Mi\" requests : cpu : \"50m\" memory : \"512Mi\" # Check database readiness at startup dbchecker : enabled : true database : vendor : postgres existingSecret : keycloak-postgresql hostname : keycloak-postgresql port : 5432 username : admin database : keycloak Upgrade the Keycloak Helm chart: Note The Helm chart is substituted with the new Keyacloak.X instance. Change the namespace and the values file name if required. helm upgrade keycloak codecentric/keycloakx --version 1 .6.0 --values keycloak-values.yaml -n security Note If there are error messages when upgrading via Helm, make sure that StatefulSets are removed. If they are removed and the error still persists, try to add the --force flag to the Helm command: helm upgrade keycloak codecentric/keycloakx --version 1 .6.0 --values keycloak-values.yaml -n security --force","title":"Upgrade Keycloak"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#install-postgres","text":"Add Bitnami chart repository and update Helm repos: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update Create values for Postgres: Note Postgres v.11 and Postgres v.14.5 are not compatible. Postgres image will be upgraded to a minor release v.11.17. fullnameOverride: \"keycloak-postgresql\" sets the name of the Postgres StatefulSet. It must be the same as in the previous StatefulSet . View: postgres-values.yaml fullnameOverride : \"keycloak-postgresql\" # PostgreSQL read only replica parameters readReplicas : # Number of PostgreSQL read only replicas replicaCount : 1 global : postgresql : auth : username : admin existingSecret : keycloak-postgresql secretKeys : adminPasswordKey : postgres-password userPasswordKey : password database : keycloak image : registry : docker.io repository : bitnami/postgresql tag : 11.17.0-debian-11-r3 auth : existingSecret : keycloak-postgresql secretKeys : adminPasswordKey : postgres-password userPasswordKey : password primary : persistence : enabled : true size : 3Gi # If the StorageClass with reclaimPolicy: Retain is used, install an additional StorageClass before installing PostgreSQL # (the code is given below). # If the default StorageClass will be used - change \"gp2-retain\" to \"gp2\" storageClass : \"gp2-retain\" Install the Postgres database chart: Note Change the namespace and the values file name if required. helm install postgresql bitnami/postgresql \\ --version 11 .7.6 \\ --values postgres-values.yaml \\ --namespace security Log in to Keycloak and check that everything works as expected.","title":"Install Postgres"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#clean-and-analyze-database","text":"Optionally, run the vacuumdb application on the database, to recover space occupied by \"dead tuples\" in the tables, analyze the contents of database tables, and collect statistics for PostgreSQL query engine to improve performance: PGPASSWORD = \" ${ postgresql_postgres -password } \" vacuumdb --analyze --verbose -d keycloak -U postgres For all databases, run the following command: PGPASSWORD = \" ${ postgresql_postgres -password } \" vacuumdb --analyze --verbose --all -U postgres","title":"Clean and Analyze Database"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#migrate-postgres-database-from-postgres-v11x-to-v145","text":"Info There is a Postgres database migration script at the end of this tutorial. Please read the section below before using the script. To upgrade Keycloak by migrating Postgres database from Postgres v.11.x to v.14.5, perform the steps described in the Prerequisites section of this tutorial, and then perform the following steps:","title":"Migrate Postgres Database From Postgres v.11.x to v.14.5 "},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#export-postgres-databases","text":"Log in to the current Keycloak Postgres pod and create a logical backup of all roles and databases using the pg_dumpall application. If there is no access to the Postgres Superuser, backup the Keycloak database with the pg_dump application: Note The secret key postgresql-postgres-password is for the postgres Superuser and postgresql-password is for admin user. The admin user is indicated by default in the Postgres Helm chart. The admin user may not have enough permissions to dump all Postgres databases and roles, so the preferred option for exporting all objects is using the pg_dumpall tool with the postgres Superuser. If the PGPASSWORD variable is not specified before using the pg_dumpall tool, you will be prompted to enter a password for each database during the export. If the -l keycloak parameter is specified, pg_dumpall will connect to the keycloak database for dumping global objects and discovering what other databases should be dumped. By default, pg_dumpall will try to connect to postgres or template1 databases. This parameter is optional. The pg_dumpall --clean option adds SQL commands to the dumped file for dropping databases before recreating them during import, as well as DROP commands for roles and tablespaces ( pg_dump also has this option). If the --clean parameter is specified, connect to the postgres database initially during import via psql . The psql script will attempt to drop other databases immediately, and that will fail for the database you are connected to. This flag is optional, and it is not included into this tutorial. PGPASSWORD = \" ${ postgresql_postgres -password } \" pg_dumpall -h localhost -p 5432 -U postgres -l keycloak > /tmp/keycloak_wildfly_db_dump.sql Note If there is no working password for the postgres Superuser, try the admin user using the pg_dump tool to export the keycloak database without global roles: PGPASSWORD = \" ${ postgresql_password } \" pg_dump -h localhost -p 5432 -U admin -d keycloak > /tmp/keycloak_wildfly_db_dump.sql Info Double-check that the contents of the dumped file is not empty. It usually contains more than 4000 lines. Copy the file with the database dump to a local machine. Since tar may not be present in the pod and kubectl cp will not work without tar , use the following command: kubectl exec -n security ${ postgresql_pod } -- cat /tmp/keycloak_wildfly_db_dump.sql > keycloak_wildfly_db_dump.sql Note Please find below the alternative commands for exporting the database to the local machine without copying the file to a pod for Postgres and admin users: kubectl exec -n security ${ postgresql_pod } \"--\" sh -c \"PGPASSWORD='\" ${ postgresql_postgres -password } \"' pg_dumpall -h localhost -p 5432 -U postgres\" > keycloak_wildfly_db_dump.sql kubectl exec -n security ${ postgresql_pod } \"--\" sh -c \"PGPASSWORD='\" ${ postgresql_password } \"' pg_dump -h localhost -p 5432 -U admin -d keycloak\" > keycloak_wildfly_db_dump.sql Delete the dumped file from the pod for security reasons: kubectl exec -n security ${ postgresql_pod } \"--\" sh -c \"rm /tmp/keycloak_wildfly_db_dump.sql\"","title":"Export Postgres Databases"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#delete-keycloak-resources_1","text":"Delete all previous Keycloak resources along with the Postgres database and keycloak StatefulSets , Ingress , and custom resources via Helm, or via the tool used for their deployment. helm list -n security helm delete keycloak -n security Warning Don't delete the whole namespace. Keep the keycloak-postgresql and keycloak-admin-creds secrets. Delete the volume in AWS, from which a snapshot has been created. Then delete the PVC: kubectl delete pvc data-keycloak-postgresql-0 -n security","title":"Delete Keycloak Resources"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#install-postgres_1","text":"Add Bitnami chart repository and update Helm repos: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update Create Postgres values: Note fullnameOverride: \"keycloak-postgresql\" sets the name of the Postgres StatefulSet. It must be same as in the previous StatefulSet . View: postgres-values.yaml nameOverride : \"keycloak-postgresql\" # PostgreSQL read only replica parameters readReplicas : # Number of PostgreSQL read only replicas replicaCount : 1 global : postgresql : auth : username : admin existingSecret : keycloak-postgresql secretKeys : adminPasswordKey : postgres-password userPasswordKey : password database : keycloak auth : existingSecret : keycloak-postgresql secretKeys : adminPasswordKey : postgres-password userPasswordKey : password primary : persistence : enabled : true size : 3Gi # If the StorageClass with reclaimPolicy: Retain is used, install an additional StorageClass before installing PostgreSQL # (the code is given below). # If the default StorageClass will be used - change \"gp2-retain\" to \"gp2\" storageClass : \"gp2-retain\" Install the Postgres database: Note Change the namespace and the values file name if required. helm install postgresql bitnami/postgresql \\ --version 11 .7.6 \\ --values postgres-values.yaml \\ --namespace security Wait for the database to be ready.","title":"Install Postgres"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#import-postgres-databases","text":"Upload the database dump to the new Keycloak Postgres pod: cat keycloak_wildfly_db_dump.sql | kubectl exec -i -n security ${ postgresql_pod } \"--\" sh -c \"cat > /tmp/keycloak_wildfly_db_dump.sql\" Warning Database import must be done before deploying Keycloak, because Keycloak will write its own data to the database during the start, and the import will partially fail. If that happened, scale down the keycloak StatefulSet , and try to drop the Keycloak database in the Postgres pod: dropdb -i -e keycloak -p 5432 -h localhost -U postgres If there still are some conflicting objects like roles, drop them via the DROP ROLE command. If the previous steps do not help, downscale the Keycloak and Postgres StatefulSets and delete the attached PVC (save the volumeID before removing), and delete the volume on AWS if using gp2-retain . In case of using gp2 , the volume will be deleted automatically after removing PVC. After that, redeploy the Postgres database, so that the new PVC is automatically created. Import the SQL dump file to the Postgres database cluster: Info Since the databases were exported in the sql format, the psql tool will be used to restore (reload) them. pg_restore does not support this plain-text format. If the entire Postgres database cluster was migrated with the postgres Superuser using pg_dumpall , use the import command without indicating the database: psql -U postgres -f /tmp/keycloak_wildfly_db_dump.sql If the database was migrated with the admin user using pg_dump , the postgres Superuser still can be used to restore it, but, in this case, a database must be indicated: Warning If the database name was not indicated during the import for the file dumped with pg_dump , the psql tool will import this database to a default Postgres database called postgres . psql -U postgres -d keycloak -f /tmp/keycloak_wildfly_db_dump.sql If the postgres Superuser is not accessible in the Postgres pod, run the command under the admin or any other user that has the database permissions. In this case, indicate the database as well: psql -U admin -d keycloak -f /tmp/keycloak_wildfly_db_dump.sql After a successful import, delete the dump file from the pod for security reasons: kubectl exec -n security ${ postgresql_pod } \"--\" sh -c \"rm /tmp/keycloak_wildfly_db_dump.sql\" Note Please find below the alternative commands for importing the database from the local machine to the pod without storing the backup on a pod for postgres or admin users: cat \"keycloak_wildfly_db_dump.sql\" | kubectl exec -i -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" sh -c \"cat | PGPASSWORD='\" ${ postgresql_superuser_password } \"' psql -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \"\" cat \"keycloak_wildfly_db_dump.sql\" | kubectl exec -i -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" sh -c \"cat | PGPASSWORD='\" ${ postgresql_superuser_password } \"' psql -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \" -d \" ${ database_name } \"\" cat \"keycloak_wildfly_db_dump.sql\" | kubectl exec -i -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" sh -c \"cat | PGPASSWORD='\" ${ postgresql_admin_password } \"' psql -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \" -d \" ${ database_name } \"\"","title":"Import Postgres Databases"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#install-keycloak","text":"Make sure the Keycloak chart repository is added: helm repo add codecentric https://codecentric.github.io/helm-charts helm repo update Create Keycloak values: Note nameOverride: \"keycloak\" sets the name of the Keycloak pod. It must be the same Keycloak name as in the previous StatefulSet . Change Ingress host name to the Keycloak host name. hostname: keycloak-postgresql is the hostname of the pod with the Postgres database that is the same as Postgres StatefulSet name, for example, keycloak-postgresql . \"/opt/keycloak/bin/kc.sh start --auto-build\" was used in the legacy Keycloak version. However, it is no longer required in the new Keycloak version since it is deprecated and used by default. Optionally, use the following command for applying the old Keycloak theme: bin/kc.sh start --features-disabled = admin2 Info Automatic database migration will start after the Keycloak installation. View: keycloak-values.yaml nameOverride : \"keycloak\" replicas : 1 # Deploy the latest verion image : tag : \"19.0.1\" # start: create OpenShift realm which is required by EDP extraInitContainers : | - name: realm-provider image: busybox imagePullPolicy: IfNotPresent command: - sh args: - -c - | echo '{\"realm\": \"openshift\",\"enabled\": true}' > /opt/keycloak/data/import/openshift.json volumeMounts: - name: realm mountPath: /opt/keycloak/data/import extraVolumeMounts : | - name: realm mountPath: /opt/keycloak/data/import extraVolumes : | - name: realm emptyDir: {} command : - \"/opt/keycloak/bin/kc.sh\" - \"--verbose\" - \"start\" - \"--http-enabled=true\" - \"--http-port=8080\" - \"--hostname-strict=false\" - \"--hostname-strict-https=false\" - \"--spi-events-listener-jboss-logging-success-level=info\" - \"--spi-events-listener-jboss-logging-error-level=warn\" - \"--import-realm\" extraEnv : | - name: KC_PROXY value: \"passthrough\" - name: KEYCLOAK_ADMIN valueFrom: secretKeyRef: name: keycloak-admin-creds key: username - name: KEYCLOAK_ADMIN_PASSWORD valueFrom: secretKeyRef: name: keycloak-admin-creds key: password - name: JAVA_OPTS_APPEND value: >- -XX:+UseContainerSupport -XX:MaxRAMPercentage=50.0 -Djava.awt.headless=true -Djgroups.dns.query={{ include \"keycloak.fullname\" . }}-headless # This block should be uncommented if you install Keycloak on Kubernetes ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx ingress.kubernetes.io/affinity : cookie rules : - host : keycloak.<ROOT_DOMAIN> paths : - path : '{{ tpl .Values.http.relativePath $ | trimSuffix \"/\" }}/' pathType : Prefix # This block should be uncommented if you set Keycloak to OpenShift and change the host field # route: # enabled: false # # Path for the Route # path: '/' # # Host name for the Route # host: \"keycloak.<ROOT_DOMAIN>\" # # TLS configuration # tls: # enabled: true resources : limits : memory : \"2048Mi\" requests : cpu : \"50m\" memory : \"512Mi\" # Check database readiness at startup dbchecker : enabled : true database : vendor : postgres existingSecret : keycloak-postgresql hostname : keycloak-postgresql port : 5432 username : admin database : keycloak Deploy Keycloak: Note Change the namespace and the values file name if required. helm install keycloak codecentric/keycloakx --version 1 .6.0 --values keycloak-values.yaml -n security Log in to Keycloak and check if everything has been imported correctly.","title":"Install Keycloak"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#clean-and-analyze-database_1","text":"Optionally, run the vacuumdb application on the database, to analyze the contents of database tables and collect statistics for the Postgres query optimizer: PGPASSWORD = \" ${ postgresql_postgres -password } \" vacuumdb --analyze --verbose -d keycloak -U postgres For all databases, run the following command: PGPASSWORD = \" ${ postgresql_postgres -password } \" vacuumdb --analyze --verbose --all -U postgres","title":"Clean and Analyze Database"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#postgres-database-migration-script","text":"Info Please read the Migrate Postgres Database From Postgres v.11.x to v.14.5 section of this tutorial before using the script. Note The kubectl tool is required for using this script. This script will likely work for any other Postgres database besides Keycloak after some adjustments. It queries the pg_dump , pg_dumpall , psql , and vacuumdb commands under the hood. The following script can be used for exporting and importing Postgres databases as well as optimizing them with the vacuumdb application. Please examine the code and make the adjustments if required. By default, the following command exports Keycloak Postgres databases from a Kubernetes pod to a local machine: ./script.sh After running the command, please follow the prompt. To import a database backup to a newly created Postgres Kubernetes pod, pass a database dump sql file to the script: ./script.sh path-to/db_dump.sql The -h flag prints help, and -c|-v runs the vacuumdb garbage collector and analyzer. View: keycloak_db_migration.sh #!/bin/bash # set -x db_migration_help (){ echo \"Keycloak Postgres database migration\" echo echo \"Usage:\" echo \"------------------------------------------\" echo \"Export Keycloak Postgres database from pod\" echo \"Run without parameters:\" echo \" $0 \" echo \"------------------------------------------\" echo \"Import Keycloak Postgres database to pod\" echo \"Pass filename to script:\" echo \" $0 path/to/db_dump.sql\" echo \"------------------------------------------\" echo \"Additional options: \" echo \" $0 [OPTIONS...]\" echo \"Options:\" echo \"h Print Help.\" echo \"c|v Run garbage collector and analyzer.\" } keycloak_ns (){ printf '%s\\n' 'Enter keycloak namespace: ' read -r keycloak_namespace if [ -z \" ${ keycloak_namespace } \" ] ; then echo \"Don't skip namespace\" exit 1 fi } postgres_pod (){ printf '%s\\n' 'Enter postgres pod name: ' read -r postgres_pod_name if [ -z \" ${ postgres_pod_name } \" ] ; then echo \"Don't skip pod name\" exit 1 fi } postgres_user (){ printf '%s\\n' 'Enter postgres username: ' printf '%s' \"Skip to use [postgres] superuser: \" read -r postgres_username if [ -z \" ${ postgres_username } \" ] ; then postgres_username = 'postgres' fi } pgdb_host_info (){ database_name = 'keycloak' db_host = 'localhost' db_port = '5432' } postgresql_admin_pass (){ postgresql_password = 'POSTGRES_PASSWORD' postgresql_admin_password = \" $( kubectl exec -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"printenv ${ postgresql_password } \" ) \" } postgresql_su_pass (){ postgresql_postgres_password = 'POSTGRES_POSTGRES_PASSWORD' postgresql_superuser_password = \" $( kubectl exec -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"printenv ${ postgresql_postgres_password } \" ) \" if [ -z \" ${ postgresql_superuser_password } \" ] ; then echo \"SuperUser password variable does not exist. Using user password instead...\" postgresql_admin_pass postgresql_superuser_password = \" ${ postgresql_admin_password } \" fi } keycloak_pgdb_export (){ current_cluster = \" $( kubectl config current-context ) \" exported_db_name = \"keycloak_db_dump_ ${ current_cluster } _ ${ keycloak_namespace } _ ${ postgres_username } _ $( date + \"%Y%m%d%H%M\" ) .sql\" if [ \" ${ postgres_username } \" == 'postgres' ] ; then # call a function to get a pass for postgres user postgresql_su_pass kubectl exec -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"PGPASSWORD='\" ${ postgresql_superuser_password } \"' pg_dumpall -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \"\" > \" ${ exported_db_name } \" else # call a function to get a pass for admin user postgresql_admin_pass kubectl exec -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"PGPASSWORD='\" ${ postgresql_admin_password } \"' pg_dump -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \" -d \" ${ database_name } \"\" > \" ${ exported_db_name } \" fi separate_lines = \"---------------\" if [ ! -s \" ${ exported_db_name } \" ] ; then rm -f \" ${ exported_db_name } \" echo \" ${ separate_lines } \" echo \"Something went wrong. The database dump file is empty and was not saved.\" else echo \" ${ separate_lines } \" grep 'Dumped' \" ${ exported_db_name } \" | sort -u echo \"Database has been exported to $( pwd ) / ${ exported_db_name } \" fi } keycloak_pgdb_import (){ echo \"Preparing Import\" echo \"----------------\" if [ ! -f \" $1 \" ] ; then echo \"The file $1 does not exist.\" exit 1 fi keycloak_ns postgres_pod postgres_user pgdb_host_info if [ \" ${ postgres_username } \" == 'postgres' ] ; then # restore full backup with all databases and roles as superuser or a single database postgresql_su_pass if [ -n \" $( cat \" $1 \" | grep 'CREATE ROLE' ) \" ] ; then cat \" $1 \" | kubectl exec -i -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"cat | PGPASSWORD='\" ${ postgresql_superuser_password } \"' psql -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \"\" else cat \" $1 \" | kubectl exec -i -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"cat | PGPASSWORD='\" ${ postgresql_superuser_password } \"' psql -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \" -d \" ${ database_name } \"\" fi else # restore a single database postgresql_admin_pass cat \" $1 \" | kubectl exec -i -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"cat | PGPASSWORD='\" ${ postgresql_admin_password } \"' psql -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \" -d \" ${ database_name } \"\" fi } vacuum_pgdb (){ echo \"Preparing garbage collector and analyzer\" echo \"----------------------------------------\" keycloak_ns postgres_pod postgres_user pgdb_host_info if [ \" ${ postgres_username } \" == 'postgres' ] ; then postgresql_su_pass kubectl exec -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"PGPASSWORD='\" ${ postgresql_superuser_password } \"' vacuumdb --analyze --all -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \"\" else postgresql_admin_pass kubectl exec -n \" ${ keycloak_namespace } \" \" ${ postgres_pod_name } \" \"--\" \\ sh -c \"PGPASSWORD='\" ${ postgresql_admin_password } \"' vacuumdb --analyze -h \" ${ db_host } \" -p \" ${ db_port } \" -U \" ${ postgres_username } \" -d \" ${ database_name } \"\" fi } while [ \" $# \" -eq 1 ] ; do case \" $1 \" in -h | --help ) db_migration_help exit 0 ;; -c | --clean | -v | --vacuum ) vacuum_pgdb exit 0 ;; -- ) break ;; -* ) echo \"Invalid option ' $1 '. Use -h|--help to see the valid options\" > & 2 exit 1 ;; * ) keycloak_pgdb_import \" $1 \" exit 0 ;; esac shift done if [ \" $# \" -gt 1 ] ; then echo \"Please pass a single file to the script\" exit 1 fi echo \"Preparing Export\" echo \"----------------\" keycloak_ns postgres_pod postgres_user pgdb_host_info keycloak_pgdb_export","title":"Postgres Database Migration Script"},{"location":"operator-guide/upgrade-keycloak-17.0.x-legacy-to-19.0.x/#related-articles","text":"Deploy OKD 4.10 Cluster","title":"Related Articles"},{"location":"operator-guide/velero-irsa/","text":"IAM Roles for Velero Service Accounts \u2693\ufe0e Note Make sure that IRSA is enabled and amazon-eks-pod-identity-webhook is deployed according to the Associate IAM Roles With Service Accounts documentation. Velero AWS plugin requires access to AWS resources. Follow the steps below to create a required role: Create AWS IAM Policy \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039VELERO_NAMESPACE\u203aVelero_policy\": { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DescribeSnapshots\" , \"ec2:CreateTags\" , \"ec2:CreateVolume\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetObject\" , \"s3:DeleteObject\" , \"s3:PutObject\" , \"s3:AbortMultipartUpload\" , \"s3:ListMultipartUploadParts\" ], \"Resource\" : [ \"arn:aws:s3:::velero-*/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::velero-*\" ] } ] } Create AWS IAM Role \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039VELERO_NAMESPACE\u203aVelero\" with trust relationships: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::<AWS_ACCOUNT_ID>:oidc-provider/<OIDC_PROVIDER>\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"<OIDC_PROVIDER>:sub\": \"system:serviceaccount:<VELERO_NAMESPACE>:edp-velero\" } } } ] } Attach the \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039VELERO_NAMESPACE\u203aVelero_policy\" policy to the \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039VELERO_NAMESPACE\u203aVelero\" role. Make sure that Amazon S3 bucket with name velero-\u2039CLUSTER_NAME\u203a exists. Provide key value eks.amazonaws.com/role-arn: \"arn:aws:iam:: :role/AWSIRSA\u2039CLUSTER_NAME\u203a\u2039VELERO_NAMESPACE\u203aVelero\" into the serviceAccount.server.annotations parameter in values.yaml during the Velero Installation . Related Articles \u2693\ufe0e Associate IAM Roles With Service Accounts Install Velero","title":"IAM Roles for Velero Service Accounts"},{"location":"operator-guide/velero-irsa/#iam-roles-for-velero-service-accounts","text":"Note Make sure that IRSA is enabled and amazon-eks-pod-identity-webhook is deployed according to the Associate IAM Roles With Service Accounts documentation. Velero AWS plugin requires access to AWS resources. Follow the steps below to create a required role: Create AWS IAM Policy \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039VELERO_NAMESPACE\u203aVelero_policy\": { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DescribeSnapshots\" , \"ec2:CreateTags\" , \"ec2:CreateVolume\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetObject\" , \"s3:DeleteObject\" , \"s3:PutObject\" , \"s3:AbortMultipartUpload\" , \"s3:ListMultipartUploadParts\" ], \"Resource\" : [ \"arn:aws:s3:::velero-*/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" ], \"Resource\" : [ \"arn:aws:s3:::velero-*\" ] } ] } Create AWS IAM Role \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039VELERO_NAMESPACE\u203aVelero\" with trust relationships: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::<AWS_ACCOUNT_ID>:oidc-provider/<OIDC_PROVIDER>\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"<OIDC_PROVIDER>:sub\": \"system:serviceaccount:<VELERO_NAMESPACE>:edp-velero\" } } } ] } Attach the \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039VELERO_NAMESPACE\u203aVelero_policy\" policy to the \"AWSIRSA\u2039CLUSTER_NAME\u203a\u2039VELERO_NAMESPACE\u203aVelero\" role. Make sure that Amazon S3 bucket with name velero-\u2039CLUSTER_NAME\u203a exists. Provide key value eks.amazonaws.com/role-arn: \"arn:aws:iam:: :role/AWSIRSA\u2039CLUSTER_NAME\u203a\u2039VELERO_NAMESPACE\u203aVelero\" into the serviceAccount.server.annotations parameter in values.yaml during the Velero Installation .","title":"IAM Roles for Velero Service Accounts"},{"location":"operator-guide/velero-irsa/#related-articles","text":"Associate IAM Roles With Service Accounts Install Velero","title":"Related Articles"},{"location":"operator-guide/waf-tf-configuration/","text":"Configure AWS WAF With Terraform \u2693\ufe0e This page contains accurate information on how to configure AWS WAF using Terraform with the aim to have a secured traffic exposure and to prevent the Host Header vulnerabilities. Prerequisites \u2693\ufe0e To follow the instruction, check the following prerequisites: Deployed infrastructure includes Nginx Ingress Controller Deployed services for testing Separate and exposed AWS ALB terraform 0.14.10 hishicorp/aws = 4.8.0 Solution Overview \u2693\ufe0e The solution includes two parts: Prerequisites (mostly the left part of the scheme) - AWS ALB, Compute Resources (EC2, EKS, etc.). WAF configuration (the right part of the scheme). The WAF ACL resource is the main resource used for the configuration; The default web ACL option is Block. Overview WAF Solution The ACL includes three managed AWS rules that secure the exposed traffic: AWS-AWSManagedRulesCommonRuleSet AWS-AWSManagedRulesLinuxRuleSet AWS-AWSManagedRulesKnownBadInputsRuleSet AWS provides a lot of rules such as baseline and use-case specific rules, for details, please refer to the Baseline rule groups . There is the PreventHostInjections rule that prevents the Host Header vulnerabilities. This rule includes one statement that declares that the Host Header should match Regex Pattern Set, thus only in this case it will be passed. The Regex Pattern Set is another resource that helps to organize regexes, in fact, is a set of regexes. All regexes added to the single set are matched by the OR statement, i.e. when exposing several URLs, it is necessary to add this statement to the set and refer to it in the rule. WAF ACL Configuration \u2693\ufe0e To create the Regex Pattern Set, inspect the following code: resource \"aws_wafv2_regex_pattern_set\" \"common\" { name = \"Common\" scope = \"REGIONAL\" regular_expression { regex_string = \"^.*(some-url).*((.edp-epam)+)\\\\.com$\" } # Add here additional regular expressions for other endpoints, they are merging with OR operator, e.g. /* regular_expression { regex_string = \"^.*(jenkins).*((.edp-epam)+)\\\\.com$\" } */ tags = var.tags } It includes 'regex_string', for example: url - some-url.edp-epam.com, In addition, it is possible to add other links to the same resource using the regular_expression element. There is the Terraform code for the aws_wafv2_web_acl resource: resource \"aws_wafv2_web_acl\" \"external\" { name = \"ExternalACL\" scope = \"REGIONAL\" default_action { block {} } rule { name = \"AWS-AWSManagedRulesCommonRuleSet\" priority = 1 override_action { none {} } statement { managed_rule_group_statement { name = \"AWSManagedRulesCommonRuleSet\" vendor_name = \"AWS\" } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \"AWS-AWSManagedRulesCommonRuleSet\" sampled_requests_enabled = true } } rule { name = \"AWS-AWSManagedRulesLinuxRuleSet\" priority = 2 statement { managed_rule_group_statement { name = \"AWSManagedRulesLinuxRuleSet\" vendor_name = \"AWS\" } } override_action { none {} } visibility_config { cloudwatch_metrics_enabled = true metric_name = \"AWS-AWSManagedRulesLinuxRuleSet\" sampled_requests_enabled = true } } rule { name = \"AWS-AWSManagedRulesKnownBadInputsRuleSet\" priority = 3 override_action { none {} } statement { managed_rule_group_statement { name = \"AWSManagedRulesKnownBadInputsRuleSet\" vendor_name = \"AWS\" } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \"AWS-AWSManagedRulesKnownBadInputsRuleSet\" sampled_requests_enabled = true } } rule { name = \"PreventHostInjections\" priority = 0 statement { regex_pattern_set_reference_statement { arn = aws_wafv2_regex_pattern_set.common.arn field_to_match { single_header { name = \"host\" } } text_transformation { priority = 0 type = \"NONE\" } } } action { allow {} } visibility_config { cloudwatch_metrics_enabled = true metric_name = \"PreventHostInjections\" sampled_requests_enabled = true } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \"ExternalACL\" sampled_requests_enabled = true } tags = var.tags } As mentioned previously, ACL includes three managed AWS rules (group rules), for visibility, enabling sampling, and CloudWatch in the config. The 'PreventHostInjections' custom rule refers to the created pattern set and declares the Host Header, as well as sets the 'Action' if matched to 'Allow'. Associate AWS Resource \u2693\ufe0e To have the created ACL working, it is necessary to associate an AWS resource with it, in this case, it is AWS ALB: resource \"aws_wafv2_web_acl_association\" \"waf_alb\" { resource_arn = aws_lb . <aws_alb_for_waf> . arn web_acl_arn = aws_wafv2_web_acl.external.arn } Note AWS ALB can be created in the scope of this Terraform code or created previously. When creating ALB to expose links, the ALB should have a security group that allows some external traffic. When ALB is associated with the WAF ACL, direct the traffic to the ALB by the Route53 CNAME record: module \"some_url_exposure\" { source = \"terraform-aws-modules/route53/aws//modules/records\" version = \"2.0.0\" zone_name = \"edp-epam.com\" records = [ { name = \"some-url\" type = \"CNAME\" ttl = 300 records = [ aws_lb . <aws_alb_for_waf> . dns_name ] } ] } In the sample above, the module is used, but it is also possible to use a Terraform resource.","title":"Configure AWS WAF With Terraform"},{"location":"operator-guide/waf-tf-configuration/#configure-aws-waf-with-terraform","text":"This page contains accurate information on how to configure AWS WAF using Terraform with the aim to have a secured traffic exposure and to prevent the Host Header vulnerabilities.","title":"Configure AWS WAF With Terraform"},{"location":"operator-guide/waf-tf-configuration/#prerequisites","text":"To follow the instruction, check the following prerequisites: Deployed infrastructure includes Nginx Ingress Controller Deployed services for testing Separate and exposed AWS ALB terraform 0.14.10 hishicorp/aws = 4.8.0","title":"Prerequisites"},{"location":"operator-guide/waf-tf-configuration/#solution-overview","text":"The solution includes two parts: Prerequisites (mostly the left part of the scheme) - AWS ALB, Compute Resources (EC2, EKS, etc.). WAF configuration (the right part of the scheme). The WAF ACL resource is the main resource used for the configuration; The default web ACL option is Block. Overview WAF Solution The ACL includes three managed AWS rules that secure the exposed traffic: AWS-AWSManagedRulesCommonRuleSet AWS-AWSManagedRulesLinuxRuleSet AWS-AWSManagedRulesKnownBadInputsRuleSet AWS provides a lot of rules such as baseline and use-case specific rules, for details, please refer to the Baseline rule groups . There is the PreventHostInjections rule that prevents the Host Header vulnerabilities. This rule includes one statement that declares that the Host Header should match Regex Pattern Set, thus only in this case it will be passed. The Regex Pattern Set is another resource that helps to organize regexes, in fact, is a set of regexes. All regexes added to the single set are matched by the OR statement, i.e. when exposing several URLs, it is necessary to add this statement to the set and refer to it in the rule.","title":"Solution Overview"},{"location":"operator-guide/waf-tf-configuration/#waf-acl-configuration","text":"To create the Regex Pattern Set, inspect the following code: resource \"aws_wafv2_regex_pattern_set\" \"common\" { name = \"Common\" scope = \"REGIONAL\" regular_expression { regex_string = \"^.*(some-url).*((.edp-epam)+)\\\\.com$\" } # Add here additional regular expressions for other endpoints, they are merging with OR operator, e.g. /* regular_expression { regex_string = \"^.*(jenkins).*((.edp-epam)+)\\\\.com$\" } */ tags = var.tags } It includes 'regex_string', for example: url - some-url.edp-epam.com, In addition, it is possible to add other links to the same resource using the regular_expression element. There is the Terraform code for the aws_wafv2_web_acl resource: resource \"aws_wafv2_web_acl\" \"external\" { name = \"ExternalACL\" scope = \"REGIONAL\" default_action { block {} } rule { name = \"AWS-AWSManagedRulesCommonRuleSet\" priority = 1 override_action { none {} } statement { managed_rule_group_statement { name = \"AWSManagedRulesCommonRuleSet\" vendor_name = \"AWS\" } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \"AWS-AWSManagedRulesCommonRuleSet\" sampled_requests_enabled = true } } rule { name = \"AWS-AWSManagedRulesLinuxRuleSet\" priority = 2 statement { managed_rule_group_statement { name = \"AWSManagedRulesLinuxRuleSet\" vendor_name = \"AWS\" } } override_action { none {} } visibility_config { cloudwatch_metrics_enabled = true metric_name = \"AWS-AWSManagedRulesLinuxRuleSet\" sampled_requests_enabled = true } } rule { name = \"AWS-AWSManagedRulesKnownBadInputsRuleSet\" priority = 3 override_action { none {} } statement { managed_rule_group_statement { name = \"AWSManagedRulesKnownBadInputsRuleSet\" vendor_name = \"AWS\" } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \"AWS-AWSManagedRulesKnownBadInputsRuleSet\" sampled_requests_enabled = true } } rule { name = \"PreventHostInjections\" priority = 0 statement { regex_pattern_set_reference_statement { arn = aws_wafv2_regex_pattern_set.common.arn field_to_match { single_header { name = \"host\" } } text_transformation { priority = 0 type = \"NONE\" } } } action { allow {} } visibility_config { cloudwatch_metrics_enabled = true metric_name = \"PreventHostInjections\" sampled_requests_enabled = true } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \"ExternalACL\" sampled_requests_enabled = true } tags = var.tags } As mentioned previously, ACL includes three managed AWS rules (group rules), for visibility, enabling sampling, and CloudWatch in the config. The 'PreventHostInjections' custom rule refers to the created pattern set and declares the Host Header, as well as sets the 'Action' if matched to 'Allow'.","title":"WAF ACL Configuration"},{"location":"operator-guide/waf-tf-configuration/#associate-aws-resource","text":"To have the created ACL working, it is necessary to associate an AWS resource with it, in this case, it is AWS ALB: resource \"aws_wafv2_web_acl_association\" \"waf_alb\" { resource_arn = aws_lb . <aws_alb_for_waf> . arn web_acl_arn = aws_wafv2_web_acl.external.arn } Note AWS ALB can be created in the scope of this Terraform code or created previously. When creating ALB to expose links, the ALB should have a security group that allows some external traffic. When ALB is associated with the WAF ACL, direct the traffic to the ALB by the Route53 CNAME record: module \"some_url_exposure\" { source = \"terraform-aws-modules/route53/aws//modules/records\" version = \"2.0.0\" zone_name = \"edp-epam.com\" records = [ { name = \"some-url\" type = \"CNAME\" ttl = 300 records = [ aws_lb . <aws_alb_for_waf> . dns_name ] } ] } In the sample above, the module is used, but it is also possible to use a Terraform resource.","title":"Associate AWS Resource"},{"location":"use-cases/","text":"Overview \u2693\ufe0e The Use Cases section provides useful recommendations of how to operate with the EPAM Delivery Platform tools and manage the custom resources. Get acquainted with the description of technical scenarios and solutions. Autotest as Quality Gate Promote Application in CD Pipeline","title":"Overview"},{"location":"use-cases/#overview","text":"The Use Cases section provides useful recommendations of how to operate with the EPAM Delivery Platform tools and manage the custom resources. Get acquainted with the description of technical scenarios and solutions. Autotest as Quality Gate Promote Application in CD Pipeline","title":"Overview"},{"location":"use-cases/autotest-as-quality-gate/","text":"Autotest as a Quality Gate \u2693\ufe0e This use case describes the flow of adding an autotest as a quality gate to a newly created CD pipeline with a selected build version of an application to be promoted. Make sure an application is created. Please refer to the Add Application section for details. Applications Create an autotest with the necessary parameters. Please refer to the Add Autotest section for details. Autotests Create a CD pipeline where the necessary application build version will be promoted and the autotest will be used as a quality gate. Please refer to the Add CD Pipeline section for details. Continuous delivery In the Application menu, select the application to be promoted in the CD pipeline. Application to promote In the Stages menu, add the necessary stage and indicate the Autotests quality gate. Quality gate type Note Please be aware that once created, the CD pipeline remains with the chosen quality gate type without possibility to edit it. Info To trigger the CD pipeline, first, make sure that all applications have passed the Build pipelines and autotests have passed the single Code Review pipelines. Navigate to Jenkins to see the autotest logs and Allure report. Related Articles \u2693\ufe0e Add Application Add Autotest Add CD Pipeline","title":"Autotest as Quality Gate"},{"location":"use-cases/autotest-as-quality-gate/#autotest-as-a-quality-gate","text":"This use case describes the flow of adding an autotest as a quality gate to a newly created CD pipeline with a selected build version of an application to be promoted. Make sure an application is created. Please refer to the Add Application section for details. Applications Create an autotest with the necessary parameters. Please refer to the Add Autotest section for details. Autotests Create a CD pipeline where the necessary application build version will be promoted and the autotest will be used as a quality gate. Please refer to the Add CD Pipeline section for details. Continuous delivery In the Application menu, select the application to be promoted in the CD pipeline. Application to promote In the Stages menu, add the necessary stage and indicate the Autotests quality gate. Quality gate type Note Please be aware that once created, the CD pipeline remains with the chosen quality gate type without possibility to edit it. Info To trigger the CD pipeline, first, make sure that all applications have passed the Build pipelines and autotests have passed the single Code Review pipelines. Navigate to Jenkins to see the autotest logs and Allure report.","title":"Autotest as a Quality Gate"},{"location":"use-cases/autotest-as-quality-gate/#related-articles","text":"Add Application Add Autotest Add CD Pipeline","title":"Related Articles"},{"location":"use-cases/promotion-procedure/","text":"Promote Application in CD Pipeline \u2693\ufe0e This use case describes the promotion of the selected build versions of an application in the CD pipeline. Before the promotion, make sure the necessary applications are added to the Admin Console. Please refer to the Add Application section for the details. Navigate to the Continuous Delivery section on the left-side navigation bar and click the Create button. Please refer to the Add CD Pipeline section for the details. Create cd pipeline In the Application menu, select the applications to be deployed and promoted in the pipeline. If the application is selected without the promotion option, a new build version will be available for deploy at all the stages of the CD pipeline; Select application to deploy If the promotion is selected, the build version will pass through the environments one by one through the quality gates. Select application to promote Info If there are several applications to be promoted, and some changes are made to one of them, only the application with the changes will be promoted to the next environments. In the Stages menu, set the manual trigger type to select an application for promotion. Select trigger type Navigate to Jenkins and start the CD pipeline. With the manual quality gate, click the OK button at the quality gate stage to promote the build to the next stage. Related Articles \u2693\ufe0e Add Application Add Autotest Add CD Pipeline Autotest as a Quality Gate","title":"Promote Application in CD Pipeline"},{"location":"use-cases/promotion-procedure/#promote-application-in-cd-pipeline","text":"This use case describes the promotion of the selected build versions of an application in the CD pipeline. Before the promotion, make sure the necessary applications are added to the Admin Console. Please refer to the Add Application section for the details. Navigate to the Continuous Delivery section on the left-side navigation bar and click the Create button. Please refer to the Add CD Pipeline section for the details. Create cd pipeline In the Application menu, select the applications to be deployed and promoted in the pipeline. If the application is selected without the promotion option, a new build version will be available for deploy at all the stages of the CD pipeline; Select application to deploy If the promotion is selected, the build version will pass through the environments one by one through the quality gates. Select application to promote Info If there are several applications to be promoted, and some changes are made to one of them, only the application with the changes will be promoted to the next environments. In the Stages menu, set the manual trigger type to select an application for promotion. Select trigger type Navigate to Jenkins and start the CD pipeline. With the manual quality gate, click the OK button at the quality gate stage to promote the build to the next stage.","title":"Promote Application in CD Pipeline"},{"location":"use-cases/promotion-procedure/#related-articles","text":"Add Application Add Autotest Add CD Pipeline Autotest as a Quality Gate","title":"Related Articles"},{"location":"user-guide/","text":"Overview \u2693\ufe0e The EDP User guide is intended for developers and provides details on working with EDP Admin Console, different codebase types and EDP CI/CD flow. Admin Console \u2693\ufe0e Admin Console is a central management tool in the EDP ecosystem that provides the ability to define pipelines, project resources and new technologies in a simple way. Using Admin Console enables to manage business entities: Create such codebase types as Applications, Libraries and Autotests; Create/Update CD Pipelines; Note To interact with Admin Console via REST API, explore the Create Codebase Entity page. Overview page Navigation bar \u2013 consists of six sections: Overview, Continuous Delivery, Applications, Autotests, Libraries, and Delivery Dashboard Diagram. Click the necessary section to add an entity, open a home page or check the diagram. User name \u2013 displays the registered user name. Main links \u2013 displays the corresponding links to the major adjusted toolset, to the management tool and to the OpenShift cluster. Admin Console is a complete tool allowing to manage and control added to the environment codebases (applications, autotests, libraries) as well as to create a CD pipeline and check the visualization diagram. Inspect the main features available in Admin Console by following the corresponding link: Add Application Add Autotest Add Library Add CD Pipeline Delivery Dashboard Diagram","title":"Overview"},{"location":"user-guide/#overview","text":"The EDP User guide is intended for developers and provides details on working with EDP Admin Console, different codebase types and EDP CI/CD flow.","title":"Overview"},{"location":"user-guide/#admin-console","text":"Admin Console is a central management tool in the EDP ecosystem that provides the ability to define pipelines, project resources and new technologies in a simple way. Using Admin Console enables to manage business entities: Create such codebase types as Applications, Libraries and Autotests; Create/Update CD Pipelines; Note To interact with Admin Console via REST API, explore the Create Codebase Entity page. Overview page Navigation bar \u2013 consists of six sections: Overview, Continuous Delivery, Applications, Autotests, Libraries, and Delivery Dashboard Diagram. Click the necessary section to add an entity, open a home page or check the diagram. User name \u2013 displays the registered user name. Main links \u2013 displays the corresponding links to the major adjusted toolset, to the management tool and to the OpenShift cluster. Admin Console is a complete tool allowing to manage and control added to the environment codebases (applications, autotests, libraries) as well as to create a CD pipeline and check the visualization diagram. Inspect the main features available in Admin Console by following the corresponding link: Add Application Add Autotest Add Library Add CD Pipeline Delivery Dashboard Diagram","title":"Admin Console"},{"location":"user-guide/add-application/","text":"Add Application \u2693\ufe0e Admin Console allows to create, clone, import an application and add it to the environment. It can also be deployed in Gerrit (if the Clone or Create strategy is used) with the Code Review and Build pipelines built in Jenkins. To add an application, navigate to the Applications section on the left-side navigation bar and click the Create button. Once clicked, the three-step menu will appear: The Codebase Info Menu The Application Info Menu The Advanced Settings Menu The Codebase Info Menu \u2693\ufe0e Create application In the Codebase Integration Strategy field, select the necessary configuration strategy: Create \u2013 creates a project on the pattern in accordance with an application language, a build tool, and a framework. Clone \u2013 clones the indicated repository into EPAM Delivery Platform. While cloning the existing repository, it is required to fill in the additional fields as well. Import - allows configuring a replication from the Git server. While importing the existing repository, you have to select the Git server and define the respective path to the repository. Note In order to use the Import strategy, make sure to adjust it by following the Enable VCS Import Strategy page. The Import strategy is not applicable for Gerrit. In the Git Repository URL field, specify the link to the repository that is to be cloned. If the Import strategy is selected, specify the following fields: Select import strategy a. Git Server where the repository is located. b. Relative path to the repository on the server. With the Clone strategy, select the Codebase Authentication check box and fill in the requested fields: Repository Login \u2013 enter your login data. Repository password (or API Token) \u2013 enter your password or indicate the API Token. Note The Codebase Authentication check box should be selected only in case you clone the private repository. If you define the public repository, there is no need to enter credentials. Click the Proceed button to switch to the next menu. The Application Info Menu \u2693\ufe0e Application info Type the name of the application in the Application Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Specify the name of the default branch where you want the development to be performed. Note The default branch cannot be deleted. To create an application with an empty repository in Gerrit, select the Empty project check box. Note The empty repository option is available only for the Create strategy. Select any of the supported application languages with its framework in the Application Code Language/framework field: Java \u2013 selecting Java allows using Java 8 or Java 11. JavaScript - selecting JavaScript allows using the React framework. DotNet - selecting DotNet allows using the DotNet v.2.1 and DotNet v.3.1. Go - selecting Go allows using the Beego and Operator SDK frameworks. Python - selecting Python allows using the Python v.3.8. Other - selecting Other allows extending the default code languages when creating a codebase with the clone/import strategy. To add another code language, inspect the Add Other Code Language section. Note The Create strategy does not allow to customize the default code language set. Choose the necessary build tool in the Select Build Tool field: Java - selecting Java allows using the Gradle or Maven tool. JavaScript - selecting JavaScript allows using the NPM tool. .Net - selecting .Net allows using the .Net tool. Note The Select Build Tool field disposes of the default tools and can be changed in accordance with the selected code language. Select the Multi-Module Project check box that becomes available if the Java code language and the Maven build tool are selected. Click the Proceed button to switch to the next menu. The Advanced Settings Menu \u2693\ufe0e Advanced settings Select CI pipeline provisioner that will be handling a codebase. For details, refer to the Manage Jenkins CI Pipeline Job Provisioner instruction and become familiar with the main steps to add an additional job provisioner. Select Jenkins agent that will be used to handle a codebase. For details, refer to the Manage Jenkins Agent instruction and inspect the steps that should be done to add a new Jenkins agent. Select the necessary codebase versioning type: default - the previous versioning logic that is realized in EDP Admin Console 2.2.0 and lower versions. Using the default versioning type, in order to specify the version of the current artifacts, images, and tags in the Version Control System, a developer should navigate to the corresponding file and change the version manually . edp - the new versioning logic that is available in EDP Admin Console 2.3.0 and subsequent versions. Using the edp versioning type, a developer indicates the version number that will be used for all the artifacts stored in artifactory: binaries, pom.xml, metadata, etc. The version stored in repository (e.g. pom.xml) will not be affected or used. Using this versioning overrides any version stored in the repository files without changing actual file. When selecting the edp versioning type, the extra field will appear: Edp versioning a. Type the version number from which you want the artifacts to be versioned. Note The Start Version From field should be filled out in compliance with the semantic versioning rules, e.g. 1.2.3 or 10.10.10. Please refer to the Semantic Versioning page for details. In the Select Deployment Script field, specify one of the available options: helm-chart / openshift-template that are predefined in case it is OpenShift or EKS. In the Select CI Tool field, choose the necessary tool: Jenkins or GitLab CI, where Jenkins is the default tool and the GitLab CI tool can be additionally adjusted. For details, please refer to the Adjust GitLab CI Tool page. Note The GitLab CI tool is available only with the Import strategy and makes the Jira integration feature unavailable. Integrate with jira server Select the Integrate with Jira Server check box in case it is required to connect Jira tickets with the commits and have a respective label in the Fix Version field. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Jira Integration page, and setup the Adjust VCS Integration With Jira . Pay attention that the Jira integration feature is not available when using the GitLab CI tool. In the Select Jira Server field, select the Jira server. Indicate the pattern using any character, which is followed on the project, to validate a commit message. Indicate the pattern using any character, which is followed on the project, to find a Jira ticket number in a commit message. In the Advanced Mapping section, specify the names of the Jira fields that should be filled in with attributes from EDP. Upon clicking the question mark icon, observe the tips on how to indicate and combine variables necessary for identifying the format of values to be displayed. Advance mapping a. Select the name of the field in a Jira ticket. The available fields are the following: Fix Version/s , Component/s and Labels . b. Select the pattern of predefined variables, based on which the value from EDP will be displayed in Jira. Combine several variables to obtain the desired value. For the Fix Version/s field, select the EDP_VERSION variable that represents an EDP upgrade version, as in 2.7.0-SNAPSHOT . Combine variables to make the value more informative. For example, the pattern EDP_VERSION-EDP_COMPONENT will be displayed as 2.7.0-SNAPSHOT-nexus-operator in Jira; For the Component/s field, select the EDP_COMPONENT variable that defines the name of the existing repository. For example, nexus-operator ; For the Labels field, select the EDP_GITTAG variable that defines a tag assigned to the commit in GitHub. For example, build/2.7.0-SNAPSHOT.59 . c. Click the plus icon to add more Jira field names. d. Click the delete icon to remove the Jira field name. Integrate with perf server Select the Integrate with Perf Server check box to enable the integration with the PERF Board ( Project Performance Board ) for monitoring the overall team performance and setting up necessary metrics. Note If this option is needed, please refer to the Perf Server Integration to adjust the integration. After the integration is adjusted, the Integrate with Perf Server check box will appear in the Advanced Settings menu. In the Select Perf Server field, select the name of the Perf server with which the integration should be performed. Click the Proceed button to switch to the next menu. Perf integration Select the necessary DataSource ( Jenkins, Sonar, GitLab ) from which the data should be transferred to the Project Performance Board. Click the Create button, check the CONFIRMATION summary, and click Continue to complete the application addition. Note After the complete adding of the application, inspect the Application Overview part. Related Articles \u2693\ufe0e Application Overview Delivery Dashboard Diagram Add CD Pipelines Add Other Code Language Adjust GitLab CI Tool Adjust Jira Integration Adjust VCS Integration With Jira Enable VCS Import Strategy Manage Jenkins CI Pipeline Job Provisioner Manage Jenkins Agent Perf Server Integration Promote Application in CD Pipeline","title":"Add Application"},{"location":"user-guide/add-application/#add-application","text":"Admin Console allows to create, clone, import an application and add it to the environment. It can also be deployed in Gerrit (if the Clone or Create strategy is used) with the Code Review and Build pipelines built in Jenkins. To add an application, navigate to the Applications section on the left-side navigation bar and click the Create button. Once clicked, the three-step menu will appear: The Codebase Info Menu The Application Info Menu The Advanced Settings Menu","title":"Add Application"},{"location":"user-guide/add-application/#the-codebase-info-menu","text":"Create application In the Codebase Integration Strategy field, select the necessary configuration strategy: Create \u2013 creates a project on the pattern in accordance with an application language, a build tool, and a framework. Clone \u2013 clones the indicated repository into EPAM Delivery Platform. While cloning the existing repository, it is required to fill in the additional fields as well. Import - allows configuring a replication from the Git server. While importing the existing repository, you have to select the Git server and define the respective path to the repository. Note In order to use the Import strategy, make sure to adjust it by following the Enable VCS Import Strategy page. The Import strategy is not applicable for Gerrit. In the Git Repository URL field, specify the link to the repository that is to be cloned. If the Import strategy is selected, specify the following fields: Select import strategy a. Git Server where the repository is located. b. Relative path to the repository on the server. With the Clone strategy, select the Codebase Authentication check box and fill in the requested fields: Repository Login \u2013 enter your login data. Repository password (or API Token) \u2013 enter your password or indicate the API Token. Note The Codebase Authentication check box should be selected only in case you clone the private repository. If you define the public repository, there is no need to enter credentials. Click the Proceed button to switch to the next menu.","title":"The Codebase Info Menu"},{"location":"user-guide/add-application/#the-application-info-menu","text":"Application info Type the name of the application in the Application Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Specify the name of the default branch where you want the development to be performed. Note The default branch cannot be deleted. To create an application with an empty repository in Gerrit, select the Empty project check box. Note The empty repository option is available only for the Create strategy. Select any of the supported application languages with its framework in the Application Code Language/framework field: Java \u2013 selecting Java allows using Java 8 or Java 11. JavaScript - selecting JavaScript allows using the React framework. DotNet - selecting DotNet allows using the DotNet v.2.1 and DotNet v.3.1. Go - selecting Go allows using the Beego and Operator SDK frameworks. Python - selecting Python allows using the Python v.3.8. Other - selecting Other allows extending the default code languages when creating a codebase with the clone/import strategy. To add another code language, inspect the Add Other Code Language section. Note The Create strategy does not allow to customize the default code language set. Choose the necessary build tool in the Select Build Tool field: Java - selecting Java allows using the Gradle or Maven tool. JavaScript - selecting JavaScript allows using the NPM tool. .Net - selecting .Net allows using the .Net tool. Note The Select Build Tool field disposes of the default tools and can be changed in accordance with the selected code language. Select the Multi-Module Project check box that becomes available if the Java code language and the Maven build tool are selected. Click the Proceed button to switch to the next menu.","title":"The Application Info Menu"},{"location":"user-guide/add-application/#the-advanced-settings-menu","text":"Advanced settings Select CI pipeline provisioner that will be handling a codebase. For details, refer to the Manage Jenkins CI Pipeline Job Provisioner instruction and become familiar with the main steps to add an additional job provisioner. Select Jenkins agent that will be used to handle a codebase. For details, refer to the Manage Jenkins Agent instruction and inspect the steps that should be done to add a new Jenkins agent. Select the necessary codebase versioning type: default - the previous versioning logic that is realized in EDP Admin Console 2.2.0 and lower versions. Using the default versioning type, in order to specify the version of the current artifacts, images, and tags in the Version Control System, a developer should navigate to the corresponding file and change the version manually . edp - the new versioning logic that is available in EDP Admin Console 2.3.0 and subsequent versions. Using the edp versioning type, a developer indicates the version number that will be used for all the artifacts stored in artifactory: binaries, pom.xml, metadata, etc. The version stored in repository (e.g. pom.xml) will not be affected or used. Using this versioning overrides any version stored in the repository files without changing actual file. When selecting the edp versioning type, the extra field will appear: Edp versioning a. Type the version number from which you want the artifacts to be versioned. Note The Start Version From field should be filled out in compliance with the semantic versioning rules, e.g. 1.2.3 or 10.10.10. Please refer to the Semantic Versioning page for details. In the Select Deployment Script field, specify one of the available options: helm-chart / openshift-template that are predefined in case it is OpenShift or EKS. In the Select CI Tool field, choose the necessary tool: Jenkins or GitLab CI, where Jenkins is the default tool and the GitLab CI tool can be additionally adjusted. For details, please refer to the Adjust GitLab CI Tool page. Note The GitLab CI tool is available only with the Import strategy and makes the Jira integration feature unavailable. Integrate with jira server Select the Integrate with Jira Server check box in case it is required to connect Jira tickets with the commits and have a respective label in the Fix Version field. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Jira Integration page, and setup the Adjust VCS Integration With Jira . Pay attention that the Jira integration feature is not available when using the GitLab CI tool. In the Select Jira Server field, select the Jira server. Indicate the pattern using any character, which is followed on the project, to validate a commit message. Indicate the pattern using any character, which is followed on the project, to find a Jira ticket number in a commit message. In the Advanced Mapping section, specify the names of the Jira fields that should be filled in with attributes from EDP. Upon clicking the question mark icon, observe the tips on how to indicate and combine variables necessary for identifying the format of values to be displayed. Advance mapping a. Select the name of the field in a Jira ticket. The available fields are the following: Fix Version/s , Component/s and Labels . b. Select the pattern of predefined variables, based on which the value from EDP will be displayed in Jira. Combine several variables to obtain the desired value. For the Fix Version/s field, select the EDP_VERSION variable that represents an EDP upgrade version, as in 2.7.0-SNAPSHOT . Combine variables to make the value more informative. For example, the pattern EDP_VERSION-EDP_COMPONENT will be displayed as 2.7.0-SNAPSHOT-nexus-operator in Jira; For the Component/s field, select the EDP_COMPONENT variable that defines the name of the existing repository. For example, nexus-operator ; For the Labels field, select the EDP_GITTAG variable that defines a tag assigned to the commit in GitHub. For example, build/2.7.0-SNAPSHOT.59 . c. Click the plus icon to add more Jira field names. d. Click the delete icon to remove the Jira field name. Integrate with perf server Select the Integrate with Perf Server check box to enable the integration with the PERF Board ( Project Performance Board ) for monitoring the overall team performance and setting up necessary metrics. Note If this option is needed, please refer to the Perf Server Integration to adjust the integration. After the integration is adjusted, the Integrate with Perf Server check box will appear in the Advanced Settings menu. In the Select Perf Server field, select the name of the Perf server with which the integration should be performed. Click the Proceed button to switch to the next menu. Perf integration Select the necessary DataSource ( Jenkins, Sonar, GitLab ) from which the data should be transferred to the Project Performance Board. Click the Create button, check the CONFIRMATION summary, and click Continue to complete the application addition. Note After the complete adding of the application, inspect the Application Overview part.","title":"The Advanced Settings Menu"},{"location":"user-guide/add-application/#related-articles","text":"Application Overview Delivery Dashboard Diagram Add CD Pipelines Add Other Code Language Adjust GitLab CI Tool Adjust Jira Integration Adjust VCS Integration With Jira Enable VCS Import Strategy Manage Jenkins CI Pipeline Job Provisioner Manage Jenkins Agent Perf Server Integration Promote Application in CD Pipeline","title":"Related Articles"},{"location":"user-guide/add-autotest/","text":"Add Autotests \u2693\ufe0e Admin Console enables to clone or import an autotest, add it to the environment with its subsequent deployment in Gerrit (in case the Clone strategy is used) and building of the Code Review pipeline in Jenkins, as well as to use it for work with an application under development. It is also possible to use autotests as quality gates in a newly created CD pipeline. Info Please refer to the Add Application section for the details on how to add an application codebase type. For the details on how to use autotests as quality gates, please refer to the the Stages Menu section of the Add CD Pipeline documentation. Navigate to the Autotests section on the left-side navigation bar and click the Create button. Once clicked, the three-step menu will appear: The Codebase Info Menu The Autotest Info Menu The Advanced Settings Menu The Codebase Info Menu \u2693\ufe0e There are two available strategies: clone and import. The Clone strategy flow is displayed below: Clone autotest Clone - this strategy allows cloning the autotest from the indicated repository into EPAM Delivery Platform. While cloning the existing repository, you have to fill in the additional fields as well. In the Git Repository URL field, specify the link to the repository with the autotest. With the Clone strategy, select the Codebase Authentication check box and fill in the requested fields: Repository Login \u2013 enter your login data. Repository password (or API Token) \u2013 enter your password or indicate the API Token. Note The Codebase Authentication check box should be selected only in case you clone the private repository. If you define the public repository, there is no need to enter credentials. If there is a necessity to use the Import strategy that allows configuring a replication from the Git server, explore the steps below: Import autotest a. Import - this strategy allows configuring a replication from the Git server. Note In order to use the Import strategy, make sure to adjust it by following the Enable VCS Import Strategy page. b. In the Git Server field, select the necessary Git server from the drop-down list. c. In the Relative path field, indicate the respective path to the repository, e.g. /epmd-edp/examples/basic/edp-auto-tests-simple-example . After completing the Codebase Info menu step, click the Proceed button to switch to the next menu. The Autotest Info Menu \u2693\ufe0e Autotest info Fill in the Autotest Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Info The Import strategy does not have an Autotest Name field. Specify the name of the default branch where you want the development to be performed. Note The default branch cannot be deleted. Type the necessary description in the Description field. In the Autotest Code Language field, select the Java code language (specify Java 8 or Java 11 to be used) and get the default Maven build tool OR add another code language. Selecting Other allows extending the default code languages and get the necessary build tool, for details, inspect the Add Other Code Language section. Note Using the Create strategy does not allow to customize the default code language set. The Select Build Tool field can dispose of the default Maven tool, Gradle or other built tool in accordance with the selected code language. All the autotest reports will be created in the Allure framework that is available in the Autotest Report Framework field by default. Click the Proceed button to switch to the next menu. The Advanced Settings Menu \u2693\ufe0e Advanced settings Select CI pipeline provisioner that will be used to handle a codebase. For details, refer to the Manage Jenkins CI Pipeline Job Provisioner instruction and become familiar with the main steps to add an additional job provisioner. Select Jenkins agent that will be used to handle a codebase. For details, refer to the Manage Jenkins Agent instruction and inspect the steps that should be done to add a new Jenkins agent. Select the necessary codebase versioning type: default - the previous versioning logic that is realized in EDP Admin Console 2.2.0 and lower versions. Using the default versioning type, in order to specify the version of the current artifacts, images, and tags in the Version Control System, a developer should navigate to the corresponding file and change the version manually . edp - the new versioning logic that is available in EDP Admin Console 2.3.0 and subsequent versions. Using the edp versioning type, a developer indicates the version number from which all the artifacts will be versioned and, as a result, automatically registered in the corresponding file (e.g. pom.xml). When selecting the edp versioning type, the extra field will appear: Edp versioning a. Type the version number from which you want the artifacts to be versioned. Note The Start Version From field should be filled out in compliance with the semantic versioning rules, e.g. 1.2.3 or 10.10.10. Please refer to the Semantic Versioning page for details. In the Select CI Tool field, choose the necessary tool: Jenkins or GitLab CI, where Jenkins is the default tool and the GitLab CI tool can be additionally adjusted. For details, please refer to the Adjust GitLab CI Tool page. Note The GitLab CI tool is available only with the Import strategy and makes the Jira integration feature unavailable. Jira integration Select the Integrate with Jira Server check box in case it is required to connect Jira tickets with the commits and have a respective label in the Fix Version field. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Jira Integration page, and Adjust VCS Integration With Jira . Pay attention that the Jira integration feature is not available when using the GitLab CI tool. As soon as the Jira server is set, select it in the Select Jira Server field. Indicate the pattern using any character, which is followed on the project, to validate a commit message. Indicate the pattern using any character, which is followed on the project, to find a Jira ticket number in a commit message. In the Advanced Mapping section, specify the names of the Jira fields that should be filled in with attributes from EDP. Upon clicking the question mark icon, observe the tips on how to indicate and combine variables necessary for identifying the format of values to be displayed. Advance mapping a. Select the name of the field in a Jira ticket. The available fields are the following: Fix Version/s , Component/s and Labels . b. Select the pattern of predefined variables, based on which the value from EDP will be displayed in Jira. Combine several variables to obtain the desired value. For the Fix Version/s field, select the EDP_VERSION variable that represents an EDP upgrade version, as in 2.7.0-SNAPSHOT . Combine variables to make the value more informative. For example, the pattern EDP_VERSION-EDP_COMPONENT will be displayed as 2.7.0-SNAPSHOT-nexus-operator in Jira; For the Component/s field, select the EDP_COMPONENT variable that defines the name of the existing repository. For example, nexus-operator ; For the Labels field, select the EDP_GITTAG variable that defines a tag assigned to the commit in GitHub. For example, build/2.7.0-SNAPSHOT.59 . c. Click the plus icon to add more Jira field names. d. Click the delete icon to remove the Jira field name. Integrate with perf server Select the Integrate with Perf Server check box to enable the integration with the PERF Board ( Project Performance Board ) for monitoring the overall team performance and setting up necessary metrics. Note If this option is needed, please refer to the Perf Server Integration to adjust the integration. After the integration is adjusted, the Integrate with Perf Server check box will appear in the Advanced Settings menu. In the Select Perf Server field, select the name of the Perf server with which the integration should be performed and click the Proceed button to switch to the next menu. Perf integration Select the necessary DataSource ( Jenkins/GitLab, Sonar ) from which the data should be transferred to the Project Performance Board. Click the Create button to create an autotest, check the CONFIRMATION summary, click Continue to add an autotest to the Autotests list. Note After the complete adding of the autotest, inspect the Autotest Overview part. Related Articles \u2693\ufe0e Autotest Overview Delivery Dashboard Diagram Add Application Add CD Pipelines Add Other Code Language Adjust GitLab CI Tool Adjust Jira Integration Adjust VCS Integration With Jira Enable VCS Import Strategy Manage Jenkins CI Pipeline Job Provisioner Manage Jenkins Agent Perf Server Integration","title":"Add Autotests"},{"location":"user-guide/add-autotest/#add-autotests","text":"Admin Console enables to clone or import an autotest, add it to the environment with its subsequent deployment in Gerrit (in case the Clone strategy is used) and building of the Code Review pipeline in Jenkins, as well as to use it for work with an application under development. It is also possible to use autotests as quality gates in a newly created CD pipeline. Info Please refer to the Add Application section for the details on how to add an application codebase type. For the details on how to use autotests as quality gates, please refer to the the Stages Menu section of the Add CD Pipeline documentation. Navigate to the Autotests section on the left-side navigation bar and click the Create button. Once clicked, the three-step menu will appear: The Codebase Info Menu The Autotest Info Menu The Advanced Settings Menu","title":"Add Autotests"},{"location":"user-guide/add-autotest/#the-codebase-info-menu","text":"There are two available strategies: clone and import. The Clone strategy flow is displayed below: Clone autotest Clone - this strategy allows cloning the autotest from the indicated repository into EPAM Delivery Platform. While cloning the existing repository, you have to fill in the additional fields as well. In the Git Repository URL field, specify the link to the repository with the autotest. With the Clone strategy, select the Codebase Authentication check box and fill in the requested fields: Repository Login \u2013 enter your login data. Repository password (or API Token) \u2013 enter your password or indicate the API Token. Note The Codebase Authentication check box should be selected only in case you clone the private repository. If you define the public repository, there is no need to enter credentials. If there is a necessity to use the Import strategy that allows configuring a replication from the Git server, explore the steps below: Import autotest a. Import - this strategy allows configuring a replication from the Git server. Note In order to use the Import strategy, make sure to adjust it by following the Enable VCS Import Strategy page. b. In the Git Server field, select the necessary Git server from the drop-down list. c. In the Relative path field, indicate the respective path to the repository, e.g. /epmd-edp/examples/basic/edp-auto-tests-simple-example . After completing the Codebase Info menu step, click the Proceed button to switch to the next menu.","title":"The Codebase Info Menu"},{"location":"user-guide/add-autotest/#the-autotest-info-menu","text":"Autotest info Fill in the Autotest Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Info The Import strategy does not have an Autotest Name field. Specify the name of the default branch where you want the development to be performed. Note The default branch cannot be deleted. Type the necessary description in the Description field. In the Autotest Code Language field, select the Java code language (specify Java 8 or Java 11 to be used) and get the default Maven build tool OR add another code language. Selecting Other allows extending the default code languages and get the necessary build tool, for details, inspect the Add Other Code Language section. Note Using the Create strategy does not allow to customize the default code language set. The Select Build Tool field can dispose of the default Maven tool, Gradle or other built tool in accordance with the selected code language. All the autotest reports will be created in the Allure framework that is available in the Autotest Report Framework field by default. Click the Proceed button to switch to the next menu.","title":"The Autotest Info Menu"},{"location":"user-guide/add-autotest/#the-advanced-settings-menu","text":"Advanced settings Select CI pipeline provisioner that will be used to handle a codebase. For details, refer to the Manage Jenkins CI Pipeline Job Provisioner instruction and become familiar with the main steps to add an additional job provisioner. Select Jenkins agent that will be used to handle a codebase. For details, refer to the Manage Jenkins Agent instruction and inspect the steps that should be done to add a new Jenkins agent. Select the necessary codebase versioning type: default - the previous versioning logic that is realized in EDP Admin Console 2.2.0 and lower versions. Using the default versioning type, in order to specify the version of the current artifacts, images, and tags in the Version Control System, a developer should navigate to the corresponding file and change the version manually . edp - the new versioning logic that is available in EDP Admin Console 2.3.0 and subsequent versions. Using the edp versioning type, a developer indicates the version number from which all the artifacts will be versioned and, as a result, automatically registered in the corresponding file (e.g. pom.xml). When selecting the edp versioning type, the extra field will appear: Edp versioning a. Type the version number from which you want the artifacts to be versioned. Note The Start Version From field should be filled out in compliance with the semantic versioning rules, e.g. 1.2.3 or 10.10.10. Please refer to the Semantic Versioning page for details. In the Select CI Tool field, choose the necessary tool: Jenkins or GitLab CI, where Jenkins is the default tool and the GitLab CI tool can be additionally adjusted. For details, please refer to the Adjust GitLab CI Tool page. Note The GitLab CI tool is available only with the Import strategy and makes the Jira integration feature unavailable. Jira integration Select the Integrate with Jira Server check box in case it is required to connect Jira tickets with the commits and have a respective label in the Fix Version field. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Jira Integration page, and Adjust VCS Integration With Jira . Pay attention that the Jira integration feature is not available when using the GitLab CI tool. As soon as the Jira server is set, select it in the Select Jira Server field. Indicate the pattern using any character, which is followed on the project, to validate a commit message. Indicate the pattern using any character, which is followed on the project, to find a Jira ticket number in a commit message. In the Advanced Mapping section, specify the names of the Jira fields that should be filled in with attributes from EDP. Upon clicking the question mark icon, observe the tips on how to indicate and combine variables necessary for identifying the format of values to be displayed. Advance mapping a. Select the name of the field in a Jira ticket. The available fields are the following: Fix Version/s , Component/s and Labels . b. Select the pattern of predefined variables, based on which the value from EDP will be displayed in Jira. Combine several variables to obtain the desired value. For the Fix Version/s field, select the EDP_VERSION variable that represents an EDP upgrade version, as in 2.7.0-SNAPSHOT . Combine variables to make the value more informative. For example, the pattern EDP_VERSION-EDP_COMPONENT will be displayed as 2.7.0-SNAPSHOT-nexus-operator in Jira; For the Component/s field, select the EDP_COMPONENT variable that defines the name of the existing repository. For example, nexus-operator ; For the Labels field, select the EDP_GITTAG variable that defines a tag assigned to the commit in GitHub. For example, build/2.7.0-SNAPSHOT.59 . c. Click the plus icon to add more Jira field names. d. Click the delete icon to remove the Jira field name. Integrate with perf server Select the Integrate with Perf Server check box to enable the integration with the PERF Board ( Project Performance Board ) for monitoring the overall team performance and setting up necessary metrics. Note If this option is needed, please refer to the Perf Server Integration to adjust the integration. After the integration is adjusted, the Integrate with Perf Server check box will appear in the Advanced Settings menu. In the Select Perf Server field, select the name of the Perf server with which the integration should be performed and click the Proceed button to switch to the next menu. Perf integration Select the necessary DataSource ( Jenkins/GitLab, Sonar ) from which the data should be transferred to the Project Performance Board. Click the Create button to create an autotest, check the CONFIRMATION summary, click Continue to add an autotest to the Autotests list. Note After the complete adding of the autotest, inspect the Autotest Overview part.","title":"The Advanced Settings Menu"},{"location":"user-guide/add-autotest/#related-articles","text":"Autotest Overview Delivery Dashboard Diagram Add Application Add CD Pipelines Add Other Code Language Adjust GitLab CI Tool Adjust Jira Integration Adjust VCS Integration With Jira Enable VCS Import Strategy Manage Jenkins CI Pipeline Job Provisioner Manage Jenkins Agent Perf Server Integration","title":"Related Articles"},{"location":"user-guide/add-cd-pipeline/","text":"Add CD Pipeline \u2693\ufe0e Admin Console provides the ability to deploy an environment on your own and specify the essential components as well. Navigate to the Continuous Delivery section on the left-side navigation bar and click the Create button. Once clicked, the three-step menu will appear: The Pipeline Menu The Applications Menu The Stages Menu The creation of the CD pipeline becomes available as soon as an application is created including its provisioning in a branch and the necessary entities for the environment. After the complete adding of the CD pipeline, inspect the Check CD Pipeline Availability part. The Pipeline Menu \u2693\ufe0e Create CD pipeline Type the name of the pipeline in the Pipeline Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Note The namespace created by the CD pipeline has the following pattern combination: [cluster name]-[cd pipeline name]-[stage name] . Please be aware that the namespace length should not exceed 63 symbols. Select the deployment type from the drop-down list: Container - the pipeline will be deployed in a Docker container; Custom - this mode allows to deploy non-container applications and customize the Init stage of CD pipeline. Click the Proceed button to switch to the next menu. The Applications Menu \u2693\ufe0e CD pipeline applications Select the check box of the necessary application in the Applications menu. Specify the necessary codebase Docker stream (the output for the branch and other stages from other CD pipelines) from the drop-down menu. Select the Promote in pipeline check box in order to transfer the application from one to another stage by the specified codebase Docker stream. If the Promote in pipeline check box is not selected, the same codebase Docker stream will be deployed regardless of the stage, i.e. the codebase Docker stream input, which was selected for the pipeline, will be always used. Note The newly created CD pipeline has the following pattern combination: [pipeline name]-[branch name]. If there is another deployed CD pipeline stage with the respective codebase Docker stream (= image stream as an OpenShift term), the pattern combination will be as follows: [pipeline name]-[stage name]-[application name]-[verified]. Click the Proceed button to switch to the next menu. The Stages Menu \u2693\ufe0e CD stages Click the plus sign icon in the Stages menu and fill in the necessary fields in the Adding Stage window : Adding stage a. Type the stage name; Note The namespace created by the CD pipeline has the following pattern combination: [cluster name]-[cd pipeline name]-[stage name] . Please be aware that the namespace length should not exceed 63 symbols. b. Enter the description for this stage; c. Select the quality gate type: Manual - means that the promoting process should be confirmed in Jenkins manually; Autotests - means that the promoting process should be confirmed by the successful passing of the autotests. In the additional fields, select the previously created autotest name and specify its branch for the autotest that will be launched on the current stage. Note Please be aware that autotests used in the CD pipeline cannot be removed. For the details on how to create an autotest codebase, please refer to the Add Autotest section. d. Type the step name, which will be displayed in Jenkins, for every quality gate type; e. Add an unlimited number of quality gates by clicking a corresponding plus sign icon and remove them as well by clicking the recycle bin icon; f. Select the trigger type. The available trigger types are manual and auto . By selecting the auto trigger type, the CD pipeline will be launched automatically after the image is built. Every trigger type has a set of default stages that differ by the input stage (auto-deploy-input or manual-deploy-input). Note When changing the Trigger Type, the job-provision automatically will change the set of stages to the corresponding stages set for the CD pipeline. Note Execution sequence. The image promotion and execution of the pipelines depend on the sequence in which the environments are added. Adding stage g. Select the groovy-pipeline library; h. Select the job provisioner. In case of working with non container-based applications, there is an option to use a custom job provisioner. Please refer to the Manage Jenkins CD Job Provision page for details. i. Click the Add button to display it in the Stages menu. Info Perform the same steps as described above if there is a necessity to add one more stage. Continuous delivery menu 9.Edit the stage by clicking its name and applying changes, and remove the added stage by clicking the recycle bin icon next to its name. 10.Click the Create button to start the provisioning of the pipeline. After the CD pipeline is added, the new project with the stage name will be created in OpenShift. Check CD Pipeline Availability \u2693\ufe0e As soon as the CD pipeline is provisioned and added to the CD Pipelines list, there is an ability to: CD page Create another application by clicking the Create button and performing the same steps as described in the Add CD Pipeline section. Select a number of existing CD pipelines to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing CD pipelines in a list by clicking the Name title. The CD pipelines will be displayed in alphabetical order. Search the necessary CD pipeline by entering the corresponding name, language or the build tool into the Search field. Navigate between pages if the number of CD pipelines exceeds the capacity of a single page. Edit CD Pipeline \u2693\ufe0e Edit the CD pipeline by clicking the pen icon next to its name in the CD Pipelines list: Edit CD pipeline apply the necessary changes (the list of applications for deploy, image streams, and promotion in the pipeline) and click the Proceed button to confirm the editions: Edit CD pipeline page add new extra stages steps by clicking the plus sign icon and filling in the necessary fields in the Adding Stage window. Add stages Note The added stage will appear in the Stages menu allowing to review its details or delete. Check the CD pipeline data and details by clicking the CD pipeline name in the CD Pipelines list: Link to Jenkins the main link on the top of the details page refers to Jenkins; Edit icon the pen icon refers to the same Edit CD Pipeline page as mentioned above and allows to apply the necessary changes; the Applications menu has the main information about the applications with the respective codebase Docker streams and links to Jenkins and Gerrit as well as the signification of the promotion in CD pipeline; the Stages menu includes the stages data that was previously mentioned, the direct links to the respective to every stage Kubernetes/OpenShift page, and the link to the Autotest details page in case there are added autotests. To enable or disable auto deployment of a specific stage, click the pen icon and select the necessary trigger type from the drop-down list. Edit trigger type Note The deletion of stages is performed sequentially, starting from the latest created stage. In order to remove a stage , click the corresponding delete icon, type the CD pipeline name and confirm the deletion by clicking the Delete button. If you remove the last stage, the whole CD pipeline will be removed as the CD pipeline does not exist without stages. the Deployed Version menu indicates the applications and stages with the appropriate status. The status will be changed after stage deployment. Deployed versions the Status Info menu displays all the actions that were performed during the deployment process: Status info Remove the added CD pipeline: Remove CD pipeline Info If there is a necessity to create another CD pipeline, navigate to the Continuous Delivery section, click the Create button and perform the same steps as described above. Info In OpenShift, if the deployment fails with the ImagePullBackOff error, delete the POD. Related Articles \u2693\ufe0e Add Autotest EDP Admin Console Customize CD Pipeline Delivery Dashboard Diagram Promote Application in CD Pipeline Manage Jenkins CD Pipeline Job Provision","title":"Add CD Pipeline"},{"location":"user-guide/add-cd-pipeline/#add-cd-pipeline","text":"Admin Console provides the ability to deploy an environment on your own and specify the essential components as well. Navigate to the Continuous Delivery section on the left-side navigation bar and click the Create button. Once clicked, the three-step menu will appear: The Pipeline Menu The Applications Menu The Stages Menu The creation of the CD pipeline becomes available as soon as an application is created including its provisioning in a branch and the necessary entities for the environment. After the complete adding of the CD pipeline, inspect the Check CD Pipeline Availability part.","title":"Add CD Pipeline"},{"location":"user-guide/add-cd-pipeline/#the-pipeline-menu","text":"Create CD pipeline Type the name of the pipeline in the Pipeline Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Note The namespace created by the CD pipeline has the following pattern combination: [cluster name]-[cd pipeline name]-[stage name] . Please be aware that the namespace length should not exceed 63 symbols. Select the deployment type from the drop-down list: Container - the pipeline will be deployed in a Docker container; Custom - this mode allows to deploy non-container applications and customize the Init stage of CD pipeline. Click the Proceed button to switch to the next menu.","title":"The Pipeline Menu"},{"location":"user-guide/add-cd-pipeline/#the-applications-menu","text":"CD pipeline applications Select the check box of the necessary application in the Applications menu. Specify the necessary codebase Docker stream (the output for the branch and other stages from other CD pipelines) from the drop-down menu. Select the Promote in pipeline check box in order to transfer the application from one to another stage by the specified codebase Docker stream. If the Promote in pipeline check box is not selected, the same codebase Docker stream will be deployed regardless of the stage, i.e. the codebase Docker stream input, which was selected for the pipeline, will be always used. Note The newly created CD pipeline has the following pattern combination: [pipeline name]-[branch name]. If there is another deployed CD pipeline stage with the respective codebase Docker stream (= image stream as an OpenShift term), the pattern combination will be as follows: [pipeline name]-[stage name]-[application name]-[verified]. Click the Proceed button to switch to the next menu.","title":"The Applications Menu"},{"location":"user-guide/add-cd-pipeline/#the-stages-menu","text":"CD stages Click the plus sign icon in the Stages menu and fill in the necessary fields in the Adding Stage window : Adding stage a. Type the stage name; Note The namespace created by the CD pipeline has the following pattern combination: [cluster name]-[cd pipeline name]-[stage name] . Please be aware that the namespace length should not exceed 63 symbols. b. Enter the description for this stage; c. Select the quality gate type: Manual - means that the promoting process should be confirmed in Jenkins manually; Autotests - means that the promoting process should be confirmed by the successful passing of the autotests. In the additional fields, select the previously created autotest name and specify its branch for the autotest that will be launched on the current stage. Note Please be aware that autotests used in the CD pipeline cannot be removed. For the details on how to create an autotest codebase, please refer to the Add Autotest section. d. Type the step name, which will be displayed in Jenkins, for every quality gate type; e. Add an unlimited number of quality gates by clicking a corresponding plus sign icon and remove them as well by clicking the recycle bin icon; f. Select the trigger type. The available trigger types are manual and auto . By selecting the auto trigger type, the CD pipeline will be launched automatically after the image is built. Every trigger type has a set of default stages that differ by the input stage (auto-deploy-input or manual-deploy-input). Note When changing the Trigger Type, the job-provision automatically will change the set of stages to the corresponding stages set for the CD pipeline. Note Execution sequence. The image promotion and execution of the pipelines depend on the sequence in which the environments are added. Adding stage g. Select the groovy-pipeline library; h. Select the job provisioner. In case of working with non container-based applications, there is an option to use a custom job provisioner. Please refer to the Manage Jenkins CD Job Provision page for details. i. Click the Add button to display it in the Stages menu. Info Perform the same steps as described above if there is a necessity to add one more stage. Continuous delivery menu 9.Edit the stage by clicking its name and applying changes, and remove the added stage by clicking the recycle bin icon next to its name. 10.Click the Create button to start the provisioning of the pipeline. After the CD pipeline is added, the new project with the stage name will be created in OpenShift.","title":"The Stages Menu"},{"location":"user-guide/add-cd-pipeline/#check-cd-pipeline-availability","text":"As soon as the CD pipeline is provisioned and added to the CD Pipelines list, there is an ability to: CD page Create another application by clicking the Create button and performing the same steps as described in the Add CD Pipeline section. Select a number of existing CD pipelines to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing CD pipelines in a list by clicking the Name title. The CD pipelines will be displayed in alphabetical order. Search the necessary CD pipeline by entering the corresponding name, language or the build tool into the Search field. Navigate between pages if the number of CD pipelines exceeds the capacity of a single page.","title":"Check CD Pipeline Availability"},{"location":"user-guide/add-cd-pipeline/#edit-cd-pipeline","text":"Edit the CD pipeline by clicking the pen icon next to its name in the CD Pipelines list: Edit CD pipeline apply the necessary changes (the list of applications for deploy, image streams, and promotion in the pipeline) and click the Proceed button to confirm the editions: Edit CD pipeline page add new extra stages steps by clicking the plus sign icon and filling in the necessary fields in the Adding Stage window. Add stages Note The added stage will appear in the Stages menu allowing to review its details or delete. Check the CD pipeline data and details by clicking the CD pipeline name in the CD Pipelines list: Link to Jenkins the main link on the top of the details page refers to Jenkins; Edit icon the pen icon refers to the same Edit CD Pipeline page as mentioned above and allows to apply the necessary changes; the Applications menu has the main information about the applications with the respective codebase Docker streams and links to Jenkins and Gerrit as well as the signification of the promotion in CD pipeline; the Stages menu includes the stages data that was previously mentioned, the direct links to the respective to every stage Kubernetes/OpenShift page, and the link to the Autotest details page in case there are added autotests. To enable or disable auto deployment of a specific stage, click the pen icon and select the necessary trigger type from the drop-down list. Edit trigger type Note The deletion of stages is performed sequentially, starting from the latest created stage. In order to remove a stage , click the corresponding delete icon, type the CD pipeline name and confirm the deletion by clicking the Delete button. If you remove the last stage, the whole CD pipeline will be removed as the CD pipeline does not exist without stages. the Deployed Version menu indicates the applications and stages with the appropriate status. The status will be changed after stage deployment. Deployed versions the Status Info menu displays all the actions that were performed during the deployment process: Status info Remove the added CD pipeline: Remove CD pipeline Info If there is a necessity to create another CD pipeline, navigate to the Continuous Delivery section, click the Create button and perform the same steps as described above. Info In OpenShift, if the deployment fails with the ImagePullBackOff error, delete the POD.","title":"Edit CD Pipeline"},{"location":"user-guide/add-cd-pipeline/#related-articles","text":"Add Autotest EDP Admin Console Customize CD Pipeline Delivery Dashboard Diagram Promote Application in CD Pipeline Manage Jenkins CD Pipeline Job Provision","title":"Related Articles"},{"location":"user-guide/add-custom-global-pipeline-lib/","text":"Add a Custom Global Pipeline Library \u2693\ufe0e In order to add a new custom global pipeline library, perform the steps below: Navigate to Jenkins and go to Manage Jenkins -> Configure System -> Global Pipeline Libraries . Note It is possible to configure as many libraries as necessary. Since these libraries will be globally usable, any pipeline in the system can utilize the functionality implemented in these libraries. Specify the following values: Add custom library a - The name of a custom library; b - The version which can be branched, tagged or hashed of a commit; c - Allows pipelines the immediate using of classes or global variables defined by any libraries; d - Allows using the default version of the configured shared-library when the Load implicitly check box is selected; e - Allows using the default version of the configured shared-library if the pipeline references to the library only by the name, for example, @Library('my-shared-library') . Note If the Default version check box is not defined , the pipeline must specify a version, for example, @Library('my-shared-library@master') . If the Allow default version to be overridden check box is enabled in the Shared Library\u2019s configuration, a @Library annotation may also override the default version defined for the library. This also enables the library with the selected Load implicitly check box to be loaded from a different version if necessary. Source code management f - The URL of the repository; g - The credentials for the repository. Use the Custom Global Pipeline Libraries on the pipeline, for example: Pipeline @Library ( [ ' edp - library - stages ' , ' edp - library - pipelines ' , ' edp - custom - shared - library - name ' ] ) _ Build () Note edp-custom-shared-library-name is the name of the Custom Global Pipeline Library that should be added to the Jenkins Global Settings. Related Articles \u2693\ufe0e Jenkins Official Documentation: Extending with Shared Libraries","title":"Add Custom Pipeline Library"},{"location":"user-guide/add-custom-global-pipeline-lib/#add-a-custom-global-pipeline-library","text":"In order to add a new custom global pipeline library, perform the steps below: Navigate to Jenkins and go to Manage Jenkins -> Configure System -> Global Pipeline Libraries . Note It is possible to configure as many libraries as necessary. Since these libraries will be globally usable, any pipeline in the system can utilize the functionality implemented in these libraries. Specify the following values: Add custom library a - The name of a custom library; b - The version which can be branched, tagged or hashed of a commit; c - Allows pipelines the immediate using of classes or global variables defined by any libraries; d - Allows using the default version of the configured shared-library when the Load implicitly check box is selected; e - Allows using the default version of the configured shared-library if the pipeline references to the library only by the name, for example, @Library('my-shared-library') . Note If the Default version check box is not defined , the pipeline must specify a version, for example, @Library('my-shared-library@master') . If the Allow default version to be overridden check box is enabled in the Shared Library\u2019s configuration, a @Library annotation may also override the default version defined for the library. This also enables the library with the selected Load implicitly check box to be loaded from a different version if necessary. Source code management f - The URL of the repository; g - The credentials for the repository. Use the Custom Global Pipeline Libraries on the pipeline, for example: Pipeline @Library ( [ ' edp - library - stages ' , ' edp - library - pipelines ' , ' edp - custom - shared - library - name ' ] ) _ Build () Note edp-custom-shared-library-name is the name of the Custom Global Pipeline Library that should be added to the Jenkins Global Settings.","title":"Add a Custom Global Pipeline Library"},{"location":"user-guide/add-custom-global-pipeline-lib/#related-articles","text":"Jenkins Official Documentation: Extending with Shared Libraries","title":"Related Articles"},{"location":"user-guide/add-library/","text":"Add Library \u2693\ufe0e Admin Console helps to create, clone or import a library and add it to the environment. It can also be deployed in Gerrit (if the Clone or Create strategy is used) with the Code Review and Build pipelines built in Jenkins. Navigate to the Libraries section on the left-side navigation bar and click the Create button. Once clicked, the three-step menu will appear: The Codebase Info Menu The Library Info Menu The Advanced Settings Menu The Codebase Info Menu \u2693\ufe0e Create library In the Codebase Integration Strategy field, select the necessary option that is the configuration strategy for the replication with Gerrit: Create \u2013 creates a project on the pattern in accordance with a code language, a build tool, and a framework. Clone \u2013 clones the indicated repository into EPAM Delivery Platform. While cloning the existing repository, it is required to fill in the additional fields as well. Import - allows configuring a replication from the Git server. While importing the existing repository, select the Git server and define the respective path to the repository. Note In order to use the Import strategy, make sure to adjust it by following the Enable VCS Import Strategy page. In the Git Repository URL field, specify the link to the repository that is to be cloned. With the Clone strategy, select the Codebase Authentication check box and fill in the requested fields: Repository Login \u2013 enter your login data. Repository password (or API Token) \u2013 enter your password or indicate the API Token. Note The Codebase Authentication check box should be selected only in case you clone the private repository. If you define the public repository, there is no need to enter credentials. Click the Proceed button to switch to the next menu. The Library Info Menu \u2693\ufe0e Library info Type the name of the library in the Library Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Info If the Import strategy is used, the Library Name field will not be displayed. Specify the name of the default branch where you want the development to be performed. Note The default branch cannot be deleted. To create a library with an empty repository in Gerrit, select the Empty project check box. Note The empty repository option is available only for the Create strategy. Select any of the supported code languages in the Library Code Language block: Java \u2013 selecting Java allows specify Java 8 or Java 11, and further usage of the Gradle or Maven tool. JavaScript - selecting JavaScript allows using the NPM tool. DotNet - selecting DotNet allows using the DotNet v.2.1 and DotNet v.3.1. Groovy-pipeline - selecting Groovy-pipeline allows having the ability to customize a stages logic. For details, please refer to the Customize CD Pipeline page. Python - selecting Python allows using the Python v.3.8. Terraform - selecting Terraform allows using the Terraform different versions via the Terraform version manager ( tfenv ). EDP supports all actions available in Terraform, thus providing the ability to modify the virtual infrastructure and launch some checks with the help of linters. For details, please refer to the Use Terraform Library in EDP page. Rego - this option allows using Rego code language with an Open Policy Agent (OPA) Library. For details, please refer to the Use Open Policy Agent page. Container - this option allows using the Kaniko tool for building the container images from a Dockerfile. For details, please refer to the CI Pipeline for Container page. Other - selecting Other allows extending the default code languages when creating a codebase with the Clone/Import strategy. To add another code language, inspect the Add Other Code Language page. Note The Create strategy does not allow to customize the default code language set. The Select Build Tool field disposes of the default tools and can be changed in accordance with the selected code language. Click the Proceed button to switch to the next menu. The Advanced Settings Menu \u2693\ufe0e Advanced settings Select the CI pipeline provisioner that will be used to handle a codebase. For details, refer to the Manage Jenkins CI Pipeline Job Provisioner instruction and become familiar with the main steps to add an additional job provisioner. Select Jenkins agent that will be used to handle a codebase. For details, refer to the Manage Jenkins Agent instruction and inspect the steps that should be done to add a new Jenkins agent. Select the necessary codebase versioning type: default - the previous versioning logic that is realized in EDP Admin Console 2.2.0 and lower versions. Using the default versioning type, in order to specify the version of the current artifacts, images, and tags in the Version Control System, a developer should navigate to the corresponding file and change the version manually . edp - the new versioning logic that is available in EDP Admin Console 2.3.0 and subsequent versions. Using the edp versioning type, a developer indicates the version number from which all the artifacts will be versioned and, as a result, automatically registered in the corresponding file (e.g. pom.xml). When selecting the edp versioning type, the extra field will appear: EDP versioning a. Type the version number from which you want the artifacts to be versioned. Note The Start Version From field should be filled out in compliance with the semantic versioning rules, e.g. 1.2.3 or 10.10.10. Please refer to the Semantic Versioning page for details. In the Select CI Tool field, choose the necessary tool: Jenkins or GitLab CI, where Jenkins is the default tool and the GitLab CI tool can be additionally adjusted. For details, please refer to the Adjust GitLab CI Tool page. Note The GitLab CI tool is available only with the Import strategy and makes the Jira integration feature unavailable. Integrate with Jira server Select the Integrate with Jira Server check box in case it is required to connect Jira tickets with the commits and have a respective label in the Fix Version field. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Jira Integration page, and Adjust VCS Integration With Jira . Pay attention that the Jira integration feature is not available when using the GitLab CI tool. As soon as the Jira server is set, select it in the Select Jira Server field. Indicate the pattern using any character, which is followed on the project, to validate a commit message. Indicate the pattern using any character, which is followed on the project, to find a Jira ticket number in a commit message. In the Advanced Mapping section, specify the names of the Jira fields that should be filled in with attributes from EDP. Upon clicking the question mark icon, observe the tips on how to indicate and combine variables necessary for identifying the format of values to be displayed. Advanced mapping a. Select the name of the field in a Jira ticket. The available fields are the following: Fix Version/s , Component/s and Labels . b. Select the pattern of predefined variables, based on which the value from EDP will be displayed in Jira. Combine several variables to obtain the desired value. For the Fix Version/s field, select the EDP_VERSION variable that represents an EDP upgrade version, as in 2.7.0-SNAPSHOT . Combine variables to make the value more informative. For example, the pattern EDP_VERSION-EDP_COMPONENT will be displayed as 2.7.0-SNAPSHOT-nexus-operator in Jira; For the Component/s field select the EDP_COMPONENT variable that defines the name of the existing repository. For example, nexus-operator ; For the Labels field select the EDP_GITTAG variable that defines a tag assigned to the commit in Git Hub. For example, build/2.7.0-SNAPSHOT.59 . c. Click the plus icon to add more Jira field names. d. Click the delete icon to remove the Jira field name. Integrate with Perf server Select the Integrate with Perf Server check box to enable the integration with the PERF Board ( Project Performance Board ) for monitoring the overall team performance and setting up necessary metrics. Note If this option is needed, please refer to the Perf Server Integration to adjust the integration. After the integration is adjusted, the Integrate with Perf Server check box will appear in the Advanced Settings menu. In the Select Perf Server field, select the name of the Perf server with which the integration should be performed. Click the Proceed button to switch to the next menu. Perf integration Select the necessary DataSource ( Jenkins/GitLab, Sonar ) from which the data should be transferred to the Project Performance Board. Click the Create button, check the CONFIRMATION summary, and click Continue to add the library to the Libraries list. Note After the complete adding of the library, inspect the Library Overview part. Related Articles \u2693\ufe0e Library Overview Delivery Dashboard Diagram Add CD Pipeline Add Other Code Language Adjust GitLab CI Tool Adjust Jira Integration Adjust VCS Integration With Jira Enable VCS Import Strategy Manage Jenkins CI Pipeline Job Provisioner Manage Jenkins Agent Perf Server Integration Use Terraform Library in EDP Use Open Policy Agent Library in EDP","title":"Add Library"},{"location":"user-guide/add-library/#add-library","text":"Admin Console helps to create, clone or import a library and add it to the environment. It can also be deployed in Gerrit (if the Clone or Create strategy is used) with the Code Review and Build pipelines built in Jenkins. Navigate to the Libraries section on the left-side navigation bar and click the Create button. Once clicked, the three-step menu will appear: The Codebase Info Menu The Library Info Menu The Advanced Settings Menu","title":"Add Library"},{"location":"user-guide/add-library/#the-codebase-info-menu","text":"Create library In the Codebase Integration Strategy field, select the necessary option that is the configuration strategy for the replication with Gerrit: Create \u2013 creates a project on the pattern in accordance with a code language, a build tool, and a framework. Clone \u2013 clones the indicated repository into EPAM Delivery Platform. While cloning the existing repository, it is required to fill in the additional fields as well. Import - allows configuring a replication from the Git server. While importing the existing repository, select the Git server and define the respective path to the repository. Note In order to use the Import strategy, make sure to adjust it by following the Enable VCS Import Strategy page. In the Git Repository URL field, specify the link to the repository that is to be cloned. With the Clone strategy, select the Codebase Authentication check box and fill in the requested fields: Repository Login \u2013 enter your login data. Repository password (or API Token) \u2013 enter your password or indicate the API Token. Note The Codebase Authentication check box should be selected only in case you clone the private repository. If you define the public repository, there is no need to enter credentials. Click the Proceed button to switch to the next menu.","title":"The Codebase Info Menu"},{"location":"user-guide/add-library/#the-library-info-menu","text":"Library info Type the name of the library in the Library Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Info If the Import strategy is used, the Library Name field will not be displayed. Specify the name of the default branch where you want the development to be performed. Note The default branch cannot be deleted. To create a library with an empty repository in Gerrit, select the Empty project check box. Note The empty repository option is available only for the Create strategy. Select any of the supported code languages in the Library Code Language block: Java \u2013 selecting Java allows specify Java 8 or Java 11, and further usage of the Gradle or Maven tool. JavaScript - selecting JavaScript allows using the NPM tool. DotNet - selecting DotNet allows using the DotNet v.2.1 and DotNet v.3.1. Groovy-pipeline - selecting Groovy-pipeline allows having the ability to customize a stages logic. For details, please refer to the Customize CD Pipeline page. Python - selecting Python allows using the Python v.3.8. Terraform - selecting Terraform allows using the Terraform different versions via the Terraform version manager ( tfenv ). EDP supports all actions available in Terraform, thus providing the ability to modify the virtual infrastructure and launch some checks with the help of linters. For details, please refer to the Use Terraform Library in EDP page. Rego - this option allows using Rego code language with an Open Policy Agent (OPA) Library. For details, please refer to the Use Open Policy Agent page. Container - this option allows using the Kaniko tool for building the container images from a Dockerfile. For details, please refer to the CI Pipeline for Container page. Other - selecting Other allows extending the default code languages when creating a codebase with the Clone/Import strategy. To add another code language, inspect the Add Other Code Language page. Note The Create strategy does not allow to customize the default code language set. The Select Build Tool field disposes of the default tools and can be changed in accordance with the selected code language. Click the Proceed button to switch to the next menu.","title":"The Library Info Menu"},{"location":"user-guide/add-library/#the-advanced-settings-menu","text":"Advanced settings Select the CI pipeline provisioner that will be used to handle a codebase. For details, refer to the Manage Jenkins CI Pipeline Job Provisioner instruction and become familiar with the main steps to add an additional job provisioner. Select Jenkins agent that will be used to handle a codebase. For details, refer to the Manage Jenkins Agent instruction and inspect the steps that should be done to add a new Jenkins agent. Select the necessary codebase versioning type: default - the previous versioning logic that is realized in EDP Admin Console 2.2.0 and lower versions. Using the default versioning type, in order to specify the version of the current artifacts, images, and tags in the Version Control System, a developer should navigate to the corresponding file and change the version manually . edp - the new versioning logic that is available in EDP Admin Console 2.3.0 and subsequent versions. Using the edp versioning type, a developer indicates the version number from which all the artifacts will be versioned and, as a result, automatically registered in the corresponding file (e.g. pom.xml). When selecting the edp versioning type, the extra field will appear: EDP versioning a. Type the version number from which you want the artifacts to be versioned. Note The Start Version From field should be filled out in compliance with the semantic versioning rules, e.g. 1.2.3 or 10.10.10. Please refer to the Semantic Versioning page for details. In the Select CI Tool field, choose the necessary tool: Jenkins or GitLab CI, where Jenkins is the default tool and the GitLab CI tool can be additionally adjusted. For details, please refer to the Adjust GitLab CI Tool page. Note The GitLab CI tool is available only with the Import strategy and makes the Jira integration feature unavailable. Integrate with Jira server Select the Integrate with Jira Server check box in case it is required to connect Jira tickets with the commits and have a respective label in the Fix Version field. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Jira Integration page, and Adjust VCS Integration With Jira . Pay attention that the Jira integration feature is not available when using the GitLab CI tool. As soon as the Jira server is set, select it in the Select Jira Server field. Indicate the pattern using any character, which is followed on the project, to validate a commit message. Indicate the pattern using any character, which is followed on the project, to find a Jira ticket number in a commit message. In the Advanced Mapping section, specify the names of the Jira fields that should be filled in with attributes from EDP. Upon clicking the question mark icon, observe the tips on how to indicate and combine variables necessary for identifying the format of values to be displayed. Advanced mapping a. Select the name of the field in a Jira ticket. The available fields are the following: Fix Version/s , Component/s and Labels . b. Select the pattern of predefined variables, based on which the value from EDP will be displayed in Jira. Combine several variables to obtain the desired value. For the Fix Version/s field, select the EDP_VERSION variable that represents an EDP upgrade version, as in 2.7.0-SNAPSHOT . Combine variables to make the value more informative. For example, the pattern EDP_VERSION-EDP_COMPONENT will be displayed as 2.7.0-SNAPSHOT-nexus-operator in Jira; For the Component/s field select the EDP_COMPONENT variable that defines the name of the existing repository. For example, nexus-operator ; For the Labels field select the EDP_GITTAG variable that defines a tag assigned to the commit in Git Hub. For example, build/2.7.0-SNAPSHOT.59 . c. Click the plus icon to add more Jira field names. d. Click the delete icon to remove the Jira field name. Integrate with Perf server Select the Integrate with Perf Server check box to enable the integration with the PERF Board ( Project Performance Board ) for monitoring the overall team performance and setting up necessary metrics. Note If this option is needed, please refer to the Perf Server Integration to adjust the integration. After the integration is adjusted, the Integrate with Perf Server check box will appear in the Advanced Settings menu. In the Select Perf Server field, select the name of the Perf server with which the integration should be performed. Click the Proceed button to switch to the next menu. Perf integration Select the necessary DataSource ( Jenkins/GitLab, Sonar ) from which the data should be transferred to the Project Performance Board. Click the Create button, check the CONFIRMATION summary, and click Continue to add the library to the Libraries list. Note After the complete adding of the library, inspect the Library Overview part.","title":"The Advanced Settings Menu"},{"location":"user-guide/add-library/#related-articles","text":"Library Overview Delivery Dashboard Diagram Add CD Pipeline Add Other Code Language Adjust GitLab CI Tool Adjust Jira Integration Adjust VCS Integration With Jira Enable VCS Import Strategy Manage Jenkins CI Pipeline Job Provisioner Manage Jenkins Agent Perf Server Integration Use Terraform Library in EDP Use Open Policy Agent Library in EDP","title":"Related Articles"},{"location":"user-guide/application/","text":"Application \u2693\ufe0e This section describes the subsequent possible actions that can be performed with the newly added or existing applications. Check and Remove Application \u2693\ufe0e As soon as the application is successfully provisioned, the following will be created: Code Review and Build pipelines in Jenkins for this application. The Build pipeline will be triggered automatically if at least one environment is already added. A new project in Gerrit or another VCS. SonarQube integration will be available after the Build pipeline in Jenkins is passed. Nexus Repository Manager will be available after the Build pipeline in Jenkins is passed as well. Info To navigate quickly to OpenShift, Jenkins, Gerrit, SonarQube, Nexus, and other resources, click the Overview section on the navigation bar and hit the necessary link. The added application will be listed in the Applications list allowing you to do the following: Applications menu Create another application by clicking the Create button and performing the same steps as described in the Add Applications section. Open application data by clicking its link name. Once clicked, the following blocks will be displayed: General Info - displays common information about the created/cloned/imported application. Advanced Settings - displays the specified job provisioner, Jenkins agent, deployment script, and the versioning type with the start versioning from number (the latter two fields appear in case of edp versioning type). Branches - displays the status and name of the deployment branch, keeps the additional links to Jenkins and Gerrit. In case of edp versioning type, there are two additional fields: Build Number - indicates the current build number; Last Successful Build - indicates the last successful build number. Status Info - displays all the actions that were performed during the creation/cloning/importing process. Edit the application codebase by clicking the pencil icon. For details see the Edit Existing Codebase section. Remove application with the corresponding database and Jenkins pipelines: Click the delete icon next to the application name; Type the required application name; Confirm the deletion by clicking the Delete button. Note The application that is used in a CD pipeline cannot be removed. Applications menu Select a number of existing applications to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing applications in a list by clicking the Name title. The applications will be displayed in alphabetical order. Search the necessary application by entering the corresponding name, language or the build tool into the Search field. The search can be performed by the application name, language or a build tool. Navigate between pages if the number of applications exceeds the capacity of a single page. Add a New Branch \u2693\ufe0e Note Pay attention when specifying the branch name: the branch name is involved in the formation of the application version, so it must comply with the versioning semantic rules for the application. When adding an application, the default branch is a master branch. In order to add a new branch, follow the steps below: Navigate to the Branches block and click the Create button:] Add branch Fill in the required fields: New branch a. Release Branch - select the Release Branch check box if you need to create a release branch; b. Branch Name - type the branch name. Pay attention that this field remains static if you create a release branch. c. From Commit Hash - paste the commit hash from which the new branch will be created. Note that if the From Commit Hash field is empty, the latest commit from the branch name will be used. d. Branch Version - enter the necessary branch version for the artifact. The Release Candidate (RC) postfix is concatenated to the branch version number. e. Master Branch Version - type the branch version that will be used in a master branch after the release creation. The Snapshot postfix is concatenated to the master branch version number; f. Click the Proceed button and wait until the new branch will be added to the list. Info Adding of a new branch is indicated in the context of the edp versioning type. To get more detailed information on how to add a branch using the default versioning type, please refer here . The default application repository is cloned and changed to the new indicated version before the build, i.e. the new indicated version will not be committed to the repository; thus, the existing repository will keep the default version. Edit Existing Codebase \u2693\ufe0e The EDP Admin Console provides the ability to enable, disable or edit the Jira Integration functionality for applications via the Edit Codebase page. Perform the editing from one of the following sections on the Admin Console interface: Edit codebase Navigate to the codebase overview page and click the pencil icon, or Edit codebase Navigate to the codebase list page and click the pencil icon. Edit codebase To enable Jira integration, on the Edit Codebase page do the following: mark the Integrate with Jira server check box and fill in the necessary fields; click the Proceed button to apply the changes; navigate to Jenkins and add the create-jira-issue-metadata stage in the Build pipeline. Also add the commit-validate stage in the Code Review pipeline. To disable Jira integration, on the Edit Codebase page do the following: unmark the Integrate with Jira server check box; click the Proceed button to apply the changes; navigate to Jenkins and remove the create-jira-issue-metadata stage in the Build pipeline. Also remove the commit-validate stage in the Code Review pipeline. As a result, the necessary changes will be applied. Remove Branch \u2693\ufe0e In order to remove the added branch with the corresponding record in the Admin Console database, do the following: Navigate to the Branches block by clicking the application name link in the Applications list; Click the delete icon related to the necessary branch: Remove branch Enter the branch name and click the Delete button; Note The default master branch cannot be removed. Related Articles \u2693\ufe0e Add Application Promote Application in CD Pipeline","title":"Overview"},{"location":"user-guide/application/#application","text":"This section describes the subsequent possible actions that can be performed with the newly added or existing applications.","title":"Application"},{"location":"user-guide/application/#check-and-remove-application","text":"As soon as the application is successfully provisioned, the following will be created: Code Review and Build pipelines in Jenkins for this application. The Build pipeline will be triggered automatically if at least one environment is already added. A new project in Gerrit or another VCS. SonarQube integration will be available after the Build pipeline in Jenkins is passed. Nexus Repository Manager will be available after the Build pipeline in Jenkins is passed as well. Info To navigate quickly to OpenShift, Jenkins, Gerrit, SonarQube, Nexus, and other resources, click the Overview section on the navigation bar and hit the necessary link. The added application will be listed in the Applications list allowing you to do the following: Applications menu Create another application by clicking the Create button and performing the same steps as described in the Add Applications section. Open application data by clicking its link name. Once clicked, the following blocks will be displayed: General Info - displays common information about the created/cloned/imported application. Advanced Settings - displays the specified job provisioner, Jenkins agent, deployment script, and the versioning type with the start versioning from number (the latter two fields appear in case of edp versioning type). Branches - displays the status and name of the deployment branch, keeps the additional links to Jenkins and Gerrit. In case of edp versioning type, there are two additional fields: Build Number - indicates the current build number; Last Successful Build - indicates the last successful build number. Status Info - displays all the actions that were performed during the creation/cloning/importing process. Edit the application codebase by clicking the pencil icon. For details see the Edit Existing Codebase section. Remove application with the corresponding database and Jenkins pipelines: Click the delete icon next to the application name; Type the required application name; Confirm the deletion by clicking the Delete button. Note The application that is used in a CD pipeline cannot be removed. Applications menu Select a number of existing applications to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing applications in a list by clicking the Name title. The applications will be displayed in alphabetical order. Search the necessary application by entering the corresponding name, language or the build tool into the Search field. The search can be performed by the application name, language or a build tool. Navigate between pages if the number of applications exceeds the capacity of a single page.","title":"Check and Remove Application"},{"location":"user-guide/application/#add-a-new-branch","text":"Note Pay attention when specifying the branch name: the branch name is involved in the formation of the application version, so it must comply with the versioning semantic rules for the application. When adding an application, the default branch is a master branch. In order to add a new branch, follow the steps below: Navigate to the Branches block and click the Create button:] Add branch Fill in the required fields: New branch a. Release Branch - select the Release Branch check box if you need to create a release branch; b. Branch Name - type the branch name. Pay attention that this field remains static if you create a release branch. c. From Commit Hash - paste the commit hash from which the new branch will be created. Note that if the From Commit Hash field is empty, the latest commit from the branch name will be used. d. Branch Version - enter the necessary branch version for the artifact. The Release Candidate (RC) postfix is concatenated to the branch version number. e. Master Branch Version - type the branch version that will be used in a master branch after the release creation. The Snapshot postfix is concatenated to the master branch version number; f. Click the Proceed button and wait until the new branch will be added to the list. Info Adding of a new branch is indicated in the context of the edp versioning type. To get more detailed information on how to add a branch using the default versioning type, please refer here . The default application repository is cloned and changed to the new indicated version before the build, i.e. the new indicated version will not be committed to the repository; thus, the existing repository will keep the default version.","title":"Add a New Branch"},{"location":"user-guide/application/#edit-existing-codebase","text":"The EDP Admin Console provides the ability to enable, disable or edit the Jira Integration functionality for applications via the Edit Codebase page. Perform the editing from one of the following sections on the Admin Console interface: Edit codebase Navigate to the codebase overview page and click the pencil icon, or Edit codebase Navigate to the codebase list page and click the pencil icon. Edit codebase To enable Jira integration, on the Edit Codebase page do the following: mark the Integrate with Jira server check box and fill in the necessary fields; click the Proceed button to apply the changes; navigate to Jenkins and add the create-jira-issue-metadata stage in the Build pipeline. Also add the commit-validate stage in the Code Review pipeline. To disable Jira integration, on the Edit Codebase page do the following: unmark the Integrate with Jira server check box; click the Proceed button to apply the changes; navigate to Jenkins and remove the create-jira-issue-metadata stage in the Build pipeline. Also remove the commit-validate stage in the Code Review pipeline. As a result, the necessary changes will be applied.","title":"Edit Existing Codebase"},{"location":"user-guide/application/#remove-branch","text":"In order to remove the added branch with the corresponding record in the Admin Console database, do the following: Navigate to the Branches block by clicking the application name link in the Applications list; Click the delete icon related to the necessary branch: Remove branch Enter the branch name and click the Delete button; Note The default master branch cannot be removed.","title":"Remove Branch"},{"location":"user-guide/application/#related-articles","text":"Add Application Promote Application in CD Pipeline","title":"Related Articles"},{"location":"user-guide/autotest/","text":"Autotest \u2693\ufe0e This section describes the subsequent possible actions that can be performed with the newly added or existing autotests. Check and Remove Autotest \u2693\ufe0e As soon as the autotest is successfully provisioned, the following will be created: Code Review and Build pipelines in Jenkins for this autotest. The Build pipeline will be triggered automatically if at least one environment is already added. A new project in Gerrit or another VCS. SonarQube integration will be available after the Build pipeline in Jenkins is passed. Nexus Repository Manager will be available after the Build pipeline in Jenkins is passed as well. Info To navigate quickly to OpenShift, Jenkins, Gerrit, SonarQube, Nexus, and other resources, click the Overview section on the navigation bar and hit the necessary link. The added autotest will be listed in the Autotests list allowing you to do the following: Autotests page Add another autotest by clicking the Create button and performing the same steps as described at the Add Autotest page. Open autotest data by clicking its link name. Once clicked, the following blocks will be displayed: General Info - displays common information about the cloned/imported autotest. Advanced Settings - displays the specified job provisioner, Jenkins agent, deployment script, and the versioning type with the start versioning from number (the latter two fields appear in case of edp versioning type). Branches - displays the status and name of the deployment branch, keeps the additional links to Jenkins and Gerrit. In case of edp versioning type, there are two additional fields: Build Number - indicates the current build number; Last Successful Build - indicates the number of the last successful build. Status Info - displays all the actions that were performed during the cloning/importing process. Edit the autotest codebase by clicking the pencil icon. For details see the Edit Existing Codebase section. Remove autotest with the corresponding database and Jenkins pipelines: Click the delete icon next to the autotest name; Type the required autotest name; Confirm the deletion by clicking the Delete button. Note The autotest that is used in a CD pipeline cannot be removed. Autotests page Select a number of existing autotests to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing autotests in a list by clicking the Name title. The autotests will be displayed in alphabetical order. Search the necessary autotest by entering the corresponding name, language or the build tool into the Search field. The search can be performed by the autotest name, language or a build tool. Navigate between pages, if the number of autotests exceeds the capacity of a single page. Edit Existing Codebase \u2693\ufe0e The EDP Admin Console provides the ability to enable, disable or edit the Jira Integration functionality for autotests via the Edit Codebase page. Perform the editing from one of the following sections on the Admin Console interface: Edit autotest Navigate to the codebase overview page and click the pencil icon, or Edit autotest Navigate to the codebase list page and click the pencil icon. Edit autotest To enable Jira integration, on the Edit Codebase page do the following: mark the Integrate with Jira server check box and fill in the necessary fields; click the Proceed button to apply the changes; navigate to Jenkins and add the create-jira-issue-metadata stage in the Build pipeline. Also add the commit-validate stage in the Code-Review pipeline. Note Pay attention that the Jira integration feature is not available when using the GitLab CI tool. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Jira Integration and Adjust VCS Integration With Jira pages. To disable Jira integration, on the Edit Codebase page do the following: unmark the Integrate with Jira server check box; click the Proceed button to apply the changes; navigate to Jenkins and remove the create-jira-issue-metadata stage in the Build pipeline. Also remove the commit-validate stage in the Code Review pipeline. As a result, the necessary changes will be applied. Add a New Branch \u2693\ufe0e Note Pay attention when specifying the branch name: the branch name is involved in the formation of the application version, so it must comply with the versioning semantic rules for the application. When adding an autotest, the default branch is a master branch. In order to add a new branch, follow the steps below: Navigate to the Branches block and click the Create button: Add branch Fill in the required fields: New branch a. Release Branch - select the Release Branch check box if you need to create a release branch; b. Branch Name - type the branch name. Pay attention that this field remains static if you create a release branch. c. From Commit Hash - paste the commit hash from which the new branch will be created. Note that if the From Commit Hash field is empty, the latest commit from the branch name will be used. d. Branch Version - enter the necessary branch version for the artifact. The Release Candidate (RC) postfix is concatenated to the branch version number. e. Master Branch Version - type the branch version that will be used in a master branch after the release creation. The Snapshot postfix is concatenated to the master branch version number; f. Click the Proceed button and wait until the new branch will be added to the list. Info Adding of a new branch is indicated in the context of the edp versioning type. To get more detailed information on how to add a branch using the default versioning type, please refer to the Add Autotest instruction. The default autotest repository is cloned and changed to the new indicated version before the build, i.e. the new indicated version will not be committed to the repository; thus, the existing repository will keep the default version. Remove Branch \u2693\ufe0e In order to remove the added branch with the corresponding record in the Admin Console database, do the following: Navigate to the Branches block by clicking the autotest name link in the Autotests list; Click the delete icon related to the necessary branch: Remove branch Enter the branch name and click the Delete button; Note The default master branch cannot be removed. Add Autotest as a Quality Gate \u2693\ufe0e In order to add an autotest as a quality gate to a newly added CD pipeline, do the following: Create a CD pipeline with the necessary parameters. Please refer to the Add CD Pipeline section for the details. In the Stages menu, select the Autotest quality gate type. It means the promoting process should be confirmed by the successful passing of the autotests. In the additional fields, select the previously created autotest name and specify its branch. After filling in all the necessary fields, click the Create button to start the provisioning of the pipeline. After the CD pipeline is added, the new namespace containing the stage name will be created in Kubernetes (in OpenShift, a new project will be created) with the following name pattern: [cluster name]-[cd pipeline name]-[stage name] Configure Autotest Launch at Specific Stage \u2693\ufe0e In order to configure the added autotest launch at the specific stage with necessary parameters, do the following: Add the necessary stage to the CD pipeline. Please refer to the Add CD Pipeline documentation for the details. Navigate to the run.json file and add the stage name and the specific parameters. Launch Autotest Locally \u2693\ufe0e There is an ability to run the autotests locally using the IDEA (Integrated Development Environment Application, such as IntelliJ, NetBeans etc.). To launch the autotest project for the local verification, perform the following steps: Clone the project to the local machine. Open the project in IDEA and find the run.json file to copy out the necessary command value. Paste the copied command value into the Command line field and run it with the necessary values and namespace. As a result, all launched tests will be executed. Related Articles \u2693\ufe0e Add Application Add Autotests Add CD Pipeline Adjust Jira Integration Adjust VCS Integration With Jira","title":"Overview"},{"location":"user-guide/autotest/#autotest","text":"This section describes the subsequent possible actions that can be performed with the newly added or existing autotests.","title":"Autotest"},{"location":"user-guide/autotest/#check-and-remove-autotest","text":"As soon as the autotest is successfully provisioned, the following will be created: Code Review and Build pipelines in Jenkins for this autotest. The Build pipeline will be triggered automatically if at least one environment is already added. A new project in Gerrit or another VCS. SonarQube integration will be available after the Build pipeline in Jenkins is passed. Nexus Repository Manager will be available after the Build pipeline in Jenkins is passed as well. Info To navigate quickly to OpenShift, Jenkins, Gerrit, SonarQube, Nexus, and other resources, click the Overview section on the navigation bar and hit the necessary link. The added autotest will be listed in the Autotests list allowing you to do the following: Autotests page Add another autotest by clicking the Create button and performing the same steps as described at the Add Autotest page. Open autotest data by clicking its link name. Once clicked, the following blocks will be displayed: General Info - displays common information about the cloned/imported autotest. Advanced Settings - displays the specified job provisioner, Jenkins agent, deployment script, and the versioning type with the start versioning from number (the latter two fields appear in case of edp versioning type). Branches - displays the status and name of the deployment branch, keeps the additional links to Jenkins and Gerrit. In case of edp versioning type, there are two additional fields: Build Number - indicates the current build number; Last Successful Build - indicates the number of the last successful build. Status Info - displays all the actions that were performed during the cloning/importing process. Edit the autotest codebase by clicking the pencil icon. For details see the Edit Existing Codebase section. Remove autotest with the corresponding database and Jenkins pipelines: Click the delete icon next to the autotest name; Type the required autotest name; Confirm the deletion by clicking the Delete button. Note The autotest that is used in a CD pipeline cannot be removed. Autotests page Select a number of existing autotests to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing autotests in a list by clicking the Name title. The autotests will be displayed in alphabetical order. Search the necessary autotest by entering the corresponding name, language or the build tool into the Search field. The search can be performed by the autotest name, language or a build tool. Navigate between pages, if the number of autotests exceeds the capacity of a single page.","title":"Check and Remove Autotest"},{"location":"user-guide/autotest/#edit-existing-codebase","text":"The EDP Admin Console provides the ability to enable, disable or edit the Jira Integration functionality for autotests via the Edit Codebase page. Perform the editing from one of the following sections on the Admin Console interface: Edit autotest Navigate to the codebase overview page and click the pencil icon, or Edit autotest Navigate to the codebase list page and click the pencil icon. Edit autotest To enable Jira integration, on the Edit Codebase page do the following: mark the Integrate with Jira server check box and fill in the necessary fields; click the Proceed button to apply the changes; navigate to Jenkins and add the create-jira-issue-metadata stage in the Build pipeline. Also add the commit-validate stage in the Code-Review pipeline. Note Pay attention that the Jira integration feature is not available when using the GitLab CI tool. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Jira Integration and Adjust VCS Integration With Jira pages. To disable Jira integration, on the Edit Codebase page do the following: unmark the Integrate with Jira server check box; click the Proceed button to apply the changes; navigate to Jenkins and remove the create-jira-issue-metadata stage in the Build pipeline. Also remove the commit-validate stage in the Code Review pipeline. As a result, the necessary changes will be applied.","title":"Edit Existing Codebase"},{"location":"user-guide/autotest/#add-a-new-branch","text":"Note Pay attention when specifying the branch name: the branch name is involved in the formation of the application version, so it must comply with the versioning semantic rules for the application. When adding an autotest, the default branch is a master branch. In order to add a new branch, follow the steps below: Navigate to the Branches block and click the Create button: Add branch Fill in the required fields: New branch a. Release Branch - select the Release Branch check box if you need to create a release branch; b. Branch Name - type the branch name. Pay attention that this field remains static if you create a release branch. c. From Commit Hash - paste the commit hash from which the new branch will be created. Note that if the From Commit Hash field is empty, the latest commit from the branch name will be used. d. Branch Version - enter the necessary branch version for the artifact. The Release Candidate (RC) postfix is concatenated to the branch version number. e. Master Branch Version - type the branch version that will be used in a master branch after the release creation. The Snapshot postfix is concatenated to the master branch version number; f. Click the Proceed button and wait until the new branch will be added to the list. Info Adding of a new branch is indicated in the context of the edp versioning type. To get more detailed information on how to add a branch using the default versioning type, please refer to the Add Autotest instruction. The default autotest repository is cloned and changed to the new indicated version before the build, i.e. the new indicated version will not be committed to the repository; thus, the existing repository will keep the default version.","title":"Add a New Branch"},{"location":"user-guide/autotest/#remove-branch","text":"In order to remove the added branch with the corresponding record in the Admin Console database, do the following: Navigate to the Branches block by clicking the autotest name link in the Autotests list; Click the delete icon related to the necessary branch: Remove branch Enter the branch name and click the Delete button; Note The default master branch cannot be removed.","title":"Remove Branch"},{"location":"user-guide/autotest/#add-autotest-as-a-quality-gate","text":"In order to add an autotest as a quality gate to a newly added CD pipeline, do the following: Create a CD pipeline with the necessary parameters. Please refer to the Add CD Pipeline section for the details. In the Stages menu, select the Autotest quality gate type. It means the promoting process should be confirmed by the successful passing of the autotests. In the additional fields, select the previously created autotest name and specify its branch. After filling in all the necessary fields, click the Create button to start the provisioning of the pipeline. After the CD pipeline is added, the new namespace containing the stage name will be created in Kubernetes (in OpenShift, a new project will be created) with the following name pattern: [cluster name]-[cd pipeline name]-[stage name]","title":"Add Autotest as a Quality Gate"},{"location":"user-guide/autotest/#configure-autotest-launch-at-specific-stage","text":"In order to configure the added autotest launch at the specific stage with necessary parameters, do the following: Add the necessary stage to the CD pipeline. Please refer to the Add CD Pipeline documentation for the details. Navigate to the run.json file and add the stage name and the specific parameters.","title":"Configure Autotest Launch at Specific Stage"},{"location":"user-guide/autotest/#launch-autotest-locally","text":"There is an ability to run the autotests locally using the IDEA (Integrated Development Environment Application, such as IntelliJ, NetBeans etc.). To launch the autotest project for the local verification, perform the following steps: Clone the project to the local machine. Open the project in IDEA and find the run.json file to copy out the necessary command value. Paste the copied command value into the Command line field and run it with the necessary values and namespace. As a result, all launched tests will be executed.","title":"Launch Autotest Locally"},{"location":"user-guide/autotest/#related-articles","text":"Add Application Add Autotests Add CD Pipeline Adjust Jira Integration Adjust VCS Integration With Jira","title":"Related Articles"},{"location":"user-guide/build-pipeline/","text":"Build Pipeline \u2693\ufe0e This section provides details on the Build pipeline of the EDP CI/CD pipeline framework. Explore below the pipeline purpose, stages and possible actions to perform. Build Pipeline Purpose \u2693\ufe0e The purpose of the Build pipeline contains the following points: Check out, test, tag and build an image from the mainstream branch after a patch set is submitted in order to inspect whether the integrated with the mainstream code fits all quality gates, can be built and tested; Be triggered if any new patch set is submitted; Tag a specific commit in Gerrit in case the build is successful; Build a Docker image with an application that can be afterward deployed using the Jenkins Deploy pipeline. Find below the functional diagram of the Build pipeline with the default stages: build-pipeline Build Pipeline for Application and Library \u2693\ufe0e The Build pipeline is triggered automatically after the Code Review pipeline is completed and the changes are submitted. To review the Build pipeline, take the following steps: Open Jenkins via the created link in Gerrit or via the Admin Console Overview page. Click the Build pipeline link to open its stages for the application and library codebases: Init - initialization of the Code Review pipeline inputs; Checkout - checkout of the application code; Get-version - get the version from the pom.XML file and add the build number; Compile - code compilation; Tests - tests execution; Sonar - Sonar launch that checks the whole code; Build - artifact building and adding to Nexus; Build-image - docker image building and adding to Docker Registry. The Build pipeline for the library has the same stages as the application except the Build-image stage, i.e. the Docker image is not building. Push - artifact docker image pushing to Nexus and Docker Registry; Ecr-to-docker - the docker image, after being built, is copied from the ECR project registry to DockerHub via the Crane tool. The stage is not the default and can be set for the application codebase type. To set this stage, please refer to the EcrToDocker.groovy file and to the Promote Docker Images From ECR to Docker Hub page. Git-tag - adding of the corresponding Git tag of the current commit to relate with the image, artifact, and build version. Note For more details on stages, please refer to the Pipeline Stages documentation. After the Build pipeline runs all the stages successfully, the corresponding tag numbers will be created in Kubernetes/OpenShift and Nexus. Check the Tag in Kubernetes/OpenShift and Nexus \u2693\ufe0e After the Build pipeline is completed, check the tag name and the same with the commit revision. Simply navigate to Gerrit \u2192 Projects \u2192 List \u2192 select the project \u2192 Tags. Note For the Import strategy, navigate to the repository from which a codebase is imported \u2192 Tags. It is actual both for GitHub and GitLab. Open the Kubernetes/OpenShift Overview page and click the link to Nexus and check the build of a new version. Switch to Kubernetes \u2192 CodebaseImageStream (or OpenShift \u2192 Builds \u2192 Images) \u2192 click the image stream that will be used for deployment. Check the corresponding tag. Configure and Start Pipeline Manually \u2693\ufe0e The Build pipeline can be started manually. To set the necessary stages and trigger the pipeline manually, take the following steps: Open the Build pipeline for the created library. Click the Build with parameters option from the left-side menu. Modify the stages by removing the whole objects massive: {\"name\". \"tests\"} where name is a key and tests is a stage name that should be executed. Open Jenkins and check the successful execution of all stages. Related Articles \u2693\ufe0e Add Application Add Autotest Add Library Adjust Jira Integration Adjust VCS Integration With Jira Autotest as Quality Gate Pipeline Stages","title":"Build Pipeline"},{"location":"user-guide/build-pipeline/#build-pipeline","text":"This section provides details on the Build pipeline of the EDP CI/CD pipeline framework. Explore below the pipeline purpose, stages and possible actions to perform.","title":"Build Pipeline"},{"location":"user-guide/build-pipeline/#build-pipeline-purpose","text":"The purpose of the Build pipeline contains the following points: Check out, test, tag and build an image from the mainstream branch after a patch set is submitted in order to inspect whether the integrated with the mainstream code fits all quality gates, can be built and tested; Be triggered if any new patch set is submitted; Tag a specific commit in Gerrit in case the build is successful; Build a Docker image with an application that can be afterward deployed using the Jenkins Deploy pipeline. Find below the functional diagram of the Build pipeline with the default stages: build-pipeline","title":"Build Pipeline Purpose"},{"location":"user-guide/build-pipeline/#build-pipeline-for-application-and-library","text":"The Build pipeline is triggered automatically after the Code Review pipeline is completed and the changes are submitted. To review the Build pipeline, take the following steps: Open Jenkins via the created link in Gerrit or via the Admin Console Overview page. Click the Build pipeline link to open its stages for the application and library codebases: Init - initialization of the Code Review pipeline inputs; Checkout - checkout of the application code; Get-version - get the version from the pom.XML file and add the build number; Compile - code compilation; Tests - tests execution; Sonar - Sonar launch that checks the whole code; Build - artifact building and adding to Nexus; Build-image - docker image building and adding to Docker Registry. The Build pipeline for the library has the same stages as the application except the Build-image stage, i.e. the Docker image is not building. Push - artifact docker image pushing to Nexus and Docker Registry; Ecr-to-docker - the docker image, after being built, is copied from the ECR project registry to DockerHub via the Crane tool. The stage is not the default and can be set for the application codebase type. To set this stage, please refer to the EcrToDocker.groovy file and to the Promote Docker Images From ECR to Docker Hub page. Git-tag - adding of the corresponding Git tag of the current commit to relate with the image, artifact, and build version. Note For more details on stages, please refer to the Pipeline Stages documentation. After the Build pipeline runs all the stages successfully, the corresponding tag numbers will be created in Kubernetes/OpenShift and Nexus.","title":"Build Pipeline for Application and Library"},{"location":"user-guide/build-pipeline/#check-the-tag-in-kubernetesopenshift-and-nexus","text":"After the Build pipeline is completed, check the tag name and the same with the commit revision. Simply navigate to Gerrit \u2192 Projects \u2192 List \u2192 select the project \u2192 Tags. Note For the Import strategy, navigate to the repository from which a codebase is imported \u2192 Tags. It is actual both for GitHub and GitLab. Open the Kubernetes/OpenShift Overview page and click the link to Nexus and check the build of a new version. Switch to Kubernetes \u2192 CodebaseImageStream (or OpenShift \u2192 Builds \u2192 Images) \u2192 click the image stream that will be used for deployment. Check the corresponding tag.","title":"Check the Tag in Kubernetes/OpenShift and Nexus"},{"location":"user-guide/build-pipeline/#configure-and-start-pipeline-manually","text":"The Build pipeline can be started manually. To set the necessary stages and trigger the pipeline manually, take the following steps: Open the Build pipeline for the created library. Click the Build with parameters option from the left-side menu. Modify the stages by removing the whole objects massive: {\"name\". \"tests\"} where name is a key and tests is a stage name that should be executed. Open Jenkins and check the successful execution of all stages.","title":"Configure and Start Pipeline Manually"},{"location":"user-guide/build-pipeline/#related-articles","text":"Add Application Add Autotest Add Library Adjust Jira Integration Adjust VCS Integration With Jira Autotest as Quality Gate Pipeline Stages","title":"Related Articles"},{"location":"user-guide/cd-pipeline-details/","text":"CD Pipeline Details \u2693\ufe0e CD Pipeline (Continuous Delivery Pipeline) - an EDP business entity that describes the whole delivery process of the selected application set via the respective stages. The main idea of the CD pipeline is to promote the application build version between the stages by applying the sequential verification (i.e. the second stage will be available if the verification on the first stage is successfully completed). The CD pipeline can include the essential set of applications with its specific stages as well. In other words, the CD pipeline allows the selected image stream (Docker container in Kubernetes terms) to pass a set of stages for the verification process (SIT - system integration testing with the automatic type of a quality gate, QA - quality assurance, UAT - user acceptance testing with the manual testing). Note It is possible to change the image stream for the application in the CD pipeline. Please refer to the Edit CD Pipeline section for the details. A CI/CD pipeline helps to automate steps in a software delivery process, such as the code build initialization, automated tests running, and deploying to a staging or production environment. Automated pipelines remove manual errors, provide standardized development feedback cycle, and enable the fast product iterations. To get more information on the CI pipeline, please refer to the CI Pipeline Details chapter. The codebase stream is used as a holder for the output of the stage, i.e. after the Docker container (or an image stream in OpenShift terms) passes the stage verification, it will be placed to the new codebase stream. Every codebase has a branch that has its own codebase stream - a Docker container that is an output of the build for the corresponding branch. Note For more information on the main terms used in EPAM Delivery Platform, please refer to the EDP Glossary EDP CD pipeline Explore the details of the CD pipeline below. Deploy Pipeline \u2693\ufe0e The Deploy pipeline is used by default on any stage of the Continuous Delivery pipeline. It addresses the following concerns: Deploying the application(s) to the main STAGE (SIT, QA, UAT) environment in order to run autotests and to promote image build versions to the next environments afterwards. Deploying the application(s) to a custom STAGE environment in order to run autotests and check manually that everything is ok with the application. Deploying the latest or a stable and some particular numeric version of an image build that exists in Docker registry. Promoting the image build versions from the main STAGE (SIT, QA, UAT) environment. Auto deploying the application(s) version from the passed payload (using the CODEBASE_VERSION job parameter). Find below the functional diagram of the Deploy pipeline with the default stages: Note The input for a CD pipeline depends on the Trigger Type for a deploy stage and can be either Manual or Auto. Deploy pipeline stages Related Articles \u2693\ufe0e Add Application Add Autotest Add CD Pipeline Add Library CI Pipeline Details CI/CD Overview EDP Glossary EDP Pipeline Framework EDP Pipeline Stages Prepare for Release","title":"CD Pipeline Details"},{"location":"user-guide/cd-pipeline-details/#cd-pipeline-details","text":"CD Pipeline (Continuous Delivery Pipeline) - an EDP business entity that describes the whole delivery process of the selected application set via the respective stages. The main idea of the CD pipeline is to promote the application build version between the stages by applying the sequential verification (i.e. the second stage will be available if the verification on the first stage is successfully completed). The CD pipeline can include the essential set of applications with its specific stages as well. In other words, the CD pipeline allows the selected image stream (Docker container in Kubernetes terms) to pass a set of stages for the verification process (SIT - system integration testing with the automatic type of a quality gate, QA - quality assurance, UAT - user acceptance testing with the manual testing). Note It is possible to change the image stream for the application in the CD pipeline. Please refer to the Edit CD Pipeline section for the details. A CI/CD pipeline helps to automate steps in a software delivery process, such as the code build initialization, automated tests running, and deploying to a staging or production environment. Automated pipelines remove manual errors, provide standardized development feedback cycle, and enable the fast product iterations. To get more information on the CI pipeline, please refer to the CI Pipeline Details chapter. The codebase stream is used as a holder for the output of the stage, i.e. after the Docker container (or an image stream in OpenShift terms) passes the stage verification, it will be placed to the new codebase stream. Every codebase has a branch that has its own codebase stream - a Docker container that is an output of the build for the corresponding branch. Note For more information on the main terms used in EPAM Delivery Platform, please refer to the EDP Glossary EDP CD pipeline Explore the details of the CD pipeline below.","title":"CD Pipeline Details"},{"location":"user-guide/cd-pipeline-details/#deploy-pipeline","text":"The Deploy pipeline is used by default on any stage of the Continuous Delivery pipeline. It addresses the following concerns: Deploying the application(s) to the main STAGE (SIT, QA, UAT) environment in order to run autotests and to promote image build versions to the next environments afterwards. Deploying the application(s) to a custom STAGE environment in order to run autotests and check manually that everything is ok with the application. Deploying the latest or a stable and some particular numeric version of an image build that exists in Docker registry. Promoting the image build versions from the main STAGE (SIT, QA, UAT) environment. Auto deploying the application(s) version from the passed payload (using the CODEBASE_VERSION job parameter). Find below the functional diagram of the Deploy pipeline with the default stages: Note The input for a CD pipeline depends on the Trigger Type for a deploy stage and can be either Manual or Auto. Deploy pipeline stages","title":"Deploy Pipeline"},{"location":"user-guide/cd-pipeline-details/#related-articles","text":"Add Application Add Autotest Add CD Pipeline Add Library CI Pipeline Details CI/CD Overview EDP Glossary EDP Pipeline Framework EDP Pipeline Stages Prepare for Release","title":"Related Articles"},{"location":"user-guide/ci-pipeline-details/","text":"CI Pipeline Details \u2693\ufe0e CI Pipeline (Continuous Integration Pipeline) - an EDP business entity that describes the integration of changes made to a codebase into a single project. The main idea of the CI pipeline is to review the changes in the code submitted through a Version Control System (VCS) and build a new codebase version so that it can be transmitted to the Continuous Delivery Pipeline for the rest of the delivery process. There are three codebase types in EPAM Delivery Platform: Applications - a codebase that is developed in the Version Control System, has the full lifecycle starting from the Code Review stage to its deployment to the environment; Libraries - this codebase is similar to the Application type, but it is not deployed and stored in the Artifactory. The library can be connected to other applications/libraries; Autotests - a codebase that inspects the code and can be used as a quality gate for the CD pipeline stage. The autotest only has the Code Review pipeline and is launched for the stage verification. Note For more information on the above mentioned codebase types, please refer to the Add Application , Add Library , Add Autotests and Autotest as Quality Gate pages. EDP CI pipeline Related Articles \u2693\ufe0e Add Application Add Autotest Add Library Adjust Jira Integration Adjust VCS Integration With Jira Autotest as Quality Gate Build Pipeline Code Review Pipeline Pipeline Stages","title":"CI Pipeline Details"},{"location":"user-guide/ci-pipeline-details/#ci-pipeline-details","text":"CI Pipeline (Continuous Integration Pipeline) - an EDP business entity that describes the integration of changes made to a codebase into a single project. The main idea of the CI pipeline is to review the changes in the code submitted through a Version Control System (VCS) and build a new codebase version so that it can be transmitted to the Continuous Delivery Pipeline for the rest of the delivery process. There are three codebase types in EPAM Delivery Platform: Applications - a codebase that is developed in the Version Control System, has the full lifecycle starting from the Code Review stage to its deployment to the environment; Libraries - this codebase is similar to the Application type, but it is not deployed and stored in the Artifactory. The library can be connected to other applications/libraries; Autotests - a codebase that inspects the code and can be used as a quality gate for the CD pipeline stage. The autotest only has the Code Review pipeline and is launched for the stage verification. Note For more information on the above mentioned codebase types, please refer to the Add Application , Add Library , Add Autotests and Autotest as Quality Gate pages. EDP CI pipeline","title":"CI Pipeline Details"},{"location":"user-guide/ci-pipeline-details/#related-articles","text":"Add Application Add Autotest Add Library Adjust Jira Integration Adjust VCS Integration With Jira Autotest as Quality Gate Build Pipeline Code Review Pipeline Pipeline Stages","title":"Related Articles"},{"location":"user-guide/cicd-overview/","text":"EDP CI/CD Overview \u2693\ufe0e This chapter provides information on CI/CD basic definitions and flow, as well as its components and process. CI/CD Basic Definitions \u2693\ufe0e The Continuous Integration part means the following: all components of the application development are in the same place and perform the same processes for running; the results are published in one place and replicated into EPAM GitLab or VCS (version control system); the repository also includes a storage tool (e.g. Nexus) for all binary artifacts that are produced by the Jenkins CI server after submitting changes from Code Review tool into VCS; The Code Review and Build pipelines are used before the code is delivered. An important part of both of them is the integration tests that are launched during the testing stage. Many applications (SonarQube, Gerrit, etc,) used by the project need databases for their performance. The Continuous Delivery comprises an approach allowing to produce an application in short cycles so that it can be reliably released at any time point. This part is tightly bound with the usage of the Code Review , Build , and Deploy pipelines. The Deploy pipelines deploy the applications configuration and their specific versions, launch automated tests and control quality gates for the specified environment. As a result of the successfully completed process, the specific versions of images are promoted to the next environment. All environments are sequential and promote the build versions of applications one-by-one. The logic of each stage is described as a code of Jenkins pipelines and stored in the VCS. During the CI/CD, there are several continuous processes that run in the repository, find below the list of possible actions: Review the code with the help of Gerrit tool; Run the static analysis using SonarQube to control the quality of the source code and keep the historical data which helps to understand the trend and effectivity of particular teams and members; Analyze application source code using SAST, byte code, and binaries for coding/design conditions that are indicative of security vulnerabilities; Build the code with Jenkins and run automated tests that are written to make sure the applied changes will not break any functionality. Note For the details on autotests, please refer to the Autotest , Add Autotest , and Autotest as Quality Gate pages. The release process is divided into cycles and provides regular delivery of completed pieces of functionality while continuing the development and integration of new functionality into the product mainline. Explore the main flow that is displayed on the diagram below: EDP CI/CD pipeline Related Articles \u2693\ufe0e Add Application Add Library Add CD Pipeline CI Pipeline Details CD Pipeline Details Customize CI Pipeline EDP Pipeline Framework Customize CD Pipeline EDP Stages Glossary Use Terraform Library in EDP","title":"EDP CI/CD Overview"},{"location":"user-guide/cicd-overview/#edp-cicd-overview","text":"This chapter provides information on CI/CD basic definitions and flow, as well as its components and process.","title":"EDP CI/CD Overview"},{"location":"user-guide/cicd-overview/#cicd-basic-definitions","text":"The Continuous Integration part means the following: all components of the application development are in the same place and perform the same processes for running; the results are published in one place and replicated into EPAM GitLab or VCS (version control system); the repository also includes a storage tool (e.g. Nexus) for all binary artifacts that are produced by the Jenkins CI server after submitting changes from Code Review tool into VCS; The Code Review and Build pipelines are used before the code is delivered. An important part of both of them is the integration tests that are launched during the testing stage. Many applications (SonarQube, Gerrit, etc,) used by the project need databases for their performance. The Continuous Delivery comprises an approach allowing to produce an application in short cycles so that it can be reliably released at any time point. This part is tightly bound with the usage of the Code Review , Build , and Deploy pipelines. The Deploy pipelines deploy the applications configuration and their specific versions, launch automated tests and control quality gates for the specified environment. As a result of the successfully completed process, the specific versions of images are promoted to the next environment. All environments are sequential and promote the build versions of applications one-by-one. The logic of each stage is described as a code of Jenkins pipelines and stored in the VCS. During the CI/CD, there are several continuous processes that run in the repository, find below the list of possible actions: Review the code with the help of Gerrit tool; Run the static analysis using SonarQube to control the quality of the source code and keep the historical data which helps to understand the trend and effectivity of particular teams and members; Analyze application source code using SAST, byte code, and binaries for coding/design conditions that are indicative of security vulnerabilities; Build the code with Jenkins and run automated tests that are written to make sure the applied changes will not break any functionality. Note For the details on autotests, please refer to the Autotest , Add Autotest , and Autotest as Quality Gate pages. The release process is divided into cycles and provides regular delivery of completed pieces of functionality while continuing the development and integration of new functionality into the product mainline. Explore the main flow that is displayed on the diagram below: EDP CI/CD pipeline","title":"CI/CD Basic Definitions"},{"location":"user-guide/cicd-overview/#related-articles","text":"Add Application Add Library Add CD Pipeline CI Pipeline Details CD Pipeline Details Customize CI Pipeline EDP Pipeline Framework Customize CD Pipeline EDP Stages Glossary Use Terraform Library in EDP","title":"Related Articles"},{"location":"user-guide/code-review-pipeline/","text":"Code Review Pipeline \u2693\ufe0e This section provides details on the Code Review pipeline of the EDP CI/CD framework. Explore below the pipeline purpose, stages and possible actions to perform. Code Review Pipeline Purpose \u2693\ufe0e The purpose of the Code Review pipeline contains the following points: Check out and test a particular developer's change (Patch Set) in order to inspect whether the code fits all the quality gates and can be built and tested; Be triggered if any new Patch Set appears in Gerrit; Send feedback about the build process in Jenkins to review the card in Gerrit; Send feedback about Sonar violations that have been found during the Sonar stage. Find below the functional diagram of the Code Review pipeline with the default stages: Code review pipeline stages Code Review Pipeline for Applications and Libraries \u2693\ufe0e Note Make sure the necessary applications or libraries are added to the Admin Console. For the details on how to add a codebase, please refer to the Add Application or Add Library pages accordingly. To discover the Code Review pipeline, apply changes that will trigger the Code Review pipeline automatically and take the following steps: Navigate to Jenkins. In Admin Console, go to the Overview section on the left-side navigation bar and click the link to Jenkins. Link to Jenkins or In Gerrit, go to the Patch Set page and click the CI Jenkins link in the Change Log section Link from Gerrit Note The Code Review pipeline starts automatically for every codebase type (Application, Autotests, Library). Check the Code Review pipeline for the application of for the library . Click the application name in Jenkins and switch to the additional release-01 branch that is created with the respective Code Review and Build pipelines. Click the Code Review pipeline link to open the Code Review pipeline stages for the application: Init - initialization of the codebase information and loading of the common libraries gerrit-checkout / checkout - the checkout of patch sets from Gerrit. The stage is called gerrit-checkout for the Create and Clone strategies of adding a codebase and checkout for the Import strategy. compile - the source code compilation tests - the launch of the tests sonar - the launch of the static code analyzer that checks the whole code helm-lint - the launch of the linting tests for deployment charts dockerfile-lint - the launch of the linting tests for Dockerfile commit-validate - the stage is optional and appears under enabled integration with Jira. Please refer to the Adjust Jira Integration and Adjust VCS Integration With Jira sections for the details. Note For more details on EDP pipeline stages, please refer to the Pipeline Stages section. Code Review Pipeline for Autotests \u2693\ufe0e To discover the Code Review pipeline for autotests, first, apply changes to a codebase that will trigger the Code Review pipeline automatically. The flow for the autotest is similar for that for applications and libraries, however, there are some differences. Explore them below. Open the run.json file for the created autotest. Note Please refer to the Add Autotest page for the details on how to create an autotest. The run.json file keeps a command that is executed on this stage. Open the Code Review pipeline in Jenkins (via the link in Gerrit or via the Admin Console Overview page) and click the Configure option from the left side. There are only four stages available: Initialization - Gerrit-checkout - tests - sonar (the launch of the static code analyzer that checks the whole code). Open the Code Review pipeline in Jenkins with the successfully passed stages. Retrigger Code Review Pipeline \u2693\ufe0e The Code Review pipeline can be retriggered manually, especially if the pipeline failed before. To retrigger it, take the following steps: In Jenkins, click the Retrigger option from the drop-down menu for the specific Code Review pipeline version number. Alternatively, click the Jenkins main page and select the Query and Trigger Gerrit Patches option. Click Search and select the check box of the necessary change and patch set and then click Trigger Selected . As a result, the Code Review pipeline will be retriggered. Configure Code Review Pipeline \u2693\ufe0e The Configure option allows adding/removing the stage from the Code Review pipeline if needed. To configure the Code Review pipeline, take the following steps: Being in Jenkins, click the Configure option from the left-side menu. Define the stages set that will be executed for the current pipeline. To remove a stage, select and remove the whole objects massive: {\"name\".\"tests\" }, where name is a key and tests is a stage name that should be executed. To add a stage, define the objects massive: {\"name\".\"tests\" }, where name is a key and tests is a stage name that should be added. Note All stages are launched from the shared library on GitHub. The list of libraries is located in the edp-library-stages repository. To apply the new stage process, retrigger the Code Review pipeline. For details, please refer to the Retrigger Code Review Pipeline section. Open Jenkins and check that there is no removed stage in the Code Review pipeline. Related Articles \u2693\ufe0e Add Application Add Autotest Add Library Adjust Jira Integration Adjust VCS Integration With Jira Autotest as Quality Gate Pipeline Stages","title":"Code Review Pipeline"},{"location":"user-guide/code-review-pipeline/#code-review-pipeline","text":"This section provides details on the Code Review pipeline of the EDP CI/CD framework. Explore below the pipeline purpose, stages and possible actions to perform.","title":"Code Review Pipeline"},{"location":"user-guide/code-review-pipeline/#code-review-pipeline-purpose","text":"The purpose of the Code Review pipeline contains the following points: Check out and test a particular developer's change (Patch Set) in order to inspect whether the code fits all the quality gates and can be built and tested; Be triggered if any new Patch Set appears in Gerrit; Send feedback about the build process in Jenkins to review the card in Gerrit; Send feedback about Sonar violations that have been found during the Sonar stage. Find below the functional diagram of the Code Review pipeline with the default stages: Code review pipeline stages","title":"Code Review Pipeline Purpose"},{"location":"user-guide/code-review-pipeline/#code-review-pipeline-for-applications-and-libraries","text":"Note Make sure the necessary applications or libraries are added to the Admin Console. For the details on how to add a codebase, please refer to the Add Application or Add Library pages accordingly. To discover the Code Review pipeline, apply changes that will trigger the Code Review pipeline automatically and take the following steps: Navigate to Jenkins. In Admin Console, go to the Overview section on the left-side navigation bar and click the link to Jenkins. Link to Jenkins or In Gerrit, go to the Patch Set page and click the CI Jenkins link in the Change Log section Link from Gerrit Note The Code Review pipeline starts automatically for every codebase type (Application, Autotests, Library). Check the Code Review pipeline for the application of for the library . Click the application name in Jenkins and switch to the additional release-01 branch that is created with the respective Code Review and Build pipelines. Click the Code Review pipeline link to open the Code Review pipeline stages for the application: Init - initialization of the codebase information and loading of the common libraries gerrit-checkout / checkout - the checkout of patch sets from Gerrit. The stage is called gerrit-checkout for the Create and Clone strategies of adding a codebase and checkout for the Import strategy. compile - the source code compilation tests - the launch of the tests sonar - the launch of the static code analyzer that checks the whole code helm-lint - the launch of the linting tests for deployment charts dockerfile-lint - the launch of the linting tests for Dockerfile commit-validate - the stage is optional and appears under enabled integration with Jira. Please refer to the Adjust Jira Integration and Adjust VCS Integration With Jira sections for the details. Note For more details on EDP pipeline stages, please refer to the Pipeline Stages section.","title":"Code Review Pipeline for Applications and Libraries"},{"location":"user-guide/code-review-pipeline/#code-review-pipeline-for-autotests","text":"To discover the Code Review pipeline for autotests, first, apply changes to a codebase that will trigger the Code Review pipeline automatically. The flow for the autotest is similar for that for applications and libraries, however, there are some differences. Explore them below. Open the run.json file for the created autotest. Note Please refer to the Add Autotest page for the details on how to create an autotest. The run.json file keeps a command that is executed on this stage. Open the Code Review pipeline in Jenkins (via the link in Gerrit or via the Admin Console Overview page) and click the Configure option from the left side. There are only four stages available: Initialization - Gerrit-checkout - tests - sonar (the launch of the static code analyzer that checks the whole code). Open the Code Review pipeline in Jenkins with the successfully passed stages.","title":"Code Review Pipeline for Autotests"},{"location":"user-guide/code-review-pipeline/#retrigger-code-review-pipeline","text":"The Code Review pipeline can be retriggered manually, especially if the pipeline failed before. To retrigger it, take the following steps: In Jenkins, click the Retrigger option from the drop-down menu for the specific Code Review pipeline version number. Alternatively, click the Jenkins main page and select the Query and Trigger Gerrit Patches option. Click Search and select the check box of the necessary change and patch set and then click Trigger Selected . As a result, the Code Review pipeline will be retriggered.","title":"Retrigger Code Review Pipeline"},{"location":"user-guide/code-review-pipeline/#configure-code-review-pipeline","text":"The Configure option allows adding/removing the stage from the Code Review pipeline if needed. To configure the Code Review pipeline, take the following steps: Being in Jenkins, click the Configure option from the left-side menu. Define the stages set that will be executed for the current pipeline. To remove a stage, select and remove the whole objects massive: {\"name\".\"tests\" }, where name is a key and tests is a stage name that should be executed. To add a stage, define the objects massive: {\"name\".\"tests\" }, where name is a key and tests is a stage name that should be added. Note All stages are launched from the shared library on GitHub. The list of libraries is located in the edp-library-stages repository. To apply the new stage process, retrigger the Code Review pipeline. For details, please refer to the Retrigger Code Review Pipeline section. Open Jenkins and check that there is no removed stage in the Code Review pipeline.","title":"Configure Code Review Pipeline"},{"location":"user-guide/code-review-pipeline/#related-articles","text":"Add Application Add Autotest Add Library Adjust Jira Integration Adjust VCS Integration With Jira Autotest as Quality Gate Pipeline Stages","title":"Related Articles"},{"location":"user-guide/container-stages/","text":"CI Pipeline for Container \u2693\ufe0e EPAM Delivery Platform ensures the implemented Container support allowing to work with Dockerfile that is processed by means of stages in the Code-Review and Build pipelines. These pipelines are expected to be created after the Container Library is added. Code Review Pipeline Stages \u2693\ufe0e In the Code Review pipeline, the following stages are available: checkout stage is a standard step during which all files are checked out from a selected branch of the Git repository. dockerfile-lint stage uses the hadolint tool to perform linting tests for the Dockerfile. dockerbuild-verify stage collects artifacts and builds an image from the Dockerfile without pushing to registry. This stage is intended to check if the image is built. Build Pipeline Stages \u2693\ufe0e In the Build pipeline, the following stages are available: checkout stage is a standard step during which all files are checked out from a master branch of the Git repository. get-version stage where the library version is determined either via: 2.1. EDP versioning functionality. 2.2. Default versioning functionality. dockerfile-lint stage uses the hadolint tool to perform linting tests for Dockerfile. build-image-kaniko stage builds Dockerfile using the Kaniko tool. git-tag stage that is intended for tagging a repository in Git. Tools for Container Images Building \u2693\ufe0e EPAM Delivery Platform ensures the implemented Kaniko tool and BuildConfig object support. Using Kaniko tool allows building the container images from a Dockerfile both on the Kubernetes and OpenShift platforms. The BuildConfig object enables the building of the container images only on the OpenShift platform. EDP uses the BuildConfig object and the Kaniko tool for creating containers from a Dockerfile and pushing them to the internal container image registry. For Kaniko, it is also possible to change the Docker config file and push the containers to different container image registries. Supported Container Image Build Tools \u2693\ufe0e Platform Build Tools Kubernetes Kaniko OpenShift Kaniko, BuildConfig Change Build Tool in the Build Pipeline \u2693\ufe0e By default, EPAM Delivery Platform uses the build-image-kaniko stage for building container images on the Kubernetes platform and the build-image-from-dockerfile stage for building container images on the OpenShift platform. In order to change a build tool for the OpenShift Platform from the default buildConfig object to the Kaniko tool, perform the following steps: Modify or update a job provisioner logic, follow the instructions on the Manage Jenkins CI Pipeline Job Provisioner page. Update the required parameters for a new provisioner. For example, if it is necessary to change the build tool for Container build pipeline, update the list of stages: stages [ 'Build-library-kaniko' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"build-image-from-dockerfile\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-kaniko' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"build-image-kaniko\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' Related Articles \u2693\ufe0e Use Dockerfile Linters for Code Review Pipeline Manage Jenkins CI Pipeline Job Provisioner","title":"CI Pipeline for Container"},{"location":"user-guide/container-stages/#ci-pipeline-for-container","text":"EPAM Delivery Platform ensures the implemented Container support allowing to work with Dockerfile that is processed by means of stages in the Code-Review and Build pipelines. These pipelines are expected to be created after the Container Library is added.","title":"CI Pipeline for Container"},{"location":"user-guide/container-stages/#code-review-pipeline-stages","text":"In the Code Review pipeline, the following stages are available: checkout stage is a standard step during which all files are checked out from a selected branch of the Git repository. dockerfile-lint stage uses the hadolint tool to perform linting tests for the Dockerfile. dockerbuild-verify stage collects artifacts and builds an image from the Dockerfile without pushing to registry. This stage is intended to check if the image is built.","title":"Code Review Pipeline Stages"},{"location":"user-guide/container-stages/#build-pipeline-stages","text":"In the Build pipeline, the following stages are available: checkout stage is a standard step during which all files are checked out from a master branch of the Git repository. get-version stage where the library version is determined either via: 2.1. EDP versioning functionality. 2.2. Default versioning functionality. dockerfile-lint stage uses the hadolint tool to perform linting tests for Dockerfile. build-image-kaniko stage builds Dockerfile using the Kaniko tool. git-tag stage that is intended for tagging a repository in Git.","title":"Build Pipeline Stages"},{"location":"user-guide/container-stages/#tools-for-container-images-building","text":"EPAM Delivery Platform ensures the implemented Kaniko tool and BuildConfig object support. Using Kaniko tool allows building the container images from a Dockerfile both on the Kubernetes and OpenShift platforms. The BuildConfig object enables the building of the container images only on the OpenShift platform. EDP uses the BuildConfig object and the Kaniko tool for creating containers from a Dockerfile and pushing them to the internal container image registry. For Kaniko, it is also possible to change the Docker config file and push the containers to different container image registries.","title":"Tools for Container Images Building"},{"location":"user-guide/container-stages/#supported-container-image-build-tools","text":"Platform Build Tools Kubernetes Kaniko OpenShift Kaniko, BuildConfig","title":"Supported Container Image Build Tools"},{"location":"user-guide/container-stages/#change-build-tool-in-the-build-pipeline","text":"By default, EPAM Delivery Platform uses the build-image-kaniko stage for building container images on the Kubernetes platform and the build-image-from-dockerfile stage for building container images on the OpenShift platform. In order to change a build tool for the OpenShift Platform from the default buildConfig object to the Kaniko tool, perform the following steps: Modify or update a job provisioner logic, follow the instructions on the Manage Jenkins CI Pipeline Job Provisioner page. Update the required parameters for a new provisioner. For example, if it is necessary to change the build tool for Container build pipeline, update the list of stages: stages [ 'Build-library-kaniko' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"build-image-from-dockerfile\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-kaniko' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"}' + ',{\"name\": \"dockerfile-lint\"},{\"name\": \"build-image-kaniko\"}' + \"${createJIMStage}\" + ',{\"name\": \"git-tag\"}]'","title":"Change Build Tool in the Build Pipeline"},{"location":"user-guide/container-stages/#related-articles","text":"Use Dockerfile Linters for Code Review Pipeline Manage Jenkins CI Pipeline Job Provisioner","title":"Related Articles"},{"location":"user-guide/copy-shared-secrets/","text":"Copy Shared Secrets \u2693\ufe0e The Copy Shared Secrets stage provides the ability to copy secrets from the current Kubernetes namespace into a namespace created during CD pipeline. Shared secrets Please follow the steps described below to copy the secrets: Create a secret in the current Kubernetes namespace that should be used in the deployment. The secret label must be app.edp.epam.com/use: cicd , since the pipeline script will attempt to copy the secret by its label. For example: kind : Secret metadata : labels : app . edp . epam . com / use : cicd Add the following step to the CD pipeline {\"name\":\"copy-secrets\",\"step_name\":\"copy-secrets\"} . Alternatively, it is possible to create a custom job provisioner with this step. Run the job. The pipeline script will create a secret with the same data in the namespace generated by the cd pipeline. Note Service account tokens are not supported. Related Articles \u2693\ufe0e Customize CD Pipeline Manage Jenkins CD Pipeline Job Provisioner","title":"Copy Shared Secrets"},{"location":"user-guide/copy-shared-secrets/#copy-shared-secrets","text":"The Copy Shared Secrets stage provides the ability to copy secrets from the current Kubernetes namespace into a namespace created during CD pipeline. Shared secrets Please follow the steps described below to copy the secrets: Create a secret in the current Kubernetes namespace that should be used in the deployment. The secret label must be app.edp.epam.com/use: cicd , since the pipeline script will attempt to copy the secret by its label. For example: kind : Secret metadata : labels : app . edp . epam . com / use : cicd Add the following step to the CD pipeline {\"name\":\"copy-secrets\",\"step_name\":\"copy-secrets\"} . Alternatively, it is possible to create a custom job provisioner with this step. Run the job. The pipeline script will create a secret with the same data in the namespace generated by the cd pipeline. Note Service account tokens are not supported.","title":"Copy Shared Secrets"},{"location":"user-guide/copy-shared-secrets/#related-articles","text":"Customize CD Pipeline Manage Jenkins CD Pipeline Job Provisioner","title":"Related Articles"},{"location":"user-guide/customize-cd-pipeline/","text":"Customize CD Pipeline \u2693\ufe0e Apart from running CD pipeline stages with the default logic, there is the ability to perform the following: Create your own logic for stages; Redefine the default EDP stages of a CD pipeline. In order to have the ability to customize a stage logic, create a CD pipeline stage source as a Library: Navigate to the Libraries section of the Admin Console and create a library with the Groovy-pipeline code language: Note If you clone the library, make sure that the correct source branch is selected. Create library Select the required fields to build your library: Advanced settings Go to the Continuous Delivery section of the Admin Console and create a CD pipeline with the library stage source and its branch: Library source Add New Stage \u2693\ufe0e Follow the steps below to add a new stage: Clone the repository with the added library; Create a \"stages\" directory in the root; Create a Jenkinsfile with default content: @Library ( [ 'edp-library-stages', 'edp-library-pipelines' ] ) _ Deploy () Create a groovy file with a meaningful name, e.g. NotificationStage.groovy; Put the required construction and your own logic into the file: import com.epam.edp.stages.impl.cd.Stage @Stage ( name = \"notify\" ) class Notify { Script script void run ( context ) { --------------- Put your own logic here ------------------ script . println ( \"Send notification logic\" ) --------------- Put your own logic here ------------------ } } return Notify Add a new stage to the STAGES parameter of the Jenkins job of your CD pipeline: Stages parameter Warning To make this stage permanently present, please modify the job provisioner . Run the job to check that your new stage has been run during the execution. Redefine Existing Stage \u2693\ufe0e By default, the following stages are implemented in EDP pipeline framework: deploy, deploy-helm, autotests, manual (Manual approve), promote-images. Using one of these names for annotation in your own class will lead to redefining the default logic with your own. Find below a sample of the possible flow of the redefining deploy stage: Clone the repository with the added library; Create a \"stages\" directory in the root; Create a Jenkinsfile with default content: @Library ( [ 'edp-library-stages', 'edp-library-pipelines' ] ) _ Deploy () Create a groovy file with a meaningful name, e.g. CustomDeployStage.groovy; Put the required construction and your own logic into the file: import com.epam.edp.stages.impl.cd.Stage @Stage ( name = \"deploy\" ) class CustomDeployStage { Script script void run ( context ) { --------------- Put your own logic here ------------------ script . println ( \"Custom deploy stage logic\" ) --------------- Put your own logic here ------------------ } } return CustomDeployStage Add a New Stage Using Shared Library via Custom Global Pipeline Libraries \u2693\ufe0e Note To add a new Custom Global Pipeline Library, please refer to the Add a New Custom Global Pipeline Library page. To redefine any stage and add custom logic using global pipeline libraries, perform the steps below: Navigate to the Libraries section of the Admin Console and create a library with the Groovy-pipeline code language: Create library Select the required fields to build your library: Advanced settings Clone the repository with the added library; Create a directory with the name src/com/epam/edp/customStages/impl/cd/impl/ in the library repository, for instance: src/com/epam/edp/customStages/impl/cd/impl/EmailNotify.groovy ; Add a Groovy file with another name to the same stages catalog, for instance \u2013 EmailNotify.groovy : package com . epam . edp . customStages . impl . cd . impl import com.epam.edp.stages.impl.cd.Stage @Stage ( name = \"notify\" ) class Notify { Script script void run ( context ) { --------------- Put your own logic here ------------------ script . println ( \"Send notification logic\" ) --------------- Put your own logic here ------------------ } } Create a Jenkinsfile with default content and the added custom library to Jenkins: @Library ( [ 'edp-library-stages', 'edp-library-pipelines', 'edp-custom-shared-library-name' ] ) _ Deploy () Note edp-custom-shared-library-name is the name of your Custom Global Pipeline Library that should be added to the Jenkins Global Settings. Add a new stage to the STAGES parameter of the Jenkins job of your CD pipeline: Stages parameter Warning To make this stage permanently present, please modify the job provisioner . Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ). Run the job to check that the new stage has been running during the execution. Redefine a Default Stage Logic via Custom Global Pipeline Libraries \u2693\ufe0e Note To add a new Custom Global Pipeline Library, please refer to the Add a New Custom Global Pipeline Library page. By default, the following stages are implemented in EDP pipeline framework: deploy, deploy-helm, autotests, manual (Manual approve), promote-images. Using one of these names for annotation in your own class will lead to redefining the default logic with your own. To redefine any stage and add custom logic using global pipeline libraries, perform the steps below: Navigate to the Libraries section of the Admin Console and create a library with the Groovy-pipeline code language: Create library Select the required fields to build your library: Advanced settings Clone the repository with the added library; Create a directory with the name src/com/epam/edp/customStages/impl/cd/impl/ in the library repository, for instance: src/com/epam/edp/customStages/impl/cd/impl/CustomDeployStage.groovy; ; Add a Groovy file with another name to the same stages catalog, for instance \u2013 CustomDeployStage.groovy : package com . epam . edp . customStages . impl . cd . impl import com.epam.edp.stages.impl.cd.Stage @Stage ( name = \"deploy\" ) class CustomDeployStage { Script script void run ( context ) { --------------- Put your own logic here ------------------ script . println ( \"Custom deploy stage logic\" ) --------------- Put your own logic here ------------------ } } Create a Jenkinsfile with default content and the added custom library to Jenkins: @Library ( [ 'edp-library-stages', 'edp-library-pipelines', 'edp-custom-shared-library-name' ] ) _ Deploy () Note edp-custom-shared-library-name is the name of your Custom Global Pipeline Library that should be added to the Jenkins Global Settings. Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ). Related Articles \u2693\ufe0e Add a New Custom Global Pipeline Library Manage Jenkins CD Pipeline Job Provisioner","title":"Customize CD Pipeline"},{"location":"user-guide/customize-cd-pipeline/#customize-cd-pipeline","text":"Apart from running CD pipeline stages with the default logic, there is the ability to perform the following: Create your own logic for stages; Redefine the default EDP stages of a CD pipeline. In order to have the ability to customize a stage logic, create a CD pipeline stage source as a Library: Navigate to the Libraries section of the Admin Console and create a library with the Groovy-pipeline code language: Note If you clone the library, make sure that the correct source branch is selected. Create library Select the required fields to build your library: Advanced settings Go to the Continuous Delivery section of the Admin Console and create a CD pipeline with the library stage source and its branch: Library source","title":"Customize CD Pipeline"},{"location":"user-guide/customize-cd-pipeline/#add-new-stage","text":"Follow the steps below to add a new stage: Clone the repository with the added library; Create a \"stages\" directory in the root; Create a Jenkinsfile with default content: @Library ( [ 'edp-library-stages', 'edp-library-pipelines' ] ) _ Deploy () Create a groovy file with a meaningful name, e.g. NotificationStage.groovy; Put the required construction and your own logic into the file: import com.epam.edp.stages.impl.cd.Stage @Stage ( name = \"notify\" ) class Notify { Script script void run ( context ) { --------------- Put your own logic here ------------------ script . println ( \"Send notification logic\" ) --------------- Put your own logic here ------------------ } } return Notify Add a new stage to the STAGES parameter of the Jenkins job of your CD pipeline: Stages parameter Warning To make this stage permanently present, please modify the job provisioner . Run the job to check that your new stage has been run during the execution.","title":"Add New Stage"},{"location":"user-guide/customize-cd-pipeline/#redefine-existing-stage","text":"By default, the following stages are implemented in EDP pipeline framework: deploy, deploy-helm, autotests, manual (Manual approve), promote-images. Using one of these names for annotation in your own class will lead to redefining the default logic with your own. Find below a sample of the possible flow of the redefining deploy stage: Clone the repository with the added library; Create a \"stages\" directory in the root; Create a Jenkinsfile with default content: @Library ( [ 'edp-library-stages', 'edp-library-pipelines' ] ) _ Deploy () Create a groovy file with a meaningful name, e.g. CustomDeployStage.groovy; Put the required construction and your own logic into the file: import com.epam.edp.stages.impl.cd.Stage @Stage ( name = \"deploy\" ) class CustomDeployStage { Script script void run ( context ) { --------------- Put your own logic here ------------------ script . println ( \"Custom deploy stage logic\" ) --------------- Put your own logic here ------------------ } } return CustomDeployStage","title":"Redefine Existing Stage"},{"location":"user-guide/customize-cd-pipeline/#add-a-new-stage-using-shared-library-via-custom-global-pipeline-libraries","text":"Note To add a new Custom Global Pipeline Library, please refer to the Add a New Custom Global Pipeline Library page. To redefine any stage and add custom logic using global pipeline libraries, perform the steps below: Navigate to the Libraries section of the Admin Console and create a library with the Groovy-pipeline code language: Create library Select the required fields to build your library: Advanced settings Clone the repository with the added library; Create a directory with the name src/com/epam/edp/customStages/impl/cd/impl/ in the library repository, for instance: src/com/epam/edp/customStages/impl/cd/impl/EmailNotify.groovy ; Add a Groovy file with another name to the same stages catalog, for instance \u2013 EmailNotify.groovy : package com . epam . edp . customStages . impl . cd . impl import com.epam.edp.stages.impl.cd.Stage @Stage ( name = \"notify\" ) class Notify { Script script void run ( context ) { --------------- Put your own logic here ------------------ script . println ( \"Send notification logic\" ) --------------- Put your own logic here ------------------ } } Create a Jenkinsfile with default content and the added custom library to Jenkins: @Library ( [ 'edp-library-stages', 'edp-library-pipelines', 'edp-custom-shared-library-name' ] ) _ Deploy () Note edp-custom-shared-library-name is the name of your Custom Global Pipeline Library that should be added to the Jenkins Global Settings. Add a new stage to the STAGES parameter of the Jenkins job of your CD pipeline: Stages parameter Warning To make this stage permanently present, please modify the job provisioner . Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ). Run the job to check that the new stage has been running during the execution.","title":"Add a New Stage Using Shared Library via Custom Global Pipeline Libraries"},{"location":"user-guide/customize-cd-pipeline/#redefine-a-default-stage-logic-via-custom-global-pipeline-libraries","text":"Note To add a new Custom Global Pipeline Library, please refer to the Add a New Custom Global Pipeline Library page. By default, the following stages are implemented in EDP pipeline framework: deploy, deploy-helm, autotests, manual (Manual approve), promote-images. Using one of these names for annotation in your own class will lead to redefining the default logic with your own. To redefine any stage and add custom logic using global pipeline libraries, perform the steps below: Navigate to the Libraries section of the Admin Console and create a library with the Groovy-pipeline code language: Create library Select the required fields to build your library: Advanced settings Clone the repository with the added library; Create a directory with the name src/com/epam/edp/customStages/impl/cd/impl/ in the library repository, for instance: src/com/epam/edp/customStages/impl/cd/impl/CustomDeployStage.groovy; ; Add a Groovy file with another name to the same stages catalog, for instance \u2013 CustomDeployStage.groovy : package com . epam . edp . customStages . impl . cd . impl import com.epam.edp.stages.impl.cd.Stage @Stage ( name = \"deploy\" ) class CustomDeployStage { Script script void run ( context ) { --------------- Put your own logic here ------------------ script . println ( \"Custom deploy stage logic\" ) --------------- Put your own logic here ------------------ } } Create a Jenkinsfile with default content and the added custom library to Jenkins: @Library ( [ 'edp-library-stages', 'edp-library-pipelines', 'edp-custom-shared-library-name' ] ) _ Deploy () Note edp-custom-shared-library-name is the name of your Custom Global Pipeline Library that should be added to the Jenkins Global Settings. Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ).","title":"Redefine a Default Stage Logic via Custom Global Pipeline Libraries"},{"location":"user-guide/customize-cd-pipeline/#related-articles","text":"Add a New Custom Global Pipeline Library Manage Jenkins CD Pipeline Job Provisioner","title":"Related Articles"},{"location":"user-guide/customize-ci-pipeline/","text":"Customize CI Pipeline \u2693\ufe0e This chapter describes the main steps that should be followed when customizing a CI pipeline. Redefine a Default Stage Logic for a Particular Application \u2693\ufe0e To redefine any stage and add custom logic, perform the steps below: Open the GitHub repository: Create a directory with the name \u201cstages\u201d in the application repository; Create a Groovy file with a meaningful name for a custom stage description, for instance: CustomSonar.groovy . Paste the copied skeleton from the reference stage and insert the necessary logic. Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ). The stage logic structure is the following: CustomSonar.groovy import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"sonar\" , buildTool = [ \"maven\" ], type = [ ProjectType . APPLICATION , ProjectType . AUTOTESTS , ProjectType . LIBRARY ]) class CustomSonar { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomSonar Info There is the ability to redefine the predefined EDP stage as well as to create it from scratch, it depends on the name that is used in the @Stage annotation. For example, using name = \"sonar\" will redefine an existing sonar stage with the same name, but using name=\"new-sonar\" will create a new stage. By default, the following stages are implemented in EDP: build build-image-from-dockerfile build-image build-image-kaniko checkout compile create-branch gerrit-checkout get-version git-tag push sonar sonar-cleanup tests trigger-job Mandatory points: Importing classes com.epam.edp.stages.impl.ci.ProjectType and com.epam.edp.stages.impl.ci.Stage; Annotating \"Stage\" for class - @Stage(name = \"sonar\", buildTool = [\"maven\"], type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY]); Property with the type \"Script\"; Void the \"run\" method with the \"context input parameter\" value; Bring the custom class back to the end of the file: return CustomSonar. Open Jenkins and make sure that all the changes are correct after the completion of the customized pipeline. Add a New Stage for a Particular Application \u2693\ufe0e To add a new stage for a particular application, perform the steps below: In the GitHub repository, add a Groovy file with another name to the same stages catalog. Copy the part of a pipeline framework logic that cannot be predefined; The stage logic structure is the following: EmailNotify.groovy import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"email-notify\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class EmailNotify { Script script void run ( context ) { ------------------- 'Your custom logic here' } } return EmailNotify Open the default set of stages and add a new one into the Default Value field by saving the respective type {\"name\": \"email-notify\"}, save the changes: Add stage Open Jenkins to check the pipeline; as soon as the checkout stage is passed, the new stage will appear in the pipeline: Check stage Warning To make this stage permanently present, please modify the job provisioner . Redefine a Default Stage Logic via Custom Global Pipeline Libraries \u2693\ufe0e Note To add a new Custom Global Pipeline Library, please refer to the Add a New Custom Global Pipeline Library page. To redefine any stage and add custom logic using global pipeline libraries, perform the steps below: Open the GitHub repository: Create a directory with the name /src/com/epam/edp/customStages/impl/ci/impl/stageName/ in the library repository, for instance: /src/com/epam/edp/customStages/impl/ci/impl/sonar/ ; Create a Groovy file with a meaningful name for a custom stage description, for instance \u2013 CustomSonar.groovy . Paste the copied skeleton from the reference stage and insert the necessary logic. Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ). The stage logic structure is the following: CustomSonar.groovy package com . epam . edp . customStages . impl . ci . impl . sonar import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"sonar\" , buildTool = [ \"maven\" ], type = [ ProjectType . APPLICATION , ProjectType . AUTOTESTS , ProjectType . LIBRARY ]) class CustomSonar { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } Info There is the ability to redefine the predefined EDP stage as well as to create it from scratch, it depends on the name that is used in the @Stage annotation. For example, using name = \"sonar\" will redefine an existing sonar stage with the same name, but using name=\"new-sonar\" will create a new stage. By default, the following stages are implemented in EDP: build build-image-from-dockerfile build-image build-image-kaniko checkout compile create-branch gerrit-checkout get-version git-tag push sonar sonar-cleanup tests trigger-job Mandatory points: Defining a package com.epam.edp.customStages.impl.ci.impl.stageName; Importing classes com.epam.edp.stages.impl.ci.ProjectType and com.epam.edp.stages.impl.ci.Stage; Annotating \"Stage\" for class - @Stage(name = \"sonar\", buildTool = [\"maven\"], type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY]); Property with the type \"Script\"; Void the \"run\" method with the \"context input parameter\" value. 3.Open Jenkins and make sure that all the changes are correct after the completion of the customized pipeline. Add a New Stage Using Shared Library via Custom Global Pipeline Libraries \u2693\ufe0e Note To add a new Custom Global Pipeline Library, please refer to the Add a New Custom Global Pipeline Library page. To redefine any stage and add custom logic using global pipeline libraries, perform the steps below: Open the GitHub repository: Create a directory with the name /src/com/epam/edp/customStages/impl/ci/impl/stageName/ in the library repository, for instance: /src/com/epam/edp/customStages/impl/ci/impl/emailNotify/ ; Add a Groovy file with another name to the same stages catalog, for instance \u2013 EmailNotify.groovy . Copy the part of a pipeline framework logic that cannot be predefined; Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ). The stage logic structure is the following: EmailNotify.groovy package com . epam . edp . customStages . impl . ci . impl . emailNotify import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"email-notify\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class EmailNotify { Script script void run ( context ) { ------------------- 'Your custom logic here' } } Open the default set of stages and add a new one into the Default Value field by saving the respective type {\"name\": \"email-notify\"}, save the changes: Add stage Open Jenkins to check the pipeline; as soon as the checkout stage is passed, the new stage will appear in the pipeline: Check stage Warning To make this stage permanently present, please modify the job provisioner . Related Articles \u2693\ufe0e Add a New Custom Global Pipeline Library Manage Jenkins CI Pipeline Job Provisioner Add Security Scanner","title":"Customize CI Pipeline"},{"location":"user-guide/customize-ci-pipeline/#customize-ci-pipeline","text":"This chapter describes the main steps that should be followed when customizing a CI pipeline.","title":"Customize CI Pipeline"},{"location":"user-guide/customize-ci-pipeline/#redefine-a-default-stage-logic-for-a-particular-application","text":"To redefine any stage and add custom logic, perform the steps below: Open the GitHub repository: Create a directory with the name \u201cstages\u201d in the application repository; Create a Groovy file with a meaningful name for a custom stage description, for instance: CustomSonar.groovy . Paste the copied skeleton from the reference stage and insert the necessary logic. Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ). The stage logic structure is the following: CustomSonar.groovy import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"sonar\" , buildTool = [ \"maven\" ], type = [ ProjectType . APPLICATION , ProjectType . AUTOTESTS , ProjectType . LIBRARY ]) class CustomSonar { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomSonar Info There is the ability to redefine the predefined EDP stage as well as to create it from scratch, it depends on the name that is used in the @Stage annotation. For example, using name = \"sonar\" will redefine an existing sonar stage with the same name, but using name=\"new-sonar\" will create a new stage. By default, the following stages are implemented in EDP: build build-image-from-dockerfile build-image build-image-kaniko checkout compile create-branch gerrit-checkout get-version git-tag push sonar sonar-cleanup tests trigger-job Mandatory points: Importing classes com.epam.edp.stages.impl.ci.ProjectType and com.epam.edp.stages.impl.ci.Stage; Annotating \"Stage\" for class - @Stage(name = \"sonar\", buildTool = [\"maven\"], type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY]); Property with the type \"Script\"; Void the \"run\" method with the \"context input parameter\" value; Bring the custom class back to the end of the file: return CustomSonar. Open Jenkins and make sure that all the changes are correct after the completion of the customized pipeline.","title":"Redefine a Default Stage Logic for a Particular Application"},{"location":"user-guide/customize-ci-pipeline/#add-a-new-stage-for-a-particular-application","text":"To add a new stage for a particular application, perform the steps below: In the GitHub repository, add a Groovy file with another name to the same stages catalog. Copy the part of a pipeline framework logic that cannot be predefined; The stage logic structure is the following: EmailNotify.groovy import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"email-notify\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class EmailNotify { Script script void run ( context ) { ------------------- 'Your custom logic here' } } return EmailNotify Open the default set of stages and add a new one into the Default Value field by saving the respective type {\"name\": \"email-notify\"}, save the changes: Add stage Open Jenkins to check the pipeline; as soon as the checkout stage is passed, the new stage will appear in the pipeline: Check stage Warning To make this stage permanently present, please modify the job provisioner .","title":"Add a New Stage for a Particular Application"},{"location":"user-guide/customize-ci-pipeline/#redefine-a-default-stage-logic-via-custom-global-pipeline-libraries","text":"Note To add a new Custom Global Pipeline Library, please refer to the Add a New Custom Global Pipeline Library page. To redefine any stage and add custom logic using global pipeline libraries, perform the steps below: Open the GitHub repository: Create a directory with the name /src/com/epam/edp/customStages/impl/ci/impl/stageName/ in the library repository, for instance: /src/com/epam/edp/customStages/impl/ci/impl/sonar/ ; Create a Groovy file with a meaningful name for a custom stage description, for instance \u2013 CustomSonar.groovy . Paste the copied skeleton from the reference stage and insert the necessary logic. Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ). The stage logic structure is the following: CustomSonar.groovy package com . epam . edp . customStages . impl . ci . impl . sonar import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"sonar\" , buildTool = [ \"maven\" ], type = [ ProjectType . APPLICATION , ProjectType . AUTOTESTS , ProjectType . LIBRARY ]) class CustomSonar { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } Info There is the ability to redefine the predefined EDP stage as well as to create it from scratch, it depends on the name that is used in the @Stage annotation. For example, using name = \"sonar\" will redefine an existing sonar stage with the same name, but using name=\"new-sonar\" will create a new stage. By default, the following stages are implemented in EDP: build build-image-from-dockerfile build-image build-image-kaniko checkout compile create-branch gerrit-checkout get-version git-tag push sonar sonar-cleanup tests trigger-job Mandatory points: Defining a package com.epam.edp.customStages.impl.ci.impl.stageName; Importing classes com.epam.edp.stages.impl.ci.ProjectType and com.epam.edp.stages.impl.ci.Stage; Annotating \"Stage\" for class - @Stage(name = \"sonar\", buildTool = [\"maven\"], type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY]); Property with the type \"Script\"; Void the \"run\" method with the \"context input parameter\" value. 3.Open Jenkins and make sure that all the changes are correct after the completion of the customized pipeline.","title":"Redefine a Default Stage Logic via Custom Global Pipeline Libraries"},{"location":"user-guide/customize-ci-pipeline/#add-a-new-stage-using-shared-library-via-custom-global-pipeline-libraries","text":"Note To add a new Custom Global Pipeline Library, please refer to the Add a New Custom Global Pipeline Library page. To redefine any stage and add custom logic using global pipeline libraries, perform the steps below: Open the GitHub repository: Create a directory with the name /src/com/epam/edp/customStages/impl/ci/impl/stageName/ in the library repository, for instance: /src/com/epam/edp/customStages/impl/ci/impl/emailNotify/ ; Add a Groovy file with another name to the same stages catalog, for instance \u2013 EmailNotify.groovy . Copy the part of a pipeline framework logic that cannot be predefined; Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ). The stage logic structure is the following: EmailNotify.groovy package com . epam . edp . customStages . impl . ci . impl . emailNotify import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"email-notify\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class EmailNotify { Script script void run ( context ) { ------------------- 'Your custom logic here' } } Open the default set of stages and add a new one into the Default Value field by saving the respective type {\"name\": \"email-notify\"}, save the changes: Add stage Open Jenkins to check the pipeline; as soon as the checkout stage is passed, the new stage will appear in the pipeline: Check stage Warning To make this stage permanently present, please modify the job provisioner .","title":"Add a New Stage Using Shared Library via Custom Global Pipeline Libraries"},{"location":"user-guide/customize-ci-pipeline/#related-articles","text":"Add a New Custom Global Pipeline Library Manage Jenkins CI Pipeline Job Provisioner Add Security Scanner","title":"Related Articles"},{"location":"user-guide/d-d-diagram/","text":"Delivery Dashboard Diagram \u2693\ufe0e Admin Console allows getting the general visualization of all the relations between CD pipeline, stages, codebases, branches, and image streams that are elements with the specific icon. To open the current project diagram, navigate to the Delivery Dashboard Diagram section on the navigation bar: Delivery dashboard Info All the requested changes (deletion, creation, adding) are displayed immediately on the Delivery Dashboard Diagram. Possible actions when using dashboard: To zoom in or zoom out the diagram scale, scroll up / down. To move the diagram, click and drag. To move an element, click it and drag to the necessary place. To see the relations for one element, click this element. To see the whole diagram, click the empty space. Related Articles \u2693\ufe0e EDP Admin Console","title":"Delivery Dashboard Diagram"},{"location":"user-guide/d-d-diagram/#delivery-dashboard-diagram","text":"Admin Console allows getting the general visualization of all the relations between CD pipeline, stages, codebases, branches, and image streams that are elements with the specific icon. To open the current project diagram, navigate to the Delivery Dashboard Diagram section on the navigation bar: Delivery dashboard Info All the requested changes (deletion, creation, adding) are displayed immediately on the Delivery Dashboard Diagram. Possible actions when using dashboard: To zoom in or zoom out the diagram scale, scroll up / down. To move the diagram, click and drag. To move an element, click it and drag to the necessary place. To see the relations for one element, click this element. To see the whole diagram, click the empty space.","title":"Delivery Dashboard Diagram"},{"location":"user-guide/d-d-diagram/#related-articles","text":"EDP Admin Console","title":"Related Articles"},{"location":"user-guide/dockerfile-stages/","text":"Use Dockerfile Linters for Code Review Pipeline \u2693\ufe0e This section contains the description of dockerbuild-verify , dockerfile-lint stages which one can use in Code Review pipeline. These stages help to obtain a quick response on the validity of the code in the Code Review pipeline in Kubernetes for all types of applications supported by EDP out of the box. Add stages Inspect the functions performed by the following stages: dockerbuild-verify stage collects artifacts and builds an image from the Dockerfile without push to registry. This stage is intended to check if the image is built. dockerfile-lint stage launches the hadolint command in order to check the Dockerfile. Related Articles \u2693\ufe0e Use Terraform Library in EDP EDP Pipeline Framework Promote Docker Images From ECR to Docker Hub CI Pipeline for Container","title":"Use Dockerfile Linters for Code Review Pipeline"},{"location":"user-guide/dockerfile-stages/#use-dockerfile-linters-for-code-review-pipeline","text":"This section contains the description of dockerbuild-verify , dockerfile-lint stages which one can use in Code Review pipeline. These stages help to obtain a quick response on the validity of the code in the Code Review pipeline in Kubernetes for all types of applications supported by EDP out of the box. Add stages Inspect the functions performed by the following stages: dockerbuild-verify stage collects artifacts and builds an image from the Dockerfile without push to registry. This stage is intended to check if the image is built. dockerfile-lint stage launches the hadolint command in order to check the Dockerfile.","title":"Use Dockerfile Linters for Code Review Pipeline"},{"location":"user-guide/dockerfile-stages/#related-articles","text":"Use Terraform Library in EDP EDP Pipeline Framework Promote Docker Images From ECR to Docker Hub CI Pipeline for Container","title":"Related Articles"},{"location":"user-guide/ecr-to-docker-stages/","text":"Promote Docker Images From ECR to Docker Hub \u2693\ufe0e This section contains the description of the ecr-to-docker stage, available in the Build pipeline. The ecr-to-docker stage is intended to perform the push of Docker images collected from the Amazon ECR cluster storage to Docker Hub repositories, where the image becomes accessible to everyone who wants to use it. This stage is optional and is designed for working with various EDP components. Note When pushing the image from ECR to Docker Hub using crane , the SHA-256 value remains unchanged. To run the ecr-to-docker stage just for once, navigate to the Build with Parameters option, add this stage to the stages list, and click Build. To add the ecr-to-docker stage to the pipeline, modify the job provisioner . Note To push properly the Docker image from the ECR storage, the ecr-to-docker stage should follow the build-image-kaniko stage. Add custom lib2 The ecr-to-docker stage contains a specific script that launches the following actions: Performs authorization in AWS ECR in the EDP private storage via awsv2 . Performs authorization in the Docker Hub. Checks whether a similar image exists in the Docker Hub in order to avoid its overwriting. If a similar image exists in the Docker Hub, the script will return the message about it and stop the execution. The ecr-to-docker stage in the Build pipeline will be marked in red. If there is no similar image, the script will proceed to promote the image using crane . Create Secret for ECR-to-Docker Stage \u2693\ufe0e The ecr-to-docker stage expects the authorization credentials to be added as Kubernetes secret into EDP-installed namespace. To create the dockerhub-credentials secret, run the following command: kubectl -n <edp-project> create secret generic dockerhub-credentials \\ --from-literal=accesstoken=<dockerhub_access_token> \\ --from-literal=account=<dockerhub_account_name> \\ --from-literal=username=<dockerhub_user_name> Note The \u2039 dockerhub_access_token \u203a should be created beforehand and in accordance with the official Docker Hub instruction . The \u2039 dockerhub_account_name \u203a and \u2039 dockerhub_user_name \u203a for the organization account repository will differ and be identical for the personal account repository. Pay attention that the Docker Hub repository for images uploading should be created beforehand and named by the following pattern: \u2039dockerhub_account_name\u203a/\u2039Application Name\u203a , where the \u2039Application Name\u203a should match the application name in the EDP Admin Console. Related Articles \u2693\ufe0e EDP Pipeline Framework Manage Access Token Manage Jenkins CI Pipeline Job Provisioner","title":"Promote Docker Images From ECR to Docker Hub"},{"location":"user-guide/ecr-to-docker-stages/#promote-docker-images-from-ecr-to-docker-hub","text":"This section contains the description of the ecr-to-docker stage, available in the Build pipeline. The ecr-to-docker stage is intended to perform the push of Docker images collected from the Amazon ECR cluster storage to Docker Hub repositories, where the image becomes accessible to everyone who wants to use it. This stage is optional and is designed for working with various EDP components. Note When pushing the image from ECR to Docker Hub using crane , the SHA-256 value remains unchanged. To run the ecr-to-docker stage just for once, navigate to the Build with Parameters option, add this stage to the stages list, and click Build. To add the ecr-to-docker stage to the pipeline, modify the job provisioner . Note To push properly the Docker image from the ECR storage, the ecr-to-docker stage should follow the build-image-kaniko stage. Add custom lib2 The ecr-to-docker stage contains a specific script that launches the following actions: Performs authorization in AWS ECR in the EDP private storage via awsv2 . Performs authorization in the Docker Hub. Checks whether a similar image exists in the Docker Hub in order to avoid its overwriting. If a similar image exists in the Docker Hub, the script will return the message about it and stop the execution. The ecr-to-docker stage in the Build pipeline will be marked in red. If there is no similar image, the script will proceed to promote the image using crane .","title":"Promote Docker Images From ECR to Docker Hub"},{"location":"user-guide/ecr-to-docker-stages/#create-secret-for-ecr-to-docker-stage","text":"The ecr-to-docker stage expects the authorization credentials to be added as Kubernetes secret into EDP-installed namespace. To create the dockerhub-credentials secret, run the following command: kubectl -n <edp-project> create secret generic dockerhub-credentials \\ --from-literal=accesstoken=<dockerhub_access_token> \\ --from-literal=account=<dockerhub_account_name> \\ --from-literal=username=<dockerhub_user_name> Note The \u2039 dockerhub_access_token \u203a should be created beforehand and in accordance with the official Docker Hub instruction . The \u2039 dockerhub_account_name \u203a and \u2039 dockerhub_user_name \u203a for the organization account repository will differ and be identical for the personal account repository. Pay attention that the Docker Hub repository for images uploading should be created beforehand and named by the following pattern: \u2039dockerhub_account_name\u203a/\u2039Application Name\u203a , where the \u2039Application Name\u203a should match the application name in the EDP Admin Console.","title":"Create Secret for ECR-to-Docker Stage"},{"location":"user-guide/ecr-to-docker-stages/#related-articles","text":"EDP Pipeline Framework Manage Access Token Manage Jenkins CI Pipeline Job Provisioner","title":"Related Articles"},{"location":"user-guide/helm-release-deletion/","text":"Helm Release Deletion \u2693\ufe0e The Helm release deletion stage provides the ability to remove Helm releases from the namespace. Note Pay attention that this stages will remove all Helm releases from the namespace. To avoid loss of important data, before using this stage, make the necessary backups. To remove Helm releases, follow the steps below: Add the following step to the CD pipeline {\"name\":\"helm-uninstall\",\"step_name\":\"helm-uninstall\"} . Alternatively, with this step, it is possible to create a custom job provisioner . Run the job. The pipeline script will remove Helm releases from the namespace. Related Articles \u2693\ufe0e Customize CD Pipeline Manage Jenkins CD Pipeline Job Provisioner","title":"Helm Release Deletion"},{"location":"user-guide/helm-release-deletion/#helm-release-deletion","text":"The Helm release deletion stage provides the ability to remove Helm releases from the namespace. Note Pay attention that this stages will remove all Helm releases from the namespace. To avoid loss of important data, before using this stage, make the necessary backups. To remove Helm releases, follow the steps below: Add the following step to the CD pipeline {\"name\":\"helm-uninstall\",\"step_name\":\"helm-uninstall\"} . Alternatively, with this step, it is possible to create a custom job provisioner . Run the job. The pipeline script will remove Helm releases from the namespace.","title":"Helm Release Deletion"},{"location":"user-guide/helm-release-deletion/#related-articles","text":"Customize CD Pipeline Manage Jenkins CD Pipeline Job Provisioner","title":"Related Articles"},{"location":"user-guide/helm-stages/","text":"Helm Chart Testing and Documentation Tools \u2693\ufe0e This section contains the description of the helm-lint and helm-docs stages that can be used in the Code Review pipeline. The stages help to obtain a quick response on the validity of the helm chart code and documentation in the Code Review pipeline. Inspect the functions performed by the following stages: helm-lint stage launches the ct lint --charts-deploy-templates/ command in order to validate the chart. Helm lint chart_schema.yaml - this file contains some rules by which the chart validity is checked. These rules are necessary for the YAML scheme validation. See the current scheme: View: chart_schema.yaml name : str() home : str() version : str() type : str() apiVersion : str() appVersion : any(str(), num()) description : str() keywords : list(str(), required=False) sources : list(str(), required=True) maintainers : list(include('maintainer'), required=True) dependencies : list(include('dependency'), required=False) icon : str(required=False) engine : str(required=False) condition : str(required=False) tags : str(required=False) deprecated : bool(required=False) kubeVersion : str(required=False) annotations : map(str(), str(), required=False) --- maintainer : name : str(required=True) email : str(required=False) url : str(required=False) --- dependency : name : str() version : str() repository : str() condition : str(required=False) tags : list(str(), required=False) enabled : bool(required=False) import-values : any(list(str()), list(include('import-value')), required=False) alias : str(required=False) ct.yaml - this file contains rules that will skip the validation of certain rules. To get more information about the chart testing lint, please refer to the ct_lint documentation. helm-docs stage helps to validate the generated documentation for the Helm deployment templates in the Code Review pipeline for all types of applications supported by EDP. This stage launches the helm-docs command in order to validate the chart documentation file existence and verify its relevance. Requirements : helm-docs v1.10.0 Note The helm-docs stage is optional. To extend the pipeline with an additional stage, please refer to the Configure Code Review Pipeline page. Helm docs Note The example of the generated documentation . Related Articles \u2693\ufe0e EDP Pipeline Framework","title":"Helm Chart Testing and Documentation Tools"},{"location":"user-guide/helm-stages/#helm-chart-testing-and-documentation-tools","text":"This section contains the description of the helm-lint and helm-docs stages that can be used in the Code Review pipeline. The stages help to obtain a quick response on the validity of the helm chart code and documentation in the Code Review pipeline. Inspect the functions performed by the following stages: helm-lint stage launches the ct lint --charts-deploy-templates/ command in order to validate the chart. Helm lint chart_schema.yaml - this file contains some rules by which the chart validity is checked. These rules are necessary for the YAML scheme validation. See the current scheme: View: chart_schema.yaml name : str() home : str() version : str() type : str() apiVersion : str() appVersion : any(str(), num()) description : str() keywords : list(str(), required=False) sources : list(str(), required=True) maintainers : list(include('maintainer'), required=True) dependencies : list(include('dependency'), required=False) icon : str(required=False) engine : str(required=False) condition : str(required=False) tags : str(required=False) deprecated : bool(required=False) kubeVersion : str(required=False) annotations : map(str(), str(), required=False) --- maintainer : name : str(required=True) email : str(required=False) url : str(required=False) --- dependency : name : str() version : str() repository : str() condition : str(required=False) tags : list(str(), required=False) enabled : bool(required=False) import-values : any(list(str()), list(include('import-value')), required=False) alias : str(required=False) ct.yaml - this file contains rules that will skip the validation of certain rules. To get more information about the chart testing lint, please refer to the ct_lint documentation. helm-docs stage helps to validate the generated documentation for the Helm deployment templates in the Code Review pipeline for all types of applications supported by EDP. This stage launches the helm-docs command in order to validate the chart documentation file existence and verify its relevance. Requirements : helm-docs v1.10.0 Note The helm-docs stage is optional. To extend the pipeline with an additional stage, please refer to the Configure Code Review Pipeline page. Helm docs Note The example of the generated documentation .","title":"Helm Chart Testing and Documentation Tools"},{"location":"user-guide/helm-stages/#related-articles","text":"EDP Pipeline Framework","title":"Related Articles"},{"location":"user-guide/library/","text":"Library \u2693\ufe0e This section describes the subsequent possible actions that can be performed with the newly added or existing libraries. Check and Remove Library \u2693\ufe0e As soon as the library is successfully provisioned, the following will be created: Code Review and Build pipelines in Jenkins for this library. The Build pipeline will be triggered automatically if at least one environment is already added. A new project in Gerrit or another VCS. SonarQube integration will be available after the Build pipeline in Jenkins is passed. Nexus Repository Manager will be available after the Build pipeline in Jenkins is passed as well. Info To navigate quickly to OpenShift, Jenkins, Gerrit, SonarQube, Nexus, and other resources, click the Overview section on the navigation bar and hit the necessary link. The added library will be listed in the Libraries list allowing to do the following: Library menu Create another library by clicking the Create button and performing the same steps as described on the Add Library page; Open library data by clicking its link name. Once clicked, the following blocks will be displayed: General Info - displays common information about the created/cloned/imported library. Advanced Settings - displays the specified job provisioner, Jenkins agent, deployment script, and the versioning type with the start versioning from number (the latter two fields appear in case of edp versioning type). Branches - displays the status and name of the deployment branch, keeps the additional links to Jenkins and Gerrit. In case of edp versioning type, there are two additional fields: Build Number - indicates the current build number; Last Successful Build - indicates the last successful build number. Status Info - displays all the actions that were performed during the creation/cloning/importing process. Edit the library codebase by clicking the pencil icon. For details see the Edit Existing Codebase section. Remove library with the corresponding database and Jenkins pipelines: Click the delete icon next to the library name; Type the required library name; Confirm the deletion by clicking the Delete button. Note The library that is used in a CD pipeline cannot be removed. Library menu Select a number of existing libraries to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing libraries in a list by clicking the Name title. The libraries will be displayed in an alphabetical order. Search the necessary application by entering the corresponding name, language or the build tool into the Search field. The search can be performed by the library name, language or a build tool. Navigate between pages, if the number of libraries exceeds the capacity of a single page. Edit Existing Codebase \u2693\ufe0e The EDP Admin Console provides the ability to enable, disable or edit the Jira Integration functionality for applications via the Edit Codebase page. Perform the editing from one of the following sections on the Admin Console interface: Edit library Navigate to the codebase overview page and click the pencil icon, or Edit library Navigate to the codebase list page and click the pencil icon. Edit library To enable Jira integration, on the Edit Codebase page do the following: mark the Integrate with Jira server check box and fill in the necessary fields; click the Proceed button to apply the changes; navigate to Jenkins and add the create-jira-issue-metadata stage in the Build pipeline. Also add the commit-validate stage in the Code Review pipeline. To disable Jira integration, on the Edit Codebase page do the following: unmark the Integrate with Jira server check box; click the Proceed button to apply the changes; navigate to Jenkins and remove the create-jira-issue-metadata stage in the Build pipeline. Also remove the commit-validate stage in the Code Review pipeline. As a result, the necessary changes will be applied. Add a New Branch \u2693\ufe0e Note Pay attention when specifying the branch name: the branch name is involved in the formation of the application version, so it must comply with the versioning semantic rules for the application. When adding a library, the default branch is a master branch. In order to add a new branch, follow the steps below: Navigate to the Branches block and click the Create button: Add branch Fill in the required fields: New branch a. Release Branch - select the Release Branch check box if you need to create a release branch; b. Branch Name - type the branch name. Pay attention that this field remain static if you create a release branch. c. From Commit Hash - paste the commit hash from which the new branch will be created. Note that if the From Commit Hash field is empty, the latest commit from the branch name will be used. d. Branch Version - enter the necessary branch version for the artifact. The Release Candidate (RC) postfix is concatenated to the branch version number. e. Master Branch Version - type the branch version that will be used in a master branch after the release creation. The Snapshot postfix is concatenated to the master branch version number; f. Click the Proceed button and wait until the new branch will be added to the list. Info Adding of a new branch is indicated in the context of the edp versioning type. To get more detailed information on how to add a branch using the default versioning type, please refer to Advanced Settings Menu section of the Admin Console user guide. The default library repository is cloned and changed to the new indicated version before the build, i.e. the new indicated version will not be committed to the repository; thus, the existing repository will keep the default version. Remove Branch \u2693\ufe0e In order to remove the added branch with the corresponding record in the Admin Console database, do the following: Navigate to the Branches block by clicking the library name link in the Libraries list; Click the delete icon related to the necessary branch: Remove branch Enter the branch name and click the Delete button; Note The default master branch cannot be removed. Related Articles \u2693\ufe0e Add Library","title":"Overview"},{"location":"user-guide/library/#library","text":"This section describes the subsequent possible actions that can be performed with the newly added or existing libraries.","title":"Library"},{"location":"user-guide/library/#check-and-remove-library","text":"As soon as the library is successfully provisioned, the following will be created: Code Review and Build pipelines in Jenkins for this library. The Build pipeline will be triggered automatically if at least one environment is already added. A new project in Gerrit or another VCS. SonarQube integration will be available after the Build pipeline in Jenkins is passed. Nexus Repository Manager will be available after the Build pipeline in Jenkins is passed as well. Info To navigate quickly to OpenShift, Jenkins, Gerrit, SonarQube, Nexus, and other resources, click the Overview section on the navigation bar and hit the necessary link. The added library will be listed in the Libraries list allowing to do the following: Library menu Create another library by clicking the Create button and performing the same steps as described on the Add Library page; Open library data by clicking its link name. Once clicked, the following blocks will be displayed: General Info - displays common information about the created/cloned/imported library. Advanced Settings - displays the specified job provisioner, Jenkins agent, deployment script, and the versioning type with the start versioning from number (the latter two fields appear in case of edp versioning type). Branches - displays the status and name of the deployment branch, keeps the additional links to Jenkins and Gerrit. In case of edp versioning type, there are two additional fields: Build Number - indicates the current build number; Last Successful Build - indicates the last successful build number. Status Info - displays all the actions that were performed during the creation/cloning/importing process. Edit the library codebase by clicking the pencil icon. For details see the Edit Existing Codebase section. Remove library with the corresponding database and Jenkins pipelines: Click the delete icon next to the library name; Type the required library name; Confirm the deletion by clicking the Delete button. Note The library that is used in a CD pipeline cannot be removed. Library menu Select a number of existing libraries to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing libraries in a list by clicking the Name title. The libraries will be displayed in an alphabetical order. Search the necessary application by entering the corresponding name, language or the build tool into the Search field. The search can be performed by the library name, language or a build tool. Navigate between pages, if the number of libraries exceeds the capacity of a single page.","title":"Check and Remove Library"},{"location":"user-guide/library/#edit-existing-codebase","text":"The EDP Admin Console provides the ability to enable, disable or edit the Jira Integration functionality for applications via the Edit Codebase page. Perform the editing from one of the following sections on the Admin Console interface: Edit library Navigate to the codebase overview page and click the pencil icon, or Edit library Navigate to the codebase list page and click the pencil icon. Edit library To enable Jira integration, on the Edit Codebase page do the following: mark the Integrate with Jira server check box and fill in the necessary fields; click the Proceed button to apply the changes; navigate to Jenkins and add the create-jira-issue-metadata stage in the Build pipeline. Also add the commit-validate stage in the Code Review pipeline. To disable Jira integration, on the Edit Codebase page do the following: unmark the Integrate with Jira server check box; click the Proceed button to apply the changes; navigate to Jenkins and remove the create-jira-issue-metadata stage in the Build pipeline. Also remove the commit-validate stage in the Code Review pipeline. As a result, the necessary changes will be applied.","title":"Edit Existing Codebase"},{"location":"user-guide/library/#add-a-new-branch","text":"Note Pay attention when specifying the branch name: the branch name is involved in the formation of the application version, so it must comply with the versioning semantic rules for the application. When adding a library, the default branch is a master branch. In order to add a new branch, follow the steps below: Navigate to the Branches block and click the Create button: Add branch Fill in the required fields: New branch a. Release Branch - select the Release Branch check box if you need to create a release branch; b. Branch Name - type the branch name. Pay attention that this field remain static if you create a release branch. c. From Commit Hash - paste the commit hash from which the new branch will be created. Note that if the From Commit Hash field is empty, the latest commit from the branch name will be used. d. Branch Version - enter the necessary branch version for the artifact. The Release Candidate (RC) postfix is concatenated to the branch version number. e. Master Branch Version - type the branch version that will be used in a master branch after the release creation. The Snapshot postfix is concatenated to the master branch version number; f. Click the Proceed button and wait until the new branch will be added to the list. Info Adding of a new branch is indicated in the context of the edp versioning type. To get more detailed information on how to add a branch using the default versioning type, please refer to Advanced Settings Menu section of the Admin Console user guide. The default library repository is cloned and changed to the new indicated version before the build, i.e. the new indicated version will not be committed to the repository; thus, the existing repository will keep the default version.","title":"Add a New Branch"},{"location":"user-guide/library/#remove-branch","text":"In order to remove the added branch with the corresponding record in the Admin Console database, do the following: Navigate to the Branches block by clicking the library name link in the Libraries list; Click the delete icon related to the necessary branch: Remove branch Enter the branch name and click the Delete button; Note The default master branch cannot be removed.","title":"Remove Branch"},{"location":"user-guide/library/#related-articles","text":"Add Library","title":"Related Articles"},{"location":"user-guide/opa-stages/","text":"Use Open Policy Agent \u2693\ufe0e Open Policy Agent (OPA) is a policy engine that provides: High-level declarative policy language Rego ; API and tooling for policy execution. EPAM Delivery Platform ensures the implemented Open Policy Agent support allowing to work with Open Policy Agent bundles that is processed by means of stages in the Code Review and Build pipelines. These pipelines are expected to be created after the Rego OPA Library is added. Code Review Pipeline Stages \u2693\ufe0e In the Code Review pipeline, the following stages are available: checkout stage, a standard step during which all files are checked out from a selected branch of the Git repository. tests stage containing a script that performs the following actions: 2.1. Runs policy tests . 2.2. Converts OPA test results into JUnit format. 2.3. Publishes JUnit-formatted results to Jenkins. Build Pipeline Stages \u2693\ufe0e In the Build pipeline, the following stages are available: checkout stage, a standard step during which all files are checked out from a selected branch of the Git repository. get-version optional stage, a step where library version is determined either via: 2.1. Standard EDP versioning functionality. 2.2. Manually specified version. In this case .manifest file in a root directory MUST be provided. File must contain a JSON document with revision field. Minimal example: { \"revision\": \"1.0.0\" }\" . tests stage containing a script that performs the following actions: 3.1. Runs policy tests . 3.2. Converts OPA test results into JUnit format. 3.3. Publishes JUnit-formatted results to Jenkins. git-tag stage, a standard step where git branch is tagged with a version. Related Articles \u2693\ufe0e EDP Pipeline Framework","title":"Use Open Policy Agent"},{"location":"user-guide/opa-stages/#use-open-policy-agent","text":"Open Policy Agent (OPA) is a policy engine that provides: High-level declarative policy language Rego ; API and tooling for policy execution. EPAM Delivery Platform ensures the implemented Open Policy Agent support allowing to work with Open Policy Agent bundles that is processed by means of stages in the Code Review and Build pipelines. These pipelines are expected to be created after the Rego OPA Library is added.","title":"Use Open Policy Agent"},{"location":"user-guide/opa-stages/#code-review-pipeline-stages","text":"In the Code Review pipeline, the following stages are available: checkout stage, a standard step during which all files are checked out from a selected branch of the Git repository. tests stage containing a script that performs the following actions: 2.1. Runs policy tests . 2.2. Converts OPA test results into JUnit format. 2.3. Publishes JUnit-formatted results to Jenkins.","title":"Code Review Pipeline Stages"},{"location":"user-guide/opa-stages/#build-pipeline-stages","text":"In the Build pipeline, the following stages are available: checkout stage, a standard step during which all files are checked out from a selected branch of the Git repository. get-version optional stage, a step where library version is determined either via: 2.1. Standard EDP versioning functionality. 2.2. Manually specified version. In this case .manifest file in a root directory MUST be provided. File must contain a JSON document with revision field. Minimal example: { \"revision\": \"1.0.0\" }\" . tests stage containing a script that performs the following actions: 3.1. Runs policy tests . 3.2. Converts OPA test results into JUnit format. 3.3. Publishes JUnit-formatted results to Jenkins. git-tag stage, a standard step where git branch is tagged with a version.","title":"Build Pipeline Stages"},{"location":"user-guide/opa-stages/#related-articles","text":"EDP Pipeline Framework","title":"Related Articles"},{"location":"user-guide/pipeline-framework/","text":"EDP Pipeline Framework \u2693\ufe0e This chapter provides detailed information about the EDP pipeline framework concepts and parts, as well as the accurate data about the Code Review , Build and Deploy pipelines with the respective stages. EDP Pipeline Framework Overview \u2693\ufe0e Note The whole logic is applied to Jenkins as it is the main tool for the CI/CD processes organization. EDP pipeline framework basic The general EDP Pipeline Framework consists of several parts: Jenkinsfile - a text file that keeps the definition of a Jenkins Pipeline and is checked into source control. Every Job has its Jenkinsfile stored in the specific application repository and in Jenkins as the plain text. The behavior logic of the pipelines can be customized easily by modifying a source code which is always copied to the EDP repository after the EDP installation. Jenkinsfile example Loading Shared Libraries - a part where every job loads libraries with the help of the shared libraries mechanism for Jenkins that allows to create reproducible pipelines, write them uniformly, and manage the update process. There are two main libraries: EDP Pipelines with the common logic described for the main pipelines Code Review, Build, Deploy pipelines and EDP Stages library that keeps the description of the stages for every pipeline. Run Stages - a part where the predefined default stages are launched. Pipeline script CI/CD Jobs Comparison \u2693\ufe0e Explore the CI and CD job comparison. Please note that the dynamic stages order can be changed, meanwhile, the predefined stages order in the reference pipeline cannot be changed, i.e. only the predefined stages set can be run. CI/CD jobs comparison Context \u2693\ufe0e Context - a variable that stores and transfers all necessary parameters between stages that are used by pipeline during performing. The context type is \"Map\". Each stage has input and output context. Each stage has a mandatory input context. Note If the input context isn't transferred, the stage will be failed. Annotations for CI/CD Stages \u2693\ufe0e Annotation for CI Stages: The annotation type is \"Map\"; The annotation consists of the name, buildTool, and codebaseType. Annotation for CD Stages: The annotation type is \"Map\"; The annotation consists of a name. Code Review Pipeline \u2693\ufe0e CodeReview() \u2013 a function that allows using the EDP implementation for the Code Review pipeline. Note All values of different parameters that are used during the pipeline execution are stored in the \"Map\" context. The Code Review pipeline consists of several steps: On the master: Initialization of all objects (Platform, Job, Gerrit, Nexus, Sonar, Application, StageFactory) and loading of the default implementations of EDP stages. On a particular Jenkins agent that depends on the build tool: Creating workdir for application sources; Loading build tool implementation for a particular application; Run in a loop all stages (From) and run them either in parallel or one by one. Code Review Pipeline Overview \u2693\ufe0e Using in pipelines - @Library(['edp-library-pipelines@version']) The corresponding enums, interfaces, classes, and their methods can be used separately from the EDP Pipelines library function (please refer to Table 1 and Table 2 ). Table 1. Enums and Interfaces with the respective properties, methods, and examples. Enums Interfaces PlatformType: - OPENSHIFT - KUBERNETES JobType: - CODEREVIEW - BUILD - DEPLOY BuildToolType: - MAVEN - GRADLE - NPM - DOTNET Platform() - contains methods for working with platform CLI. At the moment only OpenShift is supported. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Methods : getJsonPathValue(String k8s_kind, String k8s_kind_name, String jsonPath): return String value of specific parameter of particular object using jsonPath utility. Example : context.platform.getJsonPathValue(''cm'', ''project-settings'', ''.data.username'') . BuildTool() - contains methods for working with different buildTool from ENUM BuildToolType. Should be invoked on Jenkins build agents. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Nexus object - Object of class Nexus. Methods : init: return parameters of buildTool that are needed for running stages. Example : context.buildTool = new BuildToolFactory(). getBuildToolImpl (context.application.config.build_tool, this, context.nexus) context.buildTool.init() . Table 2. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) PlatformFactory() - Class that contains methods getting an implementation of CLI of the platform. At the moment OpenShift and Kubernetes are supported. Methods : getPlatformImpl(PlatformType platform, Script script): return Class Platform . Example : context.platform = new PlatformFactory().getPlatformImpl(PlatformType.OPENSHIFT, this) . Application(String name, Platform platform, Script script) - Class that describes the application object. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). String name - Name for the application for creating an object. Map config - Map of configuration settings for the particular application that is loaded from config map project-settings. String version - Application version, initially empty. Is set on the get-version step. String deployableModule - The name of the deployable module for multi-module applications, initially empty. String buildVersion - Version of the built artifact, contains build number of Job initially empty. String deployableModuleDir - The name of deployable module directory for multi-module applications, initially empty. Array imageBuildArgs - List of arguments for building an application Docker image. Methods : setConfig(String gerrit_autouser, String gerrit_host, String gerrit_sshPort, String gerrit_project): set the config property with values from config map. Example : context.application = new Application(context.job, context.gerrit.project, context.platform, this) context.application.setConfig(context.gerrit.autouser, context.gerrit.host, context.gerrit.sshPort, context.gerrit.project) Job(type: JobType.value, platform: Platform, script: Script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). JobType.value type. String deployTemplatesDirectory - The name of the directory in application repository where deploy templates are located. It can be set for a particular Job through DEPLOY_TEMPLATES_DIRECTORY parameter. String edpName - The name of the EDP Project. Map stages - Contains all stages in JSON format that is retrieved from Jenkins job env variable. String envToPromote - The name of the environment for promoting images. Boolean promoteImages - Defines whether images should be promoted or not. Methods : getParameterValue(String parameter, String defaultValue = null): return parameter of ENV variable of Jenkins job. init(): set all the properties of the Job object. setDisplayName(String displayName): set display name of the Jenkins job. setDescription(String description, Boolean addDescription = false): set new or add to the existing description of the Jenkins job. printDebugInfo(Map context): print context info to the log of Jenkins' job. runStage(String stage_name, Map context): run the particular stage according to its name. Example : context.job = new Job(JobType.CODEREVIEW.value, context.platform, this) context.job.init() context.job.printDebugInfo(context) context.job.setDisplayName(\"test\") context.job.setDescription(\"Name: ${context.application.config.name}\") Gerrit(Job job, Platform platform, Script script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String credentialsId - Credential Id in Jenkins for Gerrit. String autouser - Username of an auto user in Gerrit for integration with Jenkins. String host - Gerrit host. String project - the project name of the built application. String branch - branch to build the application from. String changeNumber - change number of Gerrit commit. String changeName - change name of Gerrit commit. String refspecName - refspecName of Gerrit commit. String sshPort - Gerrit ssh port number. String patchsetNumber - patchsetNumber of Gerrit commit. Methods : init(): set all the properties of Gerrit object. Example : context.gerrit = new Gerrit(context.job, context.platform, this) context.gerrit.init() Nexus(Job job, Platform platform, Script script) - Class that describes the Nexus tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String autouser - Username of an auto user in Nexus for integration with Jenkins. String credentialsId - Credential Id in Jenkins for Nexus. String host - Nexus host. String port - Nexus http(s) port. String repositoriesUrl - Base URL of repositories in Nexus. String restUrl - URL of Rest API. Methods : init(): set all the properties of Nexus object Example : context.nexus = new Nexus(context.job, context.platform, this) context.nexus.init() Sonar(Job job, Platform platform, Script script) - Class that describes the Sonar tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String route - External route of the sonar application. Methods : init(): set all the properties of Sonar object Example : context.sonar = new Sonar(context.job, context.platform, this) context.sonar.init() Code Review Pipeline Stages \u2693\ufe0e Each EDP stage implementation has run method that is as input parameter required to pass the \"Map\" context with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. The Code Review pipeline includes the following default stages: Checkout \u2192 Gerrit Checkout \u2192 Compile \u2192 Tests \u2192 Sonar . Info To get the full description of every stage, please refer to the EDP Stages Framework section. How to Redefine or Extend the EDP Pipeline Stages Library \u2693\ufe0e Inspect the points below to redefine or extend the EDP Pipeline Stages Library: Create \u201cstage\u201d folder in your App repository. Create a Groovy file with a meaningful name for the custom stage description. For instance \u2013 CustomBuildMavenApplication.groovy. Describe the stage logic. Redefinition: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"compile\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class CustomBuildMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomBuildMavenApplication Extension: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"new-stage\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class NewStageMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return NewStageMavenApplication Using EDP Stages Library in the Pipeline \u2693\ufe0e In order to use the EDP stages, the created pipeline should fit some requirements, that`s why a developer has to do the following: import library - @Library(['edp-library-stages']) import StageFactory class - import com.epam.edp.stages.StageFactory define context Map \u2013 context = [:] define stagesFactory instance and load EDP stages: context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } After that, there is the ability to run any EDP stage beforehand by defining a necessary context: context.factory.getStage(\"checkout\",\"maven\",\"application\").run(context) For instance, the pipeline can look like : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] node ( ' maven ' ) { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] stage ( \"checkout\" ) { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } stage ( \"compile\" ) { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } Or in a declarative way : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] pipeline { agent { label ' maven ' } stages { stage ( ' Init ' ){ steps { script { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] } } } stage ( \"Checkout\" ) { steps { script { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } } } stage ( ' Compile ' ) { steps { script { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } } } } Build Pipeline \u2693\ufe0e Build() \u2013 a function that allows using the EDP implementation for the Build pipeline. All values of different parameters that are used during the pipeline execution are stored in the \"Map\" context. The Build pipeline consists of several steps: On the master: Initialization of all objects (Platform, Job, Gerrit, Nexus, Sonar, Application, StageFactory) and loading default implementations of EDP stages. On a particular Jenkins agent that depends on the build tool: Creating workdir for application sources; Loading build tool implementation for a particular application; Run in a loop all stages (From) and run them either in parallel or one by one. Build Pipeline Overview \u2693\ufe0e Using in pipelines - @Library(['edp-library-pipelines@version']) The corresponding enums, interfaces, classes, and their methods can be used separately from the EDP Pipelines library function (please refer to Table 3 and Table 4 ). Table 3. Enums and Interfaces with the respective properties, methods, and examples. Enums Interfaces PlatformType: - OPENSHIFT - KUBERNETES JobType: - CODEREVIEW - BUILD - DEPLOY BuildToolType : - MAVEN - GRADLE - NPM - DOTNET Platform() - contains methods for working with platform CLI. At the moment only OpenShift is supported. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Methods : getJsonPathValue(String k8s_kind, String k8s_kind_name, String jsonPath): return String value of specific parameter of particular object using jsonPath utility. Example : context.platform.getJsonPathValue(\"cm\",\"project-settings\", \".data.username\") BuildTool() - contains methods for working with different buildTool from ENUM BuildToolType. Should be invoked on Jenkins build agents. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Nexus object - Object of class Nexus. See description below: Methods : init: return parameters of buildTool that are needed for running stages. Example : context.buildTool = new BuildToolFactory().getBuildToolImpl (context.application.config.build_tool, this, context.nexus)context.buildTool.init() Table 4. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) PlatformFactory() - Class that contains methods getting an implementation of CLI of the platform. At the moment OpenShift and Kubernetes are supported. Methods : getPlatformImpl(PlatformType platform, Script script): return Class Platform Example : context.platform = new PlatformFactory().getPlatformImpl(PlatformType.OPENSHIFT, this) Application(String name, Platform platform, Script script) - Class that describes the application object. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). String name - Name for the application for creating an object. Map config - Map of configuration settings for the particular application that is loaded from config map project-settings. String version - Application version, initially empty. Is set on the get-version step. String deployableModule - The name of the deployable module for multi-module applications, initially empty. String buildVersion - Version of the built artifact, contains build number of Job initially empty. String deployableModuleDir - The name of deployable module directory for multi-module applications, initially empty. Array imageBuildArgs - List of arguments for building the application Docker image. Methods : setConfig(String gerrit_autouser, String gerrit_host, String gerrit_sshPort, String gerrit_project): set the config property with values from config map. Example : context.application = new Application(context.job, context.gerrit.project, context.platform, this) context.application.setConfig(context.gerrit.autouser, context.gerrit.host, context.gerrit.sshPort, context.gerrit.project) Job(type: JobType.value, platform: Platform, script: Script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). JobType.value type. String deployTemplatesDirectory - The name of the directory in application repository, where deploy templates are located. It can be set for a particular Job through DEPLOY_TEMPLATES_DIRECTORY parameter. String edpName - The name of the EDP Project. Map stages - Contains all stages in JSON format that is retrieved from Jenkins job env variable. String envToPromote - The name of the environment for promoting images. Boolean promoteImages - Defines whether images should be promoted or not. Methods : getParameterValue(String parameter, String defaultValue = null): return parameter of ENV variable of Jenkins job. init(): set all the properties of the Job object. setDisplayName(String displayName): set display name of the Jenkins job. setDescription(String description, Boolean addDescription = false): set new or add to the existing description of the Jenkins job. printDebugInfo(Map context): print context info to the log of Jenkins' job. runStage(String stage_name, Map context): run the particular stage according to its name. Example : context.job = new Job(JobType.CODEREVIEW.value, context.platform, this) context.job.init() context.job.printDebugInfo(context) context.job.setDisplayName(\"test\") context.job.setDescription(\"Name: ${context.application.config.name}\") Gerrit(Job job, Platform platform, Script script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String credentialsId - Credentials Id in Jenkins for Gerrit. String autouser - Username of an auto user in Gerrit for integration with Jenkins. String host - Gerrit host. String project - the project name of the built application. String branch - branch to build an application from. String changeNumber - change number of Gerrit commit. String changeName - change name of Gerrit commit. String refspecName - refspecName of Gerrit commit. String sshPort - Gerrit ssh port number. String patchsetNumber - patchsetNumber of Gerrit commit. Methods : init(): set all the properties of Gerrit object Example : context.gerrit = new Gerrit(context.job, context.platform, this) context.gerrit.init() Nexus(Job job, Platform platform, Script script) - Class that describes the Nexus tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String autouser - Username of an auto user in Nexus for integration with Jenkins. String credentialsId - Credentials Id in Jenkins for Nexus. String host - Nexus host. String port - Nexus http(s) port. String repositoriesUrl - Base URL of repositories in Nexus. String restUrl - URL of Rest API. Methods : init(): set all the properties of the Nexus object. Example : context.nexus = new Nexus(context.job, context.platform, this) context.nexus.init() Sonar(Job job, Platform platform, Script script) - Class that describes the Sonar tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String route - External route of the sonar application. Methods : init(): set all the properties of Sonar object. Example : context.sonar = new Sonar(context.job, context.platform, this) context.sonar.init() Build Pipeline Stages \u2693\ufe0e Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. The Build pipeline includes the following default stages: Checkout \u2192 Gerrit Checkout \u2192 Compile \u2192 Get version \u2192 Tests \u2192 Sonar \u2192 Build \u2192 Build Docker Image \u2192 Push \u2192 Git tag . Info To get the full description of every stage, please refer to the EDP Stages Framework section. How to Redefine or Extend EDP Pipeline Stages Library \u2693\ufe0e Inspect the points below to redefine or extend the EDP Pipeline Stages Library: Create a \u201cstage\u201d folder in the App repository. Create a Groovy file with a meaningful name for the custom stage description. For instance \u2013 CustomBuildMavenApplication.groovy Describe stage logic. Redefinition: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"compile\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class CustomBuildMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomBuildMavenApplication Extension: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"new-stage\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class NewStageMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return NewStageMavenApplication Using EDP Stages Library in the Pipeline \u2693\ufe0e In order to use the EDP stages, the created pipeline should fit some requirements, that`s why a developer has to do the following: import library - @Library(['edp-library-stages']) import StageFactory class - import com.epam.edp.stages.StageFactory define context Map \u2013 context = [:] define stagesFactory instance and load EDP stages: context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } After that, there is the ability to run any EDP stage beforehand by defining a requirement context context.factory.getStage(\"checkout\",\"maven\",\"application\").run(context) For instance, the pipeline can look like : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] node ( ' maven ' ) { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] stage ( \"checkout\" ) { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } stage ( \"compile\" ) { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } Or in a declarative way : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] pipeline { agent { label ' maven ' } stages { stage ( ' Init ' ){ steps { script { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] } } } stage ( \"Checkout\" ) { steps { script { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } } } stage ( ' Compile ' ) { steps { script { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } } } } EDP Library Stages Description \u2693\ufe0e Using in pipelines - @Library(['edp-library-stages@version']) The corresponding enums, classes, interfaces and their methods can be used separately from the EDP Stages library function (please refer to Table 5 ). Table 5. Enums and Classes with the respective properties, methods, and examples. Enums Classes ProjectType : - APPLICATION - AUTOTESTS - LIBRARY StageFactory() - Class that contains methods getting an implementation of the particular stage either EDP from shared library or custom from application repository. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Map stages - Map of stages implementations. Methods : loadEdpStages(): return a list of Classes that describes EDP stages implementations. loadCustomStages(String directory): return a list of Classes that describes EDP custom stages from application repository from \"directory\". The \"directory\" should have an absolute path to files with classes of custom stages implementations. Should be run from a Jenkins agent. add(Class clazz): register class for some particular stage in stages map of StageFactory class. getStage(String name, String buildTool, String type): return an object of the class for a particular stage from stages property based on stage name and buildTool, type of application. Example : context.factory = new StageFactory(script: this) context.factory.loadEdpStages().each() { context.factory.add(it) } context.factory.loadCustomStages(\"${context.workDir}/stages\").each() { context.factory.add(it) } context.factory.getStage(stageName.toLowerCase(),context.application.config.build_tool.toLowerCase(), context.application.config.type).run(context) EDP Stages Framework \u2693\ufe0e Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. Inspect the Table 6 and Table 7 that contain the full description of every stage that can be included in Code Review and Build pipelines: Checkout \u2192 Gerrit Checkout \u2192 Compile \u2192 Get version \u2192 Tests \u2192 Sonar \u2192 Build \u2192 Build Docker Image \u2192 Push \u2192 Git tag . Table 6. The Checkout, Gerrit Checkout, Compile, Get version, and Tests stages description. Checkout Gerrit Checkout Compile Get version Tests name = \"checkout\", buildTool = [\"maven\", \"npm\", \"dotnet\",\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - StageFactory context.factory - String context.gerrit.branch - String context.gerrit.credentialsId - String context.application.config.cloneUrl name = \"gerrit-checkout\", buildTool = [\"maven\", \"npm\", \"dotnet\",\"gradle\"] type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY] context required: - String context.workDir - StageFactory context.factory - String context.gerrit.changeName - String context.gerrit.refspecName - String context.gerrit.credentialsId - String context.application.config.cloneUrl name = \"compile\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.sln_filename output: - String context.buildTool.sln_filename buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.groupRepository name = \"get-version\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - Map(empty) context.application - String context.gerrit.branch - Job context.job output: -String context.application.deplyableModule - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.command - Job context.job - String context.gerrit.branch output: - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.command - Job context.job - String context.gerrit.branch output: - String context.application.deplyableModule - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - Job context.job - String context.gerrit.branch output: - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion name = \"tests\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.command buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command type = [ProjectType.AUTOTESTS] context required: - String context.workDir - String context.buildTool.command - String context.application.config.report_framework buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir Table 7. The Sonar, Build, Build Docker Image, Push, and Git tag stages description. Sonar Build Build Docker Image Push Git tag name = \"sonar\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.job.type - String context.application.name - String context.buildTool.sln_filename - String context.sonar.route - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.job.type - String context.nexus.credentialsId - String context.buildTool.command - String context.application.name - String context.sonarRoute - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) buildTool = [\"maven\"] type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY] context required: - String context.workDir - String context.job.type - String context.nexus.credentialsId - String context.application.name - String context.buildTool.command - String context.sonar.route - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.job.type - String context.sonar.route - String context.application.name - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) name = \"build\" buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.groupRepository name = \"build-image\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote name = \"push\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.gerrit.project - String context.buildTool.sln_filename - String context.buildTool.snugetApiKey - String context.buildTool.hostedRepository buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.application.version - String context.buildTool.hostedRepository - String context. buildTool.settings buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.application.version - String context.buildTool.hostedRepository - String context.buildTool.command buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.hostedRepository - String context.gerrit.autouser name = \"git-tag\" buildTool = [\"maven\", \"npm\", \"dotnet\",\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.gerrit.credentialsId - String context.gerrit.sshPort - String context.gerrit.host - String context.gerrit.autouser - String context.application.buildVersion Deploy Pipeline \u2693\ufe0e Deploy() \u2013 a function that allows using the EDP implementation for the deploy pipeline. All values of different parameters that are used during the pipeline execution are stored in the \"Map\" context. The deploy pipeline consists of several steps: On the master: Initialization of all objects (Platform, Job, Gerrit, Nexus, StageFactory) and loading the default implementations of EDP stages; Creating an environment if it doesn`t exist; Deploying the last versions of the applications; Run predefined manual gates. On a particular autotest Jenkins agent that depends on the build tool: Creating workdir for autotest sources; Run predefined autotests. EDP Library Pipelines Description \u2693\ufe0e _Using in pipelines - @Library(['edp-library-pipelines@version']) _ The corresponding enums and interfaces with their methods can be used separately from the EDP Pipelines library function (please refer to Table 8 and Table 9 ). Table 8. Enums and Interfaces with the respective properties, methods, and examples. Enums Interfaces PlatformType : - OPENSHIFT - KUBERNETES JobType : - CODEREVIEW - BUILD - DEPLOY BuildToolType : - MAVEN - GRADLE - NPM - DOTNET Platform() - contains methods for working with platform CLI. At the moment only OpenShift is supported. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Methods : getJsonPathValue(String k8s_kind, String k8s_kind_name, String jsonPath): return String value of specific parameter of particular object using jsonPath utility. Example : context.platform.getJsonPathValue(\"cm\",\"project-settings\", \".data.username\") BuildTool() - contains methods for working with different buildTool from ENUM BuildToolType. (Should be invoked on Jenkins build agents) Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Nexus object - Object of class Nexus. Methods : init: return parameters of buildTool that are needed for running stages. Example : context.buildTool = new BuildToolFactory().getBuildToolImpl (context.application.config.build_tool, this, context.nexus) context.buildTool.init() Table 9. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) PlatformFactory() - Class that contains methods getting implementation of CLI of platform. At the moment OpenShift and Kubernetes are supported. Methods : getPlatformImpl(PlatformType platform, Script script): return Class Platform Example : context.platform = new PlatformFactory().getPlatformImpl(PlatformType.OPENSHIFT, this) Application(String name, Platform platform, Script script) - Class that describe the application object. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform() String name - Name for the application for creating object Map config - Map of configuration settings for particular application that is loaded from config map project-settings String version - Application version, initially empty. Is set on get-version step. String deployableModule - The name of deployable module for multi module applications, initially empty. String buildVersion - Version of built artifact, contains build number of Job initially empty String deployableModuleDir - The name of deployable module directory for multi module applications, initially empty. Array imageBuildArgs - List of arguments for building application Docker image Methods : setConfig(String gerrit_autouser, String gerrit_host, String gerrit_sshPort, String gerrit_project): set the config property with values from config map Example : context.application = new Application(context.job, context.gerrit.project, context.platform, this) context.application.setConfig(context.gerrit.autouser, context.gerrit.host, context.gerrit.sshPort, context.gerrit.project) Job(type: JobType.value, platform: Platform, script: Script) - Class that describe the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\" Platform platform - Object of a class Platform(). JobType.value type. String deployTemplatesDirectory - The name of the directory in application repository, where deploy templates are located. Can be set for particular Job through DEPLOY_TEMPLATES_DIRECTORY parameter. String edpName - The name of the EDP Project. Map stages - Contains all stages in JSON format that is retrieved from Jenkins job env variable. String envToPromote - The name of the environment for promoting images. Boolean promoteImages - Defines whether images should be promoted or not. Methods : getParameterValue(String parameter, String defaultValue = null): return parameter of ENV variable of Jenkins job. init(): set all the properties of Job object. setDisplayName(String displayName): set display name of the Jenkins job. setDescription(String description, Boolean addDescription = false): set new or add to existing description of the Jenkins job. printDebugInfo(Map context): print context info to log of Jenkins job. runStage(String stage_name, Map context): run the particular stage according to its name. Example : context.job = new Job(JobType.DEPLOY.value, context.platform, this) context.job.init() context.job.printDebugInfo(context) context.job.setDisplayName(\"test\") context.job.setDescription(\"Name: ${context.application.config.name}\") Gerrit(Job job, Platform platform, Script script) - Class that describe the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String credentialsId - Credential Id in Jenkins for Gerrit. String autouser - Username of autouser in Gerrit for integration with Jenkins. String host - Gerrit host. String project - project name of built application. String branch - branch to build application from. String changeNumber - change number of Gerrit commit. String changeName - change name of Gerrit commit. String refspecName - refspecName of Gerrit commit. String sshPort - gerrit ssh port number. String patchsetNumber - patchsetNumber of Gerrit commit. Methods : init(): set all the properties of Gerrit object. Example : context.gerrit = new Gerrit(context.job, context.platform, this) context.gerrit.init() . Nexus(Job job, Platform platform, Script script) - Class that describe the Nexus tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String autouser - Username of autouser in Nexus for integration with Jenkins. String credentialsId - Credential Id in Jenkins for Nexus. String host - Nexus host. String port - Nexus http(s) port. String repositoriesUrl - Base URL of repositories in Nexus. String restUrl - URL of Rest API. Methods : init(): set all the properties of Nexus object. Example : context.nexus = new Nexus(context.job, context.platform, this) context.nexus.init() . EDP Library Stages Description \u2693\ufe0e Using in pipelines - @Library(['edp-library-stages@version']) _ The corresponding classes with methods can be used separately from the EDP Pipelines library function (please refer to Table 10 ). Table 10. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) StageFactory() - Class that contains methods getting implementation of particular stage either EDP from shared library or custom from application repository. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\" Map stages - Map of stages implementations Methods : loadEdpStages(): return list of Classes that describes EDP stages implementations loadCustomStages(String directory): return list of Classes that describes EDP custom stages from application repository from \"directory\". The \"directory\" should be absolute path to files with classes of custom stages implementations. Should be run from Jenkins agent. add(Class clazz): register class for some particular stage in stages map of StageFactory class getStage(String name, String buildTool, String type): return object of the class for particular stage from stages property based on stage name and buildTool, type of application Example : context.factory = new StageFactory(script: this) context.factory.loadEdpStages().each() { context.factory.add(it) } context.factory.loadCustomStages(\"${context.workDir}/stages\").each() { context.factory.add(it) } context.factory.getStage(stageName.toLowerCase(),context.application.config.build_tool.toLowerCase(), context.application.config.type).run(context) . Deploy Pipeline Stages \u2693\ufe0e Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. The stages for the deploy pipeline are independent of the build tool and application type. Find below (see Table 11 ) the full description of every stage: Deploy \u2192 Automated tests \u2192 Promote Images . Table 11. The Deploy, Automated tests, and Promote Images stages description. Deploy Automated tests Promote Images name = \"deploy\" buildTool = null type = null context required: \u2022 String context.workDir \u2022 StageFactory context.factory \u2022 String context.gerrit.autouser \u2022 String context.gerrit.host \u2022 String context.application.config.cloneUrl \u2022 String context.jenkins.token \u2022 String context.job.edpName \u2022 String context.job.buildUrl \u2022 String context.job.jenkinsUrl \u2022 String context.job.metaProject \u2022 List context.job.applicationsList [['name':'application1_name','version':'application1_version],...] \u2022 String context.job.deployTemplatesDirectory output: \u2022 List context.job.updatedApplicaions [['name':'application1_name','version':'application1_version],...] name = \"automation-tests\", buildTool = null, type = null context required: - String context.workDir - StageFactory context.factory - String context.gerrit.credentialsId - String context.autotest.config.cloneUrl - String context.autotest.name - String context.job.stageWithoutPrefixName - String context.buildTool.settings - String context.autotest.config.report_framework name = \"promote-images\" buildTool = null type = null context required: - String context.workDir - String context.buildTool.sln_filename - List context.job.updatedApplicaions [['name':'application1_name','version':'application1_version],...] How to Redefine or Extend EDP Pipeline Stages Library \u2693\ufe0e Info Currently, the redefinition of Deploy pipeline stages is prohibited. Using EDP Library Stages in the Pipeline \u2693\ufe0e In order to use the EDP stages, the created pipeline should fit some requirements, that`s why a developer has to do the following: import libraries - @Library(['edp-library-stages', 'edp-library-pipelines']) _ import reference EDP classes(See example below) define context Map \u2013 context = [:] define reference \"init\" stage After that, there is the ability to run any EDP stage beforehand by defining requirement context context.job.runStage(\"Deploy\", context) . For instance, the pipeline can look like : @Library ( [ ' edp - library - stages ' , ' edp - library - pipelines ' ] ) _ import com.epam.edp.stages.StageFactory import com.epam.edp.platform.PlatformFactory import com.epam.edp.platform.PlatformType import com.epam.edp.JobType context = [ : ] node ( ' master ' ) { stage ( \"Init\" ) { context . platform = new PlatformFactory (). getPlatformImpl ( PlatformType . OPENSHIFT , this ) context . job = new com . epam . edp . Job ( JobType . DEPLOY . value , context . platform , this ) context . job . init () context . job . initDeployJob () println ( \"[JENKINS][DEBUG] Created object job with type - ${context.job.type}\" ) context . nexus = new com . epam . edp . Nexus ( context . job , context . platform , this ) context . nexus . init () context . jenkins = new com . epam . edp . Jenkins ( context . job , context . platform , this ) context . jenkins . init () context . gerrit = new com . epam . edp . Gerrit ( context . job , context . platform , this ) context . gerrit . init () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . environment = new com . epam . edp . Environment ( context . job . deployProject , context . platform , this ) context . job . printDebugInfo ( context ) context . job . setDisplayName ( \"${currentBuild.displayName}-${context.job.deployProject}\" ) context . job . generateInputDataForDeployJob () } stage ( \"Pre Deploy Custom stage\" ) { println ( \"Some custom pre deploy logic\" ) } context . job . runStage ( \"Deploy\" , context ) stage ( \"Post Deploy Custom stage\" ) { println ( \"Some custom post deploy logic\" ) } } Or in a declarative way : @Library ( [ ' edp - library - stages ' , ' edp - library - pipelines ' ] ) _ import com.epam.edp.stages.StageFactory import com.epam.edp.platform.PlatformFactory import com.epam.edp.platform.PlatformType import com.epam.edp.JobType context = [ : ] pipeline { agent { label ' master ' } stages { stage ( ' Init ' ) { steps { script { context . platform = new PlatformFactory (). getPlatformImpl ( PlatformType . OPENSHIFT , this ) context . job = new com . epam . edp . Job ( JobType . DEPLOY . value , context . platform , this ) context . job . init () context . job . initDeployJob () println ( \"[JENKINS][DEBUG] Created object job with type - ${context.job.type}\" ) context . nexus = new com . epam . edp . Nexus ( context . job , context . platform , this ) context . nexus . init () context . jenkins = new com . epam . edp . Jenkins ( context . job , context . platform , this ) context . jenkins . init () context . gerrit = new com . epam . edp . Gerrit ( context . job , context . platform , this ) context . gerrit . init () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . environment = new com . epam . edp . Environment ( context . job . deployProject , context . platform , this ) context . job . printDebugInfo ( context ) context . job . setDisplayName ( \"${currentBuild.displayName}-${context.job.deployProject}\" ) context . job . generateInputDataForDeployJob () } } } stage ( ' Deploy ' ) { steps { script { context . factory . getStage ( \"deploy\" ). run ( context ) } } } stage ( ' Custom stage ' ) { steps { println ( \"Some custom logic\" ) } } } } Related Articles \u2693\ufe0e Add Application Add Library Add CD Pipeline CI Pipeline Details CD Pipeline Details Customize CI Pipeline Customize CD Pipeline EDP Stages Glossary Use Terraform Library in EDP","title":"EDP Pipeline Framework"},{"location":"user-guide/pipeline-framework/#edp-pipeline-framework","text":"This chapter provides detailed information about the EDP pipeline framework concepts and parts, as well as the accurate data about the Code Review , Build and Deploy pipelines with the respective stages.","title":"EDP Pipeline Framework"},{"location":"user-guide/pipeline-framework/#edp-pipeline-framework-overview","text":"Note The whole logic is applied to Jenkins as it is the main tool for the CI/CD processes organization. EDP pipeline framework basic The general EDP Pipeline Framework consists of several parts: Jenkinsfile - a text file that keeps the definition of a Jenkins Pipeline and is checked into source control. Every Job has its Jenkinsfile stored in the specific application repository and in Jenkins as the plain text. The behavior logic of the pipelines can be customized easily by modifying a source code which is always copied to the EDP repository after the EDP installation. Jenkinsfile example Loading Shared Libraries - a part where every job loads libraries with the help of the shared libraries mechanism for Jenkins that allows to create reproducible pipelines, write them uniformly, and manage the update process. There are two main libraries: EDP Pipelines with the common logic described for the main pipelines Code Review, Build, Deploy pipelines and EDP Stages library that keeps the description of the stages for every pipeline. Run Stages - a part where the predefined default stages are launched. Pipeline script","title":"EDP Pipeline Framework Overview"},{"location":"user-guide/pipeline-framework/#cicd-jobs-comparison","text":"Explore the CI and CD job comparison. Please note that the dynamic stages order can be changed, meanwhile, the predefined stages order in the reference pipeline cannot be changed, i.e. only the predefined stages set can be run. CI/CD jobs comparison","title":"CI/CD Jobs Comparison"},{"location":"user-guide/pipeline-framework/#context","text":"Context - a variable that stores and transfers all necessary parameters between stages that are used by pipeline during performing. The context type is \"Map\". Each stage has input and output context. Each stage has a mandatory input context. Note If the input context isn't transferred, the stage will be failed.","title":"Context"},{"location":"user-guide/pipeline-framework/#annotations-for-cicd-stages","text":"Annotation for CI Stages: The annotation type is \"Map\"; The annotation consists of the name, buildTool, and codebaseType. Annotation for CD Stages: The annotation type is \"Map\"; The annotation consists of a name.","title":"Annotations for CI/CD Stages"},{"location":"user-guide/pipeline-framework/#code-review-pipeline","text":"CodeReview() \u2013 a function that allows using the EDP implementation for the Code Review pipeline. Note All values of different parameters that are used during the pipeline execution are stored in the \"Map\" context. The Code Review pipeline consists of several steps: On the master: Initialization of all objects (Platform, Job, Gerrit, Nexus, Sonar, Application, StageFactory) and loading of the default implementations of EDP stages. On a particular Jenkins agent that depends on the build tool: Creating workdir for application sources; Loading build tool implementation for a particular application; Run in a loop all stages (From) and run them either in parallel or one by one.","title":"Code Review Pipeline"},{"location":"user-guide/pipeline-framework/#code-review-pipeline-overview","text":"Using in pipelines - @Library(['edp-library-pipelines@version']) The corresponding enums, interfaces, classes, and their methods can be used separately from the EDP Pipelines library function (please refer to Table 1 and Table 2 ). Table 1. Enums and Interfaces with the respective properties, methods, and examples. Enums Interfaces PlatformType: - OPENSHIFT - KUBERNETES JobType: - CODEREVIEW - BUILD - DEPLOY BuildToolType: - MAVEN - GRADLE - NPM - DOTNET Platform() - contains methods for working with platform CLI. At the moment only OpenShift is supported. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Methods : getJsonPathValue(String k8s_kind, String k8s_kind_name, String jsonPath): return String value of specific parameter of particular object using jsonPath utility. Example : context.platform.getJsonPathValue(''cm'', ''project-settings'', ''.data.username'') . BuildTool() - contains methods for working with different buildTool from ENUM BuildToolType. Should be invoked on Jenkins build agents. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Nexus object - Object of class Nexus. Methods : init: return parameters of buildTool that are needed for running stages. Example : context.buildTool = new BuildToolFactory(). getBuildToolImpl (context.application.config.build_tool, this, context.nexus) context.buildTool.init() . Table 2. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) PlatformFactory() - Class that contains methods getting an implementation of CLI of the platform. At the moment OpenShift and Kubernetes are supported. Methods : getPlatformImpl(PlatformType platform, Script script): return Class Platform . Example : context.platform = new PlatformFactory().getPlatformImpl(PlatformType.OPENSHIFT, this) . Application(String name, Platform platform, Script script) - Class that describes the application object. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). String name - Name for the application for creating an object. Map config - Map of configuration settings for the particular application that is loaded from config map project-settings. String version - Application version, initially empty. Is set on the get-version step. String deployableModule - The name of the deployable module for multi-module applications, initially empty. String buildVersion - Version of the built artifact, contains build number of Job initially empty. String deployableModuleDir - The name of deployable module directory for multi-module applications, initially empty. Array imageBuildArgs - List of arguments for building an application Docker image. Methods : setConfig(String gerrit_autouser, String gerrit_host, String gerrit_sshPort, String gerrit_project): set the config property with values from config map. Example : context.application = new Application(context.job, context.gerrit.project, context.platform, this) context.application.setConfig(context.gerrit.autouser, context.gerrit.host, context.gerrit.sshPort, context.gerrit.project) Job(type: JobType.value, platform: Platform, script: Script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). JobType.value type. String deployTemplatesDirectory - The name of the directory in application repository where deploy templates are located. It can be set for a particular Job through DEPLOY_TEMPLATES_DIRECTORY parameter. String edpName - The name of the EDP Project. Map stages - Contains all stages in JSON format that is retrieved from Jenkins job env variable. String envToPromote - The name of the environment for promoting images. Boolean promoteImages - Defines whether images should be promoted or not. Methods : getParameterValue(String parameter, String defaultValue = null): return parameter of ENV variable of Jenkins job. init(): set all the properties of the Job object. setDisplayName(String displayName): set display name of the Jenkins job. setDescription(String description, Boolean addDescription = false): set new or add to the existing description of the Jenkins job. printDebugInfo(Map context): print context info to the log of Jenkins' job. runStage(String stage_name, Map context): run the particular stage according to its name. Example : context.job = new Job(JobType.CODEREVIEW.value, context.platform, this) context.job.init() context.job.printDebugInfo(context) context.job.setDisplayName(\"test\") context.job.setDescription(\"Name: ${context.application.config.name}\") Gerrit(Job job, Platform platform, Script script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String credentialsId - Credential Id in Jenkins for Gerrit. String autouser - Username of an auto user in Gerrit for integration with Jenkins. String host - Gerrit host. String project - the project name of the built application. String branch - branch to build the application from. String changeNumber - change number of Gerrit commit. String changeName - change name of Gerrit commit. String refspecName - refspecName of Gerrit commit. String sshPort - Gerrit ssh port number. String patchsetNumber - patchsetNumber of Gerrit commit. Methods : init(): set all the properties of Gerrit object. Example : context.gerrit = new Gerrit(context.job, context.platform, this) context.gerrit.init() Nexus(Job job, Platform platform, Script script) - Class that describes the Nexus tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String autouser - Username of an auto user in Nexus for integration with Jenkins. String credentialsId - Credential Id in Jenkins for Nexus. String host - Nexus host. String port - Nexus http(s) port. String repositoriesUrl - Base URL of repositories in Nexus. String restUrl - URL of Rest API. Methods : init(): set all the properties of Nexus object Example : context.nexus = new Nexus(context.job, context.platform, this) context.nexus.init() Sonar(Job job, Platform platform, Script script) - Class that describes the Sonar tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String route - External route of the sonar application. Methods : init(): set all the properties of Sonar object Example : context.sonar = new Sonar(context.job, context.platform, this) context.sonar.init()","title":"Code Review Pipeline Overview"},{"location":"user-guide/pipeline-framework/#code-review-pipeline-stages","text":"Each EDP stage implementation has run method that is as input parameter required to pass the \"Map\" context with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. The Code Review pipeline includes the following default stages: Checkout \u2192 Gerrit Checkout \u2192 Compile \u2192 Tests \u2192 Sonar . Info To get the full description of every stage, please refer to the EDP Stages Framework section.","title":"Code Review Pipeline Stages"},{"location":"user-guide/pipeline-framework/#how-to-redefine-or-extend-the-edp-pipeline-stages-library","text":"Inspect the points below to redefine or extend the EDP Pipeline Stages Library: Create \u201cstage\u201d folder in your App repository. Create a Groovy file with a meaningful name for the custom stage description. For instance \u2013 CustomBuildMavenApplication.groovy. Describe the stage logic. Redefinition: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"compile\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class CustomBuildMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomBuildMavenApplication Extension: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"new-stage\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class NewStageMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return NewStageMavenApplication","title":"How to Redefine or Extend the EDP Pipeline Stages Library"},{"location":"user-guide/pipeline-framework/#using-edp-stages-library-in-the-pipeline","text":"In order to use the EDP stages, the created pipeline should fit some requirements, that`s why a developer has to do the following: import library - @Library(['edp-library-stages']) import StageFactory class - import com.epam.edp.stages.StageFactory define context Map \u2013 context = [:] define stagesFactory instance and load EDP stages: context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } After that, there is the ability to run any EDP stage beforehand by defining a necessary context: context.factory.getStage(\"checkout\",\"maven\",\"application\").run(context) For instance, the pipeline can look like : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] node ( ' maven ' ) { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] stage ( \"checkout\" ) { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } stage ( \"compile\" ) { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } Or in a declarative way : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] pipeline { agent { label ' maven ' } stages { stage ( ' Init ' ){ steps { script { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] } } } stage ( \"Checkout\" ) { steps { script { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } } } stage ( ' Compile ' ) { steps { script { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } } } }","title":"Using EDP Stages Library in the Pipeline"},{"location":"user-guide/pipeline-framework/#build-pipeline","text":"Build() \u2013 a function that allows using the EDP implementation for the Build pipeline. All values of different parameters that are used during the pipeline execution are stored in the \"Map\" context. The Build pipeline consists of several steps: On the master: Initialization of all objects (Platform, Job, Gerrit, Nexus, Sonar, Application, StageFactory) and loading default implementations of EDP stages. On a particular Jenkins agent that depends on the build tool: Creating workdir for application sources; Loading build tool implementation for a particular application; Run in a loop all stages (From) and run them either in parallel or one by one.","title":"Build Pipeline"},{"location":"user-guide/pipeline-framework/#build-pipeline-overview","text":"Using in pipelines - @Library(['edp-library-pipelines@version']) The corresponding enums, interfaces, classes, and their methods can be used separately from the EDP Pipelines library function (please refer to Table 3 and Table 4 ). Table 3. Enums and Interfaces with the respective properties, methods, and examples. Enums Interfaces PlatformType: - OPENSHIFT - KUBERNETES JobType: - CODEREVIEW - BUILD - DEPLOY BuildToolType : - MAVEN - GRADLE - NPM - DOTNET Platform() - contains methods for working with platform CLI. At the moment only OpenShift is supported. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Methods : getJsonPathValue(String k8s_kind, String k8s_kind_name, String jsonPath): return String value of specific parameter of particular object using jsonPath utility. Example : context.platform.getJsonPathValue(\"cm\",\"project-settings\", \".data.username\") BuildTool() - contains methods for working with different buildTool from ENUM BuildToolType. Should be invoked on Jenkins build agents. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Nexus object - Object of class Nexus. See description below: Methods : init: return parameters of buildTool that are needed for running stages. Example : context.buildTool = new BuildToolFactory().getBuildToolImpl (context.application.config.build_tool, this, context.nexus)context.buildTool.init() Table 4. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) PlatformFactory() - Class that contains methods getting an implementation of CLI of the platform. At the moment OpenShift and Kubernetes are supported. Methods : getPlatformImpl(PlatformType platform, Script script): return Class Platform Example : context.platform = new PlatformFactory().getPlatformImpl(PlatformType.OPENSHIFT, this) Application(String name, Platform platform, Script script) - Class that describes the application object. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). String name - Name for the application for creating an object. Map config - Map of configuration settings for the particular application that is loaded from config map project-settings. String version - Application version, initially empty. Is set on the get-version step. String deployableModule - The name of the deployable module for multi-module applications, initially empty. String buildVersion - Version of the built artifact, contains build number of Job initially empty. String deployableModuleDir - The name of deployable module directory for multi-module applications, initially empty. Array imageBuildArgs - List of arguments for building the application Docker image. Methods : setConfig(String gerrit_autouser, String gerrit_host, String gerrit_sshPort, String gerrit_project): set the config property with values from config map. Example : context.application = new Application(context.job, context.gerrit.project, context.platform, this) context.application.setConfig(context.gerrit.autouser, context.gerrit.host, context.gerrit.sshPort, context.gerrit.project) Job(type: JobType.value, platform: Platform, script: Script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). JobType.value type. String deployTemplatesDirectory - The name of the directory in application repository, where deploy templates are located. It can be set for a particular Job through DEPLOY_TEMPLATES_DIRECTORY parameter. String edpName - The name of the EDP Project. Map stages - Contains all stages in JSON format that is retrieved from Jenkins job env variable. String envToPromote - The name of the environment for promoting images. Boolean promoteImages - Defines whether images should be promoted or not. Methods : getParameterValue(String parameter, String defaultValue = null): return parameter of ENV variable of Jenkins job. init(): set all the properties of the Job object. setDisplayName(String displayName): set display name of the Jenkins job. setDescription(String description, Boolean addDescription = false): set new or add to the existing description of the Jenkins job. printDebugInfo(Map context): print context info to the log of Jenkins' job. runStage(String stage_name, Map context): run the particular stage according to its name. Example : context.job = new Job(JobType.CODEREVIEW.value, context.platform, this) context.job.init() context.job.printDebugInfo(context) context.job.setDisplayName(\"test\") context.job.setDescription(\"Name: ${context.application.config.name}\") Gerrit(Job job, Platform platform, Script script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String credentialsId - Credentials Id in Jenkins for Gerrit. String autouser - Username of an auto user in Gerrit for integration with Jenkins. String host - Gerrit host. String project - the project name of the built application. String branch - branch to build an application from. String changeNumber - change number of Gerrit commit. String changeName - change name of Gerrit commit. String refspecName - refspecName of Gerrit commit. String sshPort - Gerrit ssh port number. String patchsetNumber - patchsetNumber of Gerrit commit. Methods : init(): set all the properties of Gerrit object Example : context.gerrit = new Gerrit(context.job, context.platform, this) context.gerrit.init() Nexus(Job job, Platform platform, Script script) - Class that describes the Nexus tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String autouser - Username of an auto user in Nexus for integration with Jenkins. String credentialsId - Credentials Id in Jenkins for Nexus. String host - Nexus host. String port - Nexus http(s) port. String repositoriesUrl - Base URL of repositories in Nexus. String restUrl - URL of Rest API. Methods : init(): set all the properties of the Nexus object. Example : context.nexus = new Nexus(context.job, context.platform, this) context.nexus.init() Sonar(Job job, Platform platform, Script script) - Class that describes the Sonar tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String route - External route of the sonar application. Methods : init(): set all the properties of Sonar object. Example : context.sonar = new Sonar(context.job, context.platform, this) context.sonar.init()","title":"Build Pipeline Overview"},{"location":"user-guide/pipeline-framework/#build-pipeline-stages","text":"Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. The Build pipeline includes the following default stages: Checkout \u2192 Gerrit Checkout \u2192 Compile \u2192 Get version \u2192 Tests \u2192 Sonar \u2192 Build \u2192 Build Docker Image \u2192 Push \u2192 Git tag . Info To get the full description of every stage, please refer to the EDP Stages Framework section.","title":"Build Pipeline Stages"},{"location":"user-guide/pipeline-framework/#how-to-redefine-or-extend-edp-pipeline-stages-library","text":"Inspect the points below to redefine or extend the EDP Pipeline Stages Library: Create a \u201cstage\u201d folder in the App repository. Create a Groovy file with a meaningful name for the custom stage description. For instance \u2013 CustomBuildMavenApplication.groovy Describe stage logic. Redefinition: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"compile\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class CustomBuildMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomBuildMavenApplication Extension: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"new-stage\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class NewStageMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return NewStageMavenApplication","title":"How to Redefine or Extend EDP Pipeline Stages Library"},{"location":"user-guide/pipeline-framework/#using-edp-stages-library-in-the-pipeline_1","text":"In order to use the EDP stages, the created pipeline should fit some requirements, that`s why a developer has to do the following: import library - @Library(['edp-library-stages']) import StageFactory class - import com.epam.edp.stages.StageFactory define context Map \u2013 context = [:] define stagesFactory instance and load EDP stages: context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } After that, there is the ability to run any EDP stage beforehand by defining a requirement context context.factory.getStage(\"checkout\",\"maven\",\"application\").run(context) For instance, the pipeline can look like : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] node ( ' maven ' ) { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] stage ( \"checkout\" ) { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } stage ( \"compile\" ) { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } Or in a declarative way : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] pipeline { agent { label ' maven ' } stages { stage ( ' Init ' ){ steps { script { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] } } } stage ( \"Checkout\" ) { steps { script { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } } } stage ( ' Compile ' ) { steps { script { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } } } }","title":"Using EDP Stages Library in the Pipeline"},{"location":"user-guide/pipeline-framework/#edp-library-stages-description","text":"Using in pipelines - @Library(['edp-library-stages@version']) The corresponding enums, classes, interfaces and their methods can be used separately from the EDP Stages library function (please refer to Table 5 ). Table 5. Enums and Classes with the respective properties, methods, and examples. Enums Classes ProjectType : - APPLICATION - AUTOTESTS - LIBRARY StageFactory() - Class that contains methods getting an implementation of the particular stage either EDP from shared library or custom from application repository. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Map stages - Map of stages implementations. Methods : loadEdpStages(): return a list of Classes that describes EDP stages implementations. loadCustomStages(String directory): return a list of Classes that describes EDP custom stages from application repository from \"directory\". The \"directory\" should have an absolute path to files with classes of custom stages implementations. Should be run from a Jenkins agent. add(Class clazz): register class for some particular stage in stages map of StageFactory class. getStage(String name, String buildTool, String type): return an object of the class for a particular stage from stages property based on stage name and buildTool, type of application. Example : context.factory = new StageFactory(script: this) context.factory.loadEdpStages().each() { context.factory.add(it) } context.factory.loadCustomStages(\"${context.workDir}/stages\").each() { context.factory.add(it) } context.factory.getStage(stageName.toLowerCase(),context.application.config.build_tool.toLowerCase(), context.application.config.type).run(context)","title":"EDP Library Stages Description"},{"location":"user-guide/pipeline-framework/#edp-stages-framework","text":"Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. Inspect the Table 6 and Table 7 that contain the full description of every stage that can be included in Code Review and Build pipelines: Checkout \u2192 Gerrit Checkout \u2192 Compile \u2192 Get version \u2192 Tests \u2192 Sonar \u2192 Build \u2192 Build Docker Image \u2192 Push \u2192 Git tag . Table 6. The Checkout, Gerrit Checkout, Compile, Get version, and Tests stages description. Checkout Gerrit Checkout Compile Get version Tests name = \"checkout\", buildTool = [\"maven\", \"npm\", \"dotnet\",\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - StageFactory context.factory - String context.gerrit.branch - String context.gerrit.credentialsId - String context.application.config.cloneUrl name = \"gerrit-checkout\", buildTool = [\"maven\", \"npm\", \"dotnet\",\"gradle\"] type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY] context required: - String context.workDir - StageFactory context.factory - String context.gerrit.changeName - String context.gerrit.refspecName - String context.gerrit.credentialsId - String context.application.config.cloneUrl name = \"compile\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.sln_filename output: - String context.buildTool.sln_filename buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.groupRepository name = \"get-version\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - Map(empty) context.application - String context.gerrit.branch - Job context.job output: -String context.application.deplyableModule - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.command - Job context.job - String context.gerrit.branch output: - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.command - Job context.job - String context.gerrit.branch output: - String context.application.deplyableModule - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - Job context.job - String context.gerrit.branch output: - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion name = \"tests\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.command buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command type = [ProjectType.AUTOTESTS] context required: - String context.workDir - String context.buildTool.command - String context.application.config.report_framework buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir Table 7. The Sonar, Build, Build Docker Image, Push, and Git tag stages description. Sonar Build Build Docker Image Push Git tag name = \"sonar\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.job.type - String context.application.name - String context.buildTool.sln_filename - String context.sonar.route - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.job.type - String context.nexus.credentialsId - String context.buildTool.command - String context.application.name - String context.sonarRoute - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) buildTool = [\"maven\"] type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY] context required: - String context.workDir - String context.job.type - String context.nexus.credentialsId - String context.application.name - String context.buildTool.command - String context.sonar.route - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.job.type - String context.sonar.route - String context.application.name - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) name = \"build\" buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.groupRepository name = \"build-image\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote name = \"push\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.gerrit.project - String context.buildTool.sln_filename - String context.buildTool.snugetApiKey - String context.buildTool.hostedRepository buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.application.version - String context.buildTool.hostedRepository - String context. buildTool.settings buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.application.version - String context.buildTool.hostedRepository - String context.buildTool.command buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.hostedRepository - String context.gerrit.autouser name = \"git-tag\" buildTool = [\"maven\", \"npm\", \"dotnet\",\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.gerrit.credentialsId - String context.gerrit.sshPort - String context.gerrit.host - String context.gerrit.autouser - String context.application.buildVersion","title":"EDP Stages Framework"},{"location":"user-guide/pipeline-framework/#deploy-pipeline","text":"Deploy() \u2013 a function that allows using the EDP implementation for the deploy pipeline. All values of different parameters that are used during the pipeline execution are stored in the \"Map\" context. The deploy pipeline consists of several steps: On the master: Initialization of all objects (Platform, Job, Gerrit, Nexus, StageFactory) and loading the default implementations of EDP stages; Creating an environment if it doesn`t exist; Deploying the last versions of the applications; Run predefined manual gates. On a particular autotest Jenkins agent that depends on the build tool: Creating workdir for autotest sources; Run predefined autotests.","title":"Deploy Pipeline"},{"location":"user-guide/pipeline-framework/#edp-library-pipelines-description","text":"_Using in pipelines - @Library(['edp-library-pipelines@version']) _ The corresponding enums and interfaces with their methods can be used separately from the EDP Pipelines library function (please refer to Table 8 and Table 9 ). Table 8. Enums and Interfaces with the respective properties, methods, and examples. Enums Interfaces PlatformType : - OPENSHIFT - KUBERNETES JobType : - CODEREVIEW - BUILD - DEPLOY BuildToolType : - MAVEN - GRADLE - NPM - DOTNET Platform() - contains methods for working with platform CLI. At the moment only OpenShift is supported. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Methods : getJsonPathValue(String k8s_kind, String k8s_kind_name, String jsonPath): return String value of specific parameter of particular object using jsonPath utility. Example : context.platform.getJsonPathValue(\"cm\",\"project-settings\", \".data.username\") BuildTool() - contains methods for working with different buildTool from ENUM BuildToolType. (Should be invoked on Jenkins build agents) Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Nexus object - Object of class Nexus. Methods : init: return parameters of buildTool that are needed for running stages. Example : context.buildTool = new BuildToolFactory().getBuildToolImpl (context.application.config.build_tool, this, context.nexus) context.buildTool.init() Table 9. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) PlatformFactory() - Class that contains methods getting implementation of CLI of platform. At the moment OpenShift and Kubernetes are supported. Methods : getPlatformImpl(PlatformType platform, Script script): return Class Platform Example : context.platform = new PlatformFactory().getPlatformImpl(PlatformType.OPENSHIFT, this) Application(String name, Platform platform, Script script) - Class that describe the application object. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform() String name - Name for the application for creating object Map config - Map of configuration settings for particular application that is loaded from config map project-settings String version - Application version, initially empty. Is set on get-version step. String deployableModule - The name of deployable module for multi module applications, initially empty. String buildVersion - Version of built artifact, contains build number of Job initially empty String deployableModuleDir - The name of deployable module directory for multi module applications, initially empty. Array imageBuildArgs - List of arguments for building application Docker image Methods : setConfig(String gerrit_autouser, String gerrit_host, String gerrit_sshPort, String gerrit_project): set the config property with values from config map Example : context.application = new Application(context.job, context.gerrit.project, context.platform, this) context.application.setConfig(context.gerrit.autouser, context.gerrit.host, context.gerrit.sshPort, context.gerrit.project) Job(type: JobType.value, platform: Platform, script: Script) - Class that describe the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\" Platform platform - Object of a class Platform(). JobType.value type. String deployTemplatesDirectory - The name of the directory in application repository, where deploy templates are located. Can be set for particular Job through DEPLOY_TEMPLATES_DIRECTORY parameter. String edpName - The name of the EDP Project. Map stages - Contains all stages in JSON format that is retrieved from Jenkins job env variable. String envToPromote - The name of the environment for promoting images. Boolean promoteImages - Defines whether images should be promoted or not. Methods : getParameterValue(String parameter, String defaultValue = null): return parameter of ENV variable of Jenkins job. init(): set all the properties of Job object. setDisplayName(String displayName): set display name of the Jenkins job. setDescription(String description, Boolean addDescription = false): set new or add to existing description of the Jenkins job. printDebugInfo(Map context): print context info to log of Jenkins job. runStage(String stage_name, Map context): run the particular stage according to its name. Example : context.job = new Job(JobType.DEPLOY.value, context.platform, this) context.job.init() context.job.printDebugInfo(context) context.job.setDisplayName(\"test\") context.job.setDescription(\"Name: ${context.application.config.name}\") Gerrit(Job job, Platform platform, Script script) - Class that describe the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String credentialsId - Credential Id in Jenkins for Gerrit. String autouser - Username of autouser in Gerrit for integration with Jenkins. String host - Gerrit host. String project - project name of built application. String branch - branch to build application from. String changeNumber - change number of Gerrit commit. String changeName - change name of Gerrit commit. String refspecName - refspecName of Gerrit commit. String sshPort - gerrit ssh port number. String patchsetNumber - patchsetNumber of Gerrit commit. Methods : init(): set all the properties of Gerrit object. Example : context.gerrit = new Gerrit(context.job, context.platform, this) context.gerrit.init() . Nexus(Job job, Platform platform, Script script) - Class that describe the Nexus tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String autouser - Username of autouser in Nexus for integration with Jenkins. String credentialsId - Credential Id in Jenkins for Nexus. String host - Nexus host. String port - Nexus http(s) port. String repositoriesUrl - Base URL of repositories in Nexus. String restUrl - URL of Rest API. Methods : init(): set all the properties of Nexus object. Example : context.nexus = new Nexus(context.job, context.platform, this) context.nexus.init() .","title":"EDP Library Pipelines Description"},{"location":"user-guide/pipeline-framework/#edp-library-stages-description_1","text":"Using in pipelines - @Library(['edp-library-stages@version']) _ The corresponding classes with methods can be used separately from the EDP Pipelines library function (please refer to Table 10 ). Table 10. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) StageFactory() - Class that contains methods getting implementation of particular stage either EDP from shared library or custom from application repository. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\" Map stages - Map of stages implementations Methods : loadEdpStages(): return list of Classes that describes EDP stages implementations loadCustomStages(String directory): return list of Classes that describes EDP custom stages from application repository from \"directory\". The \"directory\" should be absolute path to files with classes of custom stages implementations. Should be run from Jenkins agent. add(Class clazz): register class for some particular stage in stages map of StageFactory class getStage(String name, String buildTool, String type): return object of the class for particular stage from stages property based on stage name and buildTool, type of application Example : context.factory = new StageFactory(script: this) context.factory.loadEdpStages().each() { context.factory.add(it) } context.factory.loadCustomStages(\"${context.workDir}/stages\").each() { context.factory.add(it) } context.factory.getStage(stageName.toLowerCase(),context.application.config.build_tool.toLowerCase(), context.application.config.type).run(context) .","title":"EDP Library Stages Description"},{"location":"user-guide/pipeline-framework/#deploy-pipeline-stages","text":"Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. The stages for the deploy pipeline are independent of the build tool and application type. Find below (see Table 11 ) the full description of every stage: Deploy \u2192 Automated tests \u2192 Promote Images . Table 11. The Deploy, Automated tests, and Promote Images stages description. Deploy Automated tests Promote Images name = \"deploy\" buildTool = null type = null context required: \u2022 String context.workDir \u2022 StageFactory context.factory \u2022 String context.gerrit.autouser \u2022 String context.gerrit.host \u2022 String context.application.config.cloneUrl \u2022 String context.jenkins.token \u2022 String context.job.edpName \u2022 String context.job.buildUrl \u2022 String context.job.jenkinsUrl \u2022 String context.job.metaProject \u2022 List context.job.applicationsList [['name':'application1_name','version':'application1_version],...] \u2022 String context.job.deployTemplatesDirectory output: \u2022 List context.job.updatedApplicaions [['name':'application1_name','version':'application1_version],...] name = \"automation-tests\", buildTool = null, type = null context required: - String context.workDir - StageFactory context.factory - String context.gerrit.credentialsId - String context.autotest.config.cloneUrl - String context.autotest.name - String context.job.stageWithoutPrefixName - String context.buildTool.settings - String context.autotest.config.report_framework name = \"promote-images\" buildTool = null type = null context required: - String context.workDir - String context.buildTool.sln_filename - List context.job.updatedApplicaions [['name':'application1_name','version':'application1_version],...]","title":"Deploy Pipeline Stages"},{"location":"user-guide/pipeline-framework/#how-to-redefine-or-extend-edp-pipeline-stages-library_1","text":"Info Currently, the redefinition of Deploy pipeline stages is prohibited.","title":"How to Redefine or Extend EDP Pipeline Stages Library"},{"location":"user-guide/pipeline-framework/#using-edp-library-stages-in-the-pipeline","text":"In order to use the EDP stages, the created pipeline should fit some requirements, that`s why a developer has to do the following: import libraries - @Library(['edp-library-stages', 'edp-library-pipelines']) _ import reference EDP classes(See example below) define context Map \u2013 context = [:] define reference \"init\" stage After that, there is the ability to run any EDP stage beforehand by defining requirement context context.job.runStage(\"Deploy\", context) . For instance, the pipeline can look like : @Library ( [ ' edp - library - stages ' , ' edp - library - pipelines ' ] ) _ import com.epam.edp.stages.StageFactory import com.epam.edp.platform.PlatformFactory import com.epam.edp.platform.PlatformType import com.epam.edp.JobType context = [ : ] node ( ' master ' ) { stage ( \"Init\" ) { context . platform = new PlatformFactory (). getPlatformImpl ( PlatformType . OPENSHIFT , this ) context . job = new com . epam . edp . Job ( JobType . DEPLOY . value , context . platform , this ) context . job . init () context . job . initDeployJob () println ( \"[JENKINS][DEBUG] Created object job with type - ${context.job.type}\" ) context . nexus = new com . epam . edp . Nexus ( context . job , context . platform , this ) context . nexus . init () context . jenkins = new com . epam . edp . Jenkins ( context . job , context . platform , this ) context . jenkins . init () context . gerrit = new com . epam . edp . Gerrit ( context . job , context . platform , this ) context . gerrit . init () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . environment = new com . epam . edp . Environment ( context . job . deployProject , context . platform , this ) context . job . printDebugInfo ( context ) context . job . setDisplayName ( \"${currentBuild.displayName}-${context.job.deployProject}\" ) context . job . generateInputDataForDeployJob () } stage ( \"Pre Deploy Custom stage\" ) { println ( \"Some custom pre deploy logic\" ) } context . job . runStage ( \"Deploy\" , context ) stage ( \"Post Deploy Custom stage\" ) { println ( \"Some custom post deploy logic\" ) } } Or in a declarative way : @Library ( [ ' edp - library - stages ' , ' edp - library - pipelines ' ] ) _ import com.epam.edp.stages.StageFactory import com.epam.edp.platform.PlatformFactory import com.epam.edp.platform.PlatformType import com.epam.edp.JobType context = [ : ] pipeline { agent { label ' master ' } stages { stage ( ' Init ' ) { steps { script { context . platform = new PlatformFactory (). getPlatformImpl ( PlatformType . OPENSHIFT , this ) context . job = new com . epam . edp . Job ( JobType . DEPLOY . value , context . platform , this ) context . job . init () context . job . initDeployJob () println ( \"[JENKINS][DEBUG] Created object job with type - ${context.job.type}\" ) context . nexus = new com . epam . edp . Nexus ( context . job , context . platform , this ) context . nexus . init () context . jenkins = new com . epam . edp . Jenkins ( context . job , context . platform , this ) context . jenkins . init () context . gerrit = new com . epam . edp . Gerrit ( context . job , context . platform , this ) context . gerrit . init () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . environment = new com . epam . edp . Environment ( context . job . deployProject , context . platform , this ) context . job . printDebugInfo ( context ) context . job . setDisplayName ( \"${currentBuild.displayName}-${context.job.deployProject}\" ) context . job . generateInputDataForDeployJob () } } } stage ( ' Deploy ' ) { steps { script { context . factory . getStage ( \"deploy\" ). run ( context ) } } } stage ( ' Custom stage ' ) { steps { println ( \"Some custom logic\" ) } } } }","title":"Using EDP Library Stages in the Pipeline"},{"location":"user-guide/pipeline-framework/#related-articles","text":"Add Application Add Library Add CD Pipeline CI Pipeline Details CD Pipeline Details Customize CI Pipeline Customize CD Pipeline EDP Stages Glossary Use Terraform Library in EDP","title":"Related Articles"},{"location":"user-guide/pipeline-stages/","text":"Pipeline Stages \u2693\ufe0e Get acquainted with EDP CI/CD workflow and stages description. EDP CI/CD Workflow \u2693\ufe0e Within EDP, the pipeline framework comprises the following pipelines: Code Review; Build; Deploy. Note Please refer to the EDP Pipeline Framework page for details. The diagram below shows the delivery path through these pipelines and the respective stages. Please be aware that stages may differ for different codebase types. stages Stages Description \u2693\ufe0e The table below provides the details on all the stages in the EDP pipeline framework: Name Dependency Description Pipeline Application Library Autotest Source code Documentation init Initiates information gathering Create Release, Code Review, Build + + Build.groovy checkout Performs for all files the checkout from a selected branch of the Git repository. For the main branch - from HEAD, for code review - from the commit Create Release, Build + + Checkout.groovy sast Launches vulnerability testing via Semgrep scanner. Pushes a vulnerability report to the DefectDojo. Build + Security compile Compiles the code, includes individual groovy files for each type of app or lib (NPM, DotNet, Python, Maven, Gradle) Code Review, Build + + Compile tests Launches testing procedure, includes individual groovy files for each type of app or lib Code Review, Build + + + Tests sonar Launches testing via SonarQube scanner and includes individual groovy files for each type of app or lib Code Review, Build + + Sonar build Builds the application, includes individual groovy files for each type of app or lib (Go, Maven, Gradle, NPM) Code Review, Build + Build create-branch EDP create-release process Creates default branch in Gerrit during create and clone strategies Create Release + + + CreateBranch.groovy trigger-job EDP create-release process Triggers \"build\" job Create Release + + + TriggerJob.groovy gerrit-checkout Performs checkout to the current project branch in Gerrit Code Review + + + GerritCheckout.groovy commit-validate Optional in EDP Admin Console Takes Jira parameters, when \"Jira Integration\" is enabled for the project in the Admin Console. Code Review + + CommitValidate.groovy dockerfile-lint Launches linting tests for Dockerfile Code Review + LintDockerApplicationLibrary.groovy Use Dockerfile Linters for Code Review dockerbuild-verify \"Build\" stage (if there are no \"COPY\" layers in Dockerfile) Launches build procedure for Dockerfile without pushing an image to the repository Code Review + BuildDockerfileApplicationLibrary.groovy Use Dockerfile Linters for Code Review helm-lint Launches linting tests for deployment charts Code Review + LintHelmApplicationLibrary.groovy Use helm-lint for Code Review helm-docs Checks generated documentation for deployment charts Code Review + HelmDocsApplication.groovy Use helm-docs for Code Review helm-uninstall Helm release deletion step to clear Helm releases Deploy + HelmUninstall.groovy Helm release deletion semi-auto-deploy-input Provides auto deploy with timeout and manual deploy flow Deploy + SemiAutoDeployInput.groovy Semi Auto Deploy get-version Defines the versioning of the project depending on the versioning schema selected in Admin Console Build + + GetVersion terraform-plan AWS credentials added to Jenkins Checks Terraform version, and installs default version if necessary, and launches terraform init, returns AWS username which used for action, and terraform plan command is called with an output of results to .tfplan file Build + TerraformPlan.groovy Use Terraform library in EDP terraform-apply AWS credentials added to Jenkins, the \"Terraform-plan\" stage Checks Terraform version, and installs default version if necessary, and launches terraform init, launches terraform plan from saves before .tfplan file, asks to approve, and run terraform apply from .tfplan file Build + TerraformApply.groovy Use Terraform library in EDP build-image-from-dockerfile Platform: OpenShift Builds Dockerfile Build + + .groovy files for building Dockerfile image build-image-kaniko Platform: k8s Builds Dockerfile using the Kaniko tool Build + BuildImageKaniko.groovy push Pushes an artifact to the Nexus repository Build + + Push create-Jira-issue-metadata \"get-version\" stage Creates a temporary CR in the namespace and after that pushes Jira Integration data to Jira ticket, and delete CR Build + + JiraIssueMetadata.groovy ecr-to-docker DockerHub credentials added to Jenkins Copies the docker image from the ECR project registry to DockerHub via the Crane tool after it is built Build + EcrToDocker.groovy Promote Docker Images From ECR to Docker Hub git-tag \"Get-version\" stage Creates a tag in SCM for the current build Build + + GitTagApplicationLibrary.groovy deploy Deploys the application Deploy + Deploy.groovy manual Works with the manual approve to proceed Deploy + ManualApprove.groovy promote-images Promotes docker images to the registry Deploy + PromoteImage.groovy Note The Create Release pipeline is an internal EDP mechanism for adding, importing or cloning a codebase. It is not a part of the pipeline framework. Related Articles \u2693\ufe0e Manage Jenkins CI Job Provisioner GitLab Integration GitHub Integration","title":"Overview"},{"location":"user-guide/pipeline-stages/#pipeline-stages","text":"Get acquainted with EDP CI/CD workflow and stages description.","title":"Pipeline Stages"},{"location":"user-guide/pipeline-stages/#edp-cicd-workflow","text":"Within EDP, the pipeline framework comprises the following pipelines: Code Review; Build; Deploy. Note Please refer to the EDP Pipeline Framework page for details. The diagram below shows the delivery path through these pipelines and the respective stages. Please be aware that stages may differ for different codebase types. stages","title":"EDP CI/CD Workflow"},{"location":"user-guide/pipeline-stages/#stages-description","text":"The table below provides the details on all the stages in the EDP pipeline framework: Name Dependency Description Pipeline Application Library Autotest Source code Documentation init Initiates information gathering Create Release, Code Review, Build + + Build.groovy checkout Performs for all files the checkout from a selected branch of the Git repository. For the main branch - from HEAD, for code review - from the commit Create Release, Build + + Checkout.groovy sast Launches vulnerability testing via Semgrep scanner. Pushes a vulnerability report to the DefectDojo. Build + Security compile Compiles the code, includes individual groovy files for each type of app or lib (NPM, DotNet, Python, Maven, Gradle) Code Review, Build + + Compile tests Launches testing procedure, includes individual groovy files for each type of app or lib Code Review, Build + + + Tests sonar Launches testing via SonarQube scanner and includes individual groovy files for each type of app or lib Code Review, Build + + Sonar build Builds the application, includes individual groovy files for each type of app or lib (Go, Maven, Gradle, NPM) Code Review, Build + Build create-branch EDP create-release process Creates default branch in Gerrit during create and clone strategies Create Release + + + CreateBranch.groovy trigger-job EDP create-release process Triggers \"build\" job Create Release + + + TriggerJob.groovy gerrit-checkout Performs checkout to the current project branch in Gerrit Code Review + + + GerritCheckout.groovy commit-validate Optional in EDP Admin Console Takes Jira parameters, when \"Jira Integration\" is enabled for the project in the Admin Console. Code Review + + CommitValidate.groovy dockerfile-lint Launches linting tests for Dockerfile Code Review + LintDockerApplicationLibrary.groovy Use Dockerfile Linters for Code Review dockerbuild-verify \"Build\" stage (if there are no \"COPY\" layers in Dockerfile) Launches build procedure for Dockerfile without pushing an image to the repository Code Review + BuildDockerfileApplicationLibrary.groovy Use Dockerfile Linters for Code Review helm-lint Launches linting tests for deployment charts Code Review + LintHelmApplicationLibrary.groovy Use helm-lint for Code Review helm-docs Checks generated documentation for deployment charts Code Review + HelmDocsApplication.groovy Use helm-docs for Code Review helm-uninstall Helm release deletion step to clear Helm releases Deploy + HelmUninstall.groovy Helm release deletion semi-auto-deploy-input Provides auto deploy with timeout and manual deploy flow Deploy + SemiAutoDeployInput.groovy Semi Auto Deploy get-version Defines the versioning of the project depending on the versioning schema selected in Admin Console Build + + GetVersion terraform-plan AWS credentials added to Jenkins Checks Terraform version, and installs default version if necessary, and launches terraform init, returns AWS username which used for action, and terraform plan command is called with an output of results to .tfplan file Build + TerraformPlan.groovy Use Terraform library in EDP terraform-apply AWS credentials added to Jenkins, the \"Terraform-plan\" stage Checks Terraform version, and installs default version if necessary, and launches terraform init, launches terraform plan from saves before .tfplan file, asks to approve, and run terraform apply from .tfplan file Build + TerraformApply.groovy Use Terraform library in EDP build-image-from-dockerfile Platform: OpenShift Builds Dockerfile Build + + .groovy files for building Dockerfile image build-image-kaniko Platform: k8s Builds Dockerfile using the Kaniko tool Build + BuildImageKaniko.groovy push Pushes an artifact to the Nexus repository Build + + Push create-Jira-issue-metadata \"get-version\" stage Creates a temporary CR in the namespace and after that pushes Jira Integration data to Jira ticket, and delete CR Build + + JiraIssueMetadata.groovy ecr-to-docker DockerHub credentials added to Jenkins Copies the docker image from the ECR project registry to DockerHub via the Crane tool after it is built Build + EcrToDocker.groovy Promote Docker Images From ECR to Docker Hub git-tag \"Get-version\" stage Creates a tag in SCM for the current build Build + + GitTagApplicationLibrary.groovy deploy Deploys the application Deploy + Deploy.groovy manual Works with the manual approve to proceed Deploy + ManualApprove.groovy promote-images Promotes docker images to the registry Deploy + PromoteImage.groovy Note The Create Release pipeline is an internal EDP mechanism for adding, importing or cloning a codebase. It is not a part of the pipeline framework.","title":"Stages Description"},{"location":"user-guide/pipeline-stages/#related-articles","text":"Manage Jenkins CI Job Provisioner GitLab Integration GitHub Integration","title":"Related Articles"},{"location":"user-guide/prepare-for-release/","text":"Prepare for Release \u2693\ufe0e After the necessary applications are added to EDP, they can be managed via the Admin Console. To prepare for the release, create a new branch from a selected commit with a set of CI pipelines (Code Review and Build pipelines), launch the Build pipeline, and add a new CD pipeline as well. Note Please refer to the Add Application and Add CD Pipeline for the details on how to add an application or a CD pipeline. Become familiar with the following preparation steps for release and a CD pipeline structure: Create a new branch Launch the Build pipeline Add a new CD pipeline Check CD pipeline structure Create a New Branch \u2693\ufe0e Open Gerrit via the Admin Console Overview page to have this tab available in a web browser. Being in Admin Console, open the Applications section and click an application from the list to create a new branch. Once clicked the application name, scroll down to the Branches menu and click the Create button to open the Create New Branch dialog box, fill in the Branch Name field by typing a branch name. Open the Gerrit tab in the web browser, navigate to Projects \u2192 List \u2192 select the application \u2192 Branches \u2192 gitweb for a necessary branch. Select the commit that will be the last included to a new branch commit. Copy to clipboard the commit hash. Paste the copied hash to the From Commit Hash field and click Proceed. Note If the commit hash is not added to the From Commit Hash field, the new branch will be created from the head of the master branch. Launch the Build Pipeline \u2693\ufe0e After the new branches are added, open the details page of every application and click the CI link that refers to Jenkins. Note The adding of a new branch may take some time. As soon as the new branch is created, it will be displayed in the list of the Branches menu. To build a new version of a corresponding Docker container (an image stream in OpenShift terms) for the new branch, start the Build pipeline. Being in Jenkins, select the new branch tab and click the link to the Build pipeline. Navigate to the Build with Parameters option and click the Build button to launch the Build pipeline. Warning The predefined default parameters should not be changed when triggering the Build pipeline, otherwise, it will lead to the pipeline failure. Add a New CD Pipeline \u2693\ufe0e Add a new CD pipeline and indicate the new release branch using the Admin console tool. Pay attention to the Applications menu, the necessary application(s) should be selected there, as well as the necessary branch(es) from the drop-down list. Note For the details on how to add a CD pipeline, please refer to the Add CD Pipeline page. As soon as the Build pipelines are successfully passed in Jenkins, the Docker Registry, which is used in EDP by default, will have the new image streams (Docker container in Kubernetes terms) version that corresponds to the current branch. Open the Kubernetes/OpenShift page of the project via the Admin Console Overview page \u2192 go to CodebaseImageStream (in OpenShift, go to Builds \u2192 Images) \u2192 check whether the image streams are created under the specific name (the combination of the application and branch names) and the specific tags are added. Click every image stream link. Check CD Pipeline Structure \u2693\ufe0e When the CD pipeline is added through the Admin Console, it becomes available in the CD pipelines list. Every pipeline has the details page with the additional information. To explore the CD pipeline structure, follow the steps below: Open Admin Console and navigate to Continuous Delivery section, click the newly created CD pipeline name. Discover the CD pipeline components: Applications - the list of applications with the image streams and links to Jenkins for the respective branch; Stages - a set of stages with the defined characteristics and links to Kubernetes/OpenShift project; Note Initially, an environment is empty and does not have any deployment unit. When deploying the subsequent stages, the artifacts of the selected versions will be deployed to the current project and the environment will display the current stage status. The project has a standard pattern: \u2039edp-name\u203a-\u2039pipeline-name\u203a-\u2039stage-name\u203a. Deployed Versions - the deployment status of the specific application and the predefined stage. Launch CD Pipeline Manually \u2693\ufe0e Follow the steps below to deploy the QA and UAT application stages: As soon as the Build pipelines for both applications are successfully passed, the new version of the Docker container will appear, thus allowing to launch the CD pipeline. Simply navigate to Continuous Delivery and click the pipeline name to open it in Jenkins. Click the QA stage link. Deploy the QA stage by clicking the Build Now option. After the initialization step starts, in case another menu is opened, the Pause for Input option will appear. Select the application version in the drop-down list and click Proceed. The pipeline passes the following stages: Init - initialization of the Jenkins pipeline outputs with the stages that are the Groovy scripts that execute the current code; Deploy - the deployment of the selected versions of the docker container and third-party services. As soon as the Deployed pipeline stage is completed, the respective environment will be deployed. Approve - the verification stage that enables to Proceed or Abort this stage; Promote-images - the creation of the new image streams for the current versions with the pattern combination: [pipeline name]-[stage name]-[application name]-[verified]; After all the stages are passed, the new image streams will be created in the Kubernetes/OpenShift with the new names. Deploy the UAT stage, which takes the versions that were verified during the QA stage, by clicking the Build Now option, and select the necessary application versions. The launch process is the same as for all the deploy pipelines. To get the status of the pipeline deployment, open the CD pipeline details page and check the Deployed versions state. CD Pipeline as a Team Environment \u2693\ufe0e Admin Console allows creating a CD pipeline with a part of the application set as a team environment. To do this, perform the following steps; Open the Continuous Delivery section \u2192 click the Create button \u2192 enter the pipeline name (e.g. team-a) \u2192 select ONE application and choose the master branch for it \u2192 add one DEV stage. As soon as the CD pipeline is added to the CD pipelines list, its details page will display the links to Jenkins and Kubernetes/OpenShift. Open Jenkins and deploy the DEV stage by clicking the Build Now option. Kubernetes/OpenShift keeps an independent environment that allows checking the new versions, thus speeding up the developing process when working with several microservices. As a result, the team will have the same abilities to verify the code changes when developing and during the release. Related Articles \u2693\ufe0e Add Application Add CD Pipeline Autotest as Qulity Gate Build Pipeline CD Pipeline Details Customize CD Pipeline","title":"Prepare for Release"},{"location":"user-guide/prepare-for-release/#prepare-for-release","text":"After the necessary applications are added to EDP, they can be managed via the Admin Console. To prepare for the release, create a new branch from a selected commit with a set of CI pipelines (Code Review and Build pipelines), launch the Build pipeline, and add a new CD pipeline as well. Note Please refer to the Add Application and Add CD Pipeline for the details on how to add an application or a CD pipeline. Become familiar with the following preparation steps for release and a CD pipeline structure: Create a new branch Launch the Build pipeline Add a new CD pipeline Check CD pipeline structure","title":"Prepare for Release"},{"location":"user-guide/prepare-for-release/#create-a-new-branch","text":"Open Gerrit via the Admin Console Overview page to have this tab available in a web browser. Being in Admin Console, open the Applications section and click an application from the list to create a new branch. Once clicked the application name, scroll down to the Branches menu and click the Create button to open the Create New Branch dialog box, fill in the Branch Name field by typing a branch name. Open the Gerrit tab in the web browser, navigate to Projects \u2192 List \u2192 select the application \u2192 Branches \u2192 gitweb for a necessary branch. Select the commit that will be the last included to a new branch commit. Copy to clipboard the commit hash. Paste the copied hash to the From Commit Hash field and click Proceed. Note If the commit hash is not added to the From Commit Hash field, the new branch will be created from the head of the master branch.","title":"Create a New Branch"},{"location":"user-guide/prepare-for-release/#launch-the-build-pipeline","text":"After the new branches are added, open the details page of every application and click the CI link that refers to Jenkins. Note The adding of a new branch may take some time. As soon as the new branch is created, it will be displayed in the list of the Branches menu. To build a new version of a corresponding Docker container (an image stream in OpenShift terms) for the new branch, start the Build pipeline. Being in Jenkins, select the new branch tab and click the link to the Build pipeline. Navigate to the Build with Parameters option and click the Build button to launch the Build pipeline. Warning The predefined default parameters should not be changed when triggering the Build pipeline, otherwise, it will lead to the pipeline failure.","title":"Launch the Build Pipeline"},{"location":"user-guide/prepare-for-release/#add-a-new-cd-pipeline","text":"Add a new CD pipeline and indicate the new release branch using the Admin console tool. Pay attention to the Applications menu, the necessary application(s) should be selected there, as well as the necessary branch(es) from the drop-down list. Note For the details on how to add a CD pipeline, please refer to the Add CD Pipeline page. As soon as the Build pipelines are successfully passed in Jenkins, the Docker Registry, which is used in EDP by default, will have the new image streams (Docker container in Kubernetes terms) version that corresponds to the current branch. Open the Kubernetes/OpenShift page of the project via the Admin Console Overview page \u2192 go to CodebaseImageStream (in OpenShift, go to Builds \u2192 Images) \u2192 check whether the image streams are created under the specific name (the combination of the application and branch names) and the specific tags are added. Click every image stream link.","title":"Add a New CD Pipeline"},{"location":"user-guide/prepare-for-release/#check-cd-pipeline-structure","text":"When the CD pipeline is added through the Admin Console, it becomes available in the CD pipelines list. Every pipeline has the details page with the additional information. To explore the CD pipeline structure, follow the steps below: Open Admin Console and navigate to Continuous Delivery section, click the newly created CD pipeline name. Discover the CD pipeline components: Applications - the list of applications with the image streams and links to Jenkins for the respective branch; Stages - a set of stages with the defined characteristics and links to Kubernetes/OpenShift project; Note Initially, an environment is empty and does not have any deployment unit. When deploying the subsequent stages, the artifacts of the selected versions will be deployed to the current project and the environment will display the current stage status. The project has a standard pattern: \u2039edp-name\u203a-\u2039pipeline-name\u203a-\u2039stage-name\u203a. Deployed Versions - the deployment status of the specific application and the predefined stage.","title":"Check CD Pipeline Structure"},{"location":"user-guide/prepare-for-release/#launch-cd-pipeline-manually","text":"Follow the steps below to deploy the QA and UAT application stages: As soon as the Build pipelines for both applications are successfully passed, the new version of the Docker container will appear, thus allowing to launch the CD pipeline. Simply navigate to Continuous Delivery and click the pipeline name to open it in Jenkins. Click the QA stage link. Deploy the QA stage by clicking the Build Now option. After the initialization step starts, in case another menu is opened, the Pause for Input option will appear. Select the application version in the drop-down list and click Proceed. The pipeline passes the following stages: Init - initialization of the Jenkins pipeline outputs with the stages that are the Groovy scripts that execute the current code; Deploy - the deployment of the selected versions of the docker container and third-party services. As soon as the Deployed pipeline stage is completed, the respective environment will be deployed. Approve - the verification stage that enables to Proceed or Abort this stage; Promote-images - the creation of the new image streams for the current versions with the pattern combination: [pipeline name]-[stage name]-[application name]-[verified]; After all the stages are passed, the new image streams will be created in the Kubernetes/OpenShift with the new names. Deploy the UAT stage, which takes the versions that were verified during the QA stage, by clicking the Build Now option, and select the necessary application versions. The launch process is the same as for all the deploy pipelines. To get the status of the pipeline deployment, open the CD pipeline details page and check the Deployed versions state.","title":"Launch CD Pipeline Manually"},{"location":"user-guide/prepare-for-release/#cd-pipeline-as-a-team-environment","text":"Admin Console allows creating a CD pipeline with a part of the application set as a team environment. To do this, perform the following steps; Open the Continuous Delivery section \u2192 click the Create button \u2192 enter the pipeline name (e.g. team-a) \u2192 select ONE application and choose the master branch for it \u2192 add one DEV stage. As soon as the CD pipeline is added to the CD pipelines list, its details page will display the links to Jenkins and Kubernetes/OpenShift. Open Jenkins and deploy the DEV stage by clicking the Build Now option. Kubernetes/OpenShift keeps an independent environment that allows checking the new versions, thus speeding up the developing process when working with several microservices. As a result, the team will have the same abilities to verify the code changes when developing and during the release.","title":"CD Pipeline as a Team Environment"},{"location":"user-guide/prepare-for-release/#related-articles","text":"Add Application Add CD Pipeline Autotest as Qulity Gate Build Pipeline CD Pipeline Details Customize CD Pipeline","title":"Related Articles"},{"location":"user-guide/semi-auto-deploy/","text":"Semi Auto Deploy \u2693\ufe0e The Semi Auto Deploy stage provides the ability to deploy applications with the custom logic that comprises the following behavior: When the build of an application selected for deploy in the CD pipeline is completed, the Deploy pipeline is automatically triggered; By default, the deploy stage waits for 5 minutes, and if the user does not interfere with the process (cancels or selects certain versions of the application to deploy), then the deploy stage will deploy the latest versions of all applications; The stage can be used in the manual mode. To enable the Semi Auto Deploy stage during the deploy process, follow the steps below: Create or update the CD pipeline: make sure the trigger type for the stage is set to auto . Replace the {\"name\":\"auto-deploy-input\",\"step_name\":\"auto-deploy-input\"} step to the {\"name\":\"semi-auto-deploy-input\",\"step_name\":\"semi-auto-deploy-input\"} step in the CD pipeline. Alternatively, it is possible to create a custom job provisioner with this step. Run the Build pipeline for any application selected in the CD pipeline. Exceptional Cases \u2693\ufe0e After the timeout starts and in case the pipeline has been interrupted not from the Input requested menu, the automatic deployment will be proceeding. To resolve the issue and stop the pipeline, click the Input requested menu -> Abort or being on the pipeline UI, click the Abort button. Related Articles \u2693\ufe0e Add CD Pipeline Customize CD Pipeline Manage Jenkins CD Pipeline Job Provisioner","title":"Semi Auto Deploy"},{"location":"user-guide/semi-auto-deploy/#semi-auto-deploy","text":"The Semi Auto Deploy stage provides the ability to deploy applications with the custom logic that comprises the following behavior: When the build of an application selected for deploy in the CD pipeline is completed, the Deploy pipeline is automatically triggered; By default, the deploy stage waits for 5 minutes, and if the user does not interfere with the process (cancels or selects certain versions of the application to deploy), then the deploy stage will deploy the latest versions of all applications; The stage can be used in the manual mode. To enable the Semi Auto Deploy stage during the deploy process, follow the steps below: Create or update the CD pipeline: make sure the trigger type for the stage is set to auto . Replace the {\"name\":\"auto-deploy-input\",\"step_name\":\"auto-deploy-input\"} step to the {\"name\":\"semi-auto-deploy-input\",\"step_name\":\"semi-auto-deploy-input\"} step in the CD pipeline. Alternatively, it is possible to create a custom job provisioner with this step. Run the Build pipeline for any application selected in the CD pipeline.","title":"Semi Auto Deploy"},{"location":"user-guide/semi-auto-deploy/#exceptional-cases","text":"After the timeout starts and in case the pipeline has been interrupted not from the Input requested menu, the automatic deployment will be proceeding. To resolve the issue and stop the pipeline, click the Input requested menu -> Abort or being on the pipeline UI, click the Abort button.","title":"Exceptional Cases"},{"location":"user-guide/semi-auto-deploy/#related-articles","text":"Add CD Pipeline Customize CD Pipeline Manage Jenkins CD Pipeline Job Provisioner","title":"Related Articles"},{"location":"user-guide/terraform-stages/","text":"CI Pipeline for Terraform \u2693\ufe0e EPAM Delivery Platform ensures the implemented Terraform support allowing to work with Terraform code that is processed by means of stages in the Code-Review and Build pipelines. These pipelines are expected to be created after the Terraform Library is added. Code Review Pipeline Stages \u2693\ufe0e In the Code Review pipeline, the following stages are available: checkout stage, a standard step during which all files are checked out from a selected branch of the Git repository. terraform-lint stage containing a script that performs the following actions: 2.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 2.2. Launches the terraform init command that initializes backend. 2.3. Launches the linters described below. Pay attention that if at least one of these checks is not true (returns with an error), the Code Review pipeline will fail on this step and will be displayed in red. * terraform fmt linter checks the formatting of the Terraform code; * tflint linter checks Terraform linters for possible errors and deprecated syntax; * terraform validate linter validates the Terraform code. Build Pipeline Stages \u2693\ufe0e In the Build pipeline, the following stages are available: checkout stage is a standard step during which all files are checked out from a master branch of Git repository. Note With the default versioning, in the base directory of the project, create a file named 'VERSION' with a proper Terraform version (e.g.1.0.0). terraform-lint stage containing a script that performs the same actions as in the Code Review pipeline, namely: 2.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 2.2. Launches the terraform init stage that initializes backend. 2.3. Launches the linters described below. Pay attention that if at least one of these checks is not true (returns with an error), the Build pipeline will fail on this step and will be displayed in red. - terraform fmt linter checks the formatting of the Terraform code; - tflint linter checks Terraform linters for possible errors and deprecated syntax; - terraform validate linter validates the Terraform code. terraform-plan stage containing a script that performs the following actions: 3.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 3.2. Launches the terraform init command that initializes backend. 3.3. Returns the name of the user, on behalf of whom the actions will be performed, with the help of aws . 3.4. Launches the terraform-plan command saving the results in the .tfplan file. Note EDP expects AWS credentials to be added in Jenkins under the name aws.user . To learn how to create credentials for the terraform-plan and terraform-apply stages, see the section Create AWS Credentials . terraform-apply stage containing a script that performs the following actions: 4.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 4.2. Launches the terraform init command that initializes backend. 4.3. Launches the terraform-plan command saving the results in the tfplan file. 4.4. Approves the application of Terraform code in your project by manually clicking the Proceed button. To decline the Terraform code, click the Abort button. If none of the buttons is selected within 30 minutes, by default the terraform-plan command will not be applied. 4.5. Launches the terraform-apply command. Create AWS Credentials \u2693\ufe0e To create credentials that will be used in terraform-plan and terraform-apply stages, perform the following steps: 1. Go to Jenkins -> Manage Jenkins -> Manage Credentials . In the Store scoped to Jenkins section select global as Domains . Jenkins credential 2. Click the Add Credentials tab and select AWS Credentials in the Kind dropdown. Jenkins credential 3. Enter the ID name. By default, EDP expects AWS credentials to be under the ID aws.user . 4. Enter values into the Access Key ID and Secret Access Key fields (credentials should belong to a user in AWS). 5. Click OK to save these credentials. Now the ID of the credentials is visible in the Global credentials table in Jenkins. Use Existing AWS Credentials \u2693\ufe0e To use other existing credentials (e.g. from other accounts) instead of the expected ones in the Build pipeline and in the terraform-plan and terraform-apply stages, perform the following steps: Navigate to the Build pipeline and select the Configure tab. Click the Add Parameter button and select the String Parameter option. Add string parameter Fill in the respective fields with the variable name AWS_CREDENTIALS , description, and the default value (e.g., aws.user , used previously in pipelines). Set value Now during the launch of the Build pipeline, it is possible to select the desired credentials, added in Jenkins, in the AWS_CREDENTIALS field of the Build pipeline settings. Related Articles \u2693\ufe0e EDP Pipeline Framework Associate IAM Roles With Service Accounts","title":"CI Pipeline for Terraform"},{"location":"user-guide/terraform-stages/#ci-pipeline-for-terraform","text":"EPAM Delivery Platform ensures the implemented Terraform support allowing to work with Terraform code that is processed by means of stages in the Code-Review and Build pipelines. These pipelines are expected to be created after the Terraform Library is added.","title":"CI Pipeline for Terraform"},{"location":"user-guide/terraform-stages/#code-review-pipeline-stages","text":"In the Code Review pipeline, the following stages are available: checkout stage, a standard step during which all files are checked out from a selected branch of the Git repository. terraform-lint stage containing a script that performs the following actions: 2.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 2.2. Launches the terraform init command that initializes backend. 2.3. Launches the linters described below. Pay attention that if at least one of these checks is not true (returns with an error), the Code Review pipeline will fail on this step and will be displayed in red. * terraform fmt linter checks the formatting of the Terraform code; * tflint linter checks Terraform linters for possible errors and deprecated syntax; * terraform validate linter validates the Terraform code.","title":"Code Review Pipeline Stages"},{"location":"user-guide/terraform-stages/#build-pipeline-stages","text":"In the Build pipeline, the following stages are available: checkout stage is a standard step during which all files are checked out from a master branch of Git repository. Note With the default versioning, in the base directory of the project, create a file named 'VERSION' with a proper Terraform version (e.g.1.0.0). terraform-lint stage containing a script that performs the same actions as in the Code Review pipeline, namely: 2.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 2.2. Launches the terraform init stage that initializes backend. 2.3. Launches the linters described below. Pay attention that if at least one of these checks is not true (returns with an error), the Build pipeline will fail on this step and will be displayed in red. - terraform fmt linter checks the formatting of the Terraform code; - tflint linter checks Terraform linters for possible errors and deprecated syntax; - terraform validate linter validates the Terraform code. terraform-plan stage containing a script that performs the following actions: 3.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 3.2. Launches the terraform init command that initializes backend. 3.3. Returns the name of the user, on behalf of whom the actions will be performed, with the help of aws . 3.4. Launches the terraform-plan command saving the results in the .tfplan file. Note EDP expects AWS credentials to be added in Jenkins under the name aws.user . To learn how to create credentials for the terraform-plan and terraform-apply stages, see the section Create AWS Credentials . terraform-apply stage containing a script that performs the following actions: 4.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 4.2. Launches the terraform init command that initializes backend. 4.3. Launches the terraform-plan command saving the results in the tfplan file. 4.4. Approves the application of Terraform code in your project by manually clicking the Proceed button. To decline the Terraform code, click the Abort button. If none of the buttons is selected within 30 minutes, by default the terraform-plan command will not be applied. 4.5. Launches the terraform-apply command.","title":"Build Pipeline Stages"},{"location":"user-guide/terraform-stages/#create-aws-credentials","text":"To create credentials that will be used in terraform-plan and terraform-apply stages, perform the following steps: 1. Go to Jenkins -> Manage Jenkins -> Manage Credentials . In the Store scoped to Jenkins section select global as Domains . Jenkins credential 2. Click the Add Credentials tab and select AWS Credentials in the Kind dropdown. Jenkins credential 3. Enter the ID name. By default, EDP expects AWS credentials to be under the ID aws.user . 4. Enter values into the Access Key ID and Secret Access Key fields (credentials should belong to a user in AWS). 5. Click OK to save these credentials. Now the ID of the credentials is visible in the Global credentials table in Jenkins.","title":"Create AWS Credentials"},{"location":"user-guide/terraform-stages/#use-existing-aws-credentials","text":"To use other existing credentials (e.g. from other accounts) instead of the expected ones in the Build pipeline and in the terraform-plan and terraform-apply stages, perform the following steps: Navigate to the Build pipeline and select the Configure tab. Click the Add Parameter button and select the String Parameter option. Add string parameter Fill in the respective fields with the variable name AWS_CREDENTIALS , description, and the default value (e.g., aws.user , used previously in pipelines). Set value Now during the launch of the Build pipeline, it is possible to select the desired credentials, added in Jenkins, in the AWS_CREDENTIALS field of the Build pipeline settings.","title":"Use Existing AWS Credentials"},{"location":"user-guide/terraform-stages/#related-articles","text":"EDP Pipeline Framework Associate IAM Roles With Service Accounts","title":"Related Articles"}]}