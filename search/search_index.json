{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EPAM Delivery Platform \u2693\ufe0e EPAM Delivery platform (EDP) is an open-source cloud-agnostic SaaS/PaaS solution for software development, licensed under Apache License 2.0 . It provides a pre-defined set of CI/CD patterns and tools, which allow a user to start product development quickly with established code review , release , versioning , branching , build processes. These processes include static code analysis, security checks, linters, validators, dynamic feature environments provisioning. EDP consolidates the top Open-Source CI/CD tools by running them on Kubernetes/OpenShift, which enables web/app development either in isolated (on-prem) or cloud environments. EPAM Delivery Platform, which is also called \"The Rocket\" , is a platform that allows shortening the time that is passed before an active development can be started from several months to several hours. EDP consists of the following: The platform based on managed infrastructure and container orchestration; Security covering authentication, authorization, and SSO for platform services; Development and testing toolset; Well-established engineering process and EPAM practices (EngX) reflected in CI/CD pipelines, and delivery analytics; Local development with debug capabilities. Features \u2693\ufe0e Deployed and configured CI/CD toolset ( Jenkins , Gerrit , Nexus , SonarQube ) Gerrit, GitLab or GitHub as a version control system for your code Jenkins is a pipeline orchestrator CI pipelines for Python, Java 8, Java 11, .Net, Go, React, Terraform, Jenkins Groovy Pipelines, Dockerfile, Helm Build tools: Go, Apache Maven, Apache Gradle Admin Console UI as a single entry point CD pipeline for Microservice Deployment Kubernetes native approach ( CRD, CR ) to declare CI/CD pipelines Admin Console UI \u2693\ufe0e Admin Console is a central management tool in the EDP ecosystem that provides the ability to define pipelines, project resources and new technologies in a simple way. Using Admin Console enables to manage business entities: Create Codebases as Applications, Libraries and Autotests; Create/Update CD Pipelines; Note To interact with Admin Console via REST API, explore the Create Codebase Entity page. Navigation bar \u2013 consists of seven sections: Overview, Continuous Delivery, Applications, Autotests, Libraries, and Delivery Dashboard Diagram. Click the necessary section to add an entity, open a home page or check the diagram. User name \u2013 displays the registered user name. Main links \u2013 displays the corresponding links to the major adjusted toolset, to the management tool and to the OpenShift cluster. Admin Console is a complete tool allowing to manage and control added to the environment codebases (applications, autotests, libraries) as well as to create a CD pipeline and check the visualization diagram. Inspect the main features available in Admin Console by following the corresponding link: Add Application Add Autotest Add Library Add CD Pipeline Delivery Dashboard Diagram Architecture \u2693\ufe0e EPAM Delivery Platform (EDP) is suitable for all aspects of delivery starting from development including the capability to deploy production environment. EDP architecture is represented on a diagram below. EDP consists of three cross-cutting concerns: Infrastructure as a Service; Container orchestration and centralized services; Security. On the top of these indicated concerns, EDP adds several blocks that include: EDP CI/CD Components . EDP component enables a feature in CI/CD or an instance artifacts storage and distribution (Nexus or Artifactory), static code analysis (Sonar), etc.; EDP Artifacts . This element represents an artifact that is being delivered through EDP and presented as a code. Artifact samples: frontend, backend, mobile, applications, functional and non-functional autotests, workloads for 3rd party components that can be deployed together with applications. EDP development and production environments that share the same logic. Environments wrap a set of artifacts with a specific version, and allow performing SDLC routines in order to be sure of the artifacts quality; Pipelines . Pipelines cover CI/CD process, production rollout and updates. They also connect three elements indicated above via automation allowing SDLC routines to be non-human; Technology Stack \u2693\ufe0e Explore the EDP technology stack diagram The EDP IaaS layer supports most popular public clouds AWS, Azure and GCP keeping the capability to be deployed on private/hybrid clouds based on OpenStack. EDP containers are based on Docker technology , orchestrated by Kubernetes compatible solutions. There are two main options for Kubernetes provided by EDP: Managed Kubernetes in Public Clouds to avoid installation and management of Kubernetes cluster, and get all benefits of scaling, reliability of this solution; OpenShift that is a Platform as a Service on the top of Kubernetes from Red Hat. OpenShift is the default option for on-premise installation and it can be considered whether the solution built on the top of EDP should be cloud-agnostic or require enterprise support ; There is no limitation to run EDP on vanilla Kubernetes. Note To get accurate information about EDP architecture, please refer to the EDP Architecture page.","title":"Overview"},{"location":"#epam-delivery-platform","text":"EPAM Delivery platform (EDP) is an open-source cloud-agnostic SaaS/PaaS solution for software development, licensed under Apache License 2.0 . It provides a pre-defined set of CI/CD patterns and tools, which allow a user to start product development quickly with established code review , release , versioning , branching , build processes. These processes include static code analysis, security checks, linters, validators, dynamic feature environments provisioning. EDP consolidates the top Open-Source CI/CD tools by running them on Kubernetes/OpenShift, which enables web/app development either in isolated (on-prem) or cloud environments. EPAM Delivery Platform, which is also called \"The Rocket\" , is a platform that allows shortening the time that is passed before an active development can be started from several months to several hours. EDP consists of the following: The platform based on managed infrastructure and container orchestration; Security covering authentication, authorization, and SSO for platform services; Development and testing toolset; Well-established engineering process and EPAM practices (EngX) reflected in CI/CD pipelines, and delivery analytics; Local development with debug capabilities.","title":"EPAM Delivery Platform"},{"location":"#features","text":"Deployed and configured CI/CD toolset ( Jenkins , Gerrit , Nexus , SonarQube ) Gerrit, GitLab or GitHub as a version control system for your code Jenkins is a pipeline orchestrator CI pipelines for Python, Java 8, Java 11, .Net, Go, React, Terraform, Jenkins Groovy Pipelines, Dockerfile, Helm Build tools: Go, Apache Maven, Apache Gradle Admin Console UI as a single entry point CD pipeline for Microservice Deployment Kubernetes native approach ( CRD, CR ) to declare CI/CD pipelines","title":"Features"},{"location":"#admin-console-ui","text":"Admin Console is a central management tool in the EDP ecosystem that provides the ability to define pipelines, project resources and new technologies in a simple way. Using Admin Console enables to manage business entities: Create Codebases as Applications, Libraries and Autotests; Create/Update CD Pipelines; Note To interact with Admin Console via REST API, explore the Create Codebase Entity page. Navigation bar \u2013 consists of seven sections: Overview, Continuous Delivery, Applications, Autotests, Libraries, and Delivery Dashboard Diagram. Click the necessary section to add an entity, open a home page or check the diagram. User name \u2013 displays the registered user name. Main links \u2013 displays the corresponding links to the major adjusted toolset, to the management tool and to the OpenShift cluster. Admin Console is a complete tool allowing to manage and control added to the environment codebases (applications, autotests, libraries) as well as to create a CD pipeline and check the visualization diagram. Inspect the main features available in Admin Console by following the corresponding link: Add Application Add Autotest Add Library Add CD Pipeline Delivery Dashboard Diagram","title":"Admin Console UI"},{"location":"#architecture","text":"EPAM Delivery Platform (EDP) is suitable for all aspects of delivery starting from development including the capability to deploy production environment. EDP architecture is represented on a diagram below. EDP consists of three cross-cutting concerns: Infrastructure as a Service; Container orchestration and centralized services; Security. On the top of these indicated concerns, EDP adds several blocks that include: EDP CI/CD Components . EDP component enables a feature in CI/CD or an instance artifacts storage and distribution (Nexus or Artifactory), static code analysis (Sonar), etc.; EDP Artifacts . This element represents an artifact that is being delivered through EDP and presented as a code. Artifact samples: frontend, backend, mobile, applications, functional and non-functional autotests, workloads for 3rd party components that can be deployed together with applications. EDP development and production environments that share the same logic. Environments wrap a set of artifacts with a specific version, and allow performing SDLC routines in order to be sure of the artifacts quality; Pipelines . Pipelines cover CI/CD process, production rollout and updates. They also connect three elements indicated above via automation allowing SDLC routines to be non-human;","title":"Architecture"},{"location":"#technology-stack","text":"Explore the EDP technology stack diagram The EDP IaaS layer supports most popular public clouds AWS, Azure and GCP keeping the capability to be deployed on private/hybrid clouds based on OpenStack. EDP containers are based on Docker technology , orchestrated by Kubernetes compatible solutions. There are two main options for Kubernetes provided by EDP: Managed Kubernetes in Public Clouds to avoid installation and management of Kubernetes cluster, and get all benefits of scaling, reliability of this solution; OpenShift that is a Platform as a Service on the top of Kubernetes from Red Hat. OpenShift is the default option for on-premise installation and it can be considered whether the solution built on the top of EDP should be cloud-agnostic or require enterprise support ; There is no limitation to run EDP on vanilla Kubernetes. Note To get accurate information about EDP architecture, please refer to the EDP Architecture page.","title":"Technology Stack"},{"location":"getting-started/","text":"Getting Started \u2693\ufe0e Requirements \u2693\ufe0e Kubernetes cluster 1.18+, or OpenShift 4.6+ kubectl tool helm 3.5.x+ Keycloak 11.0+ Kiosk v0.2.7 Amazon EKS Pod Identity Webhook in case of using AWS ECR as Docker Registry Hardware \u2693\ufe0e Minimal: CPU: 4 Core Memory: 16 Gb EDP Toolset \u2693\ufe0e List of Tools used on the Platform: Domain Related Tools/Solutions Artefacts Management Nexus Repository, AWS ECR AWS Amazon EKS Pod Identity Webhook, AWS ECR, AWS EFS Build .NET, Go, Apache Gradle, Apache Maven, NPM Cluster Backup Velero Code Review Gerrit, GitLab, GitHub Docker Hadolint, kaniko, crane Infrastructure as Code Terraform, TFLint Kubernetes deployment kubectl, helm, ct (Chart Testing) Kubernetes Multitenancy Kiosk Logging EFK, ELK, Loki Monitoring Prometheus, Grafana Pipeline Orchestration Jenkins, GitLab CI (basic) Policies/Rules Open Policy Agent SSO Keycloak, keycloak-proxy Static Code Analysis SonarQube Test Report Tool Allure Install prerequisites \u2693\ufe0e Install EDP \u2693\ufe0e","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#requirements","text":"Kubernetes cluster 1.18+, or OpenShift 4.6+ kubectl tool helm 3.5.x+ Keycloak 11.0+ Kiosk v0.2.7 Amazon EKS Pod Identity Webhook in case of using AWS ECR as Docker Registry","title":"Requirements"},{"location":"getting-started/#hardware","text":"Minimal: CPU: 4 Core Memory: 16 Gb","title":"Hardware"},{"location":"getting-started/#edp-toolset","text":"List of Tools used on the Platform: Domain Related Tools/Solutions Artefacts Management Nexus Repository, AWS ECR AWS Amazon EKS Pod Identity Webhook, AWS ECR, AWS EFS Build .NET, Go, Apache Gradle, Apache Maven, NPM Cluster Backup Velero Code Review Gerrit, GitLab, GitHub Docker Hadolint, kaniko, crane Infrastructure as Code Terraform, TFLint Kubernetes deployment kubectl, helm, ct (Chart Testing) Kubernetes Multitenancy Kiosk Logging EFK, ELK, Loki Monitoring Prometheus, Grafana Pipeline Orchestration Jenkins, GitLab CI (basic) Policies/Rules Open Policy Agent SSO Keycloak, keycloak-proxy Static Code Analysis SonarQube Test Report Tool Allure","title":"EDP Toolset"},{"location":"getting-started/#install-prerequisites","text":"","title":"Install prerequisites"},{"location":"getting-started/#install-edp","text":"","title":"Install EDP"},{"location":"glossary/","text":"Glossary \u2693\ufe0e Get familiar with the definitions and context for the most useful EDP terms presented in table below. Terms Details EDP Component - an item used in CI/CD process Admin Console - an EDP component that helps to manage, set up, and control the business entities. Artifactory - an EDP component that stores all the binary artifacts. NOTE : Nexus is used as a possible implementation of a repository. CI/CD Server - an EDP component that launches pipelines that perform the build, QA, and deployment code logic. NOTE : Jenkins is used as a possible implementation of a CI/CD server. Code Review tool - an EDP component that collaborates with the changes in the codebase. NOTE : Gerrit is used as a possible implementation of a code review tool. Identity Server - an authentication server providing a common way to verify requests to all of the applications. NOTE : Keycloak is used as a possible implementation of an identity server. Security Realm Tenant - a realm in identity server (e.g Keycloak) where all users' accounts and their access permissions are managed. The realm is unique for the identity server instance. Static Code Analyzer - an EDP component that inspects continuously a code quality before the necessary changes appear in a master branch. NOTE : SonarQube is used as a possible implementation of a static code analyzer. VCS (Version Control System) - a replication of the Gerrit repository that displays all the changes made by developers. NOTE : GitHub and Bitbucket are used as the possible implementation of a repository with the version control system. EDP Business Entity - a part of the CI/CD process (the integration, delivery, and deployment of any codebase changes) Application - a codebase type that is built as the binary artifact and deployable unit with the code that is stored in VCS. As a result, the application becomes a container and can be deployed in an environment. Autotests - a codebase type that inspects a product (e.g. an application set) on a stage. Autotests are not deployed to any container and launched from the respective code stage. CD Pipeline (Continuous Delivery Pipeline) - an EDP business entity that describes the whole delivery process of the selected application set via the respective stages. The main idea of the CD pipeline is to promote the application version between the stages by applying the sequential verification (i.e. the second stage will be available if the verification on the first stage is successfully completed). NOTE : The CD pipeline can include the essential set of applications with its specific stages as well. CD Pipeline Stage - an EDP business entity that is presented as the logical gate required for the application set inspection. Every stage has one OpenShift project where the selected application set is deployed. All stages are sequential and promote applications one-by-one. Codebase - an EDP business entity that possesses a code. Codebase Branch - an EDP business entity that represents a specific version in a Git branch. Every codebase branch has a Codebase Docker Stream entity. Codebase Docker Stream - a deployable component that leads to the application build and displays that the last build was verified on the specific stage. Every CD pipeline stage accepts a set of Codebase Docker Streams (CDS) that are input and output. SAMPLE: if an application1 has a master branch, the input CDS will be named as [app name]-[pipeline name]-[stage name]-[master] and the output after the passing of the DEV stage will be as follows: [app name]-[pipeline name]-[stage name]-[dev]-[verified]. Library - a codebase type that is built as the binary artifact, i.e. it`s stored in the Artifactory and can be uploaded by other applications, autotests or libraries. Quality Gate - an EDP business entity that represents the minimum acceptable results after the testing. Every stage has a quality gate that should be passed to promote the application. The stage quality gate can be a manual approve from a QA specialist OR a successful autotest launch. Quality Gate Type - this value defines trigger type that promotes artifacts (images) to the next environment in CD Pipeline. There are manual and automatic types of quality gates. The manual type means that the promoting process should be confirmed in Jenkins. The automatic type promotes the images automatically in case there are no errors in the Allure Report. NOTE : If any of the test types is not passed, the CD pipeline will fail. Trigger Type - a value that defines a trigger type used for the CD pipeline triggering. There are manual and automatic types of triggering. The manual type means that the CD pipeline should be triggered manually. The automatic type triggers the CD pipeline automatically as soon as the Codebase Docker Stream was changed. EDP CI/CD Pipelines Framework - a library that allows extending the Jenkins pipelines and stages to develop an application. Pipelines are presented as the shared library that can be connected in Jenkins. The library is connected using the Git repository link (a public repository that is supported by EDP) on the GitHub. Allure Report - a tool that represents test results in one brief report in a clear form. Automated Tests - different types of automated tests that can be run on the environment for a specific stage. Build Pipeline - a Jenkins pipeline that builds a corresponding codebase branch in the Codebase. Build Stage - a stage that takes place after the code has been submitted/merged to the repository of the main branch ( the pull request from the feature branch is merged to the main one, the Patch set is submitted in Gerrit ). Code Review Pipeline - a Jenkins pipeline that inspects the code candidate in the Code Review tool. Code Review Stage - a stage where code is reviewed before it goes to the main branch repository of the version control system ( the commit to the feature branch is pushed, the Patch set is created in Gerrit ). Deploy Pipeline - a Jenkins pipeline that is responsible for the CD Pipeline Stage deployment with the full set of applications and autotests. Deployment Stage - a part of the Continuous Delivery where artifacts are being deployed to environments. EDP CI/CD Pipelines - an orchestrator for stages that is responsible for the common technical events, e.g. initialization, in Jenkins pipeline. The set of stages for the pipeline is defined as an input JSON file for the respective Jenkins job. NOTE : There is the ability to create the necessary realization of the library pipeline on your own as well. EDP CI/CD Stages - a repository that is launched in the Jenkins pipeline. Every stage is presented as an individual Groovy file in a corresponding repository. Such single responsibility realization allows rewriting of one essential stage without changing the whole pipeline. Environment - a part of the stage where the built and packed into an image application are deployed for further testing. It`s possible to deploy several applications to several environments (Team and Integration environments) within one stage. Integration Environment - an environment type that is always deployed as soon as the new application version is built in order to launch the integration test and promote images to the next stages. The Integration Environment can be triggered manually or in case a new image appears in the Docker registry. Jenkinsfile - a text file that keeps the definition of a Jenkins Pipeline and is checked into source control. Every Job has its Jenkinsfile that is stored in the specific application repository and in Jenkins as the plain text. Jenkins Node - a machine that is a part of the Jenkins environment that is capable of executing a pipeline. Jenkins Pipeline - a user-defined model of a CD pipeline. The pipeline code defines the entire build process. Jenkins Stage - a part of the whole CI/CD process that should pass the source code in order to be released and deployed on the production. Team Environment - an environment type that can be deployed at any time by the manual trigger of the Deploy pipeline where team or developers can check out their applications. NOTE : The promotion from such kind of environment is prohibited and developed only for the local testing. OpenShift / Kubernetes (K8S) ConfigMap - a resource that stores configuration data and processes the strings that do not contain sensitive information. Docker Container - is a lightweight, standalone, and executable package. Docker Registry - a store for the Docker Container that is created for the application after the Build pipeline performance. OpenShift Web Console - a web console that enables to view, manage, and change OpenShift / K8S resources using browser. Operator Framework - a deployable unit in OpenShift that is responsible for one or a set of resources and performs its life circle (adding, displaying, and provisioning). Path - a route component that helps to find a specified path (e.g. /api) at once and skip the other. Pod - the smallest deployable unit of the large microservice application that is responsible for the application launch. The pod is presented as the one launched Docker container. When the Docker container is collected, it will be kept in Docker Registry and then saved as Pod in the OpenShift project. NOTE : The Deployment Config is responsible for the Pod push, restart, and stop processes. PV (Persistent Volume) - a cluster resource that captures the details of the storage implementation and has an independent lifecycle of any individual pod. PVC (Persistent Volume Claim) - a user request for storage that can request specific size and access mode. PV resources are consumed by PVCs. Route - a resource in OpenShift that allows getting the external access to the pushed application. Secret - an object that stores and manages all the sensitive information (e.g. passwords, tokens, and SSH keys). Service - an external connection point with Pod that is responsible for the network. A specific Service is connected to a specific Pod using labels and redirects all the requests to Pod as well. Site - a route component (link name) that is created from the indicated application name and applies automatically the project name and a wildcard DNS record.","title":"Glossary"},{"location":"glossary/#glossary","text":"Get familiar with the definitions and context for the most useful EDP terms presented in table below. Terms Details EDP Component - an item used in CI/CD process Admin Console - an EDP component that helps to manage, set up, and control the business entities. Artifactory - an EDP component that stores all the binary artifacts. NOTE : Nexus is used as a possible implementation of a repository. CI/CD Server - an EDP component that launches pipelines that perform the build, QA, and deployment code logic. NOTE : Jenkins is used as a possible implementation of a CI/CD server. Code Review tool - an EDP component that collaborates with the changes in the codebase. NOTE : Gerrit is used as a possible implementation of a code review tool. Identity Server - an authentication server providing a common way to verify requests to all of the applications. NOTE : Keycloak is used as a possible implementation of an identity server. Security Realm Tenant - a realm in identity server (e.g Keycloak) where all users' accounts and their access permissions are managed. The realm is unique for the identity server instance. Static Code Analyzer - an EDP component that inspects continuously a code quality before the necessary changes appear in a master branch. NOTE : SonarQube is used as a possible implementation of a static code analyzer. VCS (Version Control System) - a replication of the Gerrit repository that displays all the changes made by developers. NOTE : GitHub and Bitbucket are used as the possible implementation of a repository with the version control system. EDP Business Entity - a part of the CI/CD process (the integration, delivery, and deployment of any codebase changes) Application - a codebase type that is built as the binary artifact and deployable unit with the code that is stored in VCS. As a result, the application becomes a container and can be deployed in an environment. Autotests - a codebase type that inspects a product (e.g. an application set) on a stage. Autotests are not deployed to any container and launched from the respective code stage. CD Pipeline (Continuous Delivery Pipeline) - an EDP business entity that describes the whole delivery process of the selected application set via the respective stages. The main idea of the CD pipeline is to promote the application version between the stages by applying the sequential verification (i.e. the second stage will be available if the verification on the first stage is successfully completed). NOTE : The CD pipeline can include the essential set of applications with its specific stages as well. CD Pipeline Stage - an EDP business entity that is presented as the logical gate required for the application set inspection. Every stage has one OpenShift project where the selected application set is deployed. All stages are sequential and promote applications one-by-one. Codebase - an EDP business entity that possesses a code. Codebase Branch - an EDP business entity that represents a specific version in a Git branch. Every codebase branch has a Codebase Docker Stream entity. Codebase Docker Stream - a deployable component that leads to the application build and displays that the last build was verified on the specific stage. Every CD pipeline stage accepts a set of Codebase Docker Streams (CDS) that are input and output. SAMPLE: if an application1 has a master branch, the input CDS will be named as [app name]-[pipeline name]-[stage name]-[master] and the output after the passing of the DEV stage will be as follows: [app name]-[pipeline name]-[stage name]-[dev]-[verified]. Library - a codebase type that is built as the binary artifact, i.e. it`s stored in the Artifactory and can be uploaded by other applications, autotests or libraries. Quality Gate - an EDP business entity that represents the minimum acceptable results after the testing. Every stage has a quality gate that should be passed to promote the application. The stage quality gate can be a manual approve from a QA specialist OR a successful autotest launch. Quality Gate Type - this value defines trigger type that promotes artifacts (images) to the next environment in CD Pipeline. There are manual and automatic types of quality gates. The manual type means that the promoting process should be confirmed in Jenkins. The automatic type promotes the images automatically in case there are no errors in the Allure Report. NOTE : If any of the test types is not passed, the CD pipeline will fail. Trigger Type - a value that defines a trigger type used for the CD pipeline triggering. There are manual and automatic types of triggering. The manual type means that the CD pipeline should be triggered manually. The automatic type triggers the CD pipeline automatically as soon as the Codebase Docker Stream was changed. EDP CI/CD Pipelines Framework - a library that allows extending the Jenkins pipelines and stages to develop an application. Pipelines are presented as the shared library that can be connected in Jenkins. The library is connected using the Git repository link (a public repository that is supported by EDP) on the GitHub. Allure Report - a tool that represents test results in one brief report in a clear form. Automated Tests - different types of automated tests that can be run on the environment for a specific stage. Build Pipeline - a Jenkins pipeline that builds a corresponding codebase branch in the Codebase. Build Stage - a stage that takes place after the code has been submitted/merged to the repository of the main branch ( the pull request from the feature branch is merged to the main one, the Patch set is submitted in Gerrit ). Code Review Pipeline - a Jenkins pipeline that inspects the code candidate in the Code Review tool. Code Review Stage - a stage where code is reviewed before it goes to the main branch repository of the version control system ( the commit to the feature branch is pushed, the Patch set is created in Gerrit ). Deploy Pipeline - a Jenkins pipeline that is responsible for the CD Pipeline Stage deployment with the full set of applications and autotests. Deployment Stage - a part of the Continuous Delivery where artifacts are being deployed to environments. EDP CI/CD Pipelines - an orchestrator for stages that is responsible for the common technical events, e.g. initialization, in Jenkins pipeline. The set of stages for the pipeline is defined as an input JSON file for the respective Jenkins job. NOTE : There is the ability to create the necessary realization of the library pipeline on your own as well. EDP CI/CD Stages - a repository that is launched in the Jenkins pipeline. Every stage is presented as an individual Groovy file in a corresponding repository. Such single responsibility realization allows rewriting of one essential stage without changing the whole pipeline. Environment - a part of the stage where the built and packed into an image application are deployed for further testing. It`s possible to deploy several applications to several environments (Team and Integration environments) within one stage. Integration Environment - an environment type that is always deployed as soon as the new application version is built in order to launch the integration test and promote images to the next stages. The Integration Environment can be triggered manually or in case a new image appears in the Docker registry. Jenkinsfile - a text file that keeps the definition of a Jenkins Pipeline and is checked into source control. Every Job has its Jenkinsfile that is stored in the specific application repository and in Jenkins as the plain text. Jenkins Node - a machine that is a part of the Jenkins environment that is capable of executing a pipeline. Jenkins Pipeline - a user-defined model of a CD pipeline. The pipeline code defines the entire build process. Jenkins Stage - a part of the whole CI/CD process that should pass the source code in order to be released and deployed on the production. Team Environment - an environment type that can be deployed at any time by the manual trigger of the Deploy pipeline where team or developers can check out their applications. NOTE : The promotion from such kind of environment is prohibited and developed only for the local testing. OpenShift / Kubernetes (K8S) ConfigMap - a resource that stores configuration data and processes the strings that do not contain sensitive information. Docker Container - is a lightweight, standalone, and executable package. Docker Registry - a store for the Docker Container that is created for the application after the Build pipeline performance. OpenShift Web Console - a web console that enables to view, manage, and change OpenShift / K8S resources using browser. Operator Framework - a deployable unit in OpenShift that is responsible for one or a set of resources and performs its life circle (adding, displaying, and provisioning). Path - a route component that helps to find a specified path (e.g. /api) at once and skip the other. Pod - the smallest deployable unit of the large microservice application that is responsible for the application launch. The pod is presented as the one launched Docker container. When the Docker container is collected, it will be kept in Docker Registry and then saved as Pod in the OpenShift project. NOTE : The Deployment Config is responsible for the Pod push, restart, and stop processes. PV (Persistent Volume) - a cluster resource that captures the details of the storage implementation and has an independent lifecycle of any individual pod. PVC (Persistent Volume Claim) - a user request for storage that can request specific size and access mode. PV resources are consumed by PVCs. Route - a resource in OpenShift that allows getting the external access to the pushed application. Secret - an object that stores and manages all the sensitive information (e.g. passwords, tokens, and SSH keys). Service - an external connection point with Pod that is responsible for the network. A specific Service is connected to a specific Pod using labels and redirects all the requests to Pod as well. Site - a route component (link name) that is created from the indicated application name and applies automatically the project name and a wildcard DNS record.","title":"Glossary"},{"location":"roadmap/","text":"RoadMap \u2693\ufe0e RoadMap consists of three streams: Architecture Building Blocks Admin Console I. Architecture \u2693\ufe0e Goals: Improve reusability for EDP components Integrate Kubernetes Native Deployment solutions Introduce abstraction layer for CI/CD components Build processes around the GitOps approach Introduce secrets management Kubernetes Multitenancy \u2693\ufe0e Multiple instances of EDP are run in a single Kubernetes cluster. One way to achieve this is to use Multitenancy . Initially, Kiosk was selected as tools that provides this capability. An alternative option that EDP Team took into consideration is Capsule . Another tool which goes far beyond multitenancy is vcluster going a good candidate for e2e testing scenarios where one needs simple lightweight kubernetes cluster in CI pipelines. Microservice Reference Architecture Framework \u2693\ufe0e EDP provides basic Application Templates for a number of technology stacks (Java, .Net, NPM, Python) and Helm is used as a deployment tool. The goal is to extend this library and provide: Application Templates which are built on pre-defined architecture patterns (e.g., Microservice, API Gateway, Circuit Breaker, CQRS, Event Driven) and Deployment Approaches : Canary, Blue/Green. This requires additional tools installation on cluster as well. Policy Enforcement for Kubernetes \u2693\ufe0e Running workload in Kubernetes calls for extra effort from Cluster Administrators to ensure those workloads do follow best practices or specific requirements defined on organization level. Those requirements can be formalized in policies and integrated into: CI Pipelines and Kubernetes Cluster (through Admission Controller approach) - to guarantee proper resource management during development and runtime phases. EDP uses Open Policy Agent (from version 2.8.0), since it supports compliance check for more use-cases: Kubernetes Workloads, Terraform and Java code, HTTP APIs and many others . Kyverno is another option being checked in scope of this activity. Secrets Management \u2693\ufe0e EDP should provide secrets management as a part of platform. There are multiple tools providing secrets management capabilities. The aim is to be aligned with GitOps and Operator Pattern approaches so HashiCorp Vault , Banzaicloud Bank Vaults , Bitnami Sealed Secrets are currently used for internal projects and some of them should be made publicly available - as a part of EDP Deployment. Release Management \u2693\ufe0e Conventional Commits and Conventional Changelog are two approaches to be used as part of release process. Today EDP provides only capabilities to manage Release Branches . This activity should address this gap by formalizing and implementing Release Process as a part of EDP. Topics to be covered: Versioning, Tagging, Artifacts Promotion. Kubernetes Native CI/CD Pipelines \u2693\ufe0e EDP uses Jenkins as Pipeline Orchestrator. Jenkins runs workload for CI and CD parts. There is also basic support for GitLab CI , but it provides Docker image build functionality only. EDP works on providing an alternative to Jenkins and use Kubernetes Native Approach for pipeline management. There are a number of tools, which provides such capability: ArgoCD Argo Workflows Argo Rollouts Tekton Drone Flux This list is under investigation and solution is going to be implemented in two steps: Introduce tool that provide Continues Deployment approach. ArgoCD is one of the best to go with. Integrate EDP with tool that provides Continues Integration capabilities. Advanced EDP Role-based Model \u2693\ufe0e EDP has a number of base roles which are used across EDP. In some cases we want to provide more granular permissions for specific users. We also want to make this using Kubernetes Native approach. Notifications Framework \u2693\ufe0e EDP has a number of components which need to report their statuses: Build/Code Review/Deploy Pipelines, changes in Environments, updates with artifacts. The goal for this activity is to onboard Kubernetes Native approach which provides Notification capabilities with different sources/channels integration (e.g. Email, Slack, MS Teams). Some of these tools are Argo Events , Botkube . Reconciler Component Retirement \u2693\ufe0e Persistent layer, which is based on edp-db (PostgreSQL) and reconciler component should be retired in favour of Kubernetes Custom Resource (CR) . The latest features in EDP are implemented using CR approach. II. Building Blocks \u2693\ufe0e Goals: Introduce best practices from Microservice Reference Architecture deployment and observability using Kubernetes Native Tools Enable integration with the Centralized Test Reporting Frameworks Onboard SAST/DAST tool as a part of CI pipelines and Non-Functional Testing activities Infrastructure as Code \u2693\ufe0e EDP Target tool for Infrastructure as Code (IaC) is Terraform . EDP sees two CI/CD scenarios while working with IaC: Module Development and Live Environment Deployment . Today, EDP provides basic capabilities (CI Pipelines) for Terraform Module Development . At the same time, currently EDP doesn't provide Deployment pipelines for Live Environments and we are working on this feature. Terragrunt is an option to use in Live Environment deployment. Another Kubernetes Native approach to provision infrastructure components is Crossplane . Database Schema Management \u2693\ufe0e One of the challenges for Application running in Kubernetes is to manage database schema. There are a number of tools which provides such capabilities, e.g. Liquibase , Flyway . Both tools provide versioning control for database schemas. There are different approaches on how to run migration scripts in Kubernetes : in init container , as separate Job or as a separate CD stage. Purpose of this activity is to provide database schema management solution in Kubernetes as a part of EDP. EDP Team investigates SchemaHero tool and use-cases which suits Kubernetes native approach for database schema migrations. Open Policy Agent \u2693\ufe0e Open Policy Agent is introduced in version 2.8.0 . EDP now supports CI for Rego Language , so you can develop your own policies. The next goal is to provide pipeline steps for running compliance policies check for Terraform, Java, Helm Chart as a part of CI process. Report Portal \u2693\ufe0e EDP uses Allure Framework as a Test Report tool . Another option is to integrate Report Portal into EDP ecosystem. Carrier \u2693\ufe0e Carrier provides Non-functional testing capabilities. Java 17 \u2693\ufe0e EDP supports two LTS versions of Java: 8 and 11. The goal is to provide Java 17 (LTS) support. Velero \u2693\ufe0e Velero is used as a cluster backup tool and is deployed as a part of Platform. Currently, Multitenancy/On-premise support for backup capabilities is in process.. Istio \u2693\ufe0e Istio is to be used as a Service Mesh and to address challenges for Microservice or Distributed Architectures. Kong \u2693\ufe0e Kong is one of tools which is planned to use as an API Gateway solution provider. Another possible candidate for investigation is Ambassador API Gateway OpenShift 4.X \u2693\ufe0e OpenShift 4.6 is a platform that EDP supports. III. Admin Console (UI) \u2693\ufe0e Goals: Improve U\u0425 for different user types to address their concerns in the delivery model Introduce user management capabilities Enrich with traceability metrics for products Users Management \u2693\ufe0e EDP uses Keycloak as Identity and Access provider. We manage EDP roles/groups inside Keycloak realm, then these changes are propagated across EDP Tools. We want to provide this fun functionality in EDP Admin Console and using Kubernetes native approach with Custom Resources. Delivery Pipelines Dashboard \u2693\ufe0e Our [CD Pipeline] section (/user-guide/add-cd-pipeline.md) in Admin Console provides basic information like: environments, artifact versions deployed per each environment, direct links to namespaces. We want to enrich this panel with metrics (from prometheus, custom resources, events, etc). Another option is to use existing dashboards and expose EDP metrics to them, e.g. plugin for Lens , OpenShift UI Console Split Jira and Commit Validation Sections \u2693\ufe0e Commit Validate step was initially designed to be aligned with Jira Integration and cannot be used as single feature. Target state is to ensure features CommitMessage Validation and Jira Integration both can be used independently. We also want to add support for Conventional Commits . IV. Documentation as Code \u2693\ufe0e Goal: Transparent documentation and clear development guidelines for EDP customization. Consolidate documentation in a single repository edp-install , use mkdocs tool to generate docs and GitHub Pages as hosting solution.","title":"RoadMap"},{"location":"roadmap/#roadmap","text":"RoadMap consists of three streams: Architecture Building Blocks Admin Console","title":"RoadMap"},{"location":"roadmap/#i-architecture","text":"Goals: Improve reusability for EDP components Integrate Kubernetes Native Deployment solutions Introduce abstraction layer for CI/CD components Build processes around the GitOps approach Introduce secrets management","title":"I. Architecture"},{"location":"roadmap/#kubernetes-multitenancy","text":"Multiple instances of EDP are run in a single Kubernetes cluster. One way to achieve this is to use Multitenancy . Initially, Kiosk was selected as tools that provides this capability. An alternative option that EDP Team took into consideration is Capsule . Another tool which goes far beyond multitenancy is vcluster going a good candidate for e2e testing scenarios where one needs simple lightweight kubernetes cluster in CI pipelines.","title":"Kubernetes Multitenancy"},{"location":"roadmap/#microservice-reference-architecture-framework","text":"EDP provides basic Application Templates for a number of technology stacks (Java, .Net, NPM, Python) and Helm is used as a deployment tool. The goal is to extend this library and provide: Application Templates which are built on pre-defined architecture patterns (e.g., Microservice, API Gateway, Circuit Breaker, CQRS, Event Driven) and Deployment Approaches : Canary, Blue/Green. This requires additional tools installation on cluster as well.","title":"Microservice Reference Architecture Framework"},{"location":"roadmap/#policy-enforcement-for-kubernetes","text":"Running workload in Kubernetes calls for extra effort from Cluster Administrators to ensure those workloads do follow best practices or specific requirements defined on organization level. Those requirements can be formalized in policies and integrated into: CI Pipelines and Kubernetes Cluster (through Admission Controller approach) - to guarantee proper resource management during development and runtime phases. EDP uses Open Policy Agent (from version 2.8.0), since it supports compliance check for more use-cases: Kubernetes Workloads, Terraform and Java code, HTTP APIs and many others . Kyverno is another option being checked in scope of this activity.","title":"Policy Enforcement for Kubernetes"},{"location":"roadmap/#secrets-management","text":"EDP should provide secrets management as a part of platform. There are multiple tools providing secrets management capabilities. The aim is to be aligned with GitOps and Operator Pattern approaches so HashiCorp Vault , Banzaicloud Bank Vaults , Bitnami Sealed Secrets are currently used for internal projects and some of them should be made publicly available - as a part of EDP Deployment.","title":"Secrets Management"},{"location":"roadmap/#release-management","text":"Conventional Commits and Conventional Changelog are two approaches to be used as part of release process. Today EDP provides only capabilities to manage Release Branches . This activity should address this gap by formalizing and implementing Release Process as a part of EDP. Topics to be covered: Versioning, Tagging, Artifacts Promotion.","title":"Release Management"},{"location":"roadmap/#kubernetes-native-cicd-pipelines","text":"EDP uses Jenkins as Pipeline Orchestrator. Jenkins runs workload for CI and CD parts. There is also basic support for GitLab CI , but it provides Docker image build functionality only. EDP works on providing an alternative to Jenkins and use Kubernetes Native Approach for pipeline management. There are a number of tools, which provides such capability: ArgoCD Argo Workflows Argo Rollouts Tekton Drone Flux This list is under investigation and solution is going to be implemented in two steps: Introduce tool that provide Continues Deployment approach. ArgoCD is one of the best to go with. Integrate EDP with tool that provides Continues Integration capabilities.","title":"Kubernetes Native CI/CD Pipelines"},{"location":"roadmap/#advanced-edp-role-based-model","text":"EDP has a number of base roles which are used across EDP. In some cases we want to provide more granular permissions for specific users. We also want to make this using Kubernetes Native approach.","title":"Advanced EDP Role-based Model"},{"location":"roadmap/#notifications-framework","text":"EDP has a number of components which need to report their statuses: Build/Code Review/Deploy Pipelines, changes in Environments, updates with artifacts. The goal for this activity is to onboard Kubernetes Native approach which provides Notification capabilities with different sources/channels integration (e.g. Email, Slack, MS Teams). Some of these tools are Argo Events , Botkube .","title":"Notifications Framework"},{"location":"roadmap/#reconciler-component-retirement","text":"Persistent layer, which is based on edp-db (PostgreSQL) and reconciler component should be retired in favour of Kubernetes Custom Resource (CR) . The latest features in EDP are implemented using CR approach.","title":"Reconciler Component Retirement"},{"location":"roadmap/#ii-building-blocks","text":"Goals: Introduce best practices from Microservice Reference Architecture deployment and observability using Kubernetes Native Tools Enable integration with the Centralized Test Reporting Frameworks Onboard SAST/DAST tool as a part of CI pipelines and Non-Functional Testing activities","title":"II. Building Blocks"},{"location":"roadmap/#infrastructure-as-code","text":"EDP Target tool for Infrastructure as Code (IaC) is Terraform . EDP sees two CI/CD scenarios while working with IaC: Module Development and Live Environment Deployment . Today, EDP provides basic capabilities (CI Pipelines) for Terraform Module Development . At the same time, currently EDP doesn't provide Deployment pipelines for Live Environments and we are working on this feature. Terragrunt is an option to use in Live Environment deployment. Another Kubernetes Native approach to provision infrastructure components is Crossplane .","title":"Infrastructure as Code"},{"location":"roadmap/#database-schema-management","text":"One of the challenges for Application running in Kubernetes is to manage database schema. There are a number of tools which provides such capabilities, e.g. Liquibase , Flyway . Both tools provide versioning control for database schemas. There are different approaches on how to run migration scripts in Kubernetes : in init container , as separate Job or as a separate CD stage. Purpose of this activity is to provide database schema management solution in Kubernetes as a part of EDP. EDP Team investigates SchemaHero tool and use-cases which suits Kubernetes native approach for database schema migrations.","title":"Database Schema Management"},{"location":"roadmap/#open-policy-agent","text":"Open Policy Agent is introduced in version 2.8.0 . EDP now supports CI for Rego Language , so you can develop your own policies. The next goal is to provide pipeline steps for running compliance policies check for Terraform, Java, Helm Chart as a part of CI process.","title":"Open Policy Agent"},{"location":"roadmap/#report-portal","text":"EDP uses Allure Framework as a Test Report tool . Another option is to integrate Report Portal into EDP ecosystem.","title":"Report Portal"},{"location":"roadmap/#carrier","text":"Carrier provides Non-functional testing capabilities.","title":"Carrier"},{"location":"roadmap/#java-17","text":"EDP supports two LTS versions of Java: 8 and 11. The goal is to provide Java 17 (LTS) support.","title":"Java 17"},{"location":"roadmap/#velero","text":"Velero is used as a cluster backup tool and is deployed as a part of Platform. Currently, Multitenancy/On-premise support for backup capabilities is in process..","title":"Velero"},{"location":"roadmap/#istio","text":"Istio is to be used as a Service Mesh and to address challenges for Microservice or Distributed Architectures.","title":"Istio"},{"location":"roadmap/#kong","text":"Kong is one of tools which is planned to use as an API Gateway solution provider. Another possible candidate for investigation is Ambassador API Gateway","title":"Kong"},{"location":"roadmap/#openshift-4x","text":"OpenShift 4.6 is a platform that EDP supports.","title":"OpenShift 4.X"},{"location":"roadmap/#iii-admin-console-ui","text":"Goals: Improve U\u0425 for different user types to address their concerns in the delivery model Introduce user management capabilities Enrich with traceability metrics for products","title":"III. Admin Console (UI)"},{"location":"roadmap/#users-management","text":"EDP uses Keycloak as Identity and Access provider. We manage EDP roles/groups inside Keycloak realm, then these changes are propagated across EDP Tools. We want to provide this fun functionality in EDP Admin Console and using Kubernetes native approach with Custom Resources.","title":"Users Management"},{"location":"roadmap/#delivery-pipelines-dashboard","text":"Our [CD Pipeline] section (/user-guide/add-cd-pipeline.md) in Admin Console provides basic information like: environments, artifact versions deployed per each environment, direct links to namespaces. We want to enrich this panel with metrics (from prometheus, custom resources, events, etc). Another option is to use existing dashboards and expose EDP metrics to them, e.g. plugin for Lens , OpenShift UI Console","title":"Delivery Pipelines Dashboard"},{"location":"roadmap/#split-jira-and-commit-validation-sections","text":"Commit Validate step was initially designed to be aligned with Jira Integration and cannot be used as single feature. Target state is to ensure features CommitMessage Validation and Jira Integration both can be used independently. We also want to add support for Conventional Commits .","title":"Split Jira and Commit Validation Sections"},{"location":"roadmap/#iv-documentation-as-code","text":"Goal: Transparent documentation and clear development guidelines for EDP customization. Consolidate documentation in a single repository edp-install , use mkdocs tool to generate docs and GitHub Pages as hosting solution.","title":"IV. Documentation as Code"},{"location":"developer-guide/","text":"Overview \u2693\ufe0e This guide is for developers who want to extend EDP functionality","title":"Overview"},{"location":"developer-guide/#overview","text":"This guide is for developers who want to extend EDP functionality","title":"Overview"},{"location":"developer-guide/local-development/","text":"Local Development \u2693\ufe0e Requirements \u2693\ufe0e GoLang version higher than 1.13; Note The GOPATH and GOROOT environment variables should be added in PATH. PostgreSQL client version higher than 9.5; Configured access to the VCS; GoLand Intellij IDEA or another IDE. Start Operator \u2693\ufe0e In order to run the operator, follow the steps below: Clone repository; Open folder in GoLand Intellij IDEA, click the button and select the Go Build option: In Configuration tab, fill in the following: 3.1. In the Field field, indicate the path to the main.go file; 3.2. In the Working directory field, indicate the path to the operator; 3.3. In the Environment field, specify the platform name (OpenShift/Kubernetes); Create the PostgreSQL database, schema, and a user for the EDP Admin Console operator: Create database with a user: CREATE DATABASE edp-db WITH ENCODING 'UTF8'; CREATE USER postgres WITH PASSWORD 'password'; GRANT ALL PRIVILEGES ON DATABASE 'edp-db' to postgres; Create a schema: CREATE SCHEMA [ IF NOT EXISTS ] ' develop ' ; EDP Admin Console operator supports two modes for running: local and prod. For local deploy, modify edp-admin-console/conf/app.conf and set the following parameters: runmode = local [ local ] dbEnabled = true pgHost = localhost pgPort = 5432 pgDatabase = edp - db pgUser = postgres pgPassword = password edpName = develop Run go build main.go (Shift+F10); After the successful setup, follow the http://localhost:8080 URL address to check the result: Exceptional Cases \u2693\ufe0e After starting the Go build process, the following error will appear: go : finding github . com / openshift / api v3 .9.0 go : finding github . com / openshift / client - go v3 .9.0 go : errors parsing go . mod : C : \\ Users \\ << username >> \\ Desktop \\ EDP \\ edp - admin - console \\ go . mod : 36 : require github . com / openshift / api : version \"v3.9.0\" invalid : unknown revision v3 .9.0 Compilation finished with exit code 1 To resolve the issue, update the go dependency by applying the Golang command: go get github . com / openshift / api @v3 .9.0","title":"Local Development"},{"location":"developer-guide/local-development/#local-development","text":"","title":"Local Development"},{"location":"developer-guide/local-development/#requirements","text":"GoLang version higher than 1.13; Note The GOPATH and GOROOT environment variables should be added in PATH. PostgreSQL client version higher than 9.5; Configured access to the VCS; GoLand Intellij IDEA or another IDE.","title":"Requirements"},{"location":"developer-guide/local-development/#start-operator","text":"In order to run the operator, follow the steps below: Clone repository; Open folder in GoLand Intellij IDEA, click the button and select the Go Build option: In Configuration tab, fill in the following: 3.1. In the Field field, indicate the path to the main.go file; 3.2. In the Working directory field, indicate the path to the operator; 3.3. In the Environment field, specify the platform name (OpenShift/Kubernetes); Create the PostgreSQL database, schema, and a user for the EDP Admin Console operator: Create database with a user: CREATE DATABASE edp-db WITH ENCODING 'UTF8'; CREATE USER postgres WITH PASSWORD 'password'; GRANT ALL PRIVILEGES ON DATABASE 'edp-db' to postgres; Create a schema: CREATE SCHEMA [ IF NOT EXISTS ] ' develop ' ; EDP Admin Console operator supports two modes for running: local and prod. For local deploy, modify edp-admin-console/conf/app.conf and set the following parameters: runmode = local [ local ] dbEnabled = true pgHost = localhost pgPort = 5432 pgDatabase = edp - db pgUser = postgres pgPassword = password edpName = develop Run go build main.go (Shift+F10); After the successful setup, follow the http://localhost:8080 URL address to check the result:","title":"Start Operator"},{"location":"developer-guide/local-development/#exceptional-cases","text":"After starting the Go build process, the following error will appear: go : finding github . com / openshift / api v3 .9.0 go : finding github . com / openshift / client - go v3 .9.0 go : errors parsing go . mod : C : \\ Users \\ << username >> \\ Desktop \\ EDP \\ edp - admin - console \\ go . mod : 36 : require github . com / openshift / api : version \"v3.9.0\" invalid : unknown revision v3 .9.0 Compilation finished with exit code 1 To resolve the issue, update the go dependency by applying the Golang command: go get github . com / openshift / api @v3 .9.0","title":"Exceptional Cases"},{"location":"developer-guide/rest-api/","text":"EDP API \u2693\ufe0e Create Codebase Entity \u2693\ufe0e EDP allows you to create three codebase types: Application, Autotest and Library. There are also several strategy types for each codebase: Create, Clone and Import. Depending on the selected codebase type and the respective strategy, you should specify a different set of fields in a request. Note The Route, Database and VCS are optional fields. In accordance with the necessary deploy set, you have to add the necessary fields into request Request \u2693\ufe0e POST /api/v1/edp/codebase Application (Create) \u2693\ufe0e { \"name\": \"app01\", \"type\": \"application\", \"strategy\": \"create\", \"lang\": \"java\", \"framework\": \"springboot\", \"buildTool\": \"maven\", \"multiModule\": false, \"route\": { \"site\": \"api\", \"path\": \"/\" }, \"database\": { \"kind\": \"postgresql\", \"version\": \"postgres:9.6\", \"capacity\": \"1Gi\", \"storage\": \"efs\" }, \"description\": \"Description\", \"gitServer\": \"gerrit\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", \"deploymentScript\": \"openshift-template\" } Application (Clone) \u2693\ufe0e { \" name \" : \" app01 \" , \" type \" : \" application \" , \" strategy \" : \" clone \" , \" lang \" : \" java \" , \" framework \" : \" springboot \" , \" buildTool \" : \" maven \" , \" multiModule \" : false , \" repository \" : { \" url \" : \" http(s)://git.sample.com/sample.git \" , // login and password are required only if repo is private \" login \" : \" login \" , \" password \" : \" password \" }, \" description \" : \" Description \" , \" gitServer \" : \" gerrit \" , \" jenkinsSlave \" : \" maven \" , \" jobProvisioning \" : \" default \" , \" deploymentScript \" : \" openshift-template \" } Application (Import) \u2693\ufe0e { \"type\": \"application\", \"strategy\": \"import\", \"lang\": \"java\", \"framework\": \"springboot\", \"buildTool\": \"maven\", \"multiModule\": false, \"description\": \"Description\", \"gitServer\": \"git-epam\", \"gitUrlPath\": \"/relative/path/to/repo\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", \"deploymentScript\": \"openshift-template\" } Autotests (Clone) \u2693\ufe0e { \" name \" : \" aut01 \" , \" type \" : \" autotests \" , \" strategy \" : \" clone \" , \" lang \" : \" java \" , \" framework \" : \" springboot \" , \" buildTool \" : \" maven \" , \" testReportFramework \" : \" allure \" , \" repository \" : { \" url \" : \" http(s)://git.sample.com/sample.git \" , // login and password are required only if repo is private \" login \" : \" login \" , \" password \" : \" password \" }, \" description \" : \" Description \" , \" jenkinsSlave \" : \" maven \" , \" jobProvisioning \" : \" default \" } Autotests (Import) \u2693\ufe0e { \"type\": \"autotests\", \"strategy\": \"import\", \"lang\": \"java\", \"framework\": \"springboot\", \"buildTool\": \"maven\", \"testReportFramework\": \"allure\", \"description\": \"Description\", \"gitServer\": \"git-epam\", \"gitRelativePath\": \"/relative/path/to/repo\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\" } Library (Create) \u2693\ufe0e { \"name\": \"lib01\", \"type\": \"library\", \"strategy\": \"create\", \"lang\": \"java\", \"buildTool\": \"maven\", \"multiModule\": false, \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", } Library (Clone) \u2693\ufe0e { \" name \" : \" lib01 \" , \" type \" : \" library \" , \" strategy \" : \" clone \" , \" lang \" : \" java \" , \" buildTool \" : \" maven \" , \" multiModule \" : false , \" repository \" : { \" url \" : \" http(s)://git.sample.com/sample.git \" , // login and password are required only if repo is private \" login \" : \" login \" , \" password \" : \" password \" }, \" vcs \" : null , \" jenkinsSlave \" : \" maven \" , \" jobProvisioning \" : \" default \" , } Library (Import) \u2693\ufe0e { \"type\": \"library\", \"strategy\": \"import\", \"lang\": \"java\", \"buildTool\": \"maven\", \"multiModule\": false, \"gitServer\": \"git-epam\", \"gitUrlPath\": \"/relative/path/to/repo\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", } Response \u2693\ufe0e Status 200 OK Get Codebase by Name \u2693\ufe0e Request \u2693\ufe0e GET /api/v1/edp/codebase/{codebaseName} example: localhost/api/v1/edp/codebase/app01 Response \u2693\ufe0e Status 200 OK { \"id\" : 1 , \"name\" : \"app01\" , \"language\" : \"java\" , \"build_tool\" : \"maven\" , \"framework\" : \"springboot\" , \"strategy\" : \"create\" , \"git_url\" : \"\" , \"route_site\" : \"api\" , \"route_path\" : \"/\" , \"type\" : \"application\" , \"status\" : \"active\" , \"testReportFramework\" : \"\" , \"description\" : \"Description\" , \"codebase_branch\" : [ { \"id\" : 1 , \"branchName\" : \"master\" , \"from_commit\" : \"\" , \"status\" : \"active\" , \"branchLink\" : \"\" , \"jenkinsLink\" : \"\" , \"appName\" : \"\" , \"codebaseDockerStream\" : null } ], \"gitServer\" : \"gerrit\" , \"gitProjectPath\" : null , \"jenkinsSlave\" : \"maven\" , \"jobProvisioning\" : \"default\" , \"deploymentScript\" : \"openshift-template\" } Get All Codebases \u2693\ufe0e Request \u2693\ufe0e GET /api/v1/edp/codebase?type={codebaseType} example: localhost/api/v1/edp/codebase?type=application Response \u2693\ufe0e Status 200 OK [ { \"id\" : 1 , \"name\" : \"app01\" , \"language\" : \"java\" , \"build_tool\" : \"maven\" , \"framework\" : \"springboot\" , \"strategy\" : \"create\" , \"git_url\" : \"\" , \"route_site\" : \"api\" , \"route_path\" : \"/\" , \"type\" : \"application\" , \"status\" : \"active\" , \"testReportFramework\" : \"\" , \"description\" : \"Description\" , \"codebase_branch\" : [ { \"id\" : 1 , \"branchName\" : \"master\" , \"from_commit\" : \"\" , \"status\" : \"active\" , \"branchLink\" : \"\" , \"jenkinsLink\" : \"\" , \"appName\" : \"\" , \"codebaseDockerStream\" : [ { \"id\" : 1 , \"ocImageStreamName\" : \"app01-master\" , \"imageLink\" : \"\" , \"jenkinsLink\" : \"\" } ] } ], \"gitServer\" : \"gerrit\" , \"gitProjectPath\" : null , \"jenkinsSlave\" : \"maven\" , \"jobProvisioning\" : \"\" , \"deploymentScript\" : \"openshift-template\" }, { \"id\" : 2 , \"name\" : \"app02\" , \"language\" : \"java\" , \"build_tool\" : \"maven\" , \"framework\" : \"springboot\" , \"strategy\" : \"create\" , \"git_url\" : \"\" , \"route_site\" : \"app\" , \"route_path\" : \"/\" , \"type\" : \"application\" , \"status\" : \"failed\" , \"testReportFramework\" : \"\" , \"description\" : \"\" , \"codebase_branch\" : [ { \"id\" : 2 , \"branchName\" : \"master\" , \"from_commit\" : \"\" , \"status\" : \"inactive\" , \"branchLink\" : \"\" , \"jenkinsLink\" : \"\" , \"appName\" : \"\" , \"codebaseDockerStream\" : [ { \"id\" : 2 , \"ocImageStreamName\" : \"app02-master\" , \"imageLink\" : \"\" , \"jenkinsLink\" : \"\" } ] } ], \"gitServer\" : \"gerrit\" , \"gitProjectPath\" : null , \"jenkinsSlave\" : \"maven\" , \"jobProvisioning\" : \"\" , \"deploymentScript\" : \"openshift-template\" } ] Create CD Pipeline Entity \u2693\ufe0e Request \u2693\ufe0e POST /api/v1/edp/cd-pipeline { \"name\":\"pipe1\", \"applications\":[ { \"appName\":\"app01\", \"inputDockerStream\":\"app01-master\" } ], \"stages\":[ { \"name\":\"sit\", \"description\":\"description-sit\", \"qualityGateType\":\"manual\", \"stepName\":\"approve\", \"triggerType\":\"manual\", \"order\":0, \"qualityGates\": [ { \"qualityGateType\":\"manual\", \"stepName\":\"step-one-one\", \"autotestName\": null, \"branchName\": null }, { \"qualityGateType\":\"manual\", \"stepName\":\"step-two-two\", \"autotestName\": null, \"branchName\": null } ] } ] } Response \u2693\ufe0e Status 200 OK Get CD Pipeline Entity by Name \u2693\ufe0e Request \u2693\ufe0e GET /api/v1/edp/cd-pipeline/{cdPipelineName} example: localhost/api/v1/edp/cd-pipeline/pipe1 Response \u2693\ufe0e Status 200 OK { \"id\": 1, \"name\": \"pipe1\", \"status\": \"active\", \"jenkinsLink\": \"\", \"codebaseBranches\": [ { \"id\": 1, \"branchName\": \"master\", \"from_commit\": \"\", \"status\": \"active\", \"branchLink\": \"\", \"jenkinsLink\": \"\", \"appName\": \"java-springboot-helloworld\", \"codebaseDockerStream\": [ { \"id\": 1, \"ocImageStreamName\": \"java-springboot-helloworld-master\", \"imageLink\": \"\", \"jenkinsLink\": \"\" } ] } ], \"stages\": [ { \"id\": 1, \"name\": \"sit\", \"description\": \"sit\", \"triggerType\": \"manual\", \"order\": 0, \"platformProjectLink\": \"\", \"platformProjectName\": env-am-test-deploy-sit\", \"qualityGates\": [ { \"id\": 1, \"qualityGateType\": \"manual\", \"stepName\": \"manual\", \"cdStageId\": 1, \"autotest\": null, \"codebaseBranch\": null } ], \"source\": { \"type\": \"library\", \"library\": { \"name\": \"lib01\", \"branch\": \"master\" } } } ], \"services\": [], \"applicationsToPromote\": [ \"java-springboot-helloworld\" ] } Get CD Stage Entity by Pipeline and Stage Names \u2693\ufe0e Request \u2693\ufe0e GET /api/v1/edp/cd-pipeline/{cdPipelineName}/stage/{stageName} example: `localhost/api/v1/edp/cd-pipeline/pipe1/stage/sit Response \u2693\ufe0e { \"name\": \"sit\", \"cdPipeline\": \"pipe1\", \"description\": \"sit\", \"triggerType\": \"manual\", \"order\": \"0\", \"applications\": [ { \"name\": \"java-springboot-helloworld\", \"branchName\": \"master\", \"inputIs\": \"java-springboot-helloworld-master\", \"outputIs\": \"am-test-deploy-sit-java-springboot-helloworld-verified\" } ], \"qualityGates\": [ { \"id\": 1, \"qualityGateType\": \"manual\", \"stepName\": \"manual\", \"cdStageId\": 1, \"autotest\": null, \"codebaseBranch\": null } ] } Update CD Pipeline Entity \u2693\ufe0e Request \u2693\ufe0e PUT /api/v1/edp/cd-pipeline/{cdPipelineName}/update example: localhost/api/v1/edp/cd-pipeline/pipe1/update Change Set of Applications \u2693\ufe0e { \"applications\":[ { \"appName\":\"app01\", \"inputDockerStream\":\"app01-master\" }, { \"appName\":\"app02\", \"inputDockerStream\":\"app02\" } ] } Response \u2693\ufe0e 204 No Cont ent Change Set of Stages \u2693\ufe0e { \"stages\": [ { \"name\": \"sit\", \"description\": \"sit\", \"triggerType\": \"manual\", \"order\": 0, \"platformProjectLink\": \"\", \"platformProjectName\": env-deploy-sit\", \"qualityGates\": [ { \"id\": 1, \"qualityGateType\": \"manual\", \"stepName\": \"manual\", \"cdStageId\": 1, \"autotest\": null, \"codebaseBranch\": null } ], \"source\": { \"type\": \"library\", \"library\": { \"name\": \"lib01\", \"branch\": \"master\" } } } ] } Response \u2693\ufe0e 204 No Cont ent","title":"EDP API"},{"location":"developer-guide/rest-api/#edp-api","text":"","title":"EDP API"},{"location":"developer-guide/rest-api/#create-codebase-entity","text":"EDP allows you to create three codebase types: Application, Autotest and Library. There are also several strategy types for each codebase: Create, Clone and Import. Depending on the selected codebase type and the respective strategy, you should specify a different set of fields in a request. Note The Route, Database and VCS are optional fields. In accordance with the necessary deploy set, you have to add the necessary fields into request","title":"Create Codebase Entity"},{"location":"developer-guide/rest-api/#request","text":"POST /api/v1/edp/codebase","title":"Request"},{"location":"developer-guide/rest-api/#application-create","text":"{ \"name\": \"app01\", \"type\": \"application\", \"strategy\": \"create\", \"lang\": \"java\", \"framework\": \"springboot\", \"buildTool\": \"maven\", \"multiModule\": false, \"route\": { \"site\": \"api\", \"path\": \"/\" }, \"database\": { \"kind\": \"postgresql\", \"version\": \"postgres:9.6\", \"capacity\": \"1Gi\", \"storage\": \"efs\" }, \"description\": \"Description\", \"gitServer\": \"gerrit\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", \"deploymentScript\": \"openshift-template\" }","title":"Application (Create)"},{"location":"developer-guide/rest-api/#application-clone","text":"{ \" name \" : \" app01 \" , \" type \" : \" application \" , \" strategy \" : \" clone \" , \" lang \" : \" java \" , \" framework \" : \" springboot \" , \" buildTool \" : \" maven \" , \" multiModule \" : false , \" repository \" : { \" url \" : \" http(s)://git.sample.com/sample.git \" , // login and password are required only if repo is private \" login \" : \" login \" , \" password \" : \" password \" }, \" description \" : \" Description \" , \" gitServer \" : \" gerrit \" , \" jenkinsSlave \" : \" maven \" , \" jobProvisioning \" : \" default \" , \" deploymentScript \" : \" openshift-template \" }","title":"Application (Clone)"},{"location":"developer-guide/rest-api/#application-import","text":"{ \"type\": \"application\", \"strategy\": \"import\", \"lang\": \"java\", \"framework\": \"springboot\", \"buildTool\": \"maven\", \"multiModule\": false, \"description\": \"Description\", \"gitServer\": \"git-epam\", \"gitUrlPath\": \"/relative/path/to/repo\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", \"deploymentScript\": \"openshift-template\" }","title":"Application (Import)"},{"location":"developer-guide/rest-api/#autotests-clone","text":"{ \" name \" : \" aut01 \" , \" type \" : \" autotests \" , \" strategy \" : \" clone \" , \" lang \" : \" java \" , \" framework \" : \" springboot \" , \" buildTool \" : \" maven \" , \" testReportFramework \" : \" allure \" , \" repository \" : { \" url \" : \" http(s)://git.sample.com/sample.git \" , // login and password are required only if repo is private \" login \" : \" login \" , \" password \" : \" password \" }, \" description \" : \" Description \" , \" jenkinsSlave \" : \" maven \" , \" jobProvisioning \" : \" default \" }","title":"Autotests (Clone)"},{"location":"developer-guide/rest-api/#autotests-import","text":"{ \"type\": \"autotests\", \"strategy\": \"import\", \"lang\": \"java\", \"framework\": \"springboot\", \"buildTool\": \"maven\", \"testReportFramework\": \"allure\", \"description\": \"Description\", \"gitServer\": \"git-epam\", \"gitRelativePath\": \"/relative/path/to/repo\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\" }","title":"Autotests (Import)"},{"location":"developer-guide/rest-api/#library-create","text":"{ \"name\": \"lib01\", \"type\": \"library\", \"strategy\": \"create\", \"lang\": \"java\", \"buildTool\": \"maven\", \"multiModule\": false, \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", }","title":"Library (Create)"},{"location":"developer-guide/rest-api/#library-clone","text":"{ \" name \" : \" lib01 \" , \" type \" : \" library \" , \" strategy \" : \" clone \" , \" lang \" : \" java \" , \" buildTool \" : \" maven \" , \" multiModule \" : false , \" repository \" : { \" url \" : \" http(s)://git.sample.com/sample.git \" , // login and password are required only if repo is private \" login \" : \" login \" , \" password \" : \" password \" }, \" vcs \" : null , \" jenkinsSlave \" : \" maven \" , \" jobProvisioning \" : \" default \" , }","title":"Library (Clone)"},{"location":"developer-guide/rest-api/#library-import","text":"{ \"type\": \"library\", \"strategy\": \"import\", \"lang\": \"java\", \"buildTool\": \"maven\", \"multiModule\": false, \"gitServer\": \"git-epam\", \"gitUrlPath\": \"/relative/path/to/repo\", \"jenkinsSlave\": \"maven\", \"jobProvisioning\": \"default\", }","title":"Library (Import)"},{"location":"developer-guide/rest-api/#response","text":"Status 200 OK","title":"Response"},{"location":"developer-guide/rest-api/#get-codebase-by-name","text":"","title":"Get Codebase by Name"},{"location":"developer-guide/rest-api/#request_1","text":"GET /api/v1/edp/codebase/{codebaseName} example: localhost/api/v1/edp/codebase/app01","title":"Request"},{"location":"developer-guide/rest-api/#response_1","text":"Status 200 OK { \"id\" : 1 , \"name\" : \"app01\" , \"language\" : \"java\" , \"build_tool\" : \"maven\" , \"framework\" : \"springboot\" , \"strategy\" : \"create\" , \"git_url\" : \"\" , \"route_site\" : \"api\" , \"route_path\" : \"/\" , \"type\" : \"application\" , \"status\" : \"active\" , \"testReportFramework\" : \"\" , \"description\" : \"Description\" , \"codebase_branch\" : [ { \"id\" : 1 , \"branchName\" : \"master\" , \"from_commit\" : \"\" , \"status\" : \"active\" , \"branchLink\" : \"\" , \"jenkinsLink\" : \"\" , \"appName\" : \"\" , \"codebaseDockerStream\" : null } ], \"gitServer\" : \"gerrit\" , \"gitProjectPath\" : null , \"jenkinsSlave\" : \"maven\" , \"jobProvisioning\" : \"default\" , \"deploymentScript\" : \"openshift-template\" }","title":"Response"},{"location":"developer-guide/rest-api/#get-all-codebases","text":"","title":"Get All Codebases"},{"location":"developer-guide/rest-api/#request_2","text":"GET /api/v1/edp/codebase?type={codebaseType} example: localhost/api/v1/edp/codebase?type=application","title":"Request"},{"location":"developer-guide/rest-api/#response_2","text":"Status 200 OK [ { \"id\" : 1 , \"name\" : \"app01\" , \"language\" : \"java\" , \"build_tool\" : \"maven\" , \"framework\" : \"springboot\" , \"strategy\" : \"create\" , \"git_url\" : \"\" , \"route_site\" : \"api\" , \"route_path\" : \"/\" , \"type\" : \"application\" , \"status\" : \"active\" , \"testReportFramework\" : \"\" , \"description\" : \"Description\" , \"codebase_branch\" : [ { \"id\" : 1 , \"branchName\" : \"master\" , \"from_commit\" : \"\" , \"status\" : \"active\" , \"branchLink\" : \"\" , \"jenkinsLink\" : \"\" , \"appName\" : \"\" , \"codebaseDockerStream\" : [ { \"id\" : 1 , \"ocImageStreamName\" : \"app01-master\" , \"imageLink\" : \"\" , \"jenkinsLink\" : \"\" } ] } ], \"gitServer\" : \"gerrit\" , \"gitProjectPath\" : null , \"jenkinsSlave\" : \"maven\" , \"jobProvisioning\" : \"\" , \"deploymentScript\" : \"openshift-template\" }, { \"id\" : 2 , \"name\" : \"app02\" , \"language\" : \"java\" , \"build_tool\" : \"maven\" , \"framework\" : \"springboot\" , \"strategy\" : \"create\" , \"git_url\" : \"\" , \"route_site\" : \"app\" , \"route_path\" : \"/\" , \"type\" : \"application\" , \"status\" : \"failed\" , \"testReportFramework\" : \"\" , \"description\" : \"\" , \"codebase_branch\" : [ { \"id\" : 2 , \"branchName\" : \"master\" , \"from_commit\" : \"\" , \"status\" : \"inactive\" , \"branchLink\" : \"\" , \"jenkinsLink\" : \"\" , \"appName\" : \"\" , \"codebaseDockerStream\" : [ { \"id\" : 2 , \"ocImageStreamName\" : \"app02-master\" , \"imageLink\" : \"\" , \"jenkinsLink\" : \"\" } ] } ], \"gitServer\" : \"gerrit\" , \"gitProjectPath\" : null , \"jenkinsSlave\" : \"maven\" , \"jobProvisioning\" : \"\" , \"deploymentScript\" : \"openshift-template\" } ]","title":"Response"},{"location":"developer-guide/rest-api/#create-cd-pipeline-entity","text":"","title":"Create CD Pipeline Entity"},{"location":"developer-guide/rest-api/#request_3","text":"POST /api/v1/edp/cd-pipeline { \"name\":\"pipe1\", \"applications\":[ { \"appName\":\"app01\", \"inputDockerStream\":\"app01-master\" } ], \"stages\":[ { \"name\":\"sit\", \"description\":\"description-sit\", \"qualityGateType\":\"manual\", \"stepName\":\"approve\", \"triggerType\":\"manual\", \"order\":0, \"qualityGates\": [ { \"qualityGateType\":\"manual\", \"stepName\":\"step-one-one\", \"autotestName\": null, \"branchName\": null }, { \"qualityGateType\":\"manual\", \"stepName\":\"step-two-two\", \"autotestName\": null, \"branchName\": null } ] } ] }","title":"Request"},{"location":"developer-guide/rest-api/#response_3","text":"Status 200 OK","title":"Response"},{"location":"developer-guide/rest-api/#get-cd-pipeline-entity-by-name","text":"","title":"Get CD Pipeline Entity by Name"},{"location":"developer-guide/rest-api/#request_4","text":"GET /api/v1/edp/cd-pipeline/{cdPipelineName} example: localhost/api/v1/edp/cd-pipeline/pipe1","title":"Request"},{"location":"developer-guide/rest-api/#response_4","text":"Status 200 OK { \"id\": 1, \"name\": \"pipe1\", \"status\": \"active\", \"jenkinsLink\": \"\", \"codebaseBranches\": [ { \"id\": 1, \"branchName\": \"master\", \"from_commit\": \"\", \"status\": \"active\", \"branchLink\": \"\", \"jenkinsLink\": \"\", \"appName\": \"java-springboot-helloworld\", \"codebaseDockerStream\": [ { \"id\": 1, \"ocImageStreamName\": \"java-springboot-helloworld-master\", \"imageLink\": \"\", \"jenkinsLink\": \"\" } ] } ], \"stages\": [ { \"id\": 1, \"name\": \"sit\", \"description\": \"sit\", \"triggerType\": \"manual\", \"order\": 0, \"platformProjectLink\": \"\", \"platformProjectName\": env-am-test-deploy-sit\", \"qualityGates\": [ { \"id\": 1, \"qualityGateType\": \"manual\", \"stepName\": \"manual\", \"cdStageId\": 1, \"autotest\": null, \"codebaseBranch\": null } ], \"source\": { \"type\": \"library\", \"library\": { \"name\": \"lib01\", \"branch\": \"master\" } } } ], \"services\": [], \"applicationsToPromote\": [ \"java-springboot-helloworld\" ] }","title":"Response"},{"location":"developer-guide/rest-api/#get-cd-stage-entity-by-pipeline-and-stage-names","text":"","title":"Get CD Stage Entity by Pipeline and Stage Names"},{"location":"developer-guide/rest-api/#request_5","text":"GET /api/v1/edp/cd-pipeline/{cdPipelineName}/stage/{stageName} example: `localhost/api/v1/edp/cd-pipeline/pipe1/stage/sit","title":"Request"},{"location":"developer-guide/rest-api/#response_5","text":"{ \"name\": \"sit\", \"cdPipeline\": \"pipe1\", \"description\": \"sit\", \"triggerType\": \"manual\", \"order\": \"0\", \"applications\": [ { \"name\": \"java-springboot-helloworld\", \"branchName\": \"master\", \"inputIs\": \"java-springboot-helloworld-master\", \"outputIs\": \"am-test-deploy-sit-java-springboot-helloworld-verified\" } ], \"qualityGates\": [ { \"id\": 1, \"qualityGateType\": \"manual\", \"stepName\": \"manual\", \"cdStageId\": 1, \"autotest\": null, \"codebaseBranch\": null } ] }","title":"Response"},{"location":"developer-guide/rest-api/#update-cd-pipeline-entity","text":"","title":"Update CD Pipeline Entity"},{"location":"developer-guide/rest-api/#request_6","text":"PUT /api/v1/edp/cd-pipeline/{cdPipelineName}/update example: localhost/api/v1/edp/cd-pipeline/pipe1/update","title":"Request"},{"location":"developer-guide/rest-api/#change-set-of-applications","text":"{ \"applications\":[ { \"appName\":\"app01\", \"inputDockerStream\":\"app01-master\" }, { \"appName\":\"app02\", \"inputDockerStream\":\"app02\" } ] }","title":"Change Set of Applications"},{"location":"developer-guide/rest-api/#response_6","text":"204 No Cont ent","title":"Response"},{"location":"developer-guide/rest-api/#change-set-of-stages","text":"{ \"stages\": [ { \"name\": \"sit\", \"description\": \"sit\", \"triggerType\": \"manual\", \"order\": 0, \"platformProjectLink\": \"\", \"platformProjectName\": env-deploy-sit\", \"qualityGates\": [ { \"id\": 1, \"qualityGateType\": \"manual\", \"stepName\": \"manual\", \"cdStageId\": 1, \"autotest\": null, \"codebaseBranch\": null } ], \"source\": { \"type\": \"library\", \"library\": { \"name\": \"lib01\", \"branch\": \"master\" } } } ] }","title":"Change Set of Stages"},{"location":"developer-guide/rest-api/#response_7","text":"204 No Cont ent","title":"Response"},{"location":"operator-guide/","text":"Overview \u2693\ufe0e This guide is for DevOps who installs/configure/customize EDP and also support platform","title":"Overview"},{"location":"operator-guide/#overview","text":"This guide is for DevOps who installs/configure/customize EDP and also support platform","title":"Overview"},{"location":"operator-guide/add-other-code-language/","text":"Add Other Code Language \u2693\ufe0e There is an ability to extend the default code languages when creating a codebase with the clone strategy. Warning The create strategy does not allow to customize the default code language set. In order to customize the Build Tool list, perform the following: Navigate to OpenShift, and edit the edp-admin-console deployment by adding the necessary code language into the BUILD TOOLS field. Note Use the comma sign to separate the code languages in order to make them available, e.g. maven, gradle. Add the Jenkins slave by following the Add Jenkins Slave instruction. As a result, the newly added Jenkins slave will be available in the Select Jenkins Slave dropdown list of the Advanced Settings block during the codebase creation: Extend or modify the Jenkins provisioner by following the Add Job Provisioner instruction. If it is necessary to create Code Review and Build pipelines, add corresponding entries (e.g. stages[Build-application-docker], [Code-review-application-docker]). See the example below: ... stages [ ' Code - review - application - docker ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"sonar\" } ] ' stages [ ' Build - application - docker ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"sonar\" }, ' + ' { \"name\" : \"build-image-kaniko\" } ' + \"${createJFVStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' ... Note Application is one of the available options. Another option might be to add a library. Please refer to the Add Library page for details Related Articles \u2693\ufe0e Add Jenkins Slave Add Job Provisioner Add Library","title":"Add Other Code Language"},{"location":"operator-guide/add-other-code-language/#add-other-code-language","text":"There is an ability to extend the default code languages when creating a codebase with the clone strategy. Warning The create strategy does not allow to customize the default code language set. In order to customize the Build Tool list, perform the following: Navigate to OpenShift, and edit the edp-admin-console deployment by adding the necessary code language into the BUILD TOOLS field. Note Use the comma sign to separate the code languages in order to make them available, e.g. maven, gradle. Add the Jenkins slave by following the Add Jenkins Slave instruction. As a result, the newly added Jenkins slave will be available in the Select Jenkins Slave dropdown list of the Advanced Settings block during the codebase creation: Extend or modify the Jenkins provisioner by following the Add Job Provisioner instruction. If it is necessary to create Code Review and Build pipelines, add corresponding entries (e.g. stages[Build-application-docker], [Code-review-application-docker]). See the example below: ... stages [ ' Code - review - application - docker ' ] = ' [ { \"name\" : \"gerrit-checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"sonar\" } ] ' stages [ ' Build - application - docker ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"sonar\" }, ' + ' { \"name\" : \"build-image-kaniko\" } ' + \"${createJFVStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' ... Note Application is one of the available options. Another option might be to add a library. Please refer to the Add Library page for details","title":"Add Other Code Language"},{"location":"operator-guide/add-other-code-language/#related-articles","text":"Add Jenkins Slave Add Job Provisioner Add Library","title":"Related Articles"},{"location":"operator-guide/aws-irsa/","text":"Associate IAM Roles With Service Accounts \u2693\ufe0e This page contains accurate information on how to associate an IAM role with the service account in EPAM Delivery Platform. Get acquainted with the AWS Official Documentation on the subject before proceeding. To successfully associate the IAM role with the service account, follow the steps below: Create an IAM role that will further be associated with the service account. This role must have the following trust policy: IAM Role { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::<AWS_ACCOUNT_ID>:oidc-provider/<OIDC_PROVIDER>\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"<OIDC_PROVIDER>:sub\": \"system:serviceaccount:<SERVICE_ACCOUNT_NAMESPACE>:<SERVICE_ACCOUNT_NAME>\" } } } ] } Deploy the amazon-eks-pod-identity-webhook as described below: 2.1. Provide the stable version of the Docker image in the deploy/deployment-base.yaml file. 2.2. Provide ${CA_BUNDLE}_in the_deploy/mutatingwebhook.yaml file: secret_name=$(kubectl -n default get sa default -o jsonpath='{.secrets[0].name}') \\ CA_BUNDLE=$(kubectl -n default get secret/$secret_name -o jsonpath='{.data.ca\\.crt}' | tr -d '\\n') 2.3. Deploy the Webhook: kubectl apply -f deploy/ 2.4. Approve the csr : csr_name=$(kubectl get csr -o jsonpath='{.items[?(@.spec.username==\"system:serviceaccount:default:pod-identity-webhook\")].metadata.name}') kubectl certificate approve $csr_name Annotate the created service account with the IAM role: Service Account apiVersion: v1 kind: ServiceAccount metadata: name: <SERVICE_ACCOUNT_NAME> namespace: <NAMESPACE> annotations: eks.amazonaws.com/role-arn: \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/<IAM_ROLE_NAME>\" All newly launched pods with this service account will be modified and then use the associated IAM role. Find below the pod specification template: Pod Template apiVersion: v1 kind: Pod metadata: name: <POD_NAME> namespace: <POD_NAMESPACE> spec: serviceAccountName: <SERVICE_ACCOUNT_NAME> securityContext: fsGroup: 65534 containers: - name: terraform image: epamedp/edp-jenkins-terraform-agent:2.0.2 command: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600'] Related Articles \u2693\ufe0e Use Terraform Library in EDP","title":"Enable IRSA (AWS)"},{"location":"operator-guide/aws-irsa/#associate-iam-roles-with-service-accounts","text":"This page contains accurate information on how to associate an IAM role with the service account in EPAM Delivery Platform. Get acquainted with the AWS Official Documentation on the subject before proceeding. To successfully associate the IAM role with the service account, follow the steps below: Create an IAM role that will further be associated with the service account. This role must have the following trust policy: IAM Role { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::<AWS_ACCOUNT_ID>:oidc-provider/<OIDC_PROVIDER>\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"<OIDC_PROVIDER>:sub\": \"system:serviceaccount:<SERVICE_ACCOUNT_NAMESPACE>:<SERVICE_ACCOUNT_NAME>\" } } } ] } Deploy the amazon-eks-pod-identity-webhook as described below: 2.1. Provide the stable version of the Docker image in the deploy/deployment-base.yaml file. 2.2. Provide ${CA_BUNDLE}_in the_deploy/mutatingwebhook.yaml file: secret_name=$(kubectl -n default get sa default -o jsonpath='{.secrets[0].name}') \\ CA_BUNDLE=$(kubectl -n default get secret/$secret_name -o jsonpath='{.data.ca\\.crt}' | tr -d '\\n') 2.3. Deploy the Webhook: kubectl apply -f deploy/ 2.4. Approve the csr : csr_name=$(kubectl get csr -o jsonpath='{.items[?(@.spec.username==\"system:serviceaccount:default:pod-identity-webhook\")].metadata.name}') kubectl certificate approve $csr_name Annotate the created service account with the IAM role: Service Account apiVersion: v1 kind: ServiceAccount metadata: name: <SERVICE_ACCOUNT_NAME> namespace: <NAMESPACE> annotations: eks.amazonaws.com/role-arn: \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/<IAM_ROLE_NAME>\" All newly launched pods with this service account will be modified and then use the associated IAM role. Find below the pod specification template: Pod Template apiVersion: v1 kind: Pod metadata: name: <POD_NAME> namespace: <POD_NAMESPACE> spec: serviceAccountName: <SERVICE_ACCOUNT_NAME> securityContext: fsGroup: 65534 containers: - name: terraform image: epamedp/edp-jenkins-terraform-agent:2.0.2 command: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600']","title":"Associate IAM Roles With Service Accounts"},{"location":"operator-guide/aws-irsa/#related-articles","text":"Use Terraform Library in EDP","title":"Related Articles"},{"location":"operator-guide/github-integration/","text":"GitHub Integration \u2693\ufe0e Discover the steps below to apply the GitHub integration correctly: Create access token for GitHub: Click the profile account and navigate to Settings; Go to Developer Settings; Select Personal access token and generate a new one with the following parameters Warning Make sure to copy your new personal access token right at this moment because there will not be any ability to see it again. Navigate to Jenkins -> Manage Jenkins -> Manage plugins , and click the Available tab and install the following plugins: GitHub and GitHub Pull Request Builder . Note If the necessary plugins are not available in the list, check out the Installed tab and verify whether they are presented. Navigate to Jenkins -> Credentials -> System -> Global credentials -> Add credentials , and create new credentials with the Secret text kind. In the Secret field, provide your GitHub API token, fill in the ID field with the github-access-token value: Generate and add a new SSH key to the GitHub account. To get more detailed information, please inspect the official GitHub documentation page. Note Use the same SSH key that was added to the GitServer definition. Add a private part of the SSH key to Jenkins by navigating to Jenkins -> Credentials -> System -> Global credentials -> Add credentials ; and create new credentials with the SSH username with private key kind: Navigate to Jenkins -> Manage Jenkins -> Configure system -> GitHub part, and configure the GitHub server: Configure the GitHub Pull Request Builder plugin: Note The Secret field is optional, for details, please refer to the official GitHub pull request builder plugin documentation . Create a new Job Provision by navigating to the Jenkins main page and opening the job-provisions folder: Click New Item; Type the name; Select the Freestyle project option and click OK; Select the This project is parameterized check box and add a few input parameters: NAME; TYPE; BUILD_TOOL; BRANCH; GIT_SERVER_CR_NAME; GIT_SERVER_CR_VERSION; GIT_CREDENTIALS_ID; REPOSITORY_PATH; JIRA_INTEGRATION_ENABLED; Check the Execute concurrent builds if necessary option; Check the Restrict where this project can be run option; Fill in the Label Expression field by typing the master branch name. In the Build section, perform the following: Select DSL Script ; Select the Use the provided DSL script check box: 9.Insert the following code: import groovy.json.* import jenkins.model.Jenkins import javaposse.jobdsl.plugin.* import com.cloudbees.hudson.plugins.folder.* Jenkins jenkins = Jenkins . instance def stages = [ : ] def jiraIntegrationEnabled = Boolean . parseBoolean ( \"${JIRA_INTEGRATION_ENABLED}\" as String ) def commitValidateStage = jiraIntegrationEnabled ? ' ,{ \"name\" : \"commit-validate\" } ' : '' def createJIMStage = jiraIntegrationEnabled ? ' ,{ \"name\" : \"create-jira-issue-metadata\" } ' : '' def platformType = \"${PLATFORM_TYPE}\" def buildStage = platformType == \"kubernetes\" ? ' ,{ \"name\" : \"build-image-kaniko\" }, ' : ' ,{ \"name\" : \"build-image-from-dockerfile\" }, ' def buildTool = \"${BUILD_TOOL}\" def goBuildStage = buildTool . toString () == \"go\" ? ' ,{ \"name\" : \"build\" } ' : ' ,{ \"name\" : \"compile\" } ' stages [ ' Code - review - application ' ] = ' [ { \"name\" : \"checkout\" } ' + \"${commitValidateStage}\" + goBuildStage + ' ,{ \"name\" : \"tests\" },{ \"name\" : \"sonar\" } ] ' stages [ ' Code - review - library ' ] = ' [ { \"name\" : \"checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"compile\" },{ \"name\" : \"tests\" }, ' + ' { \"name\" : \"sonar\" } ] ' stages [ ' Code - review - autotests ' ] = ' [ { \"name\" : \"checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"tests\" },{ \"name\" : \"sonar\" } ] ' stages [ ' Build - library - maven ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" },{ \"name\" : \"build\" },{ \"name\" : \"push\" },{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - library - npm ' ] = stages [ ' Build - library - maven ' ] stages [ ' Build - library - gradle ' ] = stages [ ' Build - library - maven ' ] stages [ ' Build - library - dotnet ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" },{ \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - maven ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" },{ \"name\" : \"build\" } ' + \"${buildStage}\" + ' { \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - npm ' ] = stages [ ' Build - application - maven ' ] stages [ ' Build - application - gradle ' ] = stages [ ' Build - application - maven ' ] stages [ ' Build - application - dotnet ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" } ' + \"${buildStage}\" + ' { \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - go ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"tests\" },{ \"name\" : \"sonar\" }, ' + ' { \"name\" : \"build\" } ' + \"${buildStage}\" + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - python ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" },{ \"name\" : \"tests\" },{ \"name\" : \"sonar\" } ' + \"${buildStage}\" + ' { \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Create - release ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"create-branch\" },{ \"name\" : \"trigger-job\" } ] ' def buildToolsOutOfTheBox = [ \"maven\" , \"npm\" , \"gradle\" , \"dotnet\" , \"none\" , \"go\" , \"python\" ] def defaultStages = ' [ { \"name\" : \"checkout\" } ' + \"${createJIMStage}\" + ']' def codebaseName = \"${NAME}\" def gitServerCrName = \"${GIT_SERVER_CR_NAME}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID ? GIT_CREDENTIALS_ID : 'gerrit-ciuser-sshkey'}\" def repositoryPath = \"${REPOSITORY_PATH.replaceAll(~/:\\d+\\\\//,\" / \")}\" def githubRepository = \"https://${repositoryPath.split(\" @ \")[1]}\" def codebaseFolder = jenkins . getItem ( codebaseName ) if ( codebaseFolder == null ) { folder ( codebaseName ) } createListView ( codebaseName , \"Releases\" ) createReleasePipeline ( \"Create-release-${codebaseName}\" , codebaseName , stages [ \"Create-release\" ] , \"create-release.groovy\" , repositoryPath , gitCredentialsId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType ) if ( buildTool . toString (). equalsIgnoreCase ( ' none ' )) { return true } if ( BRANCH ) { def branch = \"${BRANCH}\" def formattedBranch = \"${branch.toUpperCase().replaceAll(/\\\\//, \" - \")}\" createListView ( codebaseName , formattedBranch ) def type = \"${TYPE}\" def supBuildTool = buildToolsOutOfTheBox . contains ( buildTool . toString ()) def crKey = \"Code-review-${type}\" . toString () createCodeReviewPipeline ( \"Code-review-${codebaseName}\" , codebaseName , stages . get ( crKey , defaultStages ), \"code-review.groovy\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion , githubRepository ) registerWebHook ( repositoryPath ) def buildKey = \"Build-${type}-${buildTool.toLowerCase()}\" . toString () if ( type . equalsIgnoreCase ( ' application ' ) || type . equalsIgnoreCase ( ' library ' )) { def jobExists = false if ( \"${formattedBranch}-Build-${codebaseName}\" . toString () in Jenkins . instance . getAllItems (). collect { it . name }) jobExists = true createBuildPipeline ( \"Build-${codebaseName}\" , codebaseName , stages . get ( buildKey , defaultStages ), \"build.groovy\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion , githubRepository ) registerWebHook ( repositoryPath , ' build ' ) if ( ! jobExists ) queue ( \"${codebaseName}/${formattedBranch}-Build-${codebaseName}\" ) } } def createCodeReviewPipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , watchBranch = \"master\" , gitServerCrName , gitServerCrVersion , githubRepository ) { pipelineJob ( \"${codebaseName}/${watchBranch.toUpperCase().replaceAll(/\\\\//, \" - \")}-${pipelineName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) refspec ( \"+refs/pull/*:refs/remotes/origin/pr/*\" ) } branches ( \"\\${ghprbActualCommit}\" ) scriptPath ( \"${pipelineScript}\" ) } } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) stringParam ( \"BRANCH\" , \"${watchBranch}\" , \"Branch to build artifact from\" ) } } } triggers { githubPullRequest { cron ( '' ) onlyTriggerPhrase ( false ) useGitHubHooks ( true ) permitAll ( true ) autoCloseFailedPullRequests ( false ) displayBuildErrorsOnDownstreamBuilds ( false ) whiteListTargetBranches ( [ watchBranch . toString () ] ) extensions { commitStatus { context ( ' Jenkins Code - Review ' ) triggeredStatus ( ' Build is Triggered ' ) startedStatus ( ' Build is Started ' ) addTestResults ( true ) completedStatus ( ' SUCCESS ' , ' Verified ' ) completedStatus ( ' FAILURE ' , ' Failed ' ) completedStatus ( ' PENDING ' , ' Penging ' ) completedStatus ( ' ERROR ' , ' Error ' ) } } } } properties { githubProjectProperty { projectUrlStr ( \"${githubRepository}\" ) } } } } def createBuildPipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , watchBranch = \"master\" , gitServerCrName , gitServerCrVersion , githubRepository ) { pipelineJob ( \"${codebaseName}/${watchBranch.toUpperCase().replaceAll(/\\\\//, \" - \")}-${pipelineName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) } branches ( \"${watchBranch}\" ) scriptPath ( \"${pipelineScript}\" ) } } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) stringParam ( \"BRANCH\" , \"${watchBranch}\" , \"Branch to run from\" ) } } } triggers { gitHubPushTrigger () } properties { githubProjectProperty { projectUrlStr ( \"${githubRepository}\" ) } } } } def createListView ( codebaseName , branchName ) { listView ( \"${codebaseName}/${branchName}\" ) { if ( branchName . toLowerCase () == \"releases\" ) { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^Create-release.*\" ) } } } else { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^${branchName}-(Code-review|Build).*\" ) } } } columns { status () weather () name () lastSuccess () lastFailure () lastDuration () buildButton () } } } def createReleasePipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType ) { pipelineJob ( \"${codebaseName}/${pipelineName}\" ) { logRotator { numToKeep ( 14 ) daysToKeep ( 30 ) } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) } branches ( \"master\" ) scriptPath ( \"${pipelineScript}\" ) } } parameters { stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"\" ) if ( pipelineName . contains ( \"Create-release\" )) { stringParam ( \"JIRA_INTEGRATION_ENABLED\" , \"${jiraIntegrationEnabled}\" , \"Is Jira integration enabled\" ) stringParam ( \"PLATFORM_TYPE\" , \"${platformType}\" , \"Platform type\" ) stringParam ( \"GERRIT_PROJECT\" , \"${codebaseName}\" , \"\" ) stringParam ( \"RELEASE_NAME\" , \"\" , \"Name of the release(branch to be created)\" ) stringParam ( \"COMMIT_ID\" , \"\" , \"Commit ID that will be used to create branch from for new release. If empty, HEAD of master will be used\" ) stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"REPOSITORY_PATH\" , \"${repository}\" , \"Full repository path\" ) } } } } } } def registerWebHook ( repositoryPath , type = ' code - review ' ) { def url = repositoryPath . split ( '@' ) [ 1 ] . split ( '/' ) [ 0 ] def owner = repositoryPath . split ( '@' ) [ 1 ] . split ( '/' ) [ 1 ] def repo = repositoryPath . split ( '@' ) [ 1 ] . split ( '/' ) [ 2 ] def apiUrl = ' https : //api.' + url + '/repos/' + owner + '/' + repo + '/hooks' def webhookUrl = '' def webhookConfig = [ : ] def config = [ : ] def events = [] if ( type . equalsIgnoreCase ( ' build ' )) { webhookUrl = System . getenv ( ' JENKINS_UI_URL ' ) + \"/github-webhook/\" events = [ \"push\" ] config [ \"url\" ] = webhookUrl config [ \"content_type\" ] = \"json\" config [ \"insecure_ssl\" ] = 0 webhookConfig [ \"name\" ] = \"web\" webhookConfig [ \"config\" ] = config webhookConfig [ \"events\" ] = events webhookConfig [ \"active\" ] = true } else { webhookUrl = System . getenv ( ' JENKINS_UI_URL ' ) + \"/ghprbhook/\" events = [ \"issue_comment\" , \"pull_request\" ] config [ \"url\" ] = webhookUrl config [ \"content_type\" ] = \"form\" config [ \"insecure_ssl\" ] = 0 webhookConfig [ \"name\" ] = \"web\" webhookConfig [ \"config\" ] = config webhookConfig [ \"events\" ] = events webhookConfig [ \"active\" ] = true } def requestBody = JsonOutput . toJson ( webhookConfig ) def http = new URL ( apiUrl ). openConnection () as HttpURLConnection http . setRequestMethod ( ' POST ' ) http . setDoOutput ( true ) println ( apiUrl ) http . setRequestProperty ( \"Accept\" , ' application / json ' ) http . setRequestProperty ( \"Content-Type\" , ' application / json ' ) http . setRequestProperty ( \"Authorization\" , \"token ${getSecretValue('github-access-token')}\" ) http . outputStream . write ( requestBody . getBytes ( \"UTF-8\" )) http . connect () println ( http . responseCode ) if ( http . responseCode == 201 ) { response = new JsonSlurper (). parseText ( http . inputStream . getText ( ' UTF - 8 ' )) } else { response = new JsonSlurper (). parseText ( http . errorStream . getText ( ' UTF - 8 ' )) } println \"response: ${response}\" } def getSecretValue ( name ) { def creds = com . cloudbees . plugins . credentials . CredentialsProvider . lookupCredentials ( com . cloudbees . plugins . credentials . common . StandardCredentials . class , Jenkins . instance , null , null ) def secret = creds . find { it . properties [ ' id ' ] == name } return secret != null ? secret [ ' secret ' ] : null } As a result, the new custom job-provision will be available in the Advanced CI Settings menu during the application creation: Related Articles \u2693\ufe0e Adjust Import Strategy Adjust Integration With Jira Server","title":"Overview"},{"location":"operator-guide/github-integration/#github-integration","text":"Discover the steps below to apply the GitHub integration correctly: Create access token for GitHub: Click the profile account and navigate to Settings; Go to Developer Settings; Select Personal access token and generate a new one with the following parameters Warning Make sure to copy your new personal access token right at this moment because there will not be any ability to see it again. Navigate to Jenkins -> Manage Jenkins -> Manage plugins , and click the Available tab and install the following plugins: GitHub and GitHub Pull Request Builder . Note If the necessary plugins are not available in the list, check out the Installed tab and verify whether they are presented. Navigate to Jenkins -> Credentials -> System -> Global credentials -> Add credentials , and create new credentials with the Secret text kind. In the Secret field, provide your GitHub API token, fill in the ID field with the github-access-token value: Generate and add a new SSH key to the GitHub account. To get more detailed information, please inspect the official GitHub documentation page. Note Use the same SSH key that was added to the GitServer definition. Add a private part of the SSH key to Jenkins by navigating to Jenkins -> Credentials -> System -> Global credentials -> Add credentials ; and create new credentials with the SSH username with private key kind: Navigate to Jenkins -> Manage Jenkins -> Configure system -> GitHub part, and configure the GitHub server: Configure the GitHub Pull Request Builder plugin: Note The Secret field is optional, for details, please refer to the official GitHub pull request builder plugin documentation . Create a new Job Provision by navigating to the Jenkins main page and opening the job-provisions folder: Click New Item; Type the name; Select the Freestyle project option and click OK; Select the This project is parameterized check box and add a few input parameters: NAME; TYPE; BUILD_TOOL; BRANCH; GIT_SERVER_CR_NAME; GIT_SERVER_CR_VERSION; GIT_CREDENTIALS_ID; REPOSITORY_PATH; JIRA_INTEGRATION_ENABLED; Check the Execute concurrent builds if necessary option; Check the Restrict where this project can be run option; Fill in the Label Expression field by typing the master branch name. In the Build section, perform the following: Select DSL Script ; Select the Use the provided DSL script check box: 9.Insert the following code: import groovy.json.* import jenkins.model.Jenkins import javaposse.jobdsl.plugin.* import com.cloudbees.hudson.plugins.folder.* Jenkins jenkins = Jenkins . instance def stages = [ : ] def jiraIntegrationEnabled = Boolean . parseBoolean ( \"${JIRA_INTEGRATION_ENABLED}\" as String ) def commitValidateStage = jiraIntegrationEnabled ? ' ,{ \"name\" : \"commit-validate\" } ' : '' def createJIMStage = jiraIntegrationEnabled ? ' ,{ \"name\" : \"create-jira-issue-metadata\" } ' : '' def platformType = \"${PLATFORM_TYPE}\" def buildStage = platformType == \"kubernetes\" ? ' ,{ \"name\" : \"build-image-kaniko\" }, ' : ' ,{ \"name\" : \"build-image-from-dockerfile\" }, ' def buildTool = \"${BUILD_TOOL}\" def goBuildStage = buildTool . toString () == \"go\" ? ' ,{ \"name\" : \"build\" } ' : ' ,{ \"name\" : \"compile\" } ' stages [ ' Code - review - application ' ] = ' [ { \"name\" : \"checkout\" } ' + \"${commitValidateStage}\" + goBuildStage + ' ,{ \"name\" : \"tests\" },{ \"name\" : \"sonar\" } ] ' stages [ ' Code - review - library ' ] = ' [ { \"name\" : \"checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"compile\" },{ \"name\" : \"tests\" }, ' + ' { \"name\" : \"sonar\" } ] ' stages [ ' Code - review - autotests ' ] = ' [ { \"name\" : \"checkout\" } ' + \"${commitValidateStage}\" + ' ,{ \"name\" : \"tests\" },{ \"name\" : \"sonar\" } ] ' stages [ ' Build - library - maven ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" },{ \"name\" : \"build\" },{ \"name\" : \"push\" },{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - library - npm ' ] = stages [ ' Build - library - maven ' ] stages [ ' Build - library - gradle ' ] = stages [ ' Build - library - maven ' ] stages [ ' Build - library - dotnet ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" },{ \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - maven ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" },{ \"name\" : \"build\" } ' + \"${buildStage}\" + ' { \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - npm ' ] = stages [ ' Build - application - maven ' ] stages [ ' Build - application - gradle ' ] = stages [ ' Build - application - maven ' ] stages [ ' Build - application - dotnet ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" }, ' + ' { \"name\" : \"tests\" },{ \"name\" : \"sonar\" } ' + \"${buildStage}\" + ' { \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - go ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"tests\" },{ \"name\" : \"sonar\" }, ' + ' { \"name\" : \"build\" } ' + \"${buildStage}\" + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Build - application - python ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"get-version\" },{ \"name\" : \"compile\" },{ \"name\" : \"tests\" },{ \"name\" : \"sonar\" } ' + \"${buildStage}\" + ' { \"name\" : \"push\" } ' + \"${createJIMStage}\" + ' ,{ \"name\" : \"git-tag\" } ] ' stages [ ' Create - release ' ] = ' [ { \"name\" : \"checkout\" },{ \"name\" : \"create-branch\" },{ \"name\" : \"trigger-job\" } ] ' def buildToolsOutOfTheBox = [ \"maven\" , \"npm\" , \"gradle\" , \"dotnet\" , \"none\" , \"go\" , \"python\" ] def defaultStages = ' [ { \"name\" : \"checkout\" } ' + \"${createJIMStage}\" + ']' def codebaseName = \"${NAME}\" def gitServerCrName = \"${GIT_SERVER_CR_NAME}\" def gitServerCrVersion = \"${GIT_SERVER_CR_VERSION}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID ? GIT_CREDENTIALS_ID : 'gerrit-ciuser-sshkey'}\" def repositoryPath = \"${REPOSITORY_PATH.replaceAll(~/:\\d+\\\\//,\" / \")}\" def githubRepository = \"https://${repositoryPath.split(\" @ \")[1]}\" def codebaseFolder = jenkins . getItem ( codebaseName ) if ( codebaseFolder == null ) { folder ( codebaseName ) } createListView ( codebaseName , \"Releases\" ) createReleasePipeline ( \"Create-release-${codebaseName}\" , codebaseName , stages [ \"Create-release\" ] , \"create-release.groovy\" , repositoryPath , gitCredentialsId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType ) if ( buildTool . toString (). equalsIgnoreCase ( ' none ' )) { return true } if ( BRANCH ) { def branch = \"${BRANCH}\" def formattedBranch = \"${branch.toUpperCase().replaceAll(/\\\\//, \" - \")}\" createListView ( codebaseName , formattedBranch ) def type = \"${TYPE}\" def supBuildTool = buildToolsOutOfTheBox . contains ( buildTool . toString ()) def crKey = \"Code-review-${type}\" . toString () createCodeReviewPipeline ( \"Code-review-${codebaseName}\" , codebaseName , stages . get ( crKey , defaultStages ), \"code-review.groovy\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion , githubRepository ) registerWebHook ( repositoryPath ) def buildKey = \"Build-${type}-${buildTool.toLowerCase()}\" . toString () if ( type . equalsIgnoreCase ( ' application ' ) || type . equalsIgnoreCase ( ' library ' )) { def jobExists = false if ( \"${formattedBranch}-Build-${codebaseName}\" . toString () in Jenkins . instance . getAllItems (). collect { it . name }) jobExists = true createBuildPipeline ( \"Build-${codebaseName}\" , codebaseName , stages . get ( buildKey , defaultStages ), \"build.groovy\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion , githubRepository ) registerWebHook ( repositoryPath , ' build ' ) if ( ! jobExists ) queue ( \"${codebaseName}/${formattedBranch}-Build-${codebaseName}\" ) } } def createCodeReviewPipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , watchBranch = \"master\" , gitServerCrName , gitServerCrVersion , githubRepository ) { pipelineJob ( \"${codebaseName}/${watchBranch.toUpperCase().replaceAll(/\\\\//, \" - \")}-${pipelineName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) refspec ( \"+refs/pull/*:refs/remotes/origin/pr/*\" ) } branches ( \"\\${ghprbActualCommit}\" ) scriptPath ( \"${pipelineScript}\" ) } } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) stringParam ( \"BRANCH\" , \"${watchBranch}\" , \"Branch to build artifact from\" ) } } } triggers { githubPullRequest { cron ( '' ) onlyTriggerPhrase ( false ) useGitHubHooks ( true ) permitAll ( true ) autoCloseFailedPullRequests ( false ) displayBuildErrorsOnDownstreamBuilds ( false ) whiteListTargetBranches ( [ watchBranch . toString () ] ) extensions { commitStatus { context ( ' Jenkins Code - Review ' ) triggeredStatus ( ' Build is Triggered ' ) startedStatus ( ' Build is Started ' ) addTestResults ( true ) completedStatus ( ' SUCCESS ' , ' Verified ' ) completedStatus ( ' FAILURE ' , ' Failed ' ) completedStatus ( ' PENDING ' , ' Penging ' ) completedStatus ( ' ERROR ' , ' Error ' ) } } } } properties { githubProjectProperty { projectUrlStr ( \"${githubRepository}\" ) } } } } def createBuildPipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , watchBranch = \"master\" , gitServerCrName , gitServerCrVersion , githubRepository ) { pipelineJob ( \"${codebaseName}/${watchBranch.toUpperCase().replaceAll(/\\\\//, \" - \")}-${pipelineName}\" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) } branches ( \"${watchBranch}\" ) scriptPath ( \"${pipelineScript}\" ) } } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"${codebaseName}\" , \"Gerrit project name(Codebase name) to be build\" ) stringParam ( \"BRANCH\" , \"${watchBranch}\" , \"Branch to run from\" ) } } } triggers { gitHubPushTrigger () } properties { githubProjectProperty { projectUrlStr ( \"${githubRepository}\" ) } } } } def createListView ( codebaseName , branchName ) { listView ( \"${codebaseName}/${branchName}\" ) { if ( branchName . toLowerCase () == \"releases\" ) { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^Create-release.*\" ) } } } else { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^${branchName}-(Code-review|Build).*\" ) } } } columns { status () weather () name () lastSuccess () lastFailure () lastDuration () buildButton () } } } def createReleasePipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType ) { pipelineJob ( \"${codebaseName}/${pipelineName}\" ) { logRotator { numToKeep ( 14 ) daysToKeep ( 30 ) } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) } branches ( \"master\" ) scriptPath ( \"${pipelineScript}\" ) } } parameters { stringParam ( \"STAGES\" , \"${codebaseStages}\" , \"\" ) if ( pipelineName . contains ( \"Create-release\" )) { stringParam ( \"JIRA_INTEGRATION_ENABLED\" , \"${jiraIntegrationEnabled}\" , \"Is Jira integration enabled\" ) stringParam ( \"PLATFORM_TYPE\" , \"${platformType}\" , \"Platform type\" ) stringParam ( \"GERRIT_PROJECT\" , \"${codebaseName}\" , \"\" ) stringParam ( \"RELEASE_NAME\" , \"\" , \"Name of the release(branch to be created)\" ) stringParam ( \"COMMIT_ID\" , \"\" , \"Commit ID that will be used to create branch from for new release. If empty, HEAD of master will be used\" ) stringParam ( \"GIT_SERVER_CR_NAME\" , \"${gitServerCrName}\" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"${gitServerCrVersion}\" , \"Version of GitServer CR Resource\" ) stringParam ( \"REPOSITORY_PATH\" , \"${repository}\" , \"Full repository path\" ) } } } } } } def registerWebHook ( repositoryPath , type = ' code - review ' ) { def url = repositoryPath . split ( '@' ) [ 1 ] . split ( '/' ) [ 0 ] def owner = repositoryPath . split ( '@' ) [ 1 ] . split ( '/' ) [ 1 ] def repo = repositoryPath . split ( '@' ) [ 1 ] . split ( '/' ) [ 2 ] def apiUrl = ' https : //api.' + url + '/repos/' + owner + '/' + repo + '/hooks' def webhookUrl = '' def webhookConfig = [ : ] def config = [ : ] def events = [] if ( type . equalsIgnoreCase ( ' build ' )) { webhookUrl = System . getenv ( ' JENKINS_UI_URL ' ) + \"/github-webhook/\" events = [ \"push\" ] config [ \"url\" ] = webhookUrl config [ \"content_type\" ] = \"json\" config [ \"insecure_ssl\" ] = 0 webhookConfig [ \"name\" ] = \"web\" webhookConfig [ \"config\" ] = config webhookConfig [ \"events\" ] = events webhookConfig [ \"active\" ] = true } else { webhookUrl = System . getenv ( ' JENKINS_UI_URL ' ) + \"/ghprbhook/\" events = [ \"issue_comment\" , \"pull_request\" ] config [ \"url\" ] = webhookUrl config [ \"content_type\" ] = \"form\" config [ \"insecure_ssl\" ] = 0 webhookConfig [ \"name\" ] = \"web\" webhookConfig [ \"config\" ] = config webhookConfig [ \"events\" ] = events webhookConfig [ \"active\" ] = true } def requestBody = JsonOutput . toJson ( webhookConfig ) def http = new URL ( apiUrl ). openConnection () as HttpURLConnection http . setRequestMethod ( ' POST ' ) http . setDoOutput ( true ) println ( apiUrl ) http . setRequestProperty ( \"Accept\" , ' application / json ' ) http . setRequestProperty ( \"Content-Type\" , ' application / json ' ) http . setRequestProperty ( \"Authorization\" , \"token ${getSecretValue('github-access-token')}\" ) http . outputStream . write ( requestBody . getBytes ( \"UTF-8\" )) http . connect () println ( http . responseCode ) if ( http . responseCode == 201 ) { response = new JsonSlurper (). parseText ( http . inputStream . getText ( ' UTF - 8 ' )) } else { response = new JsonSlurper (). parseText ( http . errorStream . getText ( ' UTF - 8 ' )) } println \"response: ${response}\" } def getSecretValue ( name ) { def creds = com . cloudbees . plugins . credentials . CredentialsProvider . lookupCredentials ( com . cloudbees . plugins . credentials . common . StandardCredentials . class , Jenkins . instance , null , null ) def secret = creds . find { it . properties [ ' id ' ] == name } return secret != null ? secret [ ' secret ' ] : null } As a result, the new custom job-provision will be available in the Advanced CI Settings menu during the application creation:","title":"GitHub Integration"},{"location":"operator-guide/github-integration/#related-articles","text":"Adjust Import Strategy Adjust Integration With Jira Server","title":"Related Articles"},{"location":"operator-guide/gitlab-integration/","text":"GitLab Integration \u2693\ufe0e Discover the steps below to apply the GitLab integration correctly: Create access token in Gitlab : Log in to GitLab ; In the top-right corner, click your avatar and select Settings ; On the User Settings menu, select Access Tokens ; Choose a name and an optional expiry date for the token; In the Scopes block, select the api scope for the token; Click the Create personal access token button. Note Make sure to save the access token as there won`t be the ability to access it once again. Install GitLab plugin by navigating to Manage Jenkins and switching to plugin manager, select the GitLab Plugin check box: Create Jenkins Credential ID by navigating to Jenkins -> Credentials -> System -> Global Credentials -> Add Credentials : Select GitLab API token; Select Global scope; API token - the Access Token that was created earlier; ID - the gitlab-access-token ID; Description - the description of the current Credential ID; Configure Gitlab plugin by navigating to Manage Jenkins -> Configure System and fill in the GitLab plugin settings: Connection name - connection name; Gitlab host URL - a host URL to GitLab; Credentials - credentials with Access Token to GitLab ( gitlab-access-token ); Create a new Job Provision. Navigate to the Jenkins main page and open the job-provisions/ci folder: Click New Item ; Type the name; Select Freestyle project and click OK; Select the This project is parameterized check box and add a few input parameters as the following strings: NAME; TYPE; BUILD_TOOL; BRANCH; GIT_SERVER_CR_NAME; GIT_SERVER_CR_VERSION; GIT_SERVER; GIT_SSH_PORT; GIT_USERNAME; GIT_CREDENTIALS_ID; REPOSITORY_PATH; JIRA_INTEGRATION_ENABLED; Check the Execute concurrent builds if necessary option; Check the Restrict where this project can be run option; Fill in the Label Expression field by typing the master branch name. In the Build section, perform the following: Select DSL Script ; Select the Use the provided DSL script check box: As soon as all the steps above are performed, insert the code: import groovy.json. * import jenkins.model.Jenkins Jenkins jenkins = Jenkins . instance def stages = [:] def jiraIntegrationEnabled = Boolean . parseBoolean ( \"$ {JIRA_INTEGRATION_ENABLED} \" as String ) def commitValidateStage = jiraIntegrationEnabled ? ',{\"name\": \"commit-validate\"}' : '' def createJIMStage = jiraIntegrationEnabled ? ',{\"name\": \"create-jira-issue-metadata\"}' : '' def platformType = \"$ {PLATFORM_TYPE} \" def buildStage = platformType == \"kubernetes\" ? ',{\"name\": \"build-image-kaniko\"},' : ',{\"name\": \"build-image-from-dockerfile\"},' stages [ 'Code-review-application-maven' ] = '[{\"name\": \"checkout\"}' + \"$ {commitValidateStage} \" + ',{\"name\": \"compile\"}' + ',{\"name\": \"tests\"}, {\"name\": \"sonar\"}]' stages [ 'Code-review-application-npm' ] = stages [ 'Code-review-application-maven' ] stages [ 'Code-review-application-gradle' ] = stages [ 'Code-review-application-maven' ] stages [ 'Code-review-application-dotnet' ] = stages [ 'Code-review-application-maven' ] stages [ 'Code-review-application-terraform' ] = '[{\"name\": \"checkout\"},{\"name\": \"tool-init\"},{\"name\": \"lint\"}]' stages [ 'Code-review-application-helm' ] = '[{\"name\": \"checkout\"},{\"name\": \"lint\"}]' stages [ 'Code-review-application-docker' ] = '[{\"name\": \"checkout\"},{\"name\": \"lint\"}]' stages [ 'Code-review-application-go' ] = '[{\"name\": \"checkout\"}' + \"$ {commitValidateStage} \" + ',{\"name\": \"build\"},' + '{\"name\": \"tests\"}, {\"name\": \"sonar\"}]' stages [ 'Code-review-application-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"}, {\"name\": \"sonar\"}]' stages [ 'Code-review-library' ] = '[{\"name\": \"checkout\"},{\"name\": \"compile\"},{\"name\": \"tests\"},' + '{\"name\": \"sonar\"}]' stages [ 'Code-review-autotests-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"tests\"},{\"name\": \"sonar\"}]' stages [ 'Build-library-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"$ {createJIMStage} \" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-npm' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-gradle' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"$ {createJIMStage} \" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"$ {buildStage} \" + '{\"name\": \"push\"}' + \"$ {createJIMStage} \" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"$ {buildStage} \" + '{\"name\":\"push\"}' + \"$ {createJIMStage} \" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-npm' ] = stages [ 'Build-application-maven' ] stages [ 'Build-application-gradle' ] = stages [ 'Build-application-maven' ] stages [ 'Build-application-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"$ {buildStage} \" + '{\"name\": \"push\"}' + \"$ {createJIMStage} \" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-terraform' ] = '[{\"name\": \"checkout\"},{\"name\": \"tool-init\"},' + '{\"name\": \"lint\"},{\"name\": \"git-tag\"}]' stages [ 'Build-application-helm' ] = '[{\"name\": \"checkout\"},{\"name\": \"lint\"}]' stages [ 'Build-application-docker' ] = '[{\"name\": \"checkout\"},{\"name\": \"lint\"}]' stages [ 'Build-application-go' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"tests\"},{\"name\": \"sonar\"},' + '{\"name\": \"build\"}' + \"$ {buildStage} \" + \"$ {createJIMStage} \" + '{\"name\": \"git-tag\"}]' stages [ 'Create-release' ] = '[{\"name\": \"checkout\"},{\"name\": \"create-branch\"},{\"name\": \"trigger-job\"}]' def codebaseName = \"$ {NAME} \" def buildTool = \"$ {BUILD_TOOL} \" def gitServerCrName = \"$ {GIT_SERVER_CR_NAME} \" def gitServerCrVersion = \"$ {GIT_SERVER_CR_VERSION} \" def gitServer = \"${GIT_SERVER ? GIT_SERVER : 'gerrit'}\" def gitSshPort = \"${GIT_SSH_PORT ? GIT_SSH_PORT : '29418'}\" def gitUsername = \"${GIT_USERNAME ? GIT_USERNAME : 'jenkins'}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID ? GIT_CREDENTIALS_ID : 'gerrit-ciuser-sshkey'}\" def defaultRepoPath = \"ssh://$ {gitUsername} @$ {gitServer} :$ {gitSshPort} /$ {codebaseName} \" def repositoryPath = \"${REPOSITORY_PATH ? REPOSITORY_PATH : defaultRepoPath}\" def codebaseFolder = jenkins . getItem ( codebaseName ) if ( codebaseFolder == null ) { folder ( codebaseName ) } createListView ( codebaseName , \"Releases\" ) createReleasePipeline ( \"Create-release-$ {codebaseName} \" , codebaseName , stages [ \"Create-release\" ], \"create-release.groovy\" , repositoryPath , gitCredentialsId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType ) if ( BRANCH ) { def branch = \"$ {BRANCH} \" def formattedBranch = \"${branch.toUpperCase().replaceAll(/ \\\\ //, \" - \")}\" createListView ( codebaseName , formattedBranch ) def type = \"$ {TYPE} \" createCiPipeline ( \"Code-review-$ {codebaseName} \" , codebaseName , stages [ \"Code-review-$ {type} -${buildTool.toLowerCase()}\" ], \"code-review.groovy\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) if ( type . equalsIgnoreCase ( 'application' ) || type . equalsIgnoreCase ( 'library' )) { def jobExists = false if ( \"$ {formattedBranch} -Build-$ {codebaseName} \" . toString () in Jenkins . instance . getAllItems () . collect { it . name }) { jobExists = true } createCiPipeline ( \"Build-$ {codebaseName} \" , codebaseName , stages [ \"Build-$ {type} -${buildTool.toLowerCase()}\" ], \"build.groovy\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) if ( ! jobExists ) { queue ( \"$ {codebaseName} /$ {formattedBranch} -Build-$ {codebaseName} \" ) } } } def createCiPipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , watchBranch = \"master\" , gitServerCrName , gitServerCrVersion ) { def jobName = \"${watchBranch.toUpperCase().replaceAll(/ \\\\ //, \" - \")}-$ {pipelineName} \" def existingJob = Jenkins . getInstance () . getItemByFullName ( \"$ {codebaseName} /$ {jobName} \" ) def webhookToken = null if ( existingJob ) { def triggersMap = existingJob . getTriggers () triggersMap . each { key , value -> webhookToken = value . getSecretToken () } } else { def random = new byte [ 16 ] new java . security . SecureRandom () . nextBytes ( random ) webhookToken = random . encodeHex () . toString () } pipelineJob ( \"$ {codebaseName} /$ {jobName} \" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } properties { gitLabConnection { gitLabConnection ( 'git.epam.com' ) } } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) } branches ( pipelineName . contains ( \"Build\" ) ? \"$ {watchBranch} \" : \"\\$ {gitlabMergeRequestLastCommit} \" ) scriptPath ( \"$ {pipelineScript} \" ) } } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"$ {gitServerCrName} \" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"$ {gitServerCrVersion} \" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"$ {codebaseStages} \" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"$ {codebaseName} \" , \"Gerrit project name(Codebase name) to be build\" ) if ( pipelineName . contains ( \"Build\" )) stringParam ( \"BRANCH\" , \"$ {watchBranch} \" , \"Branch to build artifact from\" ) else stringParam ( \"BRANCH\" , \"\\$ {gitlabMergeRequestLastCommit} \" , \"Branch to build artifact from\" ) } } } triggers { gitlabPush { buildOnMergeRequestEvents ( pipelineName . contains ( \"Build\" ) ? false : true ) buildOnPushEvents ( pipelineName . contains ( \"Build\" ) ? true : false ) enableCiSkip ( false ) setBuildDescription ( true ) rebuildOpenMergeRequest ( pipelineName . contains ( \"Build\" ) ? 'never' : 'source' ) commentTrigger ( \"Build it please\" ) skipWorkInProgressMergeRequest ( true ) targetBranchRegex ( \"$ {watchBranch} \" ) } } configure { it / triggers / 'com.dabsquared.gitlabjenkins.GitLabPushTrigger' << secretToken ( webhookToken ) it / triggers / 'com.dabsquared.gitlabjenkins.GitLabPushTrigger' << triggerOnApprovedMergeRequest ( pipelineName . contains ( \"Build\" ) ? false : true ) it / triggers / 'com.dabsquared.gitlabjenkins.GitLabPushTrigger' << pendingBuildName ( pipelineName . contains ( \"Build\" ) ? \"\" : \"Jenkins\" ) } } registerWebHook ( repository , codebaseName , jobName , webhookToken ) } def createReleasePipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType ) { pipelineJob ( \"$ {codebaseName} /$ {pipelineName} \" ) { logRotator { numToKeep ( 14 ) daysToKeep ( 30 ) } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) } branches ( \"master\" ) scriptPath ( \"$ {pipelineScript} \" ) } } parameters { stringParam ( \"STAGES\" , \"$ {codebaseStages} \" , \"\" ) if ( pipelineName . contains ( \"Create-release\" )) { stringParam ( \"JIRA_INTEGRATION_ENABLED\" , \"$ {jiraIntegrationEnabled} \" , \"Is Jira integration enabled\" ) stringParam ( \"PLATFORM_TYPE\" , \"$ {platformType} \" , \"Platform type\" ) stringParam ( \"GERRIT_PROJECT\" , \"$ {codebaseName} \" , \"\" ) stringParam ( \"RELEASE_NAME\" , \"\" , \"Name of the release(branch to be created)\" ) stringParam ( \"COMMIT_ID\" , \"\" , \"Commit ID that will be used to create branch from for new release. If empty, HEAD of master will be used\" ) stringParam ( \"GIT_SERVER_CR_NAME\" , \"$ {gitServerCrName} \" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"$ {gitServerCrVersion} \" , \"Version of GitServer CR Resource\" ) stringParam ( \"REPOSITORY_PATH\" , \"$ {repository} \" , \"Full repository path\" ) } } } } } } def createListView ( codebaseName , branchName ) { listView ( \"$ {codebaseName} /$ {branchName} \" ) { if ( branchName . toLowerCase () == \"releases\" ) { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^Create-release.*\" ) } } } else { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^$ {branchName} -(Code-review|Build).*\" ) } } } columns { status () weather () name () lastSuccess () lastFailure () lastDuration () buildButton () } } } def registerWebHook ( repositoryPath , codebaseName , jobName , webhookToken ) { def apiUrl = 'https://' + repositoryPath . replaceAll ( \"ssh://\" , \"\" ) . split ( '@' )[ 1 ] . replace ( '/' , \" %2F \" ) . replaceAll ( ~/ : \\ d +% 2 F / , '/api/v4/projects/' ) + '/hooks' def jobWebhookUrl = \"${System.getenv('JENKINS_UI_URL')}/project/$ {codebaseName} /$ {jobName} \" def gitlabToken = getSecretValue ( 'gitlab-access-token' ) if ( checkWebHookExist ( apiUrl , jobWebhookUrl , gitlabToken )) { println ( \"[JENKINS][DEBUG] Webhook for job $ {jobName} is already exist \\r\\n \" ) return } println ( \"[JENKINS][DEBUG] Creating webhook for job $ {jobName} \" ) def webhookConfig = [:] webhookConfig [ \"url\" ] = jobWebhookUrl webhookConfig [ \"push_events\" ] = jobName . contains ( \"Build\" ) ? \"true\" : \"false\" webhookConfig [ \"merge_requests_events\" ] = jobName . contains ( \"Build\" ) ? \"false\" : \"true\" webhookConfig [ \"issues_events\" ] = \"false\" webhookConfig [ \"confidential_issues_events\" ] = \"false\" webhookConfig [ \"tag_push_events\" ] = \"false\" webhookConfig [ \"note_events\" ] = \"true\" webhookConfig [ \"job_events\" ] = \"false\" webhookConfig [ \"pipeline_events\" ] = \"false\" webhookConfig [ \"wiki_page_events\" ] = \"false\" webhookConfig [ \"enable_ssl_verification\" ] = \"true\" webhookConfig [ \"token\" ] = webhookToken def requestBody = JsonOutput . toJson ( webhookConfig ) def httpConnector = new URL ( apiUrl ) . openConnection () as HttpURLConnection httpConnector . setRequestMethod ( 'POST' ) httpConnector . setDoOutput ( true ) httpConnector . setRequestProperty ( \"Accept\" , 'application/json' ) httpConnector . setRequestProperty ( \"Content-Type\" , 'application/json' ) httpConnector . setRequestProperty ( \"PRIVATE-TOKEN\" , \"$ {gitlabToken} \" ) httpConnector . outputStream . write ( requestBody . getBytes ( \"UTF-8\" )) httpConnector . connect () if ( httpConnector . responseCode == 201 ) println ( \"[JENKINS][DEBUG] Webhook for job $ {jobName} has been created \\r\\n \" ) else { println ( \"[JENKINS][ERROR] Responce code - $ {httpConnector.responseCode} \" ) def response = new JsonSlurper () . parseText ( httpConnector . errorStream . getText ( 'UTF-8' )) println ( \"[JENKINS][ERROR] Failed to create webhook for job $ {jobName} . Response - $ {response} \" ) } } def checkWebHookExist ( apiUrl , jobWebhookUrl , gitlabToken ) { println ( \"[JENKINS][DEBUG] Checking if webhook $ {jobWebhookUrl} exists\" ) def httpConnector = new URL ( apiUrl ) . openConnection () as HttpURLConnection httpConnector . setRequestMethod ( 'GET' ) httpConnector . setDoOutput ( true ) httpConnector . setRequestProperty ( \"Accept\" , 'application/json' ) httpConnector . setRequestProperty ( \"Content-Type\" , 'application/json' ) httpConnector . setRequestProperty ( \"PRIVATE-TOKEN\" , \"$ {gitlabToken} \" ) httpConnector . connect () if ( httpConnector . responseCode == 200 ) { def response = new JsonSlurper () . parseText ( httpConnector . inputStream . getText ( 'UTF-8' )) return response . find { it . url == jobWebhookUrl } ? true : false } } def getSecretValue ( name ) { def creds = com . cloudbees . plugins . credentials . CredentialsProvider . lookupCredentials ( com . cloudbees . plugins . credentials . common . StandardCredentials . class , Jenkins . instance , null , null ) def secret = creds . find { it . properties [ 'id' ] == name } return secret != null ? secret . getApiToken () : null } Create Secret, GitServer CR and Jenkins credentials with the \"gitlab\" ID by following the instruction: Adjust Import Strategy After the steps above are performed, the new custom job-provision will be available in Advanced CI Settings during the application creation. Note Using the GitLab integration, a webhook is automatically created. After the removal of the application, the webhook stops working but not deleted. If necessary, it must be deleted manually.* Related Articles \u2693\ufe0e Adjust Import Strategy Adjust Integration With Jira Server","title":"Overview"},{"location":"operator-guide/gitlab-integration/#gitlab-integration","text":"Discover the steps below to apply the GitLab integration correctly: Create access token in Gitlab : Log in to GitLab ; In the top-right corner, click your avatar and select Settings ; On the User Settings menu, select Access Tokens ; Choose a name and an optional expiry date for the token; In the Scopes block, select the api scope for the token; Click the Create personal access token button. Note Make sure to save the access token as there won`t be the ability to access it once again. Install GitLab plugin by navigating to Manage Jenkins and switching to plugin manager, select the GitLab Plugin check box: Create Jenkins Credential ID by navigating to Jenkins -> Credentials -> System -> Global Credentials -> Add Credentials : Select GitLab API token; Select Global scope; API token - the Access Token that was created earlier; ID - the gitlab-access-token ID; Description - the description of the current Credential ID; Configure Gitlab plugin by navigating to Manage Jenkins -> Configure System and fill in the GitLab plugin settings: Connection name - connection name; Gitlab host URL - a host URL to GitLab; Credentials - credentials with Access Token to GitLab ( gitlab-access-token ); Create a new Job Provision. Navigate to the Jenkins main page and open the job-provisions/ci folder: Click New Item ; Type the name; Select Freestyle project and click OK; Select the This project is parameterized check box and add a few input parameters as the following strings: NAME; TYPE; BUILD_TOOL; BRANCH; GIT_SERVER_CR_NAME; GIT_SERVER_CR_VERSION; GIT_SERVER; GIT_SSH_PORT; GIT_USERNAME; GIT_CREDENTIALS_ID; REPOSITORY_PATH; JIRA_INTEGRATION_ENABLED; Check the Execute concurrent builds if necessary option; Check the Restrict where this project can be run option; Fill in the Label Expression field by typing the master branch name. In the Build section, perform the following: Select DSL Script ; Select the Use the provided DSL script check box: As soon as all the steps above are performed, insert the code: import groovy.json. * import jenkins.model.Jenkins Jenkins jenkins = Jenkins . instance def stages = [:] def jiraIntegrationEnabled = Boolean . parseBoolean ( \"$ {JIRA_INTEGRATION_ENABLED} \" as String ) def commitValidateStage = jiraIntegrationEnabled ? ',{\"name\": \"commit-validate\"}' : '' def createJIMStage = jiraIntegrationEnabled ? ',{\"name\": \"create-jira-issue-metadata\"}' : '' def platformType = \"$ {PLATFORM_TYPE} \" def buildStage = platformType == \"kubernetes\" ? ',{\"name\": \"build-image-kaniko\"},' : ',{\"name\": \"build-image-from-dockerfile\"},' stages [ 'Code-review-application-maven' ] = '[{\"name\": \"checkout\"}' + \"$ {commitValidateStage} \" + ',{\"name\": \"compile\"}' + ',{\"name\": \"tests\"}, {\"name\": \"sonar\"}]' stages [ 'Code-review-application-npm' ] = stages [ 'Code-review-application-maven' ] stages [ 'Code-review-application-gradle' ] = stages [ 'Code-review-application-maven' ] stages [ 'Code-review-application-dotnet' ] = stages [ 'Code-review-application-maven' ] stages [ 'Code-review-application-terraform' ] = '[{\"name\": \"checkout\"},{\"name\": \"tool-init\"},{\"name\": \"lint\"}]' stages [ 'Code-review-application-helm' ] = '[{\"name\": \"checkout\"},{\"name\": \"lint\"}]' stages [ 'Code-review-application-docker' ] = '[{\"name\": \"checkout\"},{\"name\": \"lint\"}]' stages [ 'Code-review-application-go' ] = '[{\"name\": \"checkout\"}' + \"$ {commitValidateStage} \" + ',{\"name\": \"build\"},' + '{\"name\": \"tests\"}, {\"name\": \"sonar\"}]' stages [ 'Code-review-application-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"}, {\"name\": \"sonar\"}]' stages [ 'Code-review-library' ] = '[{\"name\": \"checkout\"},{\"name\": \"compile\"},{\"name\": \"tests\"},' + '{\"name\": \"sonar\"}]' stages [ 'Code-review-autotests-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"tests\"},{\"name\": \"sonar\"}]' stages [ 'Build-library-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"$ {createJIMStage} \" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-library-npm' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-gradle' ] = stages [ 'Build-library-maven' ] stages [ 'Build-library-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"push\"}' + \"$ {createJIMStage} \" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-maven' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"},{\"name\": \"build\"}' + \"$ {buildStage} \" + '{\"name\": \"push\"}' + \"$ {createJIMStage} \" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-python' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"$ {buildStage} \" + '{\"name\":\"push\"}' + \"$ {createJIMStage} \" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-npm' ] = stages [ 'Build-application-maven' ] stages [ 'Build-application-gradle' ] = stages [ 'Build-application-maven' ] stages [ 'Build-application-dotnet' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"compile\"},' + '{\"name\": \"tests\"},{\"name\": \"sonar\"}' + \"$ {buildStage} \" + '{\"name\": \"push\"}' + \"$ {createJIMStage} \" + ',{\"name\": \"git-tag\"}]' stages [ 'Build-application-terraform' ] = '[{\"name\": \"checkout\"},{\"name\": \"tool-init\"},' + '{\"name\": \"lint\"},{\"name\": \"git-tag\"}]' stages [ 'Build-application-helm' ] = '[{\"name\": \"checkout\"},{\"name\": \"lint\"}]' stages [ 'Build-application-docker' ] = '[{\"name\": \"checkout\"},{\"name\": \"lint\"}]' stages [ 'Build-application-go' ] = '[{\"name\": \"checkout\"},{\"name\": \"get-version\"},{\"name\": \"tests\"},{\"name\": \"sonar\"},' + '{\"name\": \"build\"}' + \"$ {buildStage} \" + \"$ {createJIMStage} \" + '{\"name\": \"git-tag\"}]' stages [ 'Create-release' ] = '[{\"name\": \"checkout\"},{\"name\": \"create-branch\"},{\"name\": \"trigger-job\"}]' def codebaseName = \"$ {NAME} \" def buildTool = \"$ {BUILD_TOOL} \" def gitServerCrName = \"$ {GIT_SERVER_CR_NAME} \" def gitServerCrVersion = \"$ {GIT_SERVER_CR_VERSION} \" def gitServer = \"${GIT_SERVER ? GIT_SERVER : 'gerrit'}\" def gitSshPort = \"${GIT_SSH_PORT ? GIT_SSH_PORT : '29418'}\" def gitUsername = \"${GIT_USERNAME ? GIT_USERNAME : 'jenkins'}\" def gitCredentialsId = \"${GIT_CREDENTIALS_ID ? GIT_CREDENTIALS_ID : 'gerrit-ciuser-sshkey'}\" def defaultRepoPath = \"ssh://$ {gitUsername} @$ {gitServer} :$ {gitSshPort} /$ {codebaseName} \" def repositoryPath = \"${REPOSITORY_PATH ? REPOSITORY_PATH : defaultRepoPath}\" def codebaseFolder = jenkins . getItem ( codebaseName ) if ( codebaseFolder == null ) { folder ( codebaseName ) } createListView ( codebaseName , \"Releases\" ) createReleasePipeline ( \"Create-release-$ {codebaseName} \" , codebaseName , stages [ \"Create-release\" ], \"create-release.groovy\" , repositoryPath , gitCredentialsId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType ) if ( BRANCH ) { def branch = \"$ {BRANCH} \" def formattedBranch = \"${branch.toUpperCase().replaceAll(/ \\\\ //, \" - \")}\" createListView ( codebaseName , formattedBranch ) def type = \"$ {TYPE} \" createCiPipeline ( \"Code-review-$ {codebaseName} \" , codebaseName , stages [ \"Code-review-$ {type} -${buildTool.toLowerCase()}\" ], \"code-review.groovy\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) if ( type . equalsIgnoreCase ( 'application' ) || type . equalsIgnoreCase ( 'library' )) { def jobExists = false if ( \"$ {formattedBranch} -Build-$ {codebaseName} \" . toString () in Jenkins . instance . getAllItems () . collect { it . name }) { jobExists = true } createCiPipeline ( \"Build-$ {codebaseName} \" , codebaseName , stages [ \"Build-$ {type} -${buildTool.toLowerCase()}\" ], \"build.groovy\" , repositoryPath , gitCredentialsId , branch , gitServerCrName , gitServerCrVersion ) if ( ! jobExists ) { queue ( \"$ {codebaseName} /$ {formattedBranch} -Build-$ {codebaseName} \" ) } } } def createCiPipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , watchBranch = \"master\" , gitServerCrName , gitServerCrVersion ) { def jobName = \"${watchBranch.toUpperCase().replaceAll(/ \\\\ //, \" - \")}-$ {pipelineName} \" def existingJob = Jenkins . getInstance () . getItemByFullName ( \"$ {codebaseName} /$ {jobName} \" ) def webhookToken = null if ( existingJob ) { def triggersMap = existingJob . getTriggers () triggersMap . each { key , value -> webhookToken = value . getSecretToken () } } else { def random = new byte [ 16 ] new java . security . SecureRandom () . nextBytes ( random ) webhookToken = random . encodeHex () . toString () } pipelineJob ( \"$ {codebaseName} /$ {jobName} \" ) { logRotator { numToKeep ( 10 ) daysToKeep ( 7 ) } properties { gitLabConnection { gitLabConnection ( 'git.epam.com' ) } } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) } branches ( pipelineName . contains ( \"Build\" ) ? \"$ {watchBranch} \" : \"\\$ {gitlabMergeRequestLastCommit} \" ) scriptPath ( \"$ {pipelineScript} \" ) } } parameters { stringParam ( \"GIT_SERVER_CR_NAME\" , \"$ {gitServerCrName} \" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"$ {gitServerCrVersion} \" , \"Version of GitServer CR Resource\" ) stringParam ( \"STAGES\" , \"$ {codebaseStages} \" , \"Consequence of stages in JSON format to be run during execution\" ) stringParam ( \"GERRIT_PROJECT_NAME\" , \"$ {codebaseName} \" , \"Gerrit project name(Codebase name) to be build\" ) if ( pipelineName . contains ( \"Build\" )) stringParam ( \"BRANCH\" , \"$ {watchBranch} \" , \"Branch to build artifact from\" ) else stringParam ( \"BRANCH\" , \"\\$ {gitlabMergeRequestLastCommit} \" , \"Branch to build artifact from\" ) } } } triggers { gitlabPush { buildOnMergeRequestEvents ( pipelineName . contains ( \"Build\" ) ? false : true ) buildOnPushEvents ( pipelineName . contains ( \"Build\" ) ? true : false ) enableCiSkip ( false ) setBuildDescription ( true ) rebuildOpenMergeRequest ( pipelineName . contains ( \"Build\" ) ? 'never' : 'source' ) commentTrigger ( \"Build it please\" ) skipWorkInProgressMergeRequest ( true ) targetBranchRegex ( \"$ {watchBranch} \" ) } } configure { it / triggers / 'com.dabsquared.gitlabjenkins.GitLabPushTrigger' << secretToken ( webhookToken ) it / triggers / 'com.dabsquared.gitlabjenkins.GitLabPushTrigger' << triggerOnApprovedMergeRequest ( pipelineName . contains ( \"Build\" ) ? false : true ) it / triggers / 'com.dabsquared.gitlabjenkins.GitLabPushTrigger' << pendingBuildName ( pipelineName . contains ( \"Build\" ) ? \"\" : \"Jenkins\" ) } } registerWebHook ( repository , codebaseName , jobName , webhookToken ) } def createReleasePipeline ( pipelineName , codebaseName , codebaseStages , pipelineScript , repository , credId , gitServerCrName , gitServerCrVersion , jiraIntegrationEnabled , platformType ) { pipelineJob ( \"$ {codebaseName} /$ {pipelineName} \" ) { logRotator { numToKeep ( 14 ) daysToKeep ( 30 ) } definition { cpsScm { scm { git { remote { url ( repository ) credentials ( credId ) } branches ( \"master\" ) scriptPath ( \"$ {pipelineScript} \" ) } } parameters { stringParam ( \"STAGES\" , \"$ {codebaseStages} \" , \"\" ) if ( pipelineName . contains ( \"Create-release\" )) { stringParam ( \"JIRA_INTEGRATION_ENABLED\" , \"$ {jiraIntegrationEnabled} \" , \"Is Jira integration enabled\" ) stringParam ( \"PLATFORM_TYPE\" , \"$ {platformType} \" , \"Platform type\" ) stringParam ( \"GERRIT_PROJECT\" , \"$ {codebaseName} \" , \"\" ) stringParam ( \"RELEASE_NAME\" , \"\" , \"Name of the release(branch to be created)\" ) stringParam ( \"COMMIT_ID\" , \"\" , \"Commit ID that will be used to create branch from for new release. If empty, HEAD of master will be used\" ) stringParam ( \"GIT_SERVER_CR_NAME\" , \"$ {gitServerCrName} \" , \"Name of Git Server CR to generate link to Git server\" ) stringParam ( \"GIT_SERVER_CR_VERSION\" , \"$ {gitServerCrVersion} \" , \"Version of GitServer CR Resource\" ) stringParam ( \"REPOSITORY_PATH\" , \"$ {repository} \" , \"Full repository path\" ) } } } } } } def createListView ( codebaseName , branchName ) { listView ( \"$ {codebaseName} /$ {branchName} \" ) { if ( branchName . toLowerCase () == \"releases\" ) { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^Create-release.*\" ) } } } else { jobFilters { regex { matchType ( MatchType . INCLUDE_MATCHED ) matchValue ( RegexMatchValue . NAME ) regex ( \"^$ {branchName} -(Code-review|Build).*\" ) } } } columns { status () weather () name () lastSuccess () lastFailure () lastDuration () buildButton () } } } def registerWebHook ( repositoryPath , codebaseName , jobName , webhookToken ) { def apiUrl = 'https://' + repositoryPath . replaceAll ( \"ssh://\" , \"\" ) . split ( '@' )[ 1 ] . replace ( '/' , \" %2F \" ) . replaceAll ( ~/ : \\ d +% 2 F / , '/api/v4/projects/' ) + '/hooks' def jobWebhookUrl = \"${System.getenv('JENKINS_UI_URL')}/project/$ {codebaseName} /$ {jobName} \" def gitlabToken = getSecretValue ( 'gitlab-access-token' ) if ( checkWebHookExist ( apiUrl , jobWebhookUrl , gitlabToken )) { println ( \"[JENKINS][DEBUG] Webhook for job $ {jobName} is already exist \\r\\n \" ) return } println ( \"[JENKINS][DEBUG] Creating webhook for job $ {jobName} \" ) def webhookConfig = [:] webhookConfig [ \"url\" ] = jobWebhookUrl webhookConfig [ \"push_events\" ] = jobName . contains ( \"Build\" ) ? \"true\" : \"false\" webhookConfig [ \"merge_requests_events\" ] = jobName . contains ( \"Build\" ) ? \"false\" : \"true\" webhookConfig [ \"issues_events\" ] = \"false\" webhookConfig [ \"confidential_issues_events\" ] = \"false\" webhookConfig [ \"tag_push_events\" ] = \"false\" webhookConfig [ \"note_events\" ] = \"true\" webhookConfig [ \"job_events\" ] = \"false\" webhookConfig [ \"pipeline_events\" ] = \"false\" webhookConfig [ \"wiki_page_events\" ] = \"false\" webhookConfig [ \"enable_ssl_verification\" ] = \"true\" webhookConfig [ \"token\" ] = webhookToken def requestBody = JsonOutput . toJson ( webhookConfig ) def httpConnector = new URL ( apiUrl ) . openConnection () as HttpURLConnection httpConnector . setRequestMethod ( 'POST' ) httpConnector . setDoOutput ( true ) httpConnector . setRequestProperty ( \"Accept\" , 'application/json' ) httpConnector . setRequestProperty ( \"Content-Type\" , 'application/json' ) httpConnector . setRequestProperty ( \"PRIVATE-TOKEN\" , \"$ {gitlabToken} \" ) httpConnector . outputStream . write ( requestBody . getBytes ( \"UTF-8\" )) httpConnector . connect () if ( httpConnector . responseCode == 201 ) println ( \"[JENKINS][DEBUG] Webhook for job $ {jobName} has been created \\r\\n \" ) else { println ( \"[JENKINS][ERROR] Responce code - $ {httpConnector.responseCode} \" ) def response = new JsonSlurper () . parseText ( httpConnector . errorStream . getText ( 'UTF-8' )) println ( \"[JENKINS][ERROR] Failed to create webhook for job $ {jobName} . Response - $ {response} \" ) } } def checkWebHookExist ( apiUrl , jobWebhookUrl , gitlabToken ) { println ( \"[JENKINS][DEBUG] Checking if webhook $ {jobWebhookUrl} exists\" ) def httpConnector = new URL ( apiUrl ) . openConnection () as HttpURLConnection httpConnector . setRequestMethod ( 'GET' ) httpConnector . setDoOutput ( true ) httpConnector . setRequestProperty ( \"Accept\" , 'application/json' ) httpConnector . setRequestProperty ( \"Content-Type\" , 'application/json' ) httpConnector . setRequestProperty ( \"PRIVATE-TOKEN\" , \"$ {gitlabToken} \" ) httpConnector . connect () if ( httpConnector . responseCode == 200 ) { def response = new JsonSlurper () . parseText ( httpConnector . inputStream . getText ( 'UTF-8' )) return response . find { it . url == jobWebhookUrl } ? true : false } } def getSecretValue ( name ) { def creds = com . cloudbees . plugins . credentials . CredentialsProvider . lookupCredentials ( com . cloudbees . plugins . credentials . common . StandardCredentials . class , Jenkins . instance , null , null ) def secret = creds . find { it . properties [ 'id' ] == name } return secret != null ? secret . getApiToken () : null } Create Secret, GitServer CR and Jenkins credentials with the \"gitlab\" ID by following the instruction: Adjust Import Strategy After the steps above are performed, the new custom job-provision will be available in Advanced CI Settings during the application creation. Note Using the GitLab integration, a webhook is automatically created. After the removal of the application, the webhook stops working but not deleted. If necessary, it must be deleted manually.*","title":"GitLab Integration"},{"location":"operator-guide/gitlab-integration/#related-articles","text":"Adjust Import Strategy Adjust Integration With Jira Server","title":"Related Articles"},{"location":"operator-guide/gitlabci-integration/","text":"Adjust GitLab CI Tool \u2693\ufe0e EDP allows selecting one of two available CI (Continuous Integration) tools, namely: Jenkins or GitLab. The Jenkins tool is available by default. To use the GitLab CI tool, it is required to make it available first. Follow the steps below to adjust the GitLab CI tool: In GitLab, add the environment variables to the project. To add variables, navigate to Settings -> CI/CD -> Expand Variables -> Add Variable : Apply the necessary variables as they differ in accordance with the cluster OpenShift / Kubernetes, see below: OpenShift Environment Variables Description DOCKER_REGISTRY_URL URL to OpenShift docker registry DOCKER_REGISTRY_PASSWORD Service Account token that has an access to registry DOCKER_REGISTRY_USER user name OPENSHIFT_SA_TOKEN token that can be used to log in to OpenShift Info In order to get access to the Docker registry and OpenShift, use the gitlab-ci ServiceAccount; pay attention that SA description contains the credentials and secrets: Kubernetes Environment Variables Description DOCKER_REGISTRY_URL URL to Amazon ECR AWS_ACCESS_KEY_ID auto IAM user access key AWS_SECRET_ACCESS_KEY auto IAM user secret access key K8S_SA_TOKEN token that can be used to log in to Kubernetes Note To get the access to ECR, it is required to have an auto IAM user that has rights to push/create a repository. In Admin Console, select the CI tool in the Advanced Settings menu during the codebase creation: Note The selection of the CI tool is available only with the Import strategy. As soon as the codebase is provisioned, the .gitlab-ci.yml file will be created in the repository that describes the pipeline's stages and logic:","title":"Adjust GitLab CI Tool"},{"location":"operator-guide/gitlabci-integration/#adjust-gitlab-ci-tool","text":"EDP allows selecting one of two available CI (Continuous Integration) tools, namely: Jenkins or GitLab. The Jenkins tool is available by default. To use the GitLab CI tool, it is required to make it available first. Follow the steps below to adjust the GitLab CI tool: In GitLab, add the environment variables to the project. To add variables, navigate to Settings -> CI/CD -> Expand Variables -> Add Variable : Apply the necessary variables as they differ in accordance with the cluster OpenShift / Kubernetes, see below: OpenShift Environment Variables Description DOCKER_REGISTRY_URL URL to OpenShift docker registry DOCKER_REGISTRY_PASSWORD Service Account token that has an access to registry DOCKER_REGISTRY_USER user name OPENSHIFT_SA_TOKEN token that can be used to log in to OpenShift Info In order to get access to the Docker registry and OpenShift, use the gitlab-ci ServiceAccount; pay attention that SA description contains the credentials and secrets: Kubernetes Environment Variables Description DOCKER_REGISTRY_URL URL to Amazon ECR AWS_ACCESS_KEY_ID auto IAM user access key AWS_SECRET_ACCESS_KEY auto IAM user secret access key K8S_SA_TOKEN token that can be used to log in to Kubernetes Note To get the access to ECR, it is required to have an auto IAM user that has rights to push/create a repository. In Admin Console, select the CI tool in the Advanced Settings menu during the codebase creation: Note The selection of the CI tool is available only with the Import strategy. As soon as the codebase is provisioned, the .gitlab-ci.yml file will be created in the repository that describes the pipeline's stages and logic:","title":"Adjust GitLab CI Tool"},{"location":"operator-guide/import-strategy/","text":"Enable VCS Import Strategy \u2693\ufe0e In order to use the import strategy, it is required to add Secret with SSH key, GitServer CR, and Jenkins credentials by following the steps below: Create Secret in the OpenShift/K8S namespace for the Git account with the id_rsa , id_rsa.pub , and username fields: As a sample, it is possible to use the following command: kubectl create secret generic gitlab - n edp \\ -- from - file = id_rsa = id_rsa \\ -- from - file id_rsa . pub = id_rsa . pub \\ --from-literal=username=user@gitlab.com Create GitServer CR in the OpenShift/K8S namespace with the gitHost , gitUser , httpsPort , sshPort , nameSshKeySecret , and createCodeReviewPipeline fields: As a sample, it is possible to use the following template: apiVersion : v2 . edp . epam . com / v1alpha1 kind : GitServer metadata : name : git - example spec : createCodeReviewPipeline : false gitHost : git . example . com gitUser : git httpsPort : 443 nameSshKeySecret : gitlab - sshkey sshPort : 22 Note The value of the nameSshKeySecret property is the name of the Secret that is indicated in the first point above. Create a Credential in Jenkins with the same ID as in the nameSshKeySecret property, and with the private key. Navigate to Jenkins -> Credentials -> System -> Global credentials -> Add Credentials : Change the Deployment Config of the Admin Console by adding the Import strategy to the INTEGRATION_STRATEGIES variable: As soon as the Admin Console is redeployed, the Import strategy will be added to the Create Application page. For details, please refer to the Add Applications page.","title":"Enable VCS Import Strategy"},{"location":"operator-guide/import-strategy/#enable-vcs-import-strategy","text":"In order to use the import strategy, it is required to add Secret with SSH key, GitServer CR, and Jenkins credentials by following the steps below: Create Secret in the OpenShift/K8S namespace for the Git account with the id_rsa , id_rsa.pub , and username fields: As a sample, it is possible to use the following command: kubectl create secret generic gitlab - n edp \\ -- from - file = id_rsa = id_rsa \\ -- from - file id_rsa . pub = id_rsa . pub \\ --from-literal=username=user@gitlab.com Create GitServer CR in the OpenShift/K8S namespace with the gitHost , gitUser , httpsPort , sshPort , nameSshKeySecret , and createCodeReviewPipeline fields: As a sample, it is possible to use the following template: apiVersion : v2 . edp . epam . com / v1alpha1 kind : GitServer metadata : name : git - example spec : createCodeReviewPipeline : false gitHost : git . example . com gitUser : git httpsPort : 443 nameSshKeySecret : gitlab - sshkey sshPort : 22 Note The value of the nameSshKeySecret property is the name of the Secret that is indicated in the first point above. Create a Credential in Jenkins with the same ID as in the nameSshKeySecret property, and with the private key. Navigate to Jenkins -> Credentials -> System -> Global credentials -> Add Credentials : Change the Deployment Config of the Admin Console by adding the Import strategy to the INTEGRATION_STRATEGIES variable: As soon as the Admin Console is redeployed, the Import strategy will be added to the Create Application page. For details, please refer to the Add Applications page.","title":"Enable VCS Import Strategy"},{"location":"operator-guide/jira-gerrit-integration/","text":"Gerrit to Jira Integration \u2693\ufe0e In order to adjust the Version Control System integration with Jira Server, first make sure you have the following prerequisites: VCS Server Jira Crucible When checked the prerequisites, follow the steps below to proceed with the integration: Integrate every project in VCS Server with every project in Crucible by creating a corresponding request in EPAM Support Portal . Add the repositories links and fill in the Keep Informed field as this request must be approved. Provide additional details to the support team. If the VCS is Gerrit, inspect the sample below of its integration: 2.1 Create a new \"crucible- \" user in Gerrit with SSH key and add a new user to the \"Non-Interactive Users\" Gerrit group; 2.2 Create a new group in Gerrit \"crucible-watcher-group\" and add the \"crucible- \" user; 2.3 Provide access to All-Projects for the \"crucible-watcher-group\" group: To link commits with Jira ticket, being in Gerrit, enter a Jira ticket ID in a commit message using the specific format: [PROJECT-CODE-1234]: commit message where PROJECT-CODE is a specific code of a project, 1234 is an ID number, and a commit message. As a result, all Gerrit commits will be displayed on Crucible : Related Articles \u2693\ufe0e Adjust Integration With Jira Server","title":"Gerrit to Jira Integration"},{"location":"operator-guide/jira-gerrit-integration/#gerrit-to-jira-integration","text":"In order to adjust the Version Control System integration with Jira Server, first make sure you have the following prerequisites: VCS Server Jira Crucible When checked the prerequisites, follow the steps below to proceed with the integration: Integrate every project in VCS Server with every project in Crucible by creating a corresponding request in EPAM Support Portal . Add the repositories links and fill in the Keep Informed field as this request must be approved. Provide additional details to the support team. If the VCS is Gerrit, inspect the sample below of its integration: 2.1 Create a new \"crucible- \" user in Gerrit with SSH key and add a new user to the \"Non-Interactive Users\" Gerrit group; 2.2 Create a new group in Gerrit \"crucible-watcher-group\" and add the \"crucible- \" user; 2.3 Provide access to All-Projects for the \"crucible-watcher-group\" group: To link commits with Jira ticket, being in Gerrit, enter a Jira ticket ID in a commit message using the specific format: [PROJECT-CODE-1234]: commit message where PROJECT-CODE is a specific code of a project, 1234 is an ID number, and a commit message. As a result, all Gerrit commits will be displayed on Crucible :","title":"Gerrit to Jira Integration"},{"location":"operator-guide/jira-gerrit-integration/#related-articles","text":"Adjust Integration With Jira Server","title":"Related Articles"},{"location":"operator-guide/jira-integration/","text":"Jira Integration \u2693\ufe0e In order to adjust the Jira server integration, first add JiraServer CR by performing the following: Create Secret in the OpenShift/K8S namespace for Jira Server account with the username and password fields: apiVersion : v1 data : password : passwordInBase64 username : usernameInBase64 kind : Secret metadata : name : epam - jira - user type : kubernetes . io / basic - auth Create JiraServer CR in the OpenShift/K8S namespace with the apiUrl , credentialName and rootUrl fields: apiVersion : v2 . edp . epam . com / v1alpha1 kind : JiraServer metadata : name : epam - jira spec : apiUrl : 'https://jira-api.example.com' credentialName : jira - user rootUrl : 'https://jira.example.com' status : available : true last_time_updated : '2021-04-05T10:51:07.042048633Z' Note The value of the credentialName property is the name of the Secret, which is indicated in the first point above. Being in Admin Console, navigate to the Advanced Settings menu to check that the Integrate with Jira Server check box became available:","title":"Overview"},{"location":"operator-guide/jira-integration/#jira-integration","text":"In order to adjust the Jira server integration, first add JiraServer CR by performing the following: Create Secret in the OpenShift/K8S namespace for Jira Server account with the username and password fields: apiVersion : v1 data : password : passwordInBase64 username : usernameInBase64 kind : Secret metadata : name : epam - jira - user type : kubernetes . io / basic - auth Create JiraServer CR in the OpenShift/K8S namespace with the apiUrl , credentialName and rootUrl fields: apiVersion : v2 . edp . epam . com / v1alpha1 kind : JiraServer metadata : name : epam - jira spec : apiUrl : 'https://jira-api.example.com' credentialName : jira - user rootUrl : 'https://jira.example.com' status : available : true last_time_updated : '2021-04-05T10:51:07.042048633Z' Note The value of the credentialName property is the name of the Secret, which is indicated in the first point above. Being in Admin Console, navigate to the Advanced Settings menu to check that the Integrate with Jira Server check box became available:","title":"Jira Integration"},{"location":"use-cases/","text":"","title":"Use Cases"},{"location":"user-guide/","text":"Overview \u2693\ufe0e This guide is for developers who have EDP installed and use for CI/CD configuration.","title":"Overview"},{"location":"user-guide/#overview","text":"This guide is for developers who have EDP installed and use for CI/CD configuration.","title":"Overview"},{"location":"user-guide/add-application/","text":"Add Application \u2693\ufe0e Admin Console allows to create, clone, import an application and add it to the environment with its subsequent deployment in Gerrit and building of the Code Review and Build pipelines in Jenkins. To add an application, navigate to the Applications section on the left-side navigation bar and click the Create button. Once clicked, the six-step menu will appear: The Codebase Info Menu The Application Info Menu The Advanced Settings Menu The Version Control System Info Menu The Exposing Service Info Menu The Database Menu Note The Version Control System Info menu is available in case this option is predefined The Codebase Info Menu \u2693\ufe0e In the Codebase Integration Strategy field, select the necessary option that is the configuration strategy for the replication with Gerrit: Create \u2013 creates a project on the pattern in accordance with an application language, a build tool, and a framework. Clone \u2013 clones the indicated repository into EPAM Delivery Platform. While cloning the existing repository, you have to fill in the additional fields as well. Import - allows configuring a replication from the Git server. While importing the existing repository, you have to select the Git server and define the respective path to the repository. Note In order to use the import strategy, make sure to adjust it by following the Adjust Import Strategy page. In the Git Repository URL field, specify the link to the repository that is to be cloned. If the Import strategy is selected, specify the following fields: a. Git Server where the repository is located. b. Relative path to the repository on the server. Select the Codebase Authentication check box and fill in the requested fields: Repository Login \u2013 enter your login data. Repository password (or API Token) \u2013 enter your password or indicate the API Token. Note The Codebase Authentication check box should be selected just in case you clone the private repository. If you define the public one, there is no need to enter credentials. Click the Proceed button to be switched to the next menu. The Application Info Menu \u2693\ufe0e Type the name of the application in the Application Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Note If the Import strategy is used, the Application Name field will not be displayed. Specify the name of the default branch where you want the development to be performed. Note The default branch cannot be deleted. Select any of the supported application languages with its framework in the Application Code Language/framework field: Java \u2013 selecting Java allows using Java 8 or Java 11. JavaScript - selecting JavaScript allows using the React framework. DotNet - selecting DotNet allows using the DotNet v.2.1 and DotNet v.3.1. Go - selecting Go allows using the Beego and Operator SDK frameworks. Python - selecting Python allows using the Python v.3.8. Other - selecting Other allows extending the default code languages when creating a codebase with the clone/import strategy. To add another code language, inspect the Add Other Code Language section. Note The Create strategy does not allow to customize the default code language set. Choose the necessary build tool in the Select Build Tool field: Java - selecting Java allows using the Gradle or Maven tool. JavaScript - selecting JavaScript allows using the NPM tool. .Net - selecting .Net allows using the .Net tool. Note The Select Build Tool field disposes of the default tools and can be changed in accordance with the selected code language. Select the Multi-Module Project check box that becomes available if the Java code language and the Maven build tool are selected. Click the Proceed button to be switched to the next menu. Note If your project is a multi-modular, add a property to the project root POM-file: <deployable.module> for a Maven project. <DeployableModule> for a DotNet project. The Advanced Settings Menu \u2693\ufe0e Select CI pipeline provisioner that will be handling a codebase. For details, refer to the Add Job Provision instruction and become familiar with the main steps to add an additional job provisioner. Select Jenkins slave that will be used to handle a codebase. For details, refer to the Add Jenkins Slave instruction and inspect the steps that should be done to add a new Jenkins slave. Select the necessary codebase versioning type: default - the previous versioning logic that is realized in EDP Admin Console 2.2.0 and lower versions. Using the default versioning type, in order to specify the version of the current artifacts, images, and tags in the Version Control System, a developer should navigate to the corresponding file and change the version manually . edp - the new versioning logic that is available in EDP Admin Console 2.3.0 and subsequent versions. Using the edp versioning type, a developer indicates the version number from which all the artifacts will be versioned and, as a result, automatically registered in the corresponding file (e.g. pom.xml). When selecting the edp versioning type, the extra field will appear: a. Type the version number from which you want the artifacts to be versioned. Note The Start Version From field should be filled out in compliance with the semantic versioning rules, e.g. 1.2.3 or 10.10.10. In the Select Deployment Script field, specify one of the available options: helm-chart / openshift-template that are predefined in case it is OpenShift or EKS. In the Select CI Tool field, choose the necessary tool: Jenkins or GitLab CI, where Jenkins is the default tool and the GitLab CI tool can be additionally adjusted. For details, please refer to the Adjust GitLab CI Tool page. Note The GitLab CI tool is available only with the Import strategy and makes the Jira integration feature unavailable. Select the Integrate with Jira Server checkbox in case it is required to connect Jira tickets with the commits and have a respective label in the Fix Version field. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Integration With Jira Server page, and setup the VCS Integration With Jira Server . Pay attention that the Jira integration feature is not available when using the GitLab CI tool. In the Select Jira Server field, select the Jira server. Indicate the pattern using any character, which is followed on the project, to validate a commit message. Indicate the pattern using any character, which is followed on the project, to find a Jira ticket number in a commit message. In the Advanced Mapping section, specify the names of the Jira fields that should be filled in with attributes from EDP. Upon clicking the question mark icon, observe the tips on how to indicate and combine variables necessary for identifying the format of values to be displayed. a. Select the name of the field in a Jira ticket. The available fields are the following: Fix Version/s , Component/s and Labels . b. Select the pattern of predefined variables, based on which the value from EDP will be displayed in Jira. Combine several variables to obtain the desired value. For the Fix Version/s field, select the EDP_VERSION variable that represents an EDP upgrade version, as in 2.7.0-SNAPSHOT . Combine variables to make the value more informative. For example, the pattern EDP_VERSION-EDP_COMPONENT will be displayed as 2.7.0-SNAPSHOT-nexus-operator in Jira; For the Component/s field, select the EDP_COMPONENT variable that defines the name of the existing repository. For example, nexus-operator ; For the Labels field, select the EDP_GITTAG variable that defines a tag assigned to the commit in GitHub. For example, build/2.7.0-SNAPSHOT.59 . c. Click the plus icon to add more Jira field names. d. Click the delete icon to remove the Jira field name. Select the Integrate with Perf Server checkbox in case it is required to connect to the PERF Board ( Project Performance Board ). Such functionality allows monitoring the overall team performance and setting up necessary metrics. Note To adjust the Perf Server integration functionality, first deploy Perf Operator. To get more information about the Perf Operator installation and architecture, please refer to the PERF Operator page. In the Select Perf Server field, select the name of the Perf server with which the integration should be performed. Click the Proceed button to be switched to the next menu. Select the necessary DataSource ( Jenkins/GitLab, Sonar ) from which the data should be transferred to the Project Performance Board. Click the Proceed button to be switched to the next menu. The Version Control System Info Menu \u2693\ufe0e Enter the login credentials into the VCS Login field. Enter the password into the VCS Password (or API Token) field OR add the API Token. Click the Proceed button to be switched to the next menu. Note The VCS Info step is skipped in case there is no need to integrate the version control for the application deployment. If the cloned application includes the VCS, this step should be completed as well. The Exposing Service Info Menu \u2693\ufe0e Select the Need Route check box to create a route component in the OpenShift project for the externally reachable host name. As a result, the added application will be accessible in a browser. Fill in the necessary fields and proceed to the final menu: Name \u2013 type the name by entering at least two characters and by using the lower-case letters, numbers and inner dashes. The mentioned name will be as a prefix for the host name. Path \u2013 specify the path starting with the /api characters. The mentioned path will be at the end of the URL path. Click the Proceed button. Once clicked, the CONFIRMATION summary will appear displaying all the specified options and settings, click Continue to complete the application addition. Note After the complete adding of the application, please refer to the Application Overview page. Related Articles \u2693\ufe0e Application Overview Delivery Dashboard Diagram Add CD Pipelines Adjust Integration With Jira Server Adjust VCS Integration With Jira Server","title":"Add Application"},{"location":"user-guide/add-application/#add-application","text":"Admin Console allows to create, clone, import an application and add it to the environment with its subsequent deployment in Gerrit and building of the Code Review and Build pipelines in Jenkins. To add an application, navigate to the Applications section on the left-side navigation bar and click the Create button. Once clicked, the six-step menu will appear: The Codebase Info Menu The Application Info Menu The Advanced Settings Menu The Version Control System Info Menu The Exposing Service Info Menu The Database Menu Note The Version Control System Info menu is available in case this option is predefined","title":"Add Application"},{"location":"user-guide/add-application/#the-codebase-info-menu","text":"In the Codebase Integration Strategy field, select the necessary option that is the configuration strategy for the replication with Gerrit: Create \u2013 creates a project on the pattern in accordance with an application language, a build tool, and a framework. Clone \u2013 clones the indicated repository into EPAM Delivery Platform. While cloning the existing repository, you have to fill in the additional fields as well. Import - allows configuring a replication from the Git server. While importing the existing repository, you have to select the Git server and define the respective path to the repository. Note In order to use the import strategy, make sure to adjust it by following the Adjust Import Strategy page. In the Git Repository URL field, specify the link to the repository that is to be cloned. If the Import strategy is selected, specify the following fields: a. Git Server where the repository is located. b. Relative path to the repository on the server. Select the Codebase Authentication check box and fill in the requested fields: Repository Login \u2013 enter your login data. Repository password (or API Token) \u2013 enter your password or indicate the API Token. Note The Codebase Authentication check box should be selected just in case you clone the private repository. If you define the public one, there is no need to enter credentials. Click the Proceed button to be switched to the next menu.","title":"The Codebase Info Menu"},{"location":"user-guide/add-application/#the-application-info-menu","text":"Type the name of the application in the Application Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Note If the Import strategy is used, the Application Name field will not be displayed. Specify the name of the default branch where you want the development to be performed. Note The default branch cannot be deleted. Select any of the supported application languages with its framework in the Application Code Language/framework field: Java \u2013 selecting Java allows using Java 8 or Java 11. JavaScript - selecting JavaScript allows using the React framework. DotNet - selecting DotNet allows using the DotNet v.2.1 and DotNet v.3.1. Go - selecting Go allows using the Beego and Operator SDK frameworks. Python - selecting Python allows using the Python v.3.8. Other - selecting Other allows extending the default code languages when creating a codebase with the clone/import strategy. To add another code language, inspect the Add Other Code Language section. Note The Create strategy does not allow to customize the default code language set. Choose the necessary build tool in the Select Build Tool field: Java - selecting Java allows using the Gradle or Maven tool. JavaScript - selecting JavaScript allows using the NPM tool. .Net - selecting .Net allows using the .Net tool. Note The Select Build Tool field disposes of the default tools and can be changed in accordance with the selected code language. Select the Multi-Module Project check box that becomes available if the Java code language and the Maven build tool are selected. Click the Proceed button to be switched to the next menu. Note If your project is a multi-modular, add a property to the project root POM-file: <deployable.module> for a Maven project. <DeployableModule> for a DotNet project.","title":"The Application Info Menu"},{"location":"user-guide/add-application/#the-advanced-settings-menu","text":"Select CI pipeline provisioner that will be handling a codebase. For details, refer to the Add Job Provision instruction and become familiar with the main steps to add an additional job provisioner. Select Jenkins slave that will be used to handle a codebase. For details, refer to the Add Jenkins Slave instruction and inspect the steps that should be done to add a new Jenkins slave. Select the necessary codebase versioning type: default - the previous versioning logic that is realized in EDP Admin Console 2.2.0 and lower versions. Using the default versioning type, in order to specify the version of the current artifacts, images, and tags in the Version Control System, a developer should navigate to the corresponding file and change the version manually . edp - the new versioning logic that is available in EDP Admin Console 2.3.0 and subsequent versions. Using the edp versioning type, a developer indicates the version number from which all the artifacts will be versioned and, as a result, automatically registered in the corresponding file (e.g. pom.xml). When selecting the edp versioning type, the extra field will appear: a. Type the version number from which you want the artifacts to be versioned. Note The Start Version From field should be filled out in compliance with the semantic versioning rules, e.g. 1.2.3 or 10.10.10. In the Select Deployment Script field, specify one of the available options: helm-chart / openshift-template that are predefined in case it is OpenShift or EKS. In the Select CI Tool field, choose the necessary tool: Jenkins or GitLab CI, where Jenkins is the default tool and the GitLab CI tool can be additionally adjusted. For details, please refer to the Adjust GitLab CI Tool page. Note The GitLab CI tool is available only with the Import strategy and makes the Jira integration feature unavailable. Select the Integrate with Jira Server checkbox in case it is required to connect Jira tickets with the commits and have a respective label in the Fix Version field. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Integration With Jira Server page, and setup the VCS Integration With Jira Server . Pay attention that the Jira integration feature is not available when using the GitLab CI tool. In the Select Jira Server field, select the Jira server. Indicate the pattern using any character, which is followed on the project, to validate a commit message. Indicate the pattern using any character, which is followed on the project, to find a Jira ticket number in a commit message. In the Advanced Mapping section, specify the names of the Jira fields that should be filled in with attributes from EDP. Upon clicking the question mark icon, observe the tips on how to indicate and combine variables necessary for identifying the format of values to be displayed. a. Select the name of the field in a Jira ticket. The available fields are the following: Fix Version/s , Component/s and Labels . b. Select the pattern of predefined variables, based on which the value from EDP will be displayed in Jira. Combine several variables to obtain the desired value. For the Fix Version/s field, select the EDP_VERSION variable that represents an EDP upgrade version, as in 2.7.0-SNAPSHOT . Combine variables to make the value more informative. For example, the pattern EDP_VERSION-EDP_COMPONENT will be displayed as 2.7.0-SNAPSHOT-nexus-operator in Jira; For the Component/s field, select the EDP_COMPONENT variable that defines the name of the existing repository. For example, nexus-operator ; For the Labels field, select the EDP_GITTAG variable that defines a tag assigned to the commit in GitHub. For example, build/2.7.0-SNAPSHOT.59 . c. Click the plus icon to add more Jira field names. d. Click the delete icon to remove the Jira field name. Select the Integrate with Perf Server checkbox in case it is required to connect to the PERF Board ( Project Performance Board ). Such functionality allows monitoring the overall team performance and setting up necessary metrics. Note To adjust the Perf Server integration functionality, first deploy Perf Operator. To get more information about the Perf Operator installation and architecture, please refer to the PERF Operator page. In the Select Perf Server field, select the name of the Perf server with which the integration should be performed. Click the Proceed button to be switched to the next menu. Select the necessary DataSource ( Jenkins/GitLab, Sonar ) from which the data should be transferred to the Project Performance Board. Click the Proceed button to be switched to the next menu.","title":"The Advanced Settings Menu"},{"location":"user-guide/add-application/#the-version-control-system-info-menu","text":"Enter the login credentials into the VCS Login field. Enter the password into the VCS Password (or API Token) field OR add the API Token. Click the Proceed button to be switched to the next menu. Note The VCS Info step is skipped in case there is no need to integrate the version control for the application deployment. If the cloned application includes the VCS, this step should be completed as well.","title":"The Version Control System Info Menu"},{"location":"user-guide/add-application/#the-exposing-service-info-menu","text":"Select the Need Route check box to create a route component in the OpenShift project for the externally reachable host name. As a result, the added application will be accessible in a browser. Fill in the necessary fields and proceed to the final menu: Name \u2013 type the name by entering at least two characters and by using the lower-case letters, numbers and inner dashes. The mentioned name will be as a prefix for the host name. Path \u2013 specify the path starting with the /api characters. The mentioned path will be at the end of the URL path. Click the Proceed button. Once clicked, the CONFIRMATION summary will appear displaying all the specified options and settings, click Continue to complete the application addition. Note After the complete adding of the application, please refer to the Application Overview page.","title":"The Exposing Service Info Menu"},{"location":"user-guide/add-application/#related-articles","text":"Application Overview Delivery Dashboard Diagram Add CD Pipelines Adjust Integration With Jira Server Adjust VCS Integration With Jira Server","title":"Related Articles"},{"location":"user-guide/add-autotest/","text":"Add Autotests \u2693\ufe0e Admin Console enables to clone or import an autotest and add it to the environment with its subsequent deployment in Gerrit and building of the Code Review pipeline in Jenkins. Navigate to the Autotests section on the left-side navigation bar and click the Create button. Once clicked, the four-step menu will appear: The Codebase Info Menu The Autotest Info Menu The Advanced Settings Menu The Version Control System Info Menu The Codebase Info Menu \u2693\ufe0e There are two available strategies: clone and import. The Clone strategy flow is displayed below: Clone - this strategy allows cloning the autotest from the indicated repository into EPAM Delivery Platform. While cloning the existing repository, you have to fill in the additional fields as well. In the Git Repository URL field, specify the link to the repository with the autotest. Select the Codebase Authentication check box and fill in the requested fields: Repository Login \u2013 enter your login data. Repository password (or API Token) \u2013 enter your password or indicate the API Token. If there is a necessity to use the Import strategy that allows configuring a replication from the Git server, explore the steps below: a. Import - this strategy allows configuring a replication from the Git server. Note In order to use the import strategy, make sure to adjust it by following the Adjust Import Strategy page. b. In the Git Server field, select the necessary Git server from the drop-down list. c. In the Relative path field, indicate the respective path to the repository, e.g. /epmd-edp/examples/basic/edp-auto-tests-simple-example . After completing the Codebase Info menu step, click the Proceed button to be switched to the next menu. The Autotest Info Menu \u2693\ufe0e Fill in the Autotest Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Info The Import strategy does not have an Autotest Name field. Specify the name of the default branch where you want the development to be performed. Note The default branch cannot be deleted. Type the necessary description in the Description field. In the Autotest Code Language field, select the Java code language (specify Java 8 or Java 11 to be used) and get the default Maven build tool OR add another code language. Selecting Other allows extending the default code languages and get the necessary build tool, for details, inspect the Add Other Code Language section. The Select Build Tool field can dispose of the default Maven tool, Gradle or other built tool in accordance with the selected code language. All the autotest reports will be created in the Allure framework that is available In the Autotest Report Framework field by default. Click the Proceed button to be switched to the next menu. The Advanced Settings Menu \u2693\ufe0e Select CI pipeline provisioner that will be used to handle a codebase. For details, refer to the Add Job Provision instruction and become familiar with the main steps to add an additional job provisioner. Select Jenkins slave that will be used to handle a codebase. For details, refer to the Add Jenkins Slave instruction and inspect the steps that should be done to add a new Jenkins slave. Select the necessary codebase versioning type: default - the previous versioning logic that is realized in EDP Admin Console 2.2.0 and lower versions. Using the default versioning type, in order to specify the version of the current artifacts, images, and tags in the Version Control System, a developer should navigate to the corresponding file and change the version manually . edp - the new versioning logic that is available in EDP Admin Console 2.3.0 and subsequent versions. Using the edp versioning type, a developer indicates the version number from which all the artifacts will be versioned and, as a result, automatically registered in the corresponding file (e.g. pom.xml). When selecting the edp versioning type, the extra field will appear: a. Type the version number from which you want the artifacts to be versioned. Note The Start Version From field should be filled out in compliance with the semantic versioning rules, e.g. 1.2.3 or 10.10.10. In the Select CI Tool field, choose the necessary tool: Jenkins or GitLab CI, where Jenkins is the default tool and the GitLab CI tool can be additionally adjusted. For details, please refer to the Adjust GitLab CI Tool page. Note The GitLab CI tool is available only with the Import strategy and makes the Jira integration feature unavailable. Select the Integrate with Jira Server checkbox in case it is required to connect Jira tickets with the commits and have a respective label in the Fix Version field. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Integration With Jira Server page, and setup the VCS Integration With Jira Server . Pay attention that the Jira integration feature is not available when using the GitLab CI tool. As soon as the Jira server is set, select it in the Select Jira Server field. Indicate the pattern using any character, which is followed on the project, to validate a commit message. Indicate the pattern using any character, which is followed on the project, to find a Jira ticket number in a commit message. In the Advanced Mapping section, specify the names of the Jira fields that should be filled in with attributes from EDP. Upon clicking the question mark icon, observe the tips on how to indicate and combine variables necessary for identifying the format of values to be displayed. a. Select the name of the field in a Jira ticket. The available fields are the following: Fix Version/s , Component/s and Labels . b. Select the pattern of predefined variables, based on which the value from EDP will be displayed in Jira. Combine several variables to obtain the desired value. For the Fix Version/s field, select the EDP_VERSION variable that represents an EDP upgrade version, as in 2.7.0-SNAPSHOT . Combine variables to make the value more informative. For example, the pattern EDP_VERSION-EDP_COMPONENT will be displayed as 2.7.0-SNAPSHOT-nexus-operator in Jira; For the Component/s field, select the EDP_COMPONENT variable that defines the name of the existing repository. For example, nexus-operator ; For the Labels field, select the EDP_GITTAG variable that defines a tag assigned to the commit in GitHub. For example, build/2.7.0-SNAPSHOT.59 . c. Click the plus icon to add more Jira field names. d. Click the delete icon to remove the Jira field name. Select the Integrate with Perf Server checkbox in case it is required to connect to the PERF Board ( Project Performance Board ). Such functionality allows monitoring the overall team performance and setting up necessary metrics. Note To adjust the Perf Server integration functionality, first deploy Perf Operator. To get more information about the Perf Operator installation and architecture, please refer to the PERF Operator page. In the Select Perf Server field, select the name of the Perf server with which the integration should be performed and click the Proceed button to be switched to the next menu. Select the necessary DataSource ( Jenkins/GitLab, Sonar ) from which the data should be transferred to the Project Performance Board. Click the Create button to create an autotest or click the Proceed button to be switched to the next VCS menu that can be predefined. The Version Control System Info Menu \u2693\ufe0e Once navigated to the VCS Info menu, perform the following: Enter the login credentials into the VCS Login field. Enter the password into the VCS Password (or API Token) field OR add the API Token. Click the Create button, check the CONFIRMATION summary, click Continue to add an autotest to the Autotests list. Note After the complete adding of the autotest, inspect the Autotest Overview part. Related Articles \u2693\ufe0e Autotest Overview Delivery Dashboard Diagram Add CD Pipelines Adjust Integration With Jira Server Adjust VCS Integration With Jira Server","title":"Add Autotests"},{"location":"user-guide/add-autotest/#add-autotests","text":"Admin Console enables to clone or import an autotest and add it to the environment with its subsequent deployment in Gerrit and building of the Code Review pipeline in Jenkins. Navigate to the Autotests section on the left-side navigation bar and click the Create button. Once clicked, the four-step menu will appear: The Codebase Info Menu The Autotest Info Menu The Advanced Settings Menu The Version Control System Info Menu","title":"Add Autotests"},{"location":"user-guide/add-autotest/#the-codebase-info-menu","text":"There are two available strategies: clone and import. The Clone strategy flow is displayed below: Clone - this strategy allows cloning the autotest from the indicated repository into EPAM Delivery Platform. While cloning the existing repository, you have to fill in the additional fields as well. In the Git Repository URL field, specify the link to the repository with the autotest. Select the Codebase Authentication check box and fill in the requested fields: Repository Login \u2013 enter your login data. Repository password (or API Token) \u2013 enter your password or indicate the API Token. If there is a necessity to use the Import strategy that allows configuring a replication from the Git server, explore the steps below: a. Import - this strategy allows configuring a replication from the Git server. Note In order to use the import strategy, make sure to adjust it by following the Adjust Import Strategy page. b. In the Git Server field, select the necessary Git server from the drop-down list. c. In the Relative path field, indicate the respective path to the repository, e.g. /epmd-edp/examples/basic/edp-auto-tests-simple-example . After completing the Codebase Info menu step, click the Proceed button to be switched to the next menu.","title":"The Codebase Info Menu"},{"location":"user-guide/add-autotest/#the-autotest-info-menu","text":"Fill in the Autotest Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Info The Import strategy does not have an Autotest Name field. Specify the name of the default branch where you want the development to be performed. Note The default branch cannot be deleted. Type the necessary description in the Description field. In the Autotest Code Language field, select the Java code language (specify Java 8 or Java 11 to be used) and get the default Maven build tool OR add another code language. Selecting Other allows extending the default code languages and get the necessary build tool, for details, inspect the Add Other Code Language section. The Select Build Tool field can dispose of the default Maven tool, Gradle or other built tool in accordance with the selected code language. All the autotest reports will be created in the Allure framework that is available In the Autotest Report Framework field by default. Click the Proceed button to be switched to the next menu.","title":"The Autotest Info Menu"},{"location":"user-guide/add-autotest/#the-advanced-settings-menu","text":"Select CI pipeline provisioner that will be used to handle a codebase. For details, refer to the Add Job Provision instruction and become familiar with the main steps to add an additional job provisioner. Select Jenkins slave that will be used to handle a codebase. For details, refer to the Add Jenkins Slave instruction and inspect the steps that should be done to add a new Jenkins slave. Select the necessary codebase versioning type: default - the previous versioning logic that is realized in EDP Admin Console 2.2.0 and lower versions. Using the default versioning type, in order to specify the version of the current artifacts, images, and tags in the Version Control System, a developer should navigate to the corresponding file and change the version manually . edp - the new versioning logic that is available in EDP Admin Console 2.3.0 and subsequent versions. Using the edp versioning type, a developer indicates the version number from which all the artifacts will be versioned and, as a result, automatically registered in the corresponding file (e.g. pom.xml). When selecting the edp versioning type, the extra field will appear: a. Type the version number from which you want the artifacts to be versioned. Note The Start Version From field should be filled out in compliance with the semantic versioning rules, e.g. 1.2.3 or 10.10.10. In the Select CI Tool field, choose the necessary tool: Jenkins or GitLab CI, where Jenkins is the default tool and the GitLab CI tool can be additionally adjusted. For details, please refer to the Adjust GitLab CI Tool page. Note The GitLab CI tool is available only with the Import strategy and makes the Jira integration feature unavailable. Select the Integrate with Jira Server checkbox in case it is required to connect Jira tickets with the commits and have a respective label in the Fix Version field. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Integration With Jira Server page, and setup the VCS Integration With Jira Server . Pay attention that the Jira integration feature is not available when using the GitLab CI tool. As soon as the Jira server is set, select it in the Select Jira Server field. Indicate the pattern using any character, which is followed on the project, to validate a commit message. Indicate the pattern using any character, which is followed on the project, to find a Jira ticket number in a commit message. In the Advanced Mapping section, specify the names of the Jira fields that should be filled in with attributes from EDP. Upon clicking the question mark icon, observe the tips on how to indicate and combine variables necessary for identifying the format of values to be displayed. a. Select the name of the field in a Jira ticket. The available fields are the following: Fix Version/s , Component/s and Labels . b. Select the pattern of predefined variables, based on which the value from EDP will be displayed in Jira. Combine several variables to obtain the desired value. For the Fix Version/s field, select the EDP_VERSION variable that represents an EDP upgrade version, as in 2.7.0-SNAPSHOT . Combine variables to make the value more informative. For example, the pattern EDP_VERSION-EDP_COMPONENT will be displayed as 2.7.0-SNAPSHOT-nexus-operator in Jira; For the Component/s field, select the EDP_COMPONENT variable that defines the name of the existing repository. For example, nexus-operator ; For the Labels field, select the EDP_GITTAG variable that defines a tag assigned to the commit in GitHub. For example, build/2.7.0-SNAPSHOT.59 . c. Click the plus icon to add more Jira field names. d. Click the delete icon to remove the Jira field name. Select the Integrate with Perf Server checkbox in case it is required to connect to the PERF Board ( Project Performance Board ). Such functionality allows monitoring the overall team performance and setting up necessary metrics. Note To adjust the Perf Server integration functionality, first deploy Perf Operator. To get more information about the Perf Operator installation and architecture, please refer to the PERF Operator page. In the Select Perf Server field, select the name of the Perf server with which the integration should be performed and click the Proceed button to be switched to the next menu. Select the necessary DataSource ( Jenkins/GitLab, Sonar ) from which the data should be transferred to the Project Performance Board. Click the Create button to create an autotest or click the Proceed button to be switched to the next VCS menu that can be predefined.","title":"The Advanced Settings Menu"},{"location":"user-guide/add-autotest/#the-version-control-system-info-menu","text":"Once navigated to the VCS Info menu, perform the following: Enter the login credentials into the VCS Login field. Enter the password into the VCS Password (or API Token) field OR add the API Token. Click the Create button, check the CONFIRMATION summary, click Continue to add an autotest to the Autotests list. Note After the complete adding of the autotest, inspect the Autotest Overview part.","title":"The Version Control System Info Menu"},{"location":"user-guide/add-autotest/#related-articles","text":"Autotest Overview Delivery Dashboard Diagram Add CD Pipelines Adjust Integration With Jira Server Adjust VCS Integration With Jira Server","title":"Related Articles"},{"location":"user-guide/add-cd-pipeline/","text":"Add CD pipeline \u2693\ufe0e Admin Console provides the ability to deploy an environment on your own and specify the essential components as well. Navigate to the Continuous Delivery section on the left-side navigation bar and click the Create button. Once clicked, the four-step menu will appear: The Pipeline Menu The Applications Menu The Stages Menu The creation of the CD pipeline becomes available as soon as an application is created including its provisioning in a branch and the necessary entities for the environment. After the complete adding of the CD pipeline, inspect the Check CD Pipeline Availability part. The Pipeline Menu \u2693\ufe0e Type the name of the pipeline in the Pipeline Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Click the Proceed button to be switched to the next menu. The Applications Menu \u2693\ufe0e Select the check box of the necessary application in the Applications menu. Specify the necessary codebase Docker stream (the output for the branch and other stages from other CD pipelines) from the drop-down menu. Select the Promote in pipeline check box in order to transfer the application from one to another stage by the specified codebase Docker stream. If the Promote in pipeline check box is not selected, the same codebase Docker stream will be deployed regardless of the stage, i.e. the codebase Docker stream input, which was selected for the pipeline, will be always used. Note The newly created CD pipeline has the following pattern combination: [pipeline name]-[branch name]. If there is another deployed CD pipeline stage with the respective codebase Docker stream (= image stream as an OpenShift term), the pattern combination will be as follows: [pipeline name]-[stage name]-[application name]-[verified]; Click the Proceed button to be switched to the next menu. The Stages Menu \u2693\ufe0e Click the plus sign icon in the Stages menu and fill in the necessary fields in the Adding Stage window: a. Type the stage name; b. Enter the description for this stage; c. Select the quality gate type: Manual - means that the promoting process should be confirmed in Jenkins manually; Autotests - means that the promoting process should be confirmed by the successful passing of the autotests. In the additional fields, select the previously created autotest name and specify its branch for the autotest that will be launched on the current stage. d. Type the step name, which will be displayed in Jenkins, for every quality gate type; e. Select the trigger type that allows promoting images to the next environment. The available trigger types are manual and auto . By selecting the auto trigger type, the CD pipeline will be launched automatically. Info Add an unlimited number of quality gates by clicking a corresponding plus sign icon and remove them as well by clicking the recycle bin icon. Note Execution sequence. The image promotion and execution of the pipelines depend on the sequence in which the environments are added. Click the Add button to display it in the Stages menu: Info Perform the same steps as described above if there is a necessity to add one more stage. Edit the stage by clicking its name and applying changes, and remove the added stage by clicking the recycle bin icon next to its name. Click the Create button to start the provisioning of the pipeline. After the CD pipeline is added, the new project with the stage name will be created in OpenShift. Check CD Pipeline Availability \u2693\ufe0e As soon as the CD pipeline is provisioned and added to the CD Pipelines list, there is an ability to: Create another application by clicking the Create button and performing the same steps as described in the Add CD Pipeline section. Select a number of existing CD pipelines to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing CD pipelines in a list by clicking the Name title. The CD pipelines will be displayed in alphabetical order. Search the necessary CD pipeline by entering the corresponding name, language or the build tool into the Search field. Navigate between pages if the number of CD pipelines exceeds the capacity of a single page. Edit CD Pipeline \u2693\ufe0e Edit the CD pipeline by clicking the pen icon next to its name in the CD Pipelines list: apply the necessary changes and click the Proceed button to confirm the editions: add new extra stages steps by clicking the plus sign icon and filling in the necessary fields in the Adding Stage window. Note The added stage will appear in the Stages menu allowing to review its details or delete. Check the CD pipeline data and details by clicking the CD pipeline name in the CD Pipelines list: the main link on the top of the details page refers to Jenkins; the pen icon refers to the same Edit CD Pipeline page as mentioned above and allows to apply the necessary changes; the Applications menu has the main information about the applications with the respective codebase Docker streams and links to Jenkins and Gerrit as well as the signification of the promotion in CD pipeline; the Stages menu includes the stages data that was previously mentioned, the direct links to the respective to every stage OpenShift page, and the link to the Autotest details page in case there are added autotests. Note The deletion of stages is performed sequentially, starting from the latest created stage. In order to remove a stage , click the corresponding delete icon, type the CD pipeline name and confirm the deletion by clicking the Delete button. If you remove the last stage, the whole CD pipeline will be removed as the CD pipeline does not exist without stages. the Deployed Version menu indicates the applications and stages with the appropriate status. The status will be changed after stage deployment. the Status Info menu displays all the actions that were performed during the deployment process: Remove the added CD pipeline: Info If there is a necessity to create another CD pipeline, navigate to the Continuous Delivery section, click the Create button and perform the same steps as described above. Related Articles \u2693\ufe0e EDP Admin Console Delivery Dashboard Diagram","title":"Add CD pipeline"},{"location":"user-guide/add-cd-pipeline/#add-cd-pipeline","text":"Admin Console provides the ability to deploy an environment on your own and specify the essential components as well. Navigate to the Continuous Delivery section on the left-side navigation bar and click the Create button. Once clicked, the four-step menu will appear: The Pipeline Menu The Applications Menu The Stages Menu The creation of the CD pipeline becomes available as soon as an application is created including its provisioning in a branch and the necessary entities for the environment. After the complete adding of the CD pipeline, inspect the Check CD Pipeline Availability part.","title":"Add CD pipeline"},{"location":"user-guide/add-cd-pipeline/#the-pipeline-menu","text":"Type the name of the pipeline in the Pipeline Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Click the Proceed button to be switched to the next menu.","title":"The Pipeline Menu"},{"location":"user-guide/add-cd-pipeline/#the-applications-menu","text":"Select the check box of the necessary application in the Applications menu. Specify the necessary codebase Docker stream (the output for the branch and other stages from other CD pipelines) from the drop-down menu. Select the Promote in pipeline check box in order to transfer the application from one to another stage by the specified codebase Docker stream. If the Promote in pipeline check box is not selected, the same codebase Docker stream will be deployed regardless of the stage, i.e. the codebase Docker stream input, which was selected for the pipeline, will be always used. Note The newly created CD pipeline has the following pattern combination: [pipeline name]-[branch name]. If there is another deployed CD pipeline stage with the respective codebase Docker stream (= image stream as an OpenShift term), the pattern combination will be as follows: [pipeline name]-[stage name]-[application name]-[verified]; Click the Proceed button to be switched to the next menu.","title":"The Applications Menu"},{"location":"user-guide/add-cd-pipeline/#the-stages-menu","text":"Click the plus sign icon in the Stages menu and fill in the necessary fields in the Adding Stage window: a. Type the stage name; b. Enter the description for this stage; c. Select the quality gate type: Manual - means that the promoting process should be confirmed in Jenkins manually; Autotests - means that the promoting process should be confirmed by the successful passing of the autotests. In the additional fields, select the previously created autotest name and specify its branch for the autotest that will be launched on the current stage. d. Type the step name, which will be displayed in Jenkins, for every quality gate type; e. Select the trigger type that allows promoting images to the next environment. The available trigger types are manual and auto . By selecting the auto trigger type, the CD pipeline will be launched automatically. Info Add an unlimited number of quality gates by clicking a corresponding plus sign icon and remove them as well by clicking the recycle bin icon. Note Execution sequence. The image promotion and execution of the pipelines depend on the sequence in which the environments are added. Click the Add button to display it in the Stages menu: Info Perform the same steps as described above if there is a necessity to add one more stage. Edit the stage by clicking its name and applying changes, and remove the added stage by clicking the recycle bin icon next to its name. Click the Create button to start the provisioning of the pipeline. After the CD pipeline is added, the new project with the stage name will be created in OpenShift.","title":"The Stages Menu"},{"location":"user-guide/add-cd-pipeline/#check-cd-pipeline-availability","text":"As soon as the CD pipeline is provisioned and added to the CD Pipelines list, there is an ability to: Create another application by clicking the Create button and performing the same steps as described in the Add CD Pipeline section. Select a number of existing CD pipelines to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing CD pipelines in a list by clicking the Name title. The CD pipelines will be displayed in alphabetical order. Search the necessary CD pipeline by entering the corresponding name, language or the build tool into the Search field. Navigate between pages if the number of CD pipelines exceeds the capacity of a single page.","title":"Check CD Pipeline Availability"},{"location":"user-guide/add-cd-pipeline/#edit-cd-pipeline","text":"Edit the CD pipeline by clicking the pen icon next to its name in the CD Pipelines list: apply the necessary changes and click the Proceed button to confirm the editions: add new extra stages steps by clicking the plus sign icon and filling in the necessary fields in the Adding Stage window. Note The added stage will appear in the Stages menu allowing to review its details or delete. Check the CD pipeline data and details by clicking the CD pipeline name in the CD Pipelines list: the main link on the top of the details page refers to Jenkins; the pen icon refers to the same Edit CD Pipeline page as mentioned above and allows to apply the necessary changes; the Applications menu has the main information about the applications with the respective codebase Docker streams and links to Jenkins and Gerrit as well as the signification of the promotion in CD pipeline; the Stages menu includes the stages data that was previously mentioned, the direct links to the respective to every stage OpenShift page, and the link to the Autotest details page in case there are added autotests. Note The deletion of stages is performed sequentially, starting from the latest created stage. In order to remove a stage , click the corresponding delete icon, type the CD pipeline name and confirm the deletion by clicking the Delete button. If you remove the last stage, the whole CD pipeline will be removed as the CD pipeline does not exist without stages. the Deployed Version menu indicates the applications and stages with the appropriate status. The status will be changed after stage deployment. the Status Info menu displays all the actions that were performed during the deployment process: Remove the added CD pipeline: Info If there is a necessity to create another CD pipeline, navigate to the Continuous Delivery section, click the Create button and perform the same steps as described above.","title":"Edit CD Pipeline"},{"location":"user-guide/add-cd-pipeline/#related-articles","text":"EDP Admin Console Delivery Dashboard Diagram","title":"Related Articles"},{"location":"user-guide/add-custom-global-pipeline-lib/","text":"Add a Custom Global Pipeline Library \u2693\ufe0e In order to add a new custom global pipeline library, perform the steps below: Navigate to Jenkins, go to Manage Jenkins -> Configure System -> Global Pipeline Libraries as many libraries as necessary can be configured. Since these libraries will be globally usable, any Pipeline in the system can utilize functionality implemented in these libraries. Specify the following values: a - The name of a custom library; b - The version which can be branched, tagged or hashed of a commit. c - Allows pipelines using immediately classes or global variables defined by any libraries. d and e - Allows using the default version of the configured shared-library when the \"Load implicitly\" check box is selected, or if the pipeline references to the library only by the name, for example, @Library('my-shared-library') . Note If the \"Default version\" check box is not defined , the pipeline must specify a version, for example, @Library('my-shared-library@master') . If the \"Allow default version to be overridden\" check box is enabled in the Shared Library\u2019s configuration, a @Library annotation may also override the default version defined for the library. This also enables the library with the selected \"Load implicitly\" check box to be loaded from a different version if necessary. f - The URL of the repository; g - The credentials for the repository. 3.Use the Custom Global Pipeline Libraries on the pipeline, for example: Pipeline @Library ( [ ' edp - custom - shared - library - name ' ] ) _ Build ()","title":"Add custom pipline library"},{"location":"user-guide/add-custom-global-pipeline-lib/#add-a-custom-global-pipeline-library","text":"In order to add a new custom global pipeline library, perform the steps below: Navigate to Jenkins, go to Manage Jenkins -> Configure System -> Global Pipeline Libraries as many libraries as necessary can be configured. Since these libraries will be globally usable, any Pipeline in the system can utilize functionality implemented in these libraries. Specify the following values: a - The name of a custom library; b - The version which can be branched, tagged or hashed of a commit. c - Allows pipelines using immediately classes or global variables defined by any libraries. d and e - Allows using the default version of the configured shared-library when the \"Load implicitly\" check box is selected, or if the pipeline references to the library only by the name, for example, @Library('my-shared-library') . Note If the \"Default version\" check box is not defined , the pipeline must specify a version, for example, @Library('my-shared-library@master') . If the \"Allow default version to be overridden\" check box is enabled in the Shared Library\u2019s configuration, a @Library annotation may also override the default version defined for the library. This also enables the library with the selected \"Load implicitly\" check box to be loaded from a different version if necessary. f - The URL of the repository; g - The credentials for the repository. 3.Use the Custom Global Pipeline Libraries on the pipeline, for example: Pipeline @Library ( [ ' edp - custom - shared - library - name ' ] ) _ Build ()","title":"Add a Custom Global Pipeline Library"},{"location":"user-guide/add-library/","text":"Add Library \u2693\ufe0e Admin Console helps to create, clone or import a library and add it to the environment with its subsequent deployment in Gerrit and building of the Code Review and Build pipelines in Jenkins. Navigate to the Libraries section on the left-side navigation bar and click the Create button. Once clicked, the four-step menu will appear: The Codebase Info Menu The Library Info Menu The Advanced Settings Menu The Version Control System Info Menu Note The Version Control System Info menu is available in case this option is predefined. The Codebase Info Menu \u2693\ufe0e In the Codebase Integration Strategy field, select the necessary option that is the configuration strategy for the replication with Gerrit: Create \u2013 creates a project on the pattern in accordance with a code language, a build tool, and a framework. Clone \u2013 clones the indicated repository into EPAM Delivery Platform. Note While cloning the existing repository, you have to fill in the additional fields as well. Import - allows configuring a replication from the Git server. While importing the existing repository, you have to select the Git server and define the respective path to the repository. Note In order to use the import strategy, make sure to adjust it by following the Adjust Import Strategy page. In the Git Repository URL field, specify the link to the repository that is to be cloned. Select the Codebase Authentication check box and fill in the requested fields: Repository Login \u2013 enter your login data. Repository password (or API Token) \u2013 enter your password or indicate the API Token. Click the Proceed button to be switched to the next menu. The Library Info Menu \u2693\ufe0e Type the name of the library in the Library Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Info If the Import strategy is used, the Library Name field will not be displayed. Specify the name of the default branch where you want the development to be performed. Note The default branch cannot be deleted. Select any of the supported code languages in the Library Code Language block: Java \u2013 selecting Java allows specify Java 8 or Java 11, and further usage of the Gradle or Maven tool. JavaScript - selecting JavaScript allows using the NPM tool. DotNet - selecting DotNet allows using the DotNet v.2.1 and DotNet v.3.1. Groovy-pipeline - selecting Groovy-pipeline allows having the ability to customize a stages logic. For details, please refer to the Customize CD Pipeline page. Python - selecting Python allows using the Python v.3.8. Terraform - selecting Terraform allows using the Terraform different versions via the Terraform version manager ( tfenv ). EDP supports all actions available in Terraform, thus providing the ability to modify the virtual infrastructure and launch some checks with the help of linters. For details, please refer to the Use Terraform Library in EDP page. Other - selecting Other allows extending the default code languages when creating a codebase with the clone/import strategy. To add another code language, inspect the Add Other Code Language page. Note The Create strategy does not allow to customize the default code language set. The Select Build Tool field disposes of the default tools and can be changed in accordance with the selected code language. Click the Proceed button to be switched to the next menu. The Advanced Settings Menu \u2693\ufe0e Select the CI pipeline provisioner that will be used to handle a codebase. For details, refer to the Add Job Provision instruction and become familiar with the main steps to add an additional job provisioner. Select Jenkins slave that will be used to handle a codebase. For details, refer to the Add Jenkins Slave instruction and inspect the steps that should be done to add a new Jenkins slave. Select the necessary codebase versioning type: default - the previous versioning logic that is realized in EDP Admin Console 2.2.0 and lower versions. Using the default versioning type, in order to specify the version of the current artifacts, images, and tags in the Version Control System, a developer should navigate to the corresponding file and change the version manually . edp - the new versioning logic that is available in EDP Admin Console 2.3.0 and subsequent versions. Using the edp versioning type, a developer indicates the version number from which all the artifacts will be versioned and, as a result, automatically registered in the corresponding file (e.g. pom.xml). When selecting the edp versioning type, the extra field will appear: a. Type the version number from which you want the artifacts to be versioned. Note The Start Version From field should be filled out in compliance with the semantic versioning rules, e.g. 1.2.3 or 10.10.10. In the Select CI Tool field, choose the necessary tool: Jenkins or GitLab CI, where Jenkins is the default tool and the GitLab CI tool can be additionally adjusted. For details, please refer to the Adjust GitLab CI Tool page. Note The GitLab CI tool is available only with the Import strategy and makes the Jira integration feature unavailable. Select the Integrate with Jira Server checkbox in case it is required to connect Jira tickets with the commits and have a respective label in the Fix Version field. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Integration With Jira Server page, and setup the VCS Integration With Jira Server . Pay attention that the Jira integration feature is not available when using the GitLab CI tool. As soon as the Jira server is set, select it in the Select Jira Server field. Indicate the pattern using any character, which is followed on the project, to validate a commit message. Indicate the pattern using any character, which is followed on the project, to find a Jira ticket number in a commit message. In the Advanced Mapping section, specify the names of the Jira fields that should be filled in with attributes from EDP. Upon clicking the question mark icon, observe the tips on how to indicate and combine variables necessary for identifying the format of values to be displayed. a. Select the name of the field in a Jira ticket. The available fields are the following: Fix Version/s , Component/s and Labels . b. Select the pattern of predefined variables, based on which the value from EDP will be displayed in Jira. Combine several variables to obtain the desired value. For the Fix Version/s field, select the EDP_VERSION variable that represents an EDP upgrade version, as in 2.7.0-SNAPSHOT . Combine variables to make the value more informative. For example, the pattern EDP_VERSION-EDP_COMPONENT will be displayed as 2.7.0-SNAPSHOT-nexus-operator in Jira; For the Component/s field select the EDP_COMPONENT variable that defines the name of the existing repository. For example, nexus-operator ; For the Labels field select the EDP_GITTAG variable that defines a tag assigned to the commit in Git Hub. For example, build/2.7.0-SNAPSHOT.59 . c. Click the plus icon to add more Jira field names. d. Click the delete icon to remove the Jira field name. Select the Integrate with Perf Server checkbox in case it is required to connect to the PERF Board ( Project Performance Board ). Such functionality allows monitoring the overall team performance and setting up necessary metrics. Note To adjust the Perf Server integration functionality, first deploy Perf Operator. To get more information about the Perf Operator installation and architecture, please refer to the PERF Operator page. In the Select Perf Server field, select the name of the Perf server with which the integration should be performed. Click the Proceed button to be switched to the next menu. Select the necessary DataSource ( Jenkins/GitLab, Sonar ) from which the data should be transferred to the Project Performance Board. Click the Create button to create a library or click the Proceed button to be switched to the next VCS menu that can be predefined. The Version Control System Info Menu \u2693\ufe0e Enter the login credentials into the VCS Login field. Enter the password into the VCS Password (or API Token) field OR add the API Token. Click the Create button, check the CONFIRMATION summary, click Continue to add the library to the Libraries list. Note After the complete adding of the library, inspect the Library Overview part. Related Articles \u2693\ufe0e Library Overview Delivery Dashboard Diagram Add CD Pipeline Adjust Integration With Jira Server Adjust VCS Integration With Jira Server Use Terraform Library in EDP Use Open Policy Agent Library in EDP","title":"Add Library"},{"location":"user-guide/add-library/#add-library","text":"Admin Console helps to create, clone or import a library and add it to the environment with its subsequent deployment in Gerrit and building of the Code Review and Build pipelines in Jenkins. Navigate to the Libraries section on the left-side navigation bar and click the Create button. Once clicked, the four-step menu will appear: The Codebase Info Menu The Library Info Menu The Advanced Settings Menu The Version Control System Info Menu Note The Version Control System Info menu is available in case this option is predefined.","title":"Add Library"},{"location":"user-guide/add-library/#the-codebase-info-menu","text":"In the Codebase Integration Strategy field, select the necessary option that is the configuration strategy for the replication with Gerrit: Create \u2013 creates a project on the pattern in accordance with a code language, a build tool, and a framework. Clone \u2013 clones the indicated repository into EPAM Delivery Platform. Note While cloning the existing repository, you have to fill in the additional fields as well. Import - allows configuring a replication from the Git server. While importing the existing repository, you have to select the Git server and define the respective path to the repository. Note In order to use the import strategy, make sure to adjust it by following the Adjust Import Strategy page. In the Git Repository URL field, specify the link to the repository that is to be cloned. Select the Codebase Authentication check box and fill in the requested fields: Repository Login \u2013 enter your login data. Repository password (or API Token) \u2013 enter your password or indicate the API Token. Click the Proceed button to be switched to the next menu.","title":"The Codebase Info Menu"},{"location":"user-guide/add-library/#the-library-info-menu","text":"Type the name of the library in the Library Name field by entering at least two characters and by using the lower-case letters, numbers and inner dashes. Info If the Import strategy is used, the Library Name field will not be displayed. Specify the name of the default branch where you want the development to be performed. Note The default branch cannot be deleted. Select any of the supported code languages in the Library Code Language block: Java \u2013 selecting Java allows specify Java 8 or Java 11, and further usage of the Gradle or Maven tool. JavaScript - selecting JavaScript allows using the NPM tool. DotNet - selecting DotNet allows using the DotNet v.2.1 and DotNet v.3.1. Groovy-pipeline - selecting Groovy-pipeline allows having the ability to customize a stages logic. For details, please refer to the Customize CD Pipeline page. Python - selecting Python allows using the Python v.3.8. Terraform - selecting Terraform allows using the Terraform different versions via the Terraform version manager ( tfenv ). EDP supports all actions available in Terraform, thus providing the ability to modify the virtual infrastructure and launch some checks with the help of linters. For details, please refer to the Use Terraform Library in EDP page. Other - selecting Other allows extending the default code languages when creating a codebase with the clone/import strategy. To add another code language, inspect the Add Other Code Language page. Note The Create strategy does not allow to customize the default code language set. The Select Build Tool field disposes of the default tools and can be changed in accordance with the selected code language. Click the Proceed button to be switched to the next menu.","title":"The Library Info Menu"},{"location":"user-guide/add-library/#the-advanced-settings-menu","text":"Select the CI pipeline provisioner that will be used to handle a codebase. For details, refer to the Add Job Provision instruction and become familiar with the main steps to add an additional job provisioner. Select Jenkins slave that will be used to handle a codebase. For details, refer to the Add Jenkins Slave instruction and inspect the steps that should be done to add a new Jenkins slave. Select the necessary codebase versioning type: default - the previous versioning logic that is realized in EDP Admin Console 2.2.0 and lower versions. Using the default versioning type, in order to specify the version of the current artifacts, images, and tags in the Version Control System, a developer should navigate to the corresponding file and change the version manually . edp - the new versioning logic that is available in EDP Admin Console 2.3.0 and subsequent versions. Using the edp versioning type, a developer indicates the version number from which all the artifacts will be versioned and, as a result, automatically registered in the corresponding file (e.g. pom.xml). When selecting the edp versioning type, the extra field will appear: a. Type the version number from which you want the artifacts to be versioned. Note The Start Version From field should be filled out in compliance with the semantic versioning rules, e.g. 1.2.3 or 10.10.10. In the Select CI Tool field, choose the necessary tool: Jenkins or GitLab CI, where Jenkins is the default tool and the GitLab CI tool can be additionally adjusted. For details, please refer to the Adjust GitLab CI Tool page. Note The GitLab CI tool is available only with the Import strategy and makes the Jira integration feature unavailable. Select the Integrate with Jira Server checkbox in case it is required to connect Jira tickets with the commits and have a respective label in the Fix Version field. Note To adjust the Jira integration functionality, first apply the necessary changes described on the Adjust Integration With Jira Server page, and setup the VCS Integration With Jira Server . Pay attention that the Jira integration feature is not available when using the GitLab CI tool. As soon as the Jira server is set, select it in the Select Jira Server field. Indicate the pattern using any character, which is followed on the project, to validate a commit message. Indicate the pattern using any character, which is followed on the project, to find a Jira ticket number in a commit message. In the Advanced Mapping section, specify the names of the Jira fields that should be filled in with attributes from EDP. Upon clicking the question mark icon, observe the tips on how to indicate and combine variables necessary for identifying the format of values to be displayed. a. Select the name of the field in a Jira ticket. The available fields are the following: Fix Version/s , Component/s and Labels . b. Select the pattern of predefined variables, based on which the value from EDP will be displayed in Jira. Combine several variables to obtain the desired value. For the Fix Version/s field, select the EDP_VERSION variable that represents an EDP upgrade version, as in 2.7.0-SNAPSHOT . Combine variables to make the value more informative. For example, the pattern EDP_VERSION-EDP_COMPONENT will be displayed as 2.7.0-SNAPSHOT-nexus-operator in Jira; For the Component/s field select the EDP_COMPONENT variable that defines the name of the existing repository. For example, nexus-operator ; For the Labels field select the EDP_GITTAG variable that defines a tag assigned to the commit in Git Hub. For example, build/2.7.0-SNAPSHOT.59 . c. Click the plus icon to add more Jira field names. d. Click the delete icon to remove the Jira field name. Select the Integrate with Perf Server checkbox in case it is required to connect to the PERF Board ( Project Performance Board ). Such functionality allows monitoring the overall team performance and setting up necessary metrics. Note To adjust the Perf Server integration functionality, first deploy Perf Operator. To get more information about the Perf Operator installation and architecture, please refer to the PERF Operator page. In the Select Perf Server field, select the name of the Perf server with which the integration should be performed. Click the Proceed button to be switched to the next menu. Select the necessary DataSource ( Jenkins/GitLab, Sonar ) from which the data should be transferred to the Project Performance Board. Click the Create button to create a library or click the Proceed button to be switched to the next VCS menu that can be predefined.","title":"The Advanced Settings Menu"},{"location":"user-guide/add-library/#the-version-control-system-info-menu","text":"Enter the login credentials into the VCS Login field. Enter the password into the VCS Password (or API Token) field OR add the API Token. Click the Create button, check the CONFIRMATION summary, click Continue to add the library to the Libraries list. Note After the complete adding of the library, inspect the Library Overview part.","title":"The Version Control System Info Menu"},{"location":"user-guide/add-library/#related-articles","text":"Library Overview Delivery Dashboard Diagram Add CD Pipeline Adjust Integration With Jira Server Adjust VCS Integration With Jira Server Use Terraform Library in EDP Use Open Policy Agent Library in EDP","title":"Related Articles"},{"location":"user-guide/application/","text":"Application \u2693\ufe0e This section describes the subsequent possible actions that can be performed with the newly added or existing applications. Check and Remove Application \u2693\ufe0e As soon as the application is successfully provisioned, the following will be created: Code Review and Build pipelines in Jenkins for this application. The Build pipeline will be triggered automatically if at least one environment is already added. A new project in Gerrit or another VCS. SonarQube integration will be available after the Build pipeline in Jenkins is passed. Nexus Repository Manager will be available after the Build pipeline in Jenkins is passed as well. Info To navigate quickly to OpenShift, Jenkins, Gerrit, SonarQube, Nexus, and other resources, click the Overview section on the navigation bar and hit the necessary link. The added application will be listed in the Applications list allowing you to do the following: Create another application by clicking the Create button and performing the same steps as described in Add Applications section. Open application data by clicking its link name. Once clicked, the following blocks will be displayed: General Info - displays common information about the created/cloned/imported application. Advanced Settings - displays the specified job provisioner, Jenkins slave, deployment script, and the versioning type with the start versioning from number (the latter two fields appear in case of edp versioning type). Branches - displays the status and name of the deployment branch, keeps the additional links to Jenkins and Gerrit. In case of edp versioning type, there are two additional fields: Build Number - indicates the current build number; Last Successful Build - indicates the last successful build number. Status Info - displays all the actions that were performed during the creation/cloning/importing process. Edit the application codebase by clicking the pencil icon. For details see the Edit Existing Codebase section. Remove application with the corresponding database and Jenkins pipelines: Click the delete icon next to the application name; Type the required application name; Confirm the deletion by clicking the Delete button. Note The application that is used in a CD pipeline cannot be removed. Select a number of existing applications to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing applications in a list by clicking the Name title. The applications will be displayed in alphabetical order. Search the necessary application by entering the corresponding name, language or the build tool into the Search field. The search can be performed by the application name, language or a build tool. Navigate between pages if the number of applications exceeds the capacity of a single page. Add a New Branch \u2693\ufe0e When adding an application, the default branch is a master branch. In order to add a new branch, follow the steps below: Navigate to the Branches block and click the Create button: Fill in the required fields: a. Release Branch - select the Release Branch check box if you need to create a release branch; b. Branch Name - type the branch name. Pay attention that this field remain static if you create a release branch. c. From Commit Hash - paste the commit hash from which the new branch will be created. Note that if the From Commit Hash field is empty, the latest commit from the branch name will be used. d. Branch Version - enter the necessary branch version for the artifact. The Release Candidate (RC) postfix is concatenated to the branch version number. e. Master Branch Version - type the branch version that will be used in a master branch after the release creation. The Snapshot postfix is concatenated to the master branch version number; f. Click the Proceed button and wait until the new branch will be added to the list. Info Adding of a new branch is indicated in the context of the edp versioning type. To get more detailed information on how to add a branch using the default versioning type, please refer here . The default application repository is cloned and changed to the new indicated version before the build, i.e. the new indicated version will not be committed to the repository; thus, the existing repository will keep the default version. Edit Existing Codebase \u2693\ufe0e The EDP Admin Console provides the ability to enable, disable or edit the Jira Integration functionality for applications via the Edit Codebase page. Perform the editing from one of the following sections on the Admin Console interface: Navigate to the codebase overview page and click the pencil icon, or Navigate to the codebase list page and click the pencil icon. To enable Jira integration, on the Edit Codebase page do the following: mark the Integrate with Jira server checkbox and fill in the necessary fields; click the Proceed button to apply the changes; navigate to Jenkins and add the create-jira-issue-metadata stage in the Build pipeline. Also add the commit-validate stage in the Code-Review pipeline. To disable Jira integration, on the Edit Codebase page do the following: unmark the Integrate with Jira server checkbox; click the Proceed button to apply the changes; navigate to Jenkins and remove the create-jira-issue-metadata stage in the Build pipeline. Also remove the commit-validate stage in the Code-Review pipeline. As a result, the necessary changes will be applied. Remove Branch \u2693\ufe0e In order to remove the added branch with the corresponding record in the Admin Console database, do the following: Navigate to the Branches block by clicking the application name link in the Applications list; Click the delete icon related to the necessary branch: Enter the branch name and click the Delete button; Note The default master branch cannot be removed. Related Articles \u2693\ufe0e Add Application","title":"Overview"},{"location":"user-guide/application/#application","text":"This section describes the subsequent possible actions that can be performed with the newly added or existing applications.","title":"Application"},{"location":"user-guide/application/#check-and-remove-application","text":"As soon as the application is successfully provisioned, the following will be created: Code Review and Build pipelines in Jenkins for this application. The Build pipeline will be triggered automatically if at least one environment is already added. A new project in Gerrit or another VCS. SonarQube integration will be available after the Build pipeline in Jenkins is passed. Nexus Repository Manager will be available after the Build pipeline in Jenkins is passed as well. Info To navigate quickly to OpenShift, Jenkins, Gerrit, SonarQube, Nexus, and other resources, click the Overview section on the navigation bar and hit the necessary link. The added application will be listed in the Applications list allowing you to do the following: Create another application by clicking the Create button and performing the same steps as described in Add Applications section. Open application data by clicking its link name. Once clicked, the following blocks will be displayed: General Info - displays common information about the created/cloned/imported application. Advanced Settings - displays the specified job provisioner, Jenkins slave, deployment script, and the versioning type with the start versioning from number (the latter two fields appear in case of edp versioning type). Branches - displays the status and name of the deployment branch, keeps the additional links to Jenkins and Gerrit. In case of edp versioning type, there are two additional fields: Build Number - indicates the current build number; Last Successful Build - indicates the last successful build number. Status Info - displays all the actions that were performed during the creation/cloning/importing process. Edit the application codebase by clicking the pencil icon. For details see the Edit Existing Codebase section. Remove application with the corresponding database and Jenkins pipelines: Click the delete icon next to the application name; Type the required application name; Confirm the deletion by clicking the Delete button. Note The application that is used in a CD pipeline cannot be removed. Select a number of existing applications to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing applications in a list by clicking the Name title. The applications will be displayed in alphabetical order. Search the necessary application by entering the corresponding name, language or the build tool into the Search field. The search can be performed by the application name, language or a build tool. Navigate between pages if the number of applications exceeds the capacity of a single page.","title":"Check and Remove Application"},{"location":"user-guide/application/#add-a-new-branch","text":"When adding an application, the default branch is a master branch. In order to add a new branch, follow the steps below: Navigate to the Branches block and click the Create button: Fill in the required fields: a. Release Branch - select the Release Branch check box if you need to create a release branch; b. Branch Name - type the branch name. Pay attention that this field remain static if you create a release branch. c. From Commit Hash - paste the commit hash from which the new branch will be created. Note that if the From Commit Hash field is empty, the latest commit from the branch name will be used. d. Branch Version - enter the necessary branch version for the artifact. The Release Candidate (RC) postfix is concatenated to the branch version number. e. Master Branch Version - type the branch version that will be used in a master branch after the release creation. The Snapshot postfix is concatenated to the master branch version number; f. Click the Proceed button and wait until the new branch will be added to the list. Info Adding of a new branch is indicated in the context of the edp versioning type. To get more detailed information on how to add a branch using the default versioning type, please refer here . The default application repository is cloned and changed to the new indicated version before the build, i.e. the new indicated version will not be committed to the repository; thus, the existing repository will keep the default version.","title":"Add a New Branch"},{"location":"user-guide/application/#edit-existing-codebase","text":"The EDP Admin Console provides the ability to enable, disable or edit the Jira Integration functionality for applications via the Edit Codebase page. Perform the editing from one of the following sections on the Admin Console interface: Navigate to the codebase overview page and click the pencil icon, or Navigate to the codebase list page and click the pencil icon. To enable Jira integration, on the Edit Codebase page do the following: mark the Integrate with Jira server checkbox and fill in the necessary fields; click the Proceed button to apply the changes; navigate to Jenkins and add the create-jira-issue-metadata stage in the Build pipeline. Also add the commit-validate stage in the Code-Review pipeline. To disable Jira integration, on the Edit Codebase page do the following: unmark the Integrate with Jira server checkbox; click the Proceed button to apply the changes; navigate to Jenkins and remove the create-jira-issue-metadata stage in the Build pipeline. Also remove the commit-validate stage in the Code-Review pipeline. As a result, the necessary changes will be applied.","title":"Edit Existing Codebase"},{"location":"user-guide/application/#remove-branch","text":"In order to remove the added branch with the corresponding record in the Admin Console database, do the following: Navigate to the Branches block by clicking the application name link in the Applications list; Click the delete icon related to the necessary branch: Enter the branch name and click the Delete button; Note The default master branch cannot be removed.","title":"Remove Branch"},{"location":"user-guide/application/#related-articles","text":"Add Application","title":"Related Articles"},{"location":"user-guide/autotest/","text":"Autotest \u2693\ufe0e This section describes the subsequent possible actions that can be performed with the newly added or existing autotests. Check and Remove Autotest \u2693\ufe0e As soon as the autotest is successfully provisioned, the following will be created: Code Review and Build pipelines in Jenkins for this autotest. The Build pipeline will be triggered automatically if at least one environment is already added. A new project in Gerrit or another VCS. SonarQube integration will be available after the Build pipeline in Jenkins is passed. Nexus Repository Manager will be available after the Build pipeline in Jenkins is passed as well. Info To navigate quickly to OpenShift, Jenkins, Gerrit, SonarQube, Nexus, and other resources, click the Overview section on the navigation bar and hit the necessary link. The added autotest will be listed in the Autotests list allowing you to do the following: Add another autotest by clicking the Create button and performing the same steps as described here . Open autotest data by clicking its link name. Once clicked, the following blocks will be displayed: General Info - displays common information about the cloned/imported autotest. Advanced Settings - displays the specified job provisioner, Jenkins slave, deployment script, and the versioning type with the start versioning from number (the latter two fields appear in case of edp versioning type). Branches - displays the status and name of the deployment branch, keeps the additional links to Jenkins and Gerrit. In case of edp versioning type, there are two additional fields: Build Number - indicates the current build number; Last Successful Build - indicates the last successful build number. Status Info - displays all the actions that were performed during the cloning/importing process. Edit the autotest codebase by clicking the pencil icon. For details see the Edit Existing Codebase section. Remove autotest with the corresponding database and Jenkins pipelines: Click the delete icon next to the autotest name; Type the required autotest name; Confirm the deletion by clicking the Delete button. Note The autotest that is used in a CD pipeline cannot be removed. Select a number of existing autotests to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing autotests in a list by clicking the Name title. The autotests will be displayed in alphabetical order. Search the necessary autotest by entering the corresponding name, language or the build tool into the Search field. The search can be performed by the autotest name, language or a build tool. Navigate between pages, if the number of autotests exceeds the capacity of a single page. Edit Existing Codebase \u2693\ufe0e The EDP Admin Console provides the ability to enable, disable or edit the Jira Integration functionality for autotests via the Edit Codebase page. Perform the editing from one of the following sections on the Admin Console interface: Navigate to the codebase overview page and click the pencil icon, or Navigate to the codebase list page and click the pencil icon. To enable Jira integration, on the Edit Codebase page do the following: mark the Integrate with Jira server checkbox and fill in the necessary fields; click the Proceed button to apply the changes; navigate to Jenkins and add the create-jira-issue-metadata stage in the Build pipeline. Also add the commit-validate stage in the Code-Review pipeline. To disable Jira integration, on the Edit Codebase page do the following: unmark the Integrate with Jira server checkbox; click the Proceed button to apply the changes; navigate to Jenkins and remove the create-jira-issue-metadata stage in the Build pipeline. Also remove the commit-validate stage in the Code-Review pipeline. As a result, the necessary changes will be applied. Add a New Branch \u2693\ufe0e When adding an autotest, the default branch is a master branch. In order to add a new branch, follow the steps below: Navigate to the Branches block and click the Create button: Fill in the required fields: a. Release Branch - select the Release Branch check box if you need to create a release branch; b. Branch Name - type the branch name. Pay attention that this field remain static if you create a release branch. c. From Commit Hash - paste the commit hash from which the new branch will be created. Note that if the From Commit Hash field is empty, the latest commit from the branch name will be used. d. Branch Version - enter the necessary branch version for the artifact. The Release Candidate (RC) postfix is concatenated to the branch version number. e. Master Branch Version - type the branch version that will be used in a master branch after the release creation. The Snapshot postfix is concatenated to the master branch version number; f. Click the Proceed button and wait until the new branch will be added to the list. Info Adding of a new branch is indicated in the context of the edp versioning type. To get more detailed information on how to add a branch using the default versioning type, please refer here . The default autotest repository is cloned and changed to the new indicated version before the build, i.e. the new indicated version will not be committed to the repository; thus, the existing repository will keep the default version. Remove Branch \u2693\ufe0e In order to remove the added branch with the corresponding record in the Admin Console database, do the following: Navigate to the Branches block by clicking the autotest name link in the Autotests list; Click the delete icon related to the necessary branch: Enter the branch name and click the Delete button; Note The default master branch cannot be removed. Related Articles \u2693\ufe0e Add Autotests","title":"Overview"},{"location":"user-guide/autotest/#autotest","text":"This section describes the subsequent possible actions that can be performed with the newly added or existing autotests.","title":"Autotest"},{"location":"user-guide/autotest/#check-and-remove-autotest","text":"As soon as the autotest is successfully provisioned, the following will be created: Code Review and Build pipelines in Jenkins for this autotest. The Build pipeline will be triggered automatically if at least one environment is already added. A new project in Gerrit or another VCS. SonarQube integration will be available after the Build pipeline in Jenkins is passed. Nexus Repository Manager will be available after the Build pipeline in Jenkins is passed as well. Info To navigate quickly to OpenShift, Jenkins, Gerrit, SonarQube, Nexus, and other resources, click the Overview section on the navigation bar and hit the necessary link. The added autotest will be listed in the Autotests list allowing you to do the following: Add another autotest by clicking the Create button and performing the same steps as described here . Open autotest data by clicking its link name. Once clicked, the following blocks will be displayed: General Info - displays common information about the cloned/imported autotest. Advanced Settings - displays the specified job provisioner, Jenkins slave, deployment script, and the versioning type with the start versioning from number (the latter two fields appear in case of edp versioning type). Branches - displays the status and name of the deployment branch, keeps the additional links to Jenkins and Gerrit. In case of edp versioning type, there are two additional fields: Build Number - indicates the current build number; Last Successful Build - indicates the last successful build number. Status Info - displays all the actions that were performed during the cloning/importing process. Edit the autotest codebase by clicking the pencil icon. For details see the Edit Existing Codebase section. Remove autotest with the corresponding database and Jenkins pipelines: Click the delete icon next to the autotest name; Type the required autotest name; Confirm the deletion by clicking the Delete button. Note The autotest that is used in a CD pipeline cannot be removed. Select a number of existing autotests to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing autotests in a list by clicking the Name title. The autotests will be displayed in alphabetical order. Search the necessary autotest by entering the corresponding name, language or the build tool into the Search field. The search can be performed by the autotest name, language or a build tool. Navigate between pages, if the number of autotests exceeds the capacity of a single page.","title":"Check and Remove Autotest"},{"location":"user-guide/autotest/#edit-existing-codebase","text":"The EDP Admin Console provides the ability to enable, disable or edit the Jira Integration functionality for autotests via the Edit Codebase page. Perform the editing from one of the following sections on the Admin Console interface: Navigate to the codebase overview page and click the pencil icon, or Navigate to the codebase list page and click the pencil icon. To enable Jira integration, on the Edit Codebase page do the following: mark the Integrate with Jira server checkbox and fill in the necessary fields; click the Proceed button to apply the changes; navigate to Jenkins and add the create-jira-issue-metadata stage in the Build pipeline. Also add the commit-validate stage in the Code-Review pipeline. To disable Jira integration, on the Edit Codebase page do the following: unmark the Integrate with Jira server checkbox; click the Proceed button to apply the changes; navigate to Jenkins and remove the create-jira-issue-metadata stage in the Build pipeline. Also remove the commit-validate stage in the Code-Review pipeline. As a result, the necessary changes will be applied.","title":"Edit Existing Codebase"},{"location":"user-guide/autotest/#add-a-new-branch","text":"When adding an autotest, the default branch is a master branch. In order to add a new branch, follow the steps below: Navigate to the Branches block and click the Create button: Fill in the required fields: a. Release Branch - select the Release Branch check box if you need to create a release branch; b. Branch Name - type the branch name. Pay attention that this field remain static if you create a release branch. c. From Commit Hash - paste the commit hash from which the new branch will be created. Note that if the From Commit Hash field is empty, the latest commit from the branch name will be used. d. Branch Version - enter the necessary branch version for the artifact. The Release Candidate (RC) postfix is concatenated to the branch version number. e. Master Branch Version - type the branch version that will be used in a master branch after the release creation. The Snapshot postfix is concatenated to the master branch version number; f. Click the Proceed button and wait until the new branch will be added to the list. Info Adding of a new branch is indicated in the context of the edp versioning type. To get more detailed information on how to add a branch using the default versioning type, please refer here . The default autotest repository is cloned and changed to the new indicated version before the build, i.e. the new indicated version will not be committed to the repository; thus, the existing repository will keep the default version.","title":"Add a New Branch"},{"location":"user-guide/autotest/#remove-branch","text":"In order to remove the added branch with the corresponding record in the Admin Console database, do the following: Navigate to the Branches block by clicking the autotest name link in the Autotests list; Click the delete icon related to the necessary branch: Enter the branch name and click the Delete button; Note The default master branch cannot be removed.","title":"Remove Branch"},{"location":"user-guide/autotest/#related-articles","text":"Add Autotests","title":"Related Articles"},{"location":"user-guide/customize-cd-pipeline/","text":"Customize CD Pipeline \u2693\ufe0e Apart from running CD pipeline stages with the default logic, there is the ability to perform the following: Create your own logic for stages; Redefine the default EDP stages of a CD pipeline. In order to have the ability to customize a stage`s logic, create a CD pipeline stage source as a Library: Create a library in Admin Console with the Groovy-pipeline code language: Create a CD pipeline with the library stage`s source and its branch: Add New Stage \u2693\ufe0e Follow the steps below to add a new stage: Clone the repository with the added library; Create a \"stages\" directory in the root; Create a groovy file with a meaningful name, e.g. NotificationStage.groovy; Put the required construction and your own logic into the file: import com.epam.edp.stages.impl.cd.Stage @Stage ( name = \"notify\" ) class Notify { Script script void run ( context ) { --------------- Put your own logic here ------------------ script . println ( \"Send notification logic\" ) --------------- Put your own logic here ------------------ } } return Notify Add a new stage to the STAGES parameter of the Jenkins job of your CD pipeline: Run the job to check that your new stage has been run during the execution. Redefine Existing Stage \u2693\ufe0e By default, the following stages are implemented in EDP pipeline framework: deploy, deploy-helm, autotests, manual (Manual approve), promote-images, promote-images-ecr. If you use one of these names for annotation in your own class, it will lead to redefining the default logic with your own. Find below a sample of the possible flow of the redefining deploy stage: Clone the repository with the added library; Create a \"stages\" directory in the root; Create a Jenkinsfile with default content: @Library ( [ 'edp-library-stages', 'edp-library-pipelines' ] ) _ Deploy () Create a groovy file with a meaningful name, e.g. CustomDeployStage.groovy; Put the required construction and your own logic into the file: import com.epam.edp.stages.impl.cd.Stage @Stage ( name = \"deploy\" ) class CustomDeployStage { Script script void run ( context ) { --------------- Put your own logic here ------------------ script . println ( \"Custom deploy stage logic\" ) --------------- Put your own logic here ------------------ } } return CustomDeployStage","title":"Customize CD Pipeline"},{"location":"user-guide/customize-cd-pipeline/#customize-cd-pipeline","text":"Apart from running CD pipeline stages with the default logic, there is the ability to perform the following: Create your own logic for stages; Redefine the default EDP stages of a CD pipeline. In order to have the ability to customize a stage`s logic, create a CD pipeline stage source as a Library: Create a library in Admin Console with the Groovy-pipeline code language: Create a CD pipeline with the library stage`s source and its branch:","title":"Customize CD Pipeline"},{"location":"user-guide/customize-cd-pipeline/#add-new-stage","text":"Follow the steps below to add a new stage: Clone the repository with the added library; Create a \"stages\" directory in the root; Create a groovy file with a meaningful name, e.g. NotificationStage.groovy; Put the required construction and your own logic into the file: import com.epam.edp.stages.impl.cd.Stage @Stage ( name = \"notify\" ) class Notify { Script script void run ( context ) { --------------- Put your own logic here ------------------ script . println ( \"Send notification logic\" ) --------------- Put your own logic here ------------------ } } return Notify Add a new stage to the STAGES parameter of the Jenkins job of your CD pipeline: Run the job to check that your new stage has been run during the execution.","title":"Add New Stage"},{"location":"user-guide/customize-cd-pipeline/#redefine-existing-stage","text":"By default, the following stages are implemented in EDP pipeline framework: deploy, deploy-helm, autotests, manual (Manual approve), promote-images, promote-images-ecr. If you use one of these names for annotation in your own class, it will lead to redefining the default logic with your own. Find below a sample of the possible flow of the redefining deploy stage: Clone the repository with the added library; Create a \"stages\" directory in the root; Create a Jenkinsfile with default content: @Library ( [ 'edp-library-stages', 'edp-library-pipelines' ] ) _ Deploy () Create a groovy file with a meaningful name, e.g. CustomDeployStage.groovy; Put the required construction and your own logic into the file: import com.epam.edp.stages.impl.cd.Stage @Stage ( name = \"deploy\" ) class CustomDeployStage { Script script void run ( context ) { --------------- Put your own logic here ------------------ script . println ( \"Custom deploy stage logic\" ) --------------- Put your own logic here ------------------ } } return CustomDeployStage","title":"Redefine Existing Stage"},{"location":"user-guide/customize-ci-pipeline/","text":"Customize CI Pipeline \u2693\ufe0e This chapter describes the main steps that should be followed when customizing a CI pipeline. Redefine a Default Stage Logic for a Particular Application \u2693\ufe0e To redefine any stage and add custom logic, perform the steps below: Open the GitHub repository: Create a directory with the name \u201cstages\u201d in the application repository; Create a Groovy file with a meaningful name for a custom stage description, for instance: CustomSonar.groovy . Paste the copied skeleton from the reference stage and insert the necessary logic. Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ). The stage logic structure is the following: CustomSonar.groovy import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"sonar\" , buildTool = [ \"maven\" ], type = [ ProjectType . APPLICATION , ProjectType . AUTOTESTS , ProjectType . LIBRARY ]) class CustomSonar { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomSonar Info There is the ability to redefine the predefined EDP stage as well as to create it from scratch, it depends on the name that is used in the @Stage annotation. For example, using name = \"sonar\" will redefine an existing sonar stage with the same name, but using name=\"new-sonar\" will create a new stage. By default, the following stages are implemented in EDP: build build-image-from-dockerfile build-image build-image-kaniko checkout compile create-branch gerrit-checkout get-version git-tag push sonar sonar-cleanup tests trigger-job Mandatory points: Importing classes com.epam.edp.stages.impl.ci.ProjectType and com.epam.edp.stages.impl.ci.Stage; Annotating \"Stage\" for class - @Stage(name = \"sonar\", buildTool = [\"maven\"], type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY]); Property with the type \"Script\"; Void the \"run\" method with the \"context input parameter\" value; Bring the custom class back to the end of the file: return CustomSonar. Open Jenkins and make sure that all your changes are correct after the completion of the customized pipeline. Add a New Stage for a Particular Application \u2693\ufe0e To add a new stage for a particular application, perform the steps below: In GitHub repository, add a Groovy file with another name to the same stages catalog. Copy the part of a pipeline framework logic that cannot be predefined; The stage logic structure is the following: EmailNotify.groovy import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"email-notify\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class EmailNotify { Script script void run ( context ) { ------------------- 'Your custom logic here' } } return EmailNotify Open the default set of stages and add a new one into the Default Value field by saving the respective type {\"name\": \"email-notify\"}, save the changes: Open Jenkins to check the pipeline; as soon as the checkout stage is passed, the new stage will appear in the pipeline: Redefine a Default Stage Logic via Custom Global Pipeline Libraries \u2693\ufe0e Note To add a new Custom Global Pipeline Library, please refer to the Add a New Custom Global Pipeline Library page. To redefine any stage and add custom logic using global pipeline libraries, perform the steps below: Open the GitHub repository: Create a directory with the name /src/com/epam/edp/customStages/impl/ci/impl/stageName/ in the library repository, for instance: /src/com/epam/edp/customStages/impl/ci/impl/sonar/ ; Create a Groovy file with a meaningful name for a custom stage description, for instance \u2013 CustomSonar.groovy . Paste the copied skeleton from the reference stage and insert the necessary logic. Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ). The stage logic structure is the following: CustomSonar.groovy package com . epam . edp . customStages . impl . ci . impl . sonar import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"sonar\" , buildTool = [ \"maven\" ], type = [ ProjectType . APPLICATION , ProjectType . AUTOTESTS , ProjectType . LIBRARY ]) class CustomSonar { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomSonar Info There is the ability to redefine the predefined EDP stage as well as to create it from scratch, it depends on the name that is used in the @Stage annotation. For example, using name = \"sonar\" will redefine an existing sonar stage with the same name, but using name=\"new-sonar\" will create a new stage. By default, the following stages are implemented in EDP: build build-image-from-dockerfile build-image build-image-kaniko checkout compile create-branch gerrit-checkout get-version git-tag push sonar sonar-cleanup tests trigger-job Mandatory points: Defining a package com.epam.edp.customStages.impl.ci.impl.stageName; Importing classes com.epam.edp.stages.impl.ci.ProjectType and com.epam.edp.stages.impl.ci.Stage; Annotating \"Stage\" for class - @Stage(name = \"sonar\", buildTool = [\"maven\"], type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY]); Property with the type \"Script\"; Void the \"run\" method with the \"context input parameter\" value; Bring the custom class back to the end of the file: return CustomSonar. 3.Open Jenkins and make sure that all your changes are correct after the completion of the customized pipeline. Add a New Stage Using Shared Library via Custom Global Pipeline Libraries \u2693\ufe0e Note To add a new Custom Global Pipeline Library, please refer to the Add a New Custom Global Pipeline Library page. To redefine any stage and add custom logic using global pipeline libraries, perform the steps below: In GitHub repository, add a Groovy file with another name to the same stages catalog; Copy the part of a pipeline framework logic that cannot be predefined; The stage logic structure is the following: EmailNotify.groovy package com . epam . edp . customStages . impl . ci . impl . emailNotify import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"email-notify\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class EmailNotify { Script script void run ( context ) { ------------------- 'Your custom logic here' } } return EmailNotify Open the default set of stages and add a new one into the Default Value field by saving the respective type {\"name\": \"email-notify\"}, save the changes: Open Jenkins to check the pipeline; as soon as the checkout stage is passed, the new stage will appear in the pipeline:","title":"Customize CI Pipeline"},{"location":"user-guide/customize-ci-pipeline/#customize-ci-pipeline","text":"This chapter describes the main steps that should be followed when customizing a CI pipeline.","title":"Customize CI Pipeline"},{"location":"user-guide/customize-ci-pipeline/#redefine-a-default-stage-logic-for-a-particular-application","text":"To redefine any stage and add custom logic, perform the steps below: Open the GitHub repository: Create a directory with the name \u201cstages\u201d in the application repository; Create a Groovy file with a meaningful name for a custom stage description, for instance: CustomSonar.groovy . Paste the copied skeleton from the reference stage and insert the necessary logic. Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ). The stage logic structure is the following: CustomSonar.groovy import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"sonar\" , buildTool = [ \"maven\" ], type = [ ProjectType . APPLICATION , ProjectType . AUTOTESTS , ProjectType . LIBRARY ]) class CustomSonar { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomSonar Info There is the ability to redefine the predefined EDP stage as well as to create it from scratch, it depends on the name that is used in the @Stage annotation. For example, using name = \"sonar\" will redefine an existing sonar stage with the same name, but using name=\"new-sonar\" will create a new stage. By default, the following stages are implemented in EDP: build build-image-from-dockerfile build-image build-image-kaniko checkout compile create-branch gerrit-checkout get-version git-tag push sonar sonar-cleanup tests trigger-job Mandatory points: Importing classes com.epam.edp.stages.impl.ci.ProjectType and com.epam.edp.stages.impl.ci.Stage; Annotating \"Stage\" for class - @Stage(name = \"sonar\", buildTool = [\"maven\"], type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY]); Property with the type \"Script\"; Void the \"run\" method with the \"context input parameter\" value; Bring the custom class back to the end of the file: return CustomSonar. Open Jenkins and make sure that all your changes are correct after the completion of the customized pipeline.","title":"Redefine a Default Stage Logic for a Particular Application"},{"location":"user-guide/customize-ci-pipeline/#add-a-new-stage-for-a-particular-application","text":"To add a new stage for a particular application, perform the steps below: In GitHub repository, add a Groovy file with another name to the same stages catalog. Copy the part of a pipeline framework logic that cannot be predefined; The stage logic structure is the following: EmailNotify.groovy import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"email-notify\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class EmailNotify { Script script void run ( context ) { ------------------- 'Your custom logic here' } } return EmailNotify Open the default set of stages and add a new one into the Default Value field by saving the respective type {\"name\": \"email-notify\"}, save the changes: Open Jenkins to check the pipeline; as soon as the checkout stage is passed, the new stage will appear in the pipeline:","title":"Add a New Stage for a Particular Application"},{"location":"user-guide/customize-ci-pipeline/#redefine-a-default-stage-logic-via-custom-global-pipeline-libraries","text":"Note To add a new Custom Global Pipeline Library, please refer to the Add a New Custom Global Pipeline Library page. To redefine any stage and add custom logic using global pipeline libraries, perform the steps below: Open the GitHub repository: Create a directory with the name /src/com/epam/edp/customStages/impl/ci/impl/stageName/ in the library repository, for instance: /src/com/epam/edp/customStages/impl/ci/impl/sonar/ ; Create a Groovy file with a meaningful name for a custom stage description, for instance \u2013 CustomSonar.groovy . Paste the copied skeleton from the reference stage and insert the necessary logic. Note Pay attention to the appropriate annotation (EDP versions of all stages can be found on GitHub ). The stage logic structure is the following: CustomSonar.groovy package com . epam . edp . customStages . impl . ci . impl . sonar import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"sonar\" , buildTool = [ \"maven\" ], type = [ ProjectType . APPLICATION , ProjectType . AUTOTESTS , ProjectType . LIBRARY ]) class CustomSonar { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomSonar Info There is the ability to redefine the predefined EDP stage as well as to create it from scratch, it depends on the name that is used in the @Stage annotation. For example, using name = \"sonar\" will redefine an existing sonar stage with the same name, but using name=\"new-sonar\" will create a new stage. By default, the following stages are implemented in EDP: build build-image-from-dockerfile build-image build-image-kaniko checkout compile create-branch gerrit-checkout get-version git-tag push sonar sonar-cleanup tests trigger-job Mandatory points: Defining a package com.epam.edp.customStages.impl.ci.impl.stageName; Importing classes com.epam.edp.stages.impl.ci.ProjectType and com.epam.edp.stages.impl.ci.Stage; Annotating \"Stage\" for class - @Stage(name = \"sonar\", buildTool = [\"maven\"], type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY]); Property with the type \"Script\"; Void the \"run\" method with the \"context input parameter\" value; Bring the custom class back to the end of the file: return CustomSonar. 3.Open Jenkins and make sure that all your changes are correct after the completion of the customized pipeline.","title":"Redefine a Default Stage Logic via Custom Global Pipeline Libraries"},{"location":"user-guide/customize-ci-pipeline/#add-a-new-stage-using-shared-library-via-custom-global-pipeline-libraries","text":"Note To add a new Custom Global Pipeline Library, please refer to the Add a New Custom Global Pipeline Library page. To redefine any stage and add custom logic using global pipeline libraries, perform the steps below: In GitHub repository, add a Groovy file with another name to the same stages catalog; Copy the part of a pipeline framework logic that cannot be predefined; The stage logic structure is the following: EmailNotify.groovy package com . epam . edp . customStages . impl . ci . impl . emailNotify import com.epam.edp.stages.impl.ci.ProjectType import com.epam.edp.stages.impl.ci.Stage @Stage ( name = \"email-notify\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class EmailNotify { Script script void run ( context ) { ------------------- 'Your custom logic here' } } return EmailNotify Open the default set of stages and add a new one into the Default Value field by saving the respective type {\"name\": \"email-notify\"}, save the changes: Open Jenkins to check the pipeline; as soon as the checkout stage is passed, the new stage will appear in the pipeline:","title":"Add a New Stage Using Shared Library via Custom Global Pipeline Libraries"},{"location":"user-guide/d-d-diagram/","text":"Delivery Dashboard Diagram \u2693\ufe0e Admin Console allows getting the general visualization of all the relations between CD pipeline, stages, codebases, branches, and image streams that are elements with the specific icon. To open the current project diagram, navigate to the Delivery Dashboard Diagram section on the navigation bar: Info All the requested changes (deletion, creation, adding) are displayed immediately on the Delivery Dashboard Diagram. Possible actions when using dashboard: To zoom in or zoom out the diagram scale, scroll up / down. To move the diagram, click and drag. To move an element, click it and drag to the necessary place. To see the relations for one element, click this element. To see the whole diagram, click the empty space. Related Articles \u2693\ufe0e EDP Admin Console","title":"Delivery Dashboard Diagram"},{"location":"user-guide/d-d-diagram/#delivery-dashboard-diagram","text":"Admin Console allows getting the general visualization of all the relations between CD pipeline, stages, codebases, branches, and image streams that are elements with the specific icon. To open the current project diagram, navigate to the Delivery Dashboard Diagram section on the navigation bar: Info All the requested changes (deletion, creation, adding) are displayed immediately on the Delivery Dashboard Diagram. Possible actions when using dashboard: To zoom in or zoom out the diagram scale, scroll up / down. To move the diagram, click and drag. To move an element, click it and drag to the necessary place. To see the relations for one element, click this element. To see the whole diagram, click the empty space.","title":"Delivery Dashboard Diagram"},{"location":"user-guide/d-d-diagram/#related-articles","text":"EDP Admin Console","title":"Related Articles"},{"location":"user-guide/dockerfile-stages/","text":"Use Dockerfile linters for Code Review \u2693\ufe0e This section contains the description of dockerbuild-verify , dockerfile-lint stages which one can use in Code-Review pipeline. These stages help obtain quick call on the validity of the code in the Code-Review pipeline in Kubernetes for all types of applications supported by EDP out of the box. Inspect the functions performed by the following stages: dockerbuild-verify stage collects artifacts and builds an image from the Dockerfile without push to registry. This stage is intended to check if the image is built. dockerfile-lint stage launches the hadolint command in order to check the Dockerfile. Related Articles \u2693\ufe0e Use Terraform Library in EDP EDP Pipeline Framework Promote Docker Images from ECR to Docker Hub","title":"Dockerfile"},{"location":"user-guide/dockerfile-stages/#use-dockerfile-linters-for-code-review","text":"This section contains the description of dockerbuild-verify , dockerfile-lint stages which one can use in Code-Review pipeline. These stages help obtain quick call on the validity of the code in the Code-Review pipeline in Kubernetes for all types of applications supported by EDP out of the box. Inspect the functions performed by the following stages: dockerbuild-verify stage collects artifacts and builds an image from the Dockerfile without push to registry. This stage is intended to check if the image is built. dockerfile-lint stage launches the hadolint command in order to check the Dockerfile.","title":"Use Dockerfile linters for Code Review"},{"location":"user-guide/dockerfile-stages/#related-articles","text":"Use Terraform Library in EDP EDP Pipeline Framework Promote Docker Images from ECR to Docker Hub","title":"Related Articles"},{"location":"user-guide/ecr-to-docker-stages/","text":"Promote Docker Images From ECR to Docker Hub \u2693\ufe0e This section contains the description of the ecr-to-docker stage, available in the Build pipeline. The ecr-to-docker stage is intended to perform the push of Docker images collected from the Amazon ECR cluster storage to Docker Hub repositories, where the image becomes accessible to everyone who wants to use it. This stage is optional and is designed for working with various EDP components. Note When pushing the image from ECR to Docker Hub using crane , the SHA-256 value remains unchanged. The ecr-to-docker stage contains a specific script that launches the following actions: Performs authorization in AWS ECR in the EDP private storage via awsv2 . Performs authorization in the Docker Hub. Checks whether a similar image exists in the Docker Hub in order to avoid its overwriting. If a similar image exists in the Docker Hub, the script will return the message about it and stop the execution. The ecr-to-docker stage in the Build pipeline will be marked in red. If there is no similar image, the script will proceed to promote the image using crane . EDP expects authorization credentials to be added in Jenkins under the following names dockerCredentialsId and repoCredentialsID . To get more information on how to create credentials, see the section Create Credentials for ECR-to-Docker Stage below. Create Credentials for ECR-to-Docker Stage \u2693\ufe0e The dockerCredentialsId and repoCredentialsID credentials are expected to be set in Jenkins in order to use the ecr-to-docker stage. Inspect the instructions below on how to create the required credentials. dockerCredentialsId \u2693\ufe0e In order to create the dockerCredentialsId value, perform the following steps: Go to Jenkins -> Manage Jenkins \u2013> Manage Credentials section. Click the Add Credentials button and select Username with password in the Kind field. In the Username field, enter the login of the account from which the push will be made and add the password field. Specify the ID and description fields. Pay attention to add the dockerCredentialsId value in both fields. Click the Save button. repoCredentialsId \u2693\ufe0e The repoCredentialsId value is created in order to indicate the name of the repository that does not match the login of the Docker account. Due to the fact that the repository can be an Enterprise solution and, accordingly, users can push images to the repository from other accounts connected within the Docker Hub Organization. To create the repoCredentialsID value, follow the steps below: Go to Jenkins -> Manage Jenkins \u2013> Manage Credentials section. Click the Add Credentials button and select Secret text in the Kind field. Enter the text that will be encrypted by Jenkins, in the Secret field. Specify the ID and description fields. Pay attention to add the repoCredentialsID value in both fields. Click the Save button. As a result, the created credentials will appear in Jenkins in the Global credentials section. Related Articles \u2693\ufe0e EDP Pipeline Framework","title":"ECR-to-DockerHub"},{"location":"user-guide/ecr-to-docker-stages/#promote-docker-images-from-ecr-to-docker-hub","text":"This section contains the description of the ecr-to-docker stage, available in the Build pipeline. The ecr-to-docker stage is intended to perform the push of Docker images collected from the Amazon ECR cluster storage to Docker Hub repositories, where the image becomes accessible to everyone who wants to use it. This stage is optional and is designed for working with various EDP components. Note When pushing the image from ECR to Docker Hub using crane , the SHA-256 value remains unchanged. The ecr-to-docker stage contains a specific script that launches the following actions: Performs authorization in AWS ECR in the EDP private storage via awsv2 . Performs authorization in the Docker Hub. Checks whether a similar image exists in the Docker Hub in order to avoid its overwriting. If a similar image exists in the Docker Hub, the script will return the message about it and stop the execution. The ecr-to-docker stage in the Build pipeline will be marked in red. If there is no similar image, the script will proceed to promote the image using crane . EDP expects authorization credentials to be added in Jenkins under the following names dockerCredentialsId and repoCredentialsID . To get more information on how to create credentials, see the section Create Credentials for ECR-to-Docker Stage below.","title":"Promote Docker Images From ECR to Docker Hub"},{"location":"user-guide/ecr-to-docker-stages/#create-credentials-for-ecr-to-docker-stage","text":"The dockerCredentialsId and repoCredentialsID credentials are expected to be set in Jenkins in order to use the ecr-to-docker stage. Inspect the instructions below on how to create the required credentials.","title":"Create Credentials for ECR-to-Docker Stage"},{"location":"user-guide/ecr-to-docker-stages/#dockercredentialsid","text":"In order to create the dockerCredentialsId value, perform the following steps: Go to Jenkins -> Manage Jenkins \u2013> Manage Credentials section. Click the Add Credentials button and select Username with password in the Kind field. In the Username field, enter the login of the account from which the push will be made and add the password field. Specify the ID and description fields. Pay attention to add the dockerCredentialsId value in both fields. Click the Save button.","title":"dockerCredentialsId"},{"location":"user-guide/ecr-to-docker-stages/#repocredentialsid","text":"The repoCredentialsId value is created in order to indicate the name of the repository that does not match the login of the Docker account. Due to the fact that the repository can be an Enterprise solution and, accordingly, users can push images to the repository from other accounts connected within the Docker Hub Organization. To create the repoCredentialsID value, follow the steps below: Go to Jenkins -> Manage Jenkins \u2013> Manage Credentials section. Click the Add Credentials button and select Secret text in the Kind field. Enter the text that will be encrypted by Jenkins, in the Secret field. Specify the ID and description fields. Pay attention to add the repoCredentialsID value in both fields. Click the Save button. As a result, the created credentials will appear in Jenkins in the Global credentials section.","title":"repoCredentialsId"},{"location":"user-guide/ecr-to-docker-stages/#related-articles","text":"EDP Pipeline Framework","title":"Related Articles"},{"location":"user-guide/helm-stages/","text":"Use chart testing tool for Code Review \u2693\ufe0e This section contains the description of helm-lint stage which one can use in Code-Review pipeline. These stage helps obtain quick call on the validity of the code in the Code-Review pipeline for all types of applications supported by EDP out of the box. Inspect the functions performed by the following stages: helm-lint stage launches the ct lint --charts-deploy-templates/ command in order to validate the chart. Related Articles \u2693\ufe0e EDP Pipeline Framework","title":"Helm"},{"location":"user-guide/helm-stages/#use-chart-testing-tool-for-code-review","text":"This section contains the description of helm-lint stage which one can use in Code-Review pipeline. These stage helps obtain quick call on the validity of the code in the Code-Review pipeline for all types of applications supported by EDP out of the box. Inspect the functions performed by the following stages: helm-lint stage launches the ct lint --charts-deploy-templates/ command in order to validate the chart.","title":"Use chart testing tool for Code Review"},{"location":"user-guide/helm-stages/#related-articles","text":"EDP Pipeline Framework","title":"Related Articles"},{"location":"user-guide/library/","text":"Library \u2693\ufe0e This section describes the subsequent possible actions that can be performed with the newly added or existing libraries. Check and Remove Library \u2693\ufe0e As soon as the library is successfully provisioned, the following will be created: Code Review and Build pipelines in Jenkins for this library. The Build pipeline will be triggered automatically if at least one environment is already added. A new project in Gerrit or another VCS. SonarQube integration will be available after the Build pipeline in Jenkins is passed. Nexus Repository Manager will be available after the Build pipeline in Jenkins is passed as well. Info To navigate quickly to OpenShift, Jenkins, Gerrit, SonarQube, Nexus, and other resources, click the Overview section on the navigation bar and hit the necessary link. The added library will be listed in the Libraries list allowing you to do the following: Create another library by clicking the Create button and performing the same steps as described here ; Open library data by clicking its link name. Once clicked, the following blocks will be displayed: General Info - displays common information about the created/cloned/imported library. Advanced Settings - displays the specified job provisioner, Jenkins slave, deployment script, and the versioning type with the start versioning from number (the latter two fields appear in case of edp versioning type). Branches - displays the status and name of the deployment branch, keeps the additional links to Jenkins and Gerrit. In case of edp versioning type, there are two additional fields: Build Number - indicates the current build number; Last Successful Build - indicates the last successful build number. Status Info - displays all the actions that were performed during the creation/cloning/importing process. Edit the library codebase by clicking the pencil icon. For details see the Edit Existing Codebase section. Remove library with the corresponding database and Jenkins pipelines: Click the delete icon next to the library name; Type the required library name; Confirm the deletion by clicking the Delete button. Note The library that is used in a CD pipeline cannot be removed. Select a number of existing libraries to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing libraries in a list by clicking the Name title. The libraries will be displayed in an alphabetical order. Search the necessary application by entering the corresponding name, language or the build tool into the Search field. The search can be performed by the library name, language or a build tool. Navigate between pages, if the number of libraries exceeds the capacity of a single page. Edit Existing Codebase \u2693\ufe0e The EDP Admin Console provides the ability to enable, disable or edit the Jira Integration functionality for applications via the Edit Codebase page. Perform the editing from one of the following sections on the Admin Console interface: Navigate to the codebase overview page and click the pencil icon, or Navigate to the codebase list page and click the pencil icon. To enable Jira integration, on the Edit Codebase page do the following: mark the Integrate with Jira server checkbox and fill in the necessary fields; click the Proceed button to apply the changes; navigate to Jenkins and add the create-jira-issue-metadata stage in the Build pipeline. Also add the commit-validate stage in the Code-Review pipeline. To disable Jira integration, on the Edit Codebase page do the following: unmark the Integrate with Jira server checkbox; click the Proceed button to apply the changes; navigate to Jenkins and remove the create-jira-issue-metadata stage in the Build pipeline. Also remove the commit-validate stage in the Code-Review pipeline. As a result, the necessary changes will be applied. Add a New Branch \u2693\ufe0e When adding a library, the default branch is a master branch. In order to add a new branch, follow the steps below: Navigate to the Branches block and click the Create button: Fill in the required fields: a. Release Branch - select the Release Branch check box if you need to create a release branch; b. Branch Name - type the branch name. Pay attention that this field remain static if you create a release branch. c. From Commit Hash - paste the commit hash from which the new branch will be created. Note that if the From Commit Hash field is empty, the latest commit from the branch name will be used. d. Branch Version - enter the necessary branch version for the artifact. The Release Candidate (RC) postfix is concatenated to the branch version number. e. Master Branch Version - type the branch version that will be used in a master branch after the release creation. The Snapshot postfix is concatenated to the master branch version number; f. Click the Proceed button and wait until the new branch will be added to the list. Info Adding of a new branch is indicated in the context of the edp versioning type. To get more detailed information on how to add a branch using the default versioning type, please refer here . The default library repository is cloned and changed to the new indicated version before the build, i.e. the new indicated version will not be committed to the repository; thus, the existing repository will keep the default version. Remove Branch \u2693\ufe0e In order to remove the added branch with the corresponding record in the Admin Console database, do the following: Navigate to the Branches block by clicking the library name link in the Libraries list; Click the delete icon related to the necessary branch: Enter the branch name and click the Delete button; Note The default master branch cannot be removed. Related Articles \u2693\ufe0e Add Library","title":"Overview"},{"location":"user-guide/library/#library","text":"This section describes the subsequent possible actions that can be performed with the newly added or existing libraries.","title":"Library"},{"location":"user-guide/library/#check-and-remove-library","text":"As soon as the library is successfully provisioned, the following will be created: Code Review and Build pipelines in Jenkins for this library. The Build pipeline will be triggered automatically if at least one environment is already added. A new project in Gerrit or another VCS. SonarQube integration will be available after the Build pipeline in Jenkins is passed. Nexus Repository Manager will be available after the Build pipeline in Jenkins is passed as well. Info To navigate quickly to OpenShift, Jenkins, Gerrit, SonarQube, Nexus, and other resources, click the Overview section on the navigation bar and hit the necessary link. The added library will be listed in the Libraries list allowing you to do the following: Create another library by clicking the Create button and performing the same steps as described here ; Open library data by clicking its link name. Once clicked, the following blocks will be displayed: General Info - displays common information about the created/cloned/imported library. Advanced Settings - displays the specified job provisioner, Jenkins slave, deployment script, and the versioning type with the start versioning from number (the latter two fields appear in case of edp versioning type). Branches - displays the status and name of the deployment branch, keeps the additional links to Jenkins and Gerrit. In case of edp versioning type, there are two additional fields: Build Number - indicates the current build number; Last Successful Build - indicates the last successful build number. Status Info - displays all the actions that were performed during the creation/cloning/importing process. Edit the library codebase by clicking the pencil icon. For details see the Edit Existing Codebase section. Remove library with the corresponding database and Jenkins pipelines: Click the delete icon next to the library name; Type the required library name; Confirm the deletion by clicking the Delete button. Note The library that is used in a CD pipeline cannot be removed. Select a number of existing libraries to be displayed on one page in the Show entries field. The filter allows to show 10, 25, 50 or 100 entries per page. Sort the existing libraries in a list by clicking the Name title. The libraries will be displayed in an alphabetical order. Search the necessary application by entering the corresponding name, language or the build tool into the Search field. The search can be performed by the library name, language or a build tool. Navigate between pages, if the number of libraries exceeds the capacity of a single page.","title":"Check and Remove Library"},{"location":"user-guide/library/#edit-existing-codebase","text":"The EDP Admin Console provides the ability to enable, disable or edit the Jira Integration functionality for applications via the Edit Codebase page. Perform the editing from one of the following sections on the Admin Console interface: Navigate to the codebase overview page and click the pencil icon, or Navigate to the codebase list page and click the pencil icon. To enable Jira integration, on the Edit Codebase page do the following: mark the Integrate with Jira server checkbox and fill in the necessary fields; click the Proceed button to apply the changes; navigate to Jenkins and add the create-jira-issue-metadata stage in the Build pipeline. Also add the commit-validate stage in the Code-Review pipeline. To disable Jira integration, on the Edit Codebase page do the following: unmark the Integrate with Jira server checkbox; click the Proceed button to apply the changes; navigate to Jenkins and remove the create-jira-issue-metadata stage in the Build pipeline. Also remove the commit-validate stage in the Code-Review pipeline. As a result, the necessary changes will be applied.","title":"Edit Existing Codebase"},{"location":"user-guide/library/#add-a-new-branch","text":"When adding a library, the default branch is a master branch. In order to add a new branch, follow the steps below: Navigate to the Branches block and click the Create button: Fill in the required fields: a. Release Branch - select the Release Branch check box if you need to create a release branch; b. Branch Name - type the branch name. Pay attention that this field remain static if you create a release branch. c. From Commit Hash - paste the commit hash from which the new branch will be created. Note that if the From Commit Hash field is empty, the latest commit from the branch name will be used. d. Branch Version - enter the necessary branch version for the artifact. The Release Candidate (RC) postfix is concatenated to the branch version number. e. Master Branch Version - type the branch version that will be used in a master branch after the release creation. The Snapshot postfix is concatenated to the master branch version number; f. Click the Proceed button and wait until the new branch will be added to the list. Info Adding of a new branch is indicated in the context of the edp versioning type. To get more detailed information on how to add a branch using the default versioning type, please refer here . The default library repository is cloned and changed to the new indicated version before the build, i.e. the new indicated version will not be committed to the repository; thus, the existing repository will keep the default version.","title":"Add a New Branch"},{"location":"user-guide/library/#remove-branch","text":"In order to remove the added branch with the corresponding record in the Admin Console database, do the following: Navigate to the Branches block by clicking the library name link in the Libraries list; Click the delete icon related to the necessary branch: Enter the branch name and click the Delete button; Note The default master branch cannot be removed.","title":"Remove Branch"},{"location":"user-guide/library/#related-articles","text":"Add Library","title":"Related Articles"},{"location":"user-guide/opa-stages/","text":"Use Open Policy Agent \u2693\ufe0e Open Policy Agent (OPA) is a policy engine that provides: High-level declarative policy language Rego ; API and tooling for policy execution. EPAM Delivery Platform ensures the implemented Open Policy Agent support allowing to work with Open Policy Agent bundles that is processed by means of stages in the Code-Review and Build pipelines. These pipelines are expected to be created after the Rego OPA Library is added. Code-Review Pipeline Stages \u2693\ufe0e In the Code-Review pipeline, the following stages are available: checkout stage, a standard step during which all files are checked out from a selected branch of the Git repository. tests stage containing a script that performs the following actions: 2.1. Runs policy tests . 2.2. Converts OPA test results into JUnit format. 2.3. Publishes JUnit-formatted results to Jenkins. Build Pipeline Stages \u2693\ufe0e In the Build pipeline, the following stages are available: checkout stage, a standard step during which all files are checked out from a selected branch of the Git repository. get-version optional stage, a step where library version is determined either via: 2.1. Standard EDP versioning functionality. 2.2. Manually specified version. In this case .manifest file in a root directory MUST be provided. File must contain a JSON document with revision field. Minimal example: { \"revision\": \"1.0.0\" }\" . tests stage containing a script that performs the following actions: 3.1. Runs policy tests . 3.2. Converts OPA test results into JUnit format. 3.3. Publishes JUnit-formatted results to Jenkins. git-tag stage, a standard step where git branch is tagged with a version. Related Articles \u2693\ufe0e EDP Pipeline Framework","title":"Open Policy Agent"},{"location":"user-guide/opa-stages/#use-open-policy-agent","text":"Open Policy Agent (OPA) is a policy engine that provides: High-level declarative policy language Rego ; API and tooling for policy execution. EPAM Delivery Platform ensures the implemented Open Policy Agent support allowing to work with Open Policy Agent bundles that is processed by means of stages in the Code-Review and Build pipelines. These pipelines are expected to be created after the Rego OPA Library is added.","title":"Use Open Policy Agent"},{"location":"user-guide/opa-stages/#code-review-pipeline-stages","text":"In the Code-Review pipeline, the following stages are available: checkout stage, a standard step during which all files are checked out from a selected branch of the Git repository. tests stage containing a script that performs the following actions: 2.1. Runs policy tests . 2.2. Converts OPA test results into JUnit format. 2.3. Publishes JUnit-formatted results to Jenkins.","title":"Code-Review Pipeline Stages"},{"location":"user-guide/opa-stages/#build-pipeline-stages","text":"In the Build pipeline, the following stages are available: checkout stage, a standard step during which all files are checked out from a selected branch of the Git repository. get-version optional stage, a step where library version is determined either via: 2.1. Standard EDP versioning functionality. 2.2. Manually specified version. In this case .manifest file in a root directory MUST be provided. File must contain a JSON document with revision field. Minimal example: { \"revision\": \"1.0.0\" }\" . tests stage containing a script that performs the following actions: 3.1. Runs policy tests . 3.2. Converts OPA test results into JUnit format. 3.3. Publishes JUnit-formatted results to Jenkins. git-tag stage, a standard step where git branch is tagged with a version.","title":"Build Pipeline Stages"},{"location":"user-guide/opa-stages/#related-articles","text":"EDP Pipeline Framework","title":"Related Articles"},{"location":"user-guide/pipeline-framework/","text":"EDP Pipeline Framework \u2693\ufe0e This chapter provides detailed information about the EDP pipeline framework parts as well as the accurate data about the Code Review , Build and Deploy pipelines with the respective stages. Overview \u2693\ufe0e The general EDP Pipeline Framework consists of several parts: Jenkinsfile - a text file that keeps the definition of a Jenkins Pipeline and is checked into source control. Every Job has its Jenkinsfile that is stored in the specific application repository and in Jenkins as the plain text. Loading Shared Libraries - a part where every job loads libraries with the help of the shared libraries mechanism for Jenkins that allows to create reproducible pipelines, write them uniformly, and manage the update process. There are two main libraries: EDP Pipelines with the common logic described for the main pipelines Code Review, Build, Deploy pipelines and EDP Stages library that keeps the description of the stages for every pipeline. Run Stages - a part where the predefined default stages are launched. Note The whole logic is applied to Jenkins as it is the main tool for the CI/CD processes organization. EDP Pipelines and Stages \u2693\ufe0e Get acquainted with the sections below to get detailed information about the EDP pipelines and their stages 1. Code Review Pipeline \u2693\ufe0e CodeReview() \u2013 a function that allows using the EDP implementation for the Code Review pipeline. Note All values of different parameters that are used during the pipeline execution are stored in Map \"context\". The Code Review pipeline consists of several steps: On the master: Initialization of all objects (Platform, Job, Gerrit, Nexus, Sonar, Application, StageFactory) and loading of the default implementations of EDP stages. On a particular slave that depends on the build tool: Creating workdir for application sources Loading build tool implementation for a particular application Run in a loop all stages (From) and run them either in parallel or one by one 1.1 Overview \u2693\ufe0e Using in pipelines - @Library(['edp-library-pipelines@version']) _ The corresponding enums, interfaces, classes, and their methods can be used separately from the EDP Pipelines library function (please refer to Table 1 and Table 2 ). Table 1. Enums and Interfaces with the respective properties, methods, and examples. Enums Interfaces PlatformType: - OPENSHIFT - KUBERNETES JobType: - CODEREVIEW - BUILD - DEPLOY BuildToolType: - MAVEN - GRADLE - NPM - DOTNET Platform() - contains methods for working with platform CLI. At the moment only OpenShift is supported. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Methods : getJsonPathValue(String k8s_kind, String k8s_kind_name, String jsonPath): return String value of specific parameter of particular object using jsonPath utility. Example : context.platform.getJsonPathValue(''cm'', ''project-settings'', ''.data.username'') . BuildTool() - contains methods for working with different buildTool from ENUM BuildToolType. Should be invoked on slave build agents. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Nexus object - Object of class Nexus. Methods : init: return parameters of buildTool that are needed for running stages. Example : context.buildTool = new BuildToolFactory(). getBuildToolImpl (context.application.config.build_tool, this, context.nexus) context.buildTool.init() . Table 2. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) PlatformFactory() - Class that contains methods getting an implementation of CLI of the platform. At the moment OpenShift and Kubernetes are supported. Methods : getPlatformImpl(PlatformType platform, Script script): return Class Platform . Example : context.platform = new PlatformFactory().getPlatformImpl(PlatformType.OPENSHIFT, this) . Application(String name, Platform platform, Script script) - Class that describes the application object. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). String name - Name for the application for creating an object. Map config - Map of configuration settings for the particular application that is loaded from config map project-settings. String version - Application version, initially empty. Is set on the get-version step. String deployableModule - The name of the deployable module for multi-module applications, initially empty. String buildVersion - Version of the built artifact, contains build number of Job initially empty. String deployableModuleDir - The name of deployable module directory for multi-module applications, initially empty. Array imageBuildArgs - List of arguments for building an application Docker image. Methods : setConfig(String gerrit_autouser, String gerrit_host, String gerrit_sshPort, String gerrit_project): set the config property with values from config map. Example : context.application = new Application(context.job, context.gerrit.project, context.platform, this) context.application.setConfig(context.gerrit.autouser, context.gerrit.host, context.gerrit.sshPort, context.gerrit.project) Job(type: JobType.value, platform: Platform, script: Script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). JobType.value type. String deployTemplatesDirectory - The name of the directory in application repository where deploy templates are located. It can be set for a particular Job through DEPLOY_TEMPLATES_DIRECTORY parameter. String edpName - The name of the EDP Project. Map stages - Contains all stages in JSON format that is retrieved from Jenkins job env variable. String envToPromote - The name of the environment for promoting images. Boolean promoteImages - Defines whether images should be promoted or not. Methods : getParameterValue(String parameter, String defaultValue = null): return parameter of ENV variable of Jenkins job. init(): set all the properties of the Job object. setDisplayName(String displayName): set display name of the Jenkins job. setDescription(String description, Boolean addDescription = false): set new or add to the existing description of the Jenkins job. printDebugInfo(Map context): print context info to the log of Jenkins' job. runStage(String stage_name, Map context): run the particular stage according to its name. Example : context.job = new Job(JobType.CODEREVIEW.value, context.platform, this) context.job.init() context.job.printDebugInfo(context) context.job.setDisplayName(\"test\") context.job.setDescription(\"Name: ${context.application.config.name}\") Gerrit(Job job, Platform platform, Script script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String credentialsId - Credential Id in Jenkins for Gerrit. String autouser - Username of an auto user in Gerrit for integration with Jenkins. String host - Gerrit host. String project - the project name of the built application. String branch - branch to build the application from. String changeNumber - change number of Gerrit commit. String changeName - change name of Gerrit commit. String refspecName - refspecName of Gerrit commit. String sshPort - Gerrit ssh port number. String patchsetNumber - patchsetNumber of Gerrit commit. Methods : init(): set all the properties of Gerrit object. Example : context.gerrit = new Gerrit(context.job, context.platform, this) context.gerrit.init() Nexus(Job job, Platform platform, Script script) - Class that describes the Nexus tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String autouser - Username of an auto user in Nexus for integration with Jenkins. String credentialsId - Credential Id in Jenkins for Nexus. String host - Nexus host. String port - Nexus http(s) port. String repositoriesUrl - Base URL of repositories in Nexus. String restUrl - URL of Rest API. Methods : init(): set all the properties of Nexus object Example : context.nexus = new Nexus(context.job, context.platform, this) context.nexus.init() Sonar(Job job, Platform platform, Script script) - Class that describes the Sonar tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String route - External route of the sonar application. Methods : init(): set all the properties of Sonar object Example : context.sonar = new Sonar(context.job, context.platform, this) context.sonar.init() 1.2 Stages \u2693\ufe0e Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. The Code Review pipeline includes the following default stages: Checkout \u2192 Gerrit Checkout \u2192 Compile \u2192 Tests \u2192 Sonar . Info To get the full description of every stage, please refer to the EDP Stages Framework section. 1.3 How to Redefine or Extend the EDP Pipeline Stages Library \u2693\ufe0e Inspect the points below to redefine or extend the EDP Pipeline Stages Library: Create \u201cstage\u201d folder in your App repository. Create a Groovy file with a meaningful name for the custom stage description. For instance \u2013 CustomBuildMavenApplication.groovy. Describe the stage logic. Redefinition: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"compile\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class CustomBuildMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomBuildMavenApplication Extension: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"new-stage\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class NewStageMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return NewStageMavenApplication 1.4 Using EDP Stages Library in the Pipeline \u2693\ufe0e In order to use the EDP stages, the created pipeline should fit some requirements, that`s why a developer has to do the following: import library - @Library(['edp-library-stages']) _ import StageFactory class - import com.epam.edp.stages.StageFactory define context Map \u2013 context = [:] define stagesFactory instance and load EDP stages: context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } After that, there is the ability to run any EDP stage beforehand by defining a necessary context: context.factory.getStage(\"checkout\",\"maven\",\"application\").run(context) For instance, the pipeline can look like : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] node ( ' maven ' ) { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] stage ( \"checkout\" ) { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } stage ( \"compile\" ) { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } Or in a declarative way : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] pipeline { agent { label ' maven ' } stages { stage ( ' Init ' ){ steps { script { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] } } } stage ( \"Checkout\" ) { steps { script { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } } } stage ( ' Compile ' ) { steps { script { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } } } } 2. Build Pipeline \u2693\ufe0e Build() \u2013 a function that allows using the EDP implementation for the Build pipeline. All values of different parameters that are used during the pipeline execution are stored in Map \"context\". The Build pipeline consists of several steps: On the master: Initialization of all objects (Platform, Job, Gerrit, Nexus, Sonar, Application, StageFactory) and loading default implementations of EDP stages. On a particular slave that depends on the build tool: Creating workdir for application sources; Loading build tool implementation for a particular application; Run in a loop all stages (From) and run them either in parallel or one by one. 2.1 Overview \u2693\ufe0e _Using in pipelines - @Library(['edp-library-pipelines@version'])__ The corresponding enums, interfaces, classes, and their methods can be used separately from the EDP Pipelines library function (please refer to Table 3 and Table 4 ). Table 3. Enums and Interfaces with the respective properties, methods, and examples. Enums Interfaces PlatformType: - OPENSHIFT - KUBERNETES JobType: - CODEREVIEW - BUILD - DEPLOY BuildToolType : - MAVEN - GRADLE - NPM - DOTNET Platform() - contains methods for working with platform CLI. At the moment only OpenShift is supported. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Methods : getJsonPathValue(String k8s_kind, String k8s_kind_name, String jsonPath): return String value of specific parameter of particular object using jsonPath utility. Example : context.platform.getJsonPathValue(\"cm\",\"project-settings\", \".data.username\") BuildTool() - contains methods for working with different buildTool from ENUM BuildToolType. Should be invoked on slave build agents. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Nexus object - Object of class Nexus. See description below: Methods : init: return parameters of buildTool that are needed for running stages. Example : context.buildTool = new BuildToolFactory().getBuildToolImpl (context.application.config.build_tool, this, context.nexus)context.buildTool.init() Table 4. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) PlatformFactory() - Class that contains methods getting an implementation of CLI of the platform. At the moment OpenShift and Kubernetes are supported. Methods : getPlatformImpl(PlatformType platform, Script script): return Class Platform Example : context.platform = new PlatformFactory().getPlatformImpl(PlatformType.OPENSHIFT, this) Application(String name, Platform platform, Script script) - Class that describes the application object. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). String name - Name for the application for creating an object. Map config - Map of configuration settings for the particular application that is loaded from config map project-settings. String version - Application version, initially empty. Is set on the get-version step. String deployableModule - The name of the deployable module for multi-module applications, initially empty. String buildVersion - Version of the built artifact, contains build number of Job initially empty. String deployableModuleDir - The name of deployable module directory for multi-module applications, initially empty. Array imageBuildArgs - List of arguments for building the application Docker image. Methods : setConfig(String gerrit_autouser, String gerrit_host, String gerrit_sshPort, String gerrit_project): set the config property with values from config map. Example : context.application = new Application(context.job, context.gerrit.project, context.platform, this) context.application.setConfig(context.gerrit.autouser, context.gerrit.host, context.gerrit.sshPort, context.gerrit.project) Job(type: JobType.value, platform: Platform, script: Script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). JobType.value type. String deployTemplatesDirectory - The name of the directory in application repository, where deploy templates are located. It can be set for a particular Job through DEPLOY_TEMPLATES_DIRECTORY parameter. String edpName - The name of the EDP Project. Map stages - Contains all stages in JSON format that is retrieved from Jenkins job env variable. String envToPromote - The name of the environment for promoting images. Boolean promoteImages - Defines whether images should be promoted or not. Methods : getParameterValue(String parameter, String defaultValue = null): return parameter of ENV variable of Jenkins job. init(): set all the properties of the Job object. setDisplayName(String displayName): set display name of the Jenkins job. setDescription(String description, Boolean addDescription = false): set new or add to the existing description of the Jenkins job. printDebugInfo(Map context): print context info to the log of Jenkins' job. runStage(String stage_name, Map context): run the particular stage according to its name. Example : context.job = new Job(JobType.CODEREVIEW.value, context.platform, this) context.job.init() context.job.printDebugInfo(context) context.job.setDisplayName(\"test\") context.job.setDescription(\"Name: ${context.application.config.name}\") Gerrit(Job job, Platform platform, Script script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String credentialsId - Credentials Id in Jenkins for Gerrit. String autouser - Username of an auto user in Gerrit for integration with Jenkins. String host - Gerrit host. String project - the project name of the built application. String branch - branch to build an application from. String changeNumber - change number of Gerrit commit. String changeName - change name of Gerrit commit. String refspecName - refspecName of Gerrit commit. String sshPort - Gerrit ssh port number. String patchsetNumber - patchsetNumber of Gerrit commit. Methods : init(): set all the properties of Gerrit object Example : context.gerrit = new Gerrit(context.job, context.platform, this) context.gerrit.init() Nexus(Job job, Platform platform, Script script) - Class that describes the Nexus tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String autouser - Username of an auto user in Nexus for integration with Jenkins. String credentialsId - Credentials Id in Jenkins for Nexus. String host - Nexus host. String port - Nexus http(s) port. String repositoriesUrl - Base URL of repositories in Nexus. String restUrl - URL of Rest API. Methods : init(): set all the properties of the Nexus object. Example : context.nexus = new Nexus(context.job, context.platform, this) context.nexus.init() Sonar(Job job, Platform platform, Script script) - Class that describes the Sonar tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String route - External route of the sonar application. Methods : init(): set all the properties of Sonar object. Example : context.sonar = new Sonar(context.job, context.platform, this) context.sonar.init() 2.2 Stages \u2693\ufe0e Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. The Build pipeline includes the following default stages: Checkout \u2192 Gerrit Checkout \u2192 Compile \u2192 Get version \u2192 Tests \u2192 Sonar \u2192 Build \u2192 Build Docker Image \u2192 Push \u2192 Git tag . Info To get the full description of every stage, please refer to the EDP Stages Framework section. 2.3 How to Redefine or Extend EDP Pipeline Stages Library \u2693\ufe0e Inspect the points below to redefine or extend the EDP Pipeline Stages Library: Create a \u201cstage\u201d folder in the App repository. Create a Groovy file with a meaningful name for the custom stage description. For instance \u2013 CustomBuildMavenApplication.groovy Describe stage logic. Redefinition: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"compile\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class CustomBuildMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomBuildMavenApplication Extension: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"new-stage\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class NewStageMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return NewStageMavenApplication 2.4 Using EDP Stages Library in the Pipeline \u2693\ufe0e In order to use the EDP stages, the created pipeline should fit some requirements, that`s why a developer has to do the following: import library - @Library(['edp-library-stages']) _ import StageFactory class - import com.epam.edp.stages.StageFactory define context Map \u2013 context = [:] define stagesFactory instance and load EDP stages: context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } After that, there is the ability to run any EDP stage beforehand by defining a requirement context context.factory.getStage(\"checkout\",\"maven\",\"application\").run(context) For instance, the pipeline can look like : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] node ( ' maven ' ) { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] stage ( \"checkout\" ) { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } stage ( \"compile\" ) { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } Or in a declarative way : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] pipeline { agent { label ' maven ' } stages { stage ( ' Init ' ){ steps { script { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] } } } stage ( \"Checkout\" ) { steps { script { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } } } stage ( ' Compile ' ) { steps { script { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } } } } 3. EDP Library Stages Description \u2693\ufe0e Using in pipelines - @Library(['edp-library-stages@version']) _ The corresponding enums, classes, interfaces and their methods can be used separately from the EDP Stages library function (please refer to Table 5 ). Table 5. Enums and Classes with the respective properties, methods, and examples. Enums Classes ProjectType : - APPLICATION - AUTOTESTS - LIBRARY StageFactory() - Class that contains methods getting an implementation of the particular stage either EDP from shared library or custom from application repository. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Map stages - Map of stages implementations. Methods : loadEdpStages(): return a list of Classes that describes EDP stages implementations. loadCustomStages(String directory): return a list of Classes that describes EDP custom stages from application repository from \"directory\". The \"directory\" should have an absolute path to files with classes of custom stages implementations. Should be run from slave agent. add(Class clazz): register class for some particular stage in stages map of StageFactory class. getStage(String name, String buildTool, String type): return an object of the class for a particular stage from stages property based on stage name and buildTool, type of application. Example : context.factory = new StageFactory(script: this) context.factory.loadEdpStages().each() { context.factory.add(it) } context.factory.loadCustomStages(\"${context.workDir}/stages\").each() { context.factory.add(it) } context.factory.getStage(stageName.toLowerCase(),context.application.config.build_tool.toLowerCase(), context.application.config.type).run(context) 4. EDP Stages Framework \u2693\ufe0e Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. Inspect the Table 6 and Table 7 that contain the full description of every stage that can be included in Code Review and Build pipelines: Checkout \u2192 Gerrit Checkout \u2192 Compile \u2192 Get version \u2192 Tests \u2192 Sonar \u2192 Build \u2192 Build Docker Image \u2192 Push \u2192 Git tag . Table 6. The Checkout, Gerrit Checkout, Compile, Get version, and Tests stages description. Checkout Gerrit Checkout Compile Get version Tests name = \"checkout\", buildTool = [\"maven\", \"npm\", \"dotnet\",\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - StageFactory context.factory - String context.gerrit.branch - String context.gerrit.credentialsId - String context.application.config.cloneUrl name = \"gerrit-checkout\", buildTool = [\"maven\", \"npm\", \"dotnet\",\"gradle\"] type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY] context required: - String context.workDir - StageFactory context.factory - String context.gerrit.changeName - String context.gerrit.refspecName - String context.gerrit.credentialsId - String context.application.config.cloneUrl name = \"compile\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.sln_filename output: - String context.buildTool.sln_filename buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.groupRepository name = \"get-version\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - Map(empty) context.application - String context.gerrit.branch - Job context.job output: -String context.application.deplyableModule - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.command - Job context.job - String context.gerrit.branch output: - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.command - Job context.job - String context.gerrit.branch output: - String context.application.deplyableModule - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - Job context.job - String context.gerrit.branch output: - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion name = \"tests\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.command buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command type = [ProjectType.AUTOTESTS] context required: - String context.workDir - String context.buildTool.command - String context.application.config.report_framework buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir Table 7. The Sonar, Build, Build Docker Image, Push, and Git tag stages description. Sonar Build Build Docker Image Push Git tag name = \"sonar\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.job.type - String context.application.name - String context.buildTool.sln_filename - String context.sonar.route - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.job.type - String context.nexus.credentialsId - String context.buildTool.command - String context.application.name - String context.sonarRoute - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) buildTool = [\"maven\"] type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY] context required: - String context.workDir - String context.job.type - String context.nexus.credentialsId - String context.application.name - String context.buildTool.command - String context.sonar.route - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.job.type - String context.sonar.route - String context.application.name - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) name = \"build\" buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.groupRepository name = \"build-image\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote name = \"push\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.gerrit.project - String context.buildTool.sln_filename - String context.buildTool.snugetApiKey - String context.buildTool.hostedRepository buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.application.version - String context.buildTool.hostedRepository - String context. buildTool.settings buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.application.version - String context.buildTool.hostedRepository - String context.buildTool.command buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.hostedRepository - String context.gerrit.autouser name = \"git-tag\" buildTool = [\"maven\", \"npm\", \"dotnet\",\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.gerrit.credentialsId - String context.gerrit.sshPort - String context.gerrit.host - String context.gerrit.autouser - String context.application.buildVersion 5. Deploy Pipeline \u2693\ufe0e Deploy() \u2013 a function that allows using the EDP implementation for the deploy pipeline. All values of different parameters that are used during the pipeline execution are stored in Map \"context\". The deploy pipeline consists of several steps: On the master: * Initialization of all objects (Platform, Job, Gerrit, Nexus, StageFactory) and loading the default implementations of EDP stages * Creating an environment if it doesn`t exist * Deploying the last versions of the applications * Run predefined manual gates On a particular autotest slave that depends on the build tool: Creating workdir for autotest sources Run predefined autotests 5.1 EDP Library Pipelines Description \u2693\ufe0e Using in pipelines - @Library(['edp-library-pipelines@version']) _ The corresponding enums and interfaces with their methods can be used separately from the EDP Pipelines library function (please refer to Table 8 and Table 9 ). Table 8. Enums and Interfaces with the respective properties, methods, and examples. Enums Interfaces PlatformType : - OPENSHIFT - KUBERNETES JobType : - CODEREVIEW - BUILD - DEPLOY BuildToolType : - MAVEN - GRADLE - NPM - DOTNET Platform() - contains methods for working with platform CLI. At the moment only OpenShift is supported. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Methods : getJsonPathValue(String k8s_kind, String k8s_kind_name, String jsonPath): return String value of specific parameter of particular object using jsonPath utility. Example : context.platform.getJsonPathValue(\"cm\",\"project-settings\", \".data.username\") BuildTool() - contains methods for working with different buildTool from ENUM BuildToolType. (Should be invoked on slave build agents) Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Nexus object - Object of class Nexus. Methods : init: return parameters of buildTool that are needed for running stages. Example : context.buildTool = new BuildToolFactory().getBuildToolImpl (context.application.config.build_tool, this, context.nexus) context.buildTool.init() Table 9. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) PlatformFactory() - Class that contains methods getting implementation of CLI of platform. At the moment OpenShift and Kubernetes are supported. Methods : getPlatformImpl(PlatformType platform, Script script): return Class Platform Example : context.platform = new PlatformFactory().getPlatformImpl(PlatformType.OPENSHIFT, this) Application(String name, Platform platform, Script script) - Class that describe the application object. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform() String name - Name for the application for creating object Map config - Map of configuration settings for particular application that is loaded from config map project-settings String version - Application version, initially empty. Is set on get-version step. String deployableModule - The name of deployable module for multi module applications, initially empty. String buildVersion - Version of built artifact, contains build number of Job initially empty String deployableModuleDir - The name of deployable module directory for multi module applications, initially empty. Array imageBuildArgs - List of arguments for building application Docker image Methods : setConfig(String gerrit_autouser, String gerrit_host, String gerrit_sshPort, String gerrit_project): set the config property with values from config map Example : context.application = new Application(context.job, context.gerrit.project, context.platform, this) context.application.setConfig(context.gerrit.autouser, context.gerrit.host, context.gerrit.sshPort, context.gerrit.project) Job(type: JobType.value, platform: Platform, script: Script) - Class that describe the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\" Platform platform - Object of a class Platform(). JobType.value type. String deployTemplatesDirectory - The name of the directory in application repository, where deploy templates are located. Can be set for particular Job through DEPLOY_TEMPLATES_DIRECTORY parameter. String edpName - The name of the EDP Project. Map stages - Contains all stages in JSON format that is retrieved from Jenkins job env variable. String envToPromote - The name of the environment for promoting images. Boolean promoteImages - Defines whether images should be promoted or not. Methods : getParameterValue(String parameter, String defaultValue = null): return parameter of ENV variable of Jenkins job. init(): set all the properties of Job object. setDisplayName(String displayName): set display name of the Jenkins job. setDescription(String description, Boolean addDescription = false): set new or add to existing description of the Jenkins job. printDebugInfo(Map context): print context info to log of Jenkins job. runStage(String stage_name, Map context): run the particular stage according to its name. Example : context.job = new Job(JobType.DEPLOY.value, context.platform, this) context.job.init() context.job.printDebugInfo(context) context.job.setDisplayName(\"test\") context.job.setDescription(\"Name: ${context.application.config.name}\") Gerrit(Job job, Platform platform, Script script) - Class that describe the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String credentialsId - Credential Id in Jenkins for Gerrit. String autouser - Username of autouser in Gerrit for integration with Jenkins. String host - Gerrit host. String project - project name of built application. String branch - branch to build application from. String changeNumber - change number of Gerrit commit. String changeName - change name of Gerrit commit. String refspecName - refspecName of Gerrit commit. String sshPort - gerrit ssh port number. String patchsetNumber - patchsetNumber of Gerrit commit. Methods : init(): set all the properties of Gerrit object. Example : context.gerrit = new Gerrit(context.job, context.platform, this) context.gerrit.init() . Nexus(Job job, Platform platform, Script script) - Class that describe the Nexus tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String autouser - Username of autouser in Nexus for integration with Jenkins. String credentialsId - Credential Id in Jenkins for Nexus. String host - Nexus host. String port - Nexus http(s) port. String repositoriesUrl - Base URL of repositories in Nexus. String restUrl - URL of Rest API. Methods : init(): set all the properties of Nexus object. Example : context.nexus = new Nexus(context.job, context.platform, this) context.nexus.init() . 5.2 EDP Library Stages Description \u2693\ufe0e Using in pipelines - @Library(['edp-library-stages@version']) _ The corresponding classes with methods can be used separately from the EDP Pipelines library function (please refer to Table 10 ). Table 10. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) StageFactory() - Class that contains methods getting implementation of particular stage either EDP from shared library or custom from application repository. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\" Map stages - Map of stages implementations Methods : loadEdpStages(): return list of Classes that describes EDP stages implementations loadCustomStages(String directory): return list of Classes that describes EDP custom stages from application repository from \"directory\". The \"directory\" should be absolute path to files with classes of custom stages implementations. Should be run from slave agent. add(Class clazz): register class for some particular stage in stages map of StageFactory class getStage(String name, String buildTool, String type): return object of the class for particular stage from stages property based on stage name and buildTool, type of application Example : context.factory = new StageFactory(script: this) context.factory.loadEdpStages().each() { context.factory.add(it) } context.factory.loadCustomStages(\"${context.workDir}/stages\").each() { context.factory.add(it) } context.factory.getStage(stageName.toLowerCase(),context.application.config.build_tool.toLowerCase(), context.application.config.type).run(context) . 5.3 Deploy Pipeline Stages \u2693\ufe0e Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. The stages for the deploy pipeline are independent of the build tool and application type. Find below (see Table 11 ) the full description of every stage: Deploy \u2192 Automated tests \u2192 Promote Images . Table 11. The Deploy, Automated tests, and Promote Images stages description. Deploy Automated tests Promote Images name = \"deploy\" buildTool = null type = null context required: \u2022 String context.workDir \u2022 StageFactory context.factory \u2022 String context.gerrit.autouser \u2022 String context.gerrit.host \u2022 String context.application.config.cloneUrl \u2022 String context.jenkins.token \u2022 String context.job.edpName \u2022 String context.job.buildUrl \u2022 String context.job.jenkinsUrl \u2022 String context.job.metaProject \u2022 List context.job.applicationsList [['name':'application1_name','version':'application1_version],...] \u2022 String context.job.deployTemplatesDirectory output: \u2022 List context.job.updatedApplicaions [['name':'application1_name','version':'application1_version],...] name = \"automation-tests\", buildTool = null, type = null context required: - String context.workDir - StageFactory context.factory - String context.gerrit.credentialsId - String context.autotest.config.cloneUrl - String context.autotest.name - String context.job.stageWithoutPrefixName - String context.buildTool.settings - String context.autotest.config.report_framework name = \"promote-images\" buildTool = null type = null context required: - String context.workDir - String context.buildTool.sln_filename - List context.job.updatedApplicaions [['name':'application1_name','version':'application1_version],...] 5.4 How to Redefine or Extend EDP Pipeline Stages Library \u2693\ufe0e Info Currently, the redefinition of Deploy pipeline stages is prohibited. 5.5 Using EDP Library Stages in the Pipeline \u2693\ufe0e In order to use the EDP stages, the created pipeline should fit some requirements, that`s why a developer has to do the following: import libraries - @Library(['edp-library-stages', 'edp-library-pipelines']) _ import reference EDP classes(See example below) define context Map \u2013 context = [:] define reference \"init\" stage After that, there is the ability to run any EDP stage beforehand by defining requirement context context.job.runStage(\"Deploy\", context) . For instance, the pipeline can look like : @Library ( [ ' edp - library - stages ' , ' edp - library - pipelines ' ] ) _ import com.epam.edp.stages.StageFactory import com.epam.edp.platform.PlatformFactory import com.epam.edp.platform.PlatformType import com.epam.edp.JobType context = [ : ] node ( ' master ' ) { stage ( \"Init\" ) { context . platform = new PlatformFactory (). getPlatformImpl ( PlatformType . OPENSHIFT , this ) context . job = new com . epam . edp . Job ( JobType . DEPLOY . value , context . platform , this ) context . job . init () context . job . initDeployJob () println ( \"[JENKINS][DEBUG] Created object job with type - ${context.job.type}\" ) context . nexus = new com . epam . edp . Nexus ( context . job , context . platform , this ) context . nexus . init () context . jenkins = new com . epam . edp . Jenkins ( context . job , context . platform , this ) context . jenkins . init () context . gerrit = new com . epam . edp . Gerrit ( context . job , context . platform , this ) context . gerrit . init () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . environment = new com . epam . edp . Environment ( context . job . deployProject , context . platform , this ) context . job . printDebugInfo ( context ) context . job . setDisplayName ( \"${currentBuild.displayName}-${context.job.deployProject}\" ) context . job . generateInputDataForDeployJob () } stage ( \"Pre Deploy Custom stage\" ) { println ( \"Some custom pre deploy logic\" ) } context . job . runStage ( \"Deploy\" , context ) stage ( \"Post Deploy Custom stage\" ) { println ( \"Some custom post deploy logic\" ) } } Or in a declarative way : @Library ( [ ' edp - library - stages ' , ' edp - library - pipelines ' ] ) _ import com.epam.edp.stages.StageFactory import com.epam.edp.platform.PlatformFactory import com.epam.edp.platform.PlatformType import com.epam.edp.JobType context = [ : ] pipeline { agent { label ' master ' } stages { stage ( ' Init ' ) { steps { script { context . platform = new PlatformFactory (). getPlatformImpl ( PlatformType . OPENSHIFT , this ) context . job = new com . epam . edp . Job ( JobType . DEPLOY . value , context . platform , this ) context . job . init () context . job . initDeployJob () println ( \"[JENKINS][DEBUG] Created object job with type - ${context.job.type}\" ) context . nexus = new com . epam . edp . Nexus ( context . job , context . platform , this ) context . nexus . init () context . jenkins = new com . epam . edp . Jenkins ( context . job , context . platform , this ) context . jenkins . init () context . gerrit = new com . epam . edp . Gerrit ( context . job , context . platform , this ) context . gerrit . init () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . environment = new com . epam . edp . Environment ( context . job . deployProject , context . platform , this ) context . job . printDebugInfo ( context ) context . job . setDisplayName ( \"${currentBuild.displayName}-${context.job.deployProject}\" ) context . job . generateInputDataForDeployJob () } } } stage ( ' Deploy ' ) { steps { script { context . factory . getStage ( \"deploy\" ). run ( context ) } } } stage ( ' Custom stage ' ) { steps { println ( \"Some custom logic\" ) } } } } Related Articles \u2693\ufe0e EDP Stages Use Terraform Library in EDP","title":"Overview"},{"location":"user-guide/pipeline-framework/#edp-pipeline-framework","text":"This chapter provides detailed information about the EDP pipeline framework parts as well as the accurate data about the Code Review , Build and Deploy pipelines with the respective stages.","title":"EDP Pipeline Framework"},{"location":"user-guide/pipeline-framework/#overview","text":"The general EDP Pipeline Framework consists of several parts: Jenkinsfile - a text file that keeps the definition of a Jenkins Pipeline and is checked into source control. Every Job has its Jenkinsfile that is stored in the specific application repository and in Jenkins as the plain text. Loading Shared Libraries - a part where every job loads libraries with the help of the shared libraries mechanism for Jenkins that allows to create reproducible pipelines, write them uniformly, and manage the update process. There are two main libraries: EDP Pipelines with the common logic described for the main pipelines Code Review, Build, Deploy pipelines and EDP Stages library that keeps the description of the stages for every pipeline. Run Stages - a part where the predefined default stages are launched. Note The whole logic is applied to Jenkins as it is the main tool for the CI/CD processes organization.","title":"Overview"},{"location":"user-guide/pipeline-framework/#edp-pipelines-and-stages","text":"Get acquainted with the sections below to get detailed information about the EDP pipelines and their stages","title":"EDP Pipelines and Stages"},{"location":"user-guide/pipeline-framework/#1-code-review-pipeline","text":"CodeReview() \u2013 a function that allows using the EDP implementation for the Code Review pipeline. Note All values of different parameters that are used during the pipeline execution are stored in Map \"context\". The Code Review pipeline consists of several steps: On the master: Initialization of all objects (Platform, Job, Gerrit, Nexus, Sonar, Application, StageFactory) and loading of the default implementations of EDP stages. On a particular slave that depends on the build tool: Creating workdir for application sources Loading build tool implementation for a particular application Run in a loop all stages (From) and run them either in parallel or one by one","title":"1. Code Review Pipeline"},{"location":"user-guide/pipeline-framework/#11-overview","text":"Using in pipelines - @Library(['edp-library-pipelines@version']) _ The corresponding enums, interfaces, classes, and their methods can be used separately from the EDP Pipelines library function (please refer to Table 1 and Table 2 ). Table 1. Enums and Interfaces with the respective properties, methods, and examples. Enums Interfaces PlatformType: - OPENSHIFT - KUBERNETES JobType: - CODEREVIEW - BUILD - DEPLOY BuildToolType: - MAVEN - GRADLE - NPM - DOTNET Platform() - contains methods for working with platform CLI. At the moment only OpenShift is supported. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Methods : getJsonPathValue(String k8s_kind, String k8s_kind_name, String jsonPath): return String value of specific parameter of particular object using jsonPath utility. Example : context.platform.getJsonPathValue(''cm'', ''project-settings'', ''.data.username'') . BuildTool() - contains methods for working with different buildTool from ENUM BuildToolType. Should be invoked on slave build agents. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Nexus object - Object of class Nexus. Methods : init: return parameters of buildTool that are needed for running stages. Example : context.buildTool = new BuildToolFactory(). getBuildToolImpl (context.application.config.build_tool, this, context.nexus) context.buildTool.init() . Table 2. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) PlatformFactory() - Class that contains methods getting an implementation of CLI of the platform. At the moment OpenShift and Kubernetes are supported. Methods : getPlatformImpl(PlatformType platform, Script script): return Class Platform . Example : context.platform = new PlatformFactory().getPlatformImpl(PlatformType.OPENSHIFT, this) . Application(String name, Platform platform, Script script) - Class that describes the application object. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). String name - Name for the application for creating an object. Map config - Map of configuration settings for the particular application that is loaded from config map project-settings. String version - Application version, initially empty. Is set on the get-version step. String deployableModule - The name of the deployable module for multi-module applications, initially empty. String buildVersion - Version of the built artifact, contains build number of Job initially empty. String deployableModuleDir - The name of deployable module directory for multi-module applications, initially empty. Array imageBuildArgs - List of arguments for building an application Docker image. Methods : setConfig(String gerrit_autouser, String gerrit_host, String gerrit_sshPort, String gerrit_project): set the config property with values from config map. Example : context.application = new Application(context.job, context.gerrit.project, context.platform, this) context.application.setConfig(context.gerrit.autouser, context.gerrit.host, context.gerrit.sshPort, context.gerrit.project) Job(type: JobType.value, platform: Platform, script: Script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). JobType.value type. String deployTemplatesDirectory - The name of the directory in application repository where deploy templates are located. It can be set for a particular Job through DEPLOY_TEMPLATES_DIRECTORY parameter. String edpName - The name of the EDP Project. Map stages - Contains all stages in JSON format that is retrieved from Jenkins job env variable. String envToPromote - The name of the environment for promoting images. Boolean promoteImages - Defines whether images should be promoted or not. Methods : getParameterValue(String parameter, String defaultValue = null): return parameter of ENV variable of Jenkins job. init(): set all the properties of the Job object. setDisplayName(String displayName): set display name of the Jenkins job. setDescription(String description, Boolean addDescription = false): set new or add to the existing description of the Jenkins job. printDebugInfo(Map context): print context info to the log of Jenkins' job. runStage(String stage_name, Map context): run the particular stage according to its name. Example : context.job = new Job(JobType.CODEREVIEW.value, context.platform, this) context.job.init() context.job.printDebugInfo(context) context.job.setDisplayName(\"test\") context.job.setDescription(\"Name: ${context.application.config.name}\") Gerrit(Job job, Platform platform, Script script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String credentialsId - Credential Id in Jenkins for Gerrit. String autouser - Username of an auto user in Gerrit for integration with Jenkins. String host - Gerrit host. String project - the project name of the built application. String branch - branch to build the application from. String changeNumber - change number of Gerrit commit. String changeName - change name of Gerrit commit. String refspecName - refspecName of Gerrit commit. String sshPort - Gerrit ssh port number. String patchsetNumber - patchsetNumber of Gerrit commit. Methods : init(): set all the properties of Gerrit object. Example : context.gerrit = new Gerrit(context.job, context.platform, this) context.gerrit.init() Nexus(Job job, Platform platform, Script script) - Class that describes the Nexus tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String autouser - Username of an auto user in Nexus for integration with Jenkins. String credentialsId - Credential Id in Jenkins for Nexus. String host - Nexus host. String port - Nexus http(s) port. String repositoriesUrl - Base URL of repositories in Nexus. String restUrl - URL of Rest API. Methods : init(): set all the properties of Nexus object Example : context.nexus = new Nexus(context.job, context.platform, this) context.nexus.init() Sonar(Job job, Platform platform, Script script) - Class that describes the Sonar tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String route - External route of the sonar application. Methods : init(): set all the properties of Sonar object Example : context.sonar = new Sonar(context.job, context.platform, this) context.sonar.init()","title":"1.1 Overview"},{"location":"user-guide/pipeline-framework/#12-stages","text":"Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. The Code Review pipeline includes the following default stages: Checkout \u2192 Gerrit Checkout \u2192 Compile \u2192 Tests \u2192 Sonar . Info To get the full description of every stage, please refer to the EDP Stages Framework section.","title":"1.2 Stages"},{"location":"user-guide/pipeline-framework/#13-how-to-redefine-or-extend-the-edp-pipeline-stages-library","text":"Inspect the points below to redefine or extend the EDP Pipeline Stages Library: Create \u201cstage\u201d folder in your App repository. Create a Groovy file with a meaningful name for the custom stage description. For instance \u2013 CustomBuildMavenApplication.groovy. Describe the stage logic. Redefinition: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"compile\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class CustomBuildMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomBuildMavenApplication Extension: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"new-stage\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class NewStageMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return NewStageMavenApplication","title":"1.3 How to Redefine or Extend the EDP Pipeline Stages Library"},{"location":"user-guide/pipeline-framework/#14-using-edp-stages-library-in-the-pipeline","text":"In order to use the EDP stages, the created pipeline should fit some requirements, that`s why a developer has to do the following: import library - @Library(['edp-library-stages']) _ import StageFactory class - import com.epam.edp.stages.StageFactory define context Map \u2013 context = [:] define stagesFactory instance and load EDP stages: context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } After that, there is the ability to run any EDP stage beforehand by defining a necessary context: context.factory.getStage(\"checkout\",\"maven\",\"application\").run(context) For instance, the pipeline can look like : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] node ( ' maven ' ) { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] stage ( \"checkout\" ) { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } stage ( \"compile\" ) { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } Or in a declarative way : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] pipeline { agent { label ' maven ' } stages { stage ( ' Init ' ){ steps { script { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] } } } stage ( \"Checkout\" ) { steps { script { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } } } stage ( ' Compile ' ) { steps { script { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } } } }","title":"1.4 Using EDP Stages Library in the Pipeline"},{"location":"user-guide/pipeline-framework/#2-build-pipeline","text":"Build() \u2013 a function that allows using the EDP implementation for the Build pipeline. All values of different parameters that are used during the pipeline execution are stored in Map \"context\". The Build pipeline consists of several steps: On the master: Initialization of all objects (Platform, Job, Gerrit, Nexus, Sonar, Application, StageFactory) and loading default implementations of EDP stages. On a particular slave that depends on the build tool: Creating workdir for application sources; Loading build tool implementation for a particular application; Run in a loop all stages (From) and run them either in parallel or one by one.","title":"2. Build Pipeline"},{"location":"user-guide/pipeline-framework/#21-overview","text":"_Using in pipelines - @Library(['edp-library-pipelines@version'])__ The corresponding enums, interfaces, classes, and their methods can be used separately from the EDP Pipelines library function (please refer to Table 3 and Table 4 ). Table 3. Enums and Interfaces with the respective properties, methods, and examples. Enums Interfaces PlatformType: - OPENSHIFT - KUBERNETES JobType: - CODEREVIEW - BUILD - DEPLOY BuildToolType : - MAVEN - GRADLE - NPM - DOTNET Platform() - contains methods for working with platform CLI. At the moment only OpenShift is supported. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Methods : getJsonPathValue(String k8s_kind, String k8s_kind_name, String jsonPath): return String value of specific parameter of particular object using jsonPath utility. Example : context.platform.getJsonPathValue(\"cm\",\"project-settings\", \".data.username\") BuildTool() - contains methods for working with different buildTool from ENUM BuildToolType. Should be invoked on slave build agents. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Nexus object - Object of class Nexus. See description below: Methods : init: return parameters of buildTool that are needed for running stages. Example : context.buildTool = new BuildToolFactory().getBuildToolImpl (context.application.config.build_tool, this, context.nexus)context.buildTool.init() Table 4. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) PlatformFactory() - Class that contains methods getting an implementation of CLI of the platform. At the moment OpenShift and Kubernetes are supported. Methods : getPlatformImpl(PlatformType platform, Script script): return Class Platform Example : context.platform = new PlatformFactory().getPlatformImpl(PlatformType.OPENSHIFT, this) Application(String name, Platform platform, Script script) - Class that describes the application object. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). String name - Name for the application for creating an object. Map config - Map of configuration settings for the particular application that is loaded from config map project-settings. String version - Application version, initially empty. Is set on the get-version step. String deployableModule - The name of the deployable module for multi-module applications, initially empty. String buildVersion - Version of the built artifact, contains build number of Job initially empty. String deployableModuleDir - The name of deployable module directory for multi-module applications, initially empty. Array imageBuildArgs - List of arguments for building the application Docker image. Methods : setConfig(String gerrit_autouser, String gerrit_host, String gerrit_sshPort, String gerrit_project): set the config property with values from config map. Example : context.application = new Application(context.job, context.gerrit.project, context.platform, this) context.application.setConfig(context.gerrit.autouser, context.gerrit.host, context.gerrit.sshPort, context.gerrit.project) Job(type: JobType.value, platform: Platform, script: Script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). JobType.value type. String deployTemplatesDirectory - The name of the directory in application repository, where deploy templates are located. It can be set for a particular Job through DEPLOY_TEMPLATES_DIRECTORY parameter. String edpName - The name of the EDP Project. Map stages - Contains all stages in JSON format that is retrieved from Jenkins job env variable. String envToPromote - The name of the environment for promoting images. Boolean promoteImages - Defines whether images should be promoted or not. Methods : getParameterValue(String parameter, String defaultValue = null): return parameter of ENV variable of Jenkins job. init(): set all the properties of the Job object. setDisplayName(String displayName): set display name of the Jenkins job. setDescription(String description, Boolean addDescription = false): set new or add to the existing description of the Jenkins job. printDebugInfo(Map context): print context info to the log of Jenkins' job. runStage(String stage_name, Map context): run the particular stage according to its name. Example : context.job = new Job(JobType.CODEREVIEW.value, context.platform, this) context.job.init() context.job.printDebugInfo(context) context.job.setDisplayName(\"test\") context.job.setDescription(\"Name: ${context.application.config.name}\") Gerrit(Job job, Platform platform, Script script) - Class that describes the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String credentialsId - Credentials Id in Jenkins for Gerrit. String autouser - Username of an auto user in Gerrit for integration with Jenkins. String host - Gerrit host. String project - the project name of the built application. String branch - branch to build an application from. String changeNumber - change number of Gerrit commit. String changeName - change name of Gerrit commit. String refspecName - refspecName of Gerrit commit. String sshPort - Gerrit ssh port number. String patchsetNumber - patchsetNumber of Gerrit commit. Methods : init(): set all the properties of Gerrit object Example : context.gerrit = new Gerrit(context.job, context.platform, this) context.gerrit.init() Nexus(Job job, Platform platform, Script script) - Class that describes the Nexus tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String autouser - Username of an auto user in Nexus for integration with Jenkins. String credentialsId - Credentials Id in Jenkins for Nexus. String host - Nexus host. String port - Nexus http(s) port. String repositoriesUrl - Base URL of repositories in Nexus. String restUrl - URL of Rest API. Methods : init(): set all the properties of the Nexus object. Example : context.nexus = new Nexus(context.job, context.platform, this) context.nexus.init() Sonar(Job job, Platform platform, Script script) - Class that describes the Sonar tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String route - External route of the sonar application. Methods : init(): set all the properties of Sonar object. Example : context.sonar = new Sonar(context.job, context.platform, this) context.sonar.init()","title":"2.1 Overview"},{"location":"user-guide/pipeline-framework/#22-stages","text":"Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. The Build pipeline includes the following default stages: Checkout \u2192 Gerrit Checkout \u2192 Compile \u2192 Get version \u2192 Tests \u2192 Sonar \u2192 Build \u2192 Build Docker Image \u2192 Push \u2192 Git tag . Info To get the full description of every stage, please refer to the EDP Stages Framework section.","title":"2.2 Stages"},{"location":"user-guide/pipeline-framework/#23-how-to-redefine-or-extend-edp-pipeline-stages-library","text":"Inspect the points below to redefine or extend the EDP Pipeline Stages Library: Create a \u201cstage\u201d folder in the App repository. Create a Groovy file with a meaningful name for the custom stage description. For instance \u2013 CustomBuildMavenApplication.groovy Describe stage logic. Redefinition: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"compile\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class CustomBuildMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return CustomBuildMavenApplication Extension: import com.epam.edp.stages.ProjectType import com.epam.edp.stages.Stage @Stage ( name = \"new-stage\" , buildTool = \"maven\" , type = ProjectType . APPLICATION ) class NewStageMavenApplication { Script script void run ( context ) { script . sh \"echo 'Your custom logic of the stage'\" } } return NewStageMavenApplication","title":"2.3 How to Redefine or Extend EDP Pipeline Stages Library"},{"location":"user-guide/pipeline-framework/#24-using-edp-stages-library-in-the-pipeline","text":"In order to use the EDP stages, the created pipeline should fit some requirements, that`s why a developer has to do the following: import library - @Library(['edp-library-stages']) _ import StageFactory class - import com.epam.edp.stages.StageFactory define context Map \u2013 context = [:] define stagesFactory instance and load EDP stages: context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } After that, there is the ability to run any EDP stage beforehand by defining a requirement context context.factory.getStage(\"checkout\",\"maven\",\"application\").run(context) For instance, the pipeline can look like : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] node ( ' maven ' ) { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] stage ( \"checkout\" ) { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } stage ( \"compile\" ) { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } Or in a declarative way : @Library ( [ ' edp - library - stages ' ] ) _ import com.epam.edp.stages.StageFactory import org.apache.commons.lang.RandomStringUtils context = [ : ] pipeline { agent { label ' maven ' } stages { stage ( ' Init ' ){ steps { script { context . workDir = new File ( \"/tmp/${RandomStringUtils.random(10, true, true)}\" ) context . workDir . deleteDir () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . gerrit = [ : ] context . application = [ : ] context . application . config = [ : ] context . buildTool = [ : ] context . nexus = [ : ] } } } stage ( \"Checkout\" ) { steps { script { context . gerrit . branch = \"master\" context . gerrit . credentialsId = \"jenkins\" context . application . config . cloneUrl = \"ssh://jenkins@gerrit:32092/sit-718-cloned-java-maven-project\" context . factory . getStage ( \"checkout\" , \"maven\" , \"application\" ). run ( context ) } } } stage ( ' Compile ' ) { steps { script { context . buildTool . command = \"mvn\" context . nexus . credentialsId = \"nexus\" context . factory . getStage ( \"compile\" , \"maven\" , \"application\" ). run ( context ) } } } } }","title":"2.4 Using EDP Stages Library in the Pipeline"},{"location":"user-guide/pipeline-framework/#3-edp-library-stages-description","text":"Using in pipelines - @Library(['edp-library-stages@version']) _ The corresponding enums, classes, interfaces and their methods can be used separately from the EDP Stages library function (please refer to Table 5 ). Table 5. Enums and Classes with the respective properties, methods, and examples. Enums Classes ProjectType : - APPLICATION - AUTOTESTS - LIBRARY StageFactory() - Class that contains methods getting an implementation of the particular stage either EDP from shared library or custom from application repository. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Map stages - Map of stages implementations. Methods : loadEdpStages(): return a list of Classes that describes EDP stages implementations. loadCustomStages(String directory): return a list of Classes that describes EDP custom stages from application repository from \"directory\". The \"directory\" should have an absolute path to files with classes of custom stages implementations. Should be run from slave agent. add(Class clazz): register class for some particular stage in stages map of StageFactory class. getStage(String name, String buildTool, String type): return an object of the class for a particular stage from stages property based on stage name and buildTool, type of application. Example : context.factory = new StageFactory(script: this) context.factory.loadEdpStages().each() { context.factory.add(it) } context.factory.loadCustomStages(\"${context.workDir}/stages\").each() { context.factory.add(it) } context.factory.getStage(stageName.toLowerCase(),context.application.config.build_tool.toLowerCase(), context.application.config.type).run(context)","title":"3. EDP Library Stages Description"},{"location":"user-guide/pipeline-framework/#4-edp-stages-framework","text":"Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. Inspect the Table 6 and Table 7 that contain the full description of every stage that can be included in Code Review and Build pipelines: Checkout \u2192 Gerrit Checkout \u2192 Compile \u2192 Get version \u2192 Tests \u2192 Sonar \u2192 Build \u2192 Build Docker Image \u2192 Push \u2192 Git tag . Table 6. The Checkout, Gerrit Checkout, Compile, Get version, and Tests stages description. Checkout Gerrit Checkout Compile Get version Tests name = \"checkout\", buildTool = [\"maven\", \"npm\", \"dotnet\",\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - StageFactory context.factory - String context.gerrit.branch - String context.gerrit.credentialsId - String context.application.config.cloneUrl name = \"gerrit-checkout\", buildTool = [\"maven\", \"npm\", \"dotnet\",\"gradle\"] type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY] context required: - String context.workDir - StageFactory context.factory - String context.gerrit.changeName - String context.gerrit.refspecName - String context.gerrit.credentialsId - String context.application.config.cloneUrl name = \"compile\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.sln_filename output: - String context.buildTool.sln_filename buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.groupRepository name = \"get-version\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - Map(empty) context.application - String context.gerrit.branch - Job context.job output: -String context.application.deplyableModule - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.command - Job context.job - String context.gerrit.branch output: - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.command - Job context.job - String context.gerrit.branch output: - String context.application.deplyableModule - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - Job context.job - String context.gerrit.branch output: - String context.application.deplyableModuleDir - String context.application.version - String context.application.buildVersion name = \"tests\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.command buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command type = [ProjectType.AUTOTESTS] context required: - String context.workDir - String context.buildTool.command - String context.application.config.report_framework buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir Table 7. The Sonar, Build, Build Docker Image, Push, and Git tag stages description. Sonar Build Build Docker Image Push Git tag name = \"sonar\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.job.type - String context.application.name - String context.buildTool.sln_filename - String context.sonar.route - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.job.type - String context.nexus.credentialsId - String context.buildTool.command - String context.application.name - String context.sonarRoute - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) buildTool = [\"maven\"] type = [ProjectType.APPLICATION, ProjectType.AUTOTESTS, ProjectType.LIBRARY] context required: - String context.workDir - String context.job.type - String context.nexus.credentialsId - String context.application.name - String context.buildTool.command - String context.sonar.route - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.job.type - String context.sonar.route - String context.application.name - String context.gerrit.changeName(Only for codereview pipeline) - String context.gerrit.branch(Only for build pipeline) name = \"build\" buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.buildTool.command - String context.nexus.credentialsId buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.groupRepository name = \"build-image\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.application.deployableModule - String context.application.deployableModuleDir - String context.application.name - String context.application.config.language - String context.application.buildVersion - Boolean context.job.promoteImages - String context.job.envToPromote name = \"push\" buildTool = [\"dotnet\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.gerrit.project - String context.buildTool.sln_filename - String context.buildTool.snugetApiKey - String context.buildTool.hostedRepository buildTool = [\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.application.version - String context.buildTool.hostedRepository - String context. buildTool.settings buildTool = [\"maven\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.application.version - String context.buildTool.hostedRepository - String context.buildTool.command buildTool = [\"npm\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.nexus.credentialsId - String context.buildTool.hostedRepository - String context.gerrit.autouser name = \"git-tag\" buildTool = [\"maven\", \"npm\", \"dotnet\",\"gradle\"] type = [ProjectType.APPLICATION] context required: - String context.workDir - String context.gerrit.credentialsId - String context.gerrit.sshPort - String context.gerrit.host - String context.gerrit.autouser - String context.application.buildVersion","title":"4. EDP Stages Framework"},{"location":"user-guide/pipeline-framework/#5-deploy-pipeline","text":"Deploy() \u2013 a function that allows using the EDP implementation for the deploy pipeline. All values of different parameters that are used during the pipeline execution are stored in Map \"context\". The deploy pipeline consists of several steps: On the master: * Initialization of all objects (Platform, Job, Gerrit, Nexus, StageFactory) and loading the default implementations of EDP stages * Creating an environment if it doesn`t exist * Deploying the last versions of the applications * Run predefined manual gates On a particular autotest slave that depends on the build tool: Creating workdir for autotest sources Run predefined autotests","title":"5. Deploy Pipeline"},{"location":"user-guide/pipeline-framework/#51-edp-library-pipelines-description","text":"Using in pipelines - @Library(['edp-library-pipelines@version']) _ The corresponding enums and interfaces with their methods can be used separately from the EDP Pipelines library function (please refer to Table 8 and Table 9 ). Table 8. Enums and Interfaces with the respective properties, methods, and examples. Enums Interfaces PlatformType : - OPENSHIFT - KUBERNETES JobType : - CODEREVIEW - BUILD - DEPLOY BuildToolType : - MAVEN - GRADLE - NPM - DOTNET Platform() - contains methods for working with platform CLI. At the moment only OpenShift is supported. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Methods : getJsonPathValue(String k8s_kind, String k8s_kind_name, String jsonPath): return String value of specific parameter of particular object using jsonPath utility. Example : context.platform.getJsonPathValue(\"cm\",\"project-settings\", \".data.username\") BuildTool() - contains methods for working with different buildTool from ENUM BuildToolType. (Should be invoked on slave build agents) Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Nexus object - Object of class Nexus. Methods : init: return parameters of buildTool that are needed for running stages. Example : context.buildTool = new BuildToolFactory().getBuildToolImpl (context.application.config.build_tool, this, context.nexus) context.buildTool.init() Table 9. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) PlatformFactory() - Class that contains methods getting implementation of CLI of platform. At the moment OpenShift and Kubernetes are supported. Methods : getPlatformImpl(PlatformType platform, Script script): return Class Platform Example : context.platform = new PlatformFactory().getPlatformImpl(PlatformType.OPENSHIFT, this) Application(String name, Platform platform, Script script) - Class that describe the application object. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform() String name - Name for the application for creating object Map config - Map of configuration settings for particular application that is loaded from config map project-settings String version - Application version, initially empty. Is set on get-version step. String deployableModule - The name of deployable module for multi module applications, initially empty. String buildVersion - Version of built artifact, contains build number of Job initially empty String deployableModuleDir - The name of deployable module directory for multi module applications, initially empty. Array imageBuildArgs - List of arguments for building application Docker image Methods : setConfig(String gerrit_autouser, String gerrit_host, String gerrit_sshPort, String gerrit_project): set the config property with values from config map Example : context.application = new Application(context.job, context.gerrit.project, context.platform, this) context.application.setConfig(context.gerrit.autouser, context.gerrit.host, context.gerrit.sshPort, context.gerrit.project) Job(type: JobType.value, platform: Platform, script: Script) - Class that describe the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\" Platform platform - Object of a class Platform(). JobType.value type. String deployTemplatesDirectory - The name of the directory in application repository, where deploy templates are located. Can be set for particular Job through DEPLOY_TEMPLATES_DIRECTORY parameter. String edpName - The name of the EDP Project. Map stages - Contains all stages in JSON format that is retrieved from Jenkins job env variable. String envToPromote - The name of the environment for promoting images. Boolean promoteImages - Defines whether images should be promoted or not. Methods : getParameterValue(String parameter, String defaultValue = null): return parameter of ENV variable of Jenkins job. init(): set all the properties of Job object. setDisplayName(String displayName): set display name of the Jenkins job. setDescription(String description, Boolean addDescription = false): set new or add to existing description of the Jenkins job. printDebugInfo(Map context): print context info to log of Jenkins job. runStage(String stage_name, Map context): run the particular stage according to its name. Example : context.job = new Job(JobType.DEPLOY.value, context.platform, this) context.job.init() context.job.printDebugInfo(context) context.job.setDisplayName(\"test\") context.job.setDescription(\"Name: ${context.application.config.name}\") Gerrit(Job job, Platform platform, Script script) - Class that describe the Gerrit tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String credentialsId - Credential Id in Jenkins for Gerrit. String autouser - Username of autouser in Gerrit for integration with Jenkins. String host - Gerrit host. String project - project name of built application. String branch - branch to build application from. String changeNumber - change number of Gerrit commit. String changeName - change name of Gerrit commit. String refspecName - refspecName of Gerrit commit. String sshPort - gerrit ssh port number. String patchsetNumber - patchsetNumber of Gerrit commit. Methods : init(): set all the properties of Gerrit object. Example : context.gerrit = new Gerrit(context.job, context.platform, this) context.gerrit.init() . Nexus(Job job, Platform platform, Script script) - Class that describe the Nexus tool. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\". Platform platform - Object of a class Platform(). Job job - Object of a class Job(). String autouser - Username of autouser in Nexus for integration with Jenkins. String credentialsId - Credential Id in Jenkins for Nexus. String host - Nexus host. String port - Nexus http(s) port. String repositoriesUrl - Base URL of repositories in Nexus. String restUrl - URL of Rest API. Methods : init(): set all the properties of Nexus object. Example : context.nexus = new Nexus(context.job, context.platform, this) context.nexus.init() .","title":"5.1 EDP Library Pipelines Description"},{"location":"user-guide/pipeline-framework/#52-edp-library-stages-description","text":"Using in pipelines - @Library(['edp-library-stages@version']) _ The corresponding classes with methods can be used separately from the EDP Pipelines library function (please refer to Table 10 ). Table 10. Classes with the respective properties, methods, and examples. Classes Description (properties, methods, and examples) StageFactory() - Class that contains methods getting implementation of particular stage either EDP from shared library or custom from application repository. Properties : Script script - Object with type script, in most cases if class created from Jenkins pipelines it is \"this\" Map stages - Map of stages implementations Methods : loadEdpStages(): return list of Classes that describes EDP stages implementations loadCustomStages(String directory): return list of Classes that describes EDP custom stages from application repository from \"directory\". The \"directory\" should be absolute path to files with classes of custom stages implementations. Should be run from slave agent. add(Class clazz): register class for some particular stage in stages map of StageFactory class getStage(String name, String buildTool, String type): return object of the class for particular stage from stages property based on stage name and buildTool, type of application Example : context.factory = new StageFactory(script: this) context.factory.loadEdpStages().each() { context.factory.add(it) } context.factory.loadCustomStages(\"${context.workDir}/stages\").each() { context.factory.add(it) } context.factory.getStage(stageName.toLowerCase(),context.application.config.build_tool.toLowerCase(), context.application.config.type).run(context) .","title":"5.2 EDP Library Stages Description"},{"location":"user-guide/pipeline-framework/#53-deploy-pipeline-stages","text":"Each EDP stage implementation has run method that is as input parameter required to pass a context map with different keys. Some stages can implement the logic for several build tools and application types, some of them are specific. The stages for the deploy pipeline are independent of the build tool and application type. Find below (see Table 11 ) the full description of every stage: Deploy \u2192 Automated tests \u2192 Promote Images . Table 11. The Deploy, Automated tests, and Promote Images stages description. Deploy Automated tests Promote Images name = \"deploy\" buildTool = null type = null context required: \u2022 String context.workDir \u2022 StageFactory context.factory \u2022 String context.gerrit.autouser \u2022 String context.gerrit.host \u2022 String context.application.config.cloneUrl \u2022 String context.jenkins.token \u2022 String context.job.edpName \u2022 String context.job.buildUrl \u2022 String context.job.jenkinsUrl \u2022 String context.job.metaProject \u2022 List context.job.applicationsList [['name':'application1_name','version':'application1_version],...] \u2022 String context.job.deployTemplatesDirectory output: \u2022 List context.job.updatedApplicaions [['name':'application1_name','version':'application1_version],...] name = \"automation-tests\", buildTool = null, type = null context required: - String context.workDir - StageFactory context.factory - String context.gerrit.credentialsId - String context.autotest.config.cloneUrl - String context.autotest.name - String context.job.stageWithoutPrefixName - String context.buildTool.settings - String context.autotest.config.report_framework name = \"promote-images\" buildTool = null type = null context required: - String context.workDir - String context.buildTool.sln_filename - List context.job.updatedApplicaions [['name':'application1_name','version':'application1_version],...]","title":"5.3 Deploy Pipeline Stages"},{"location":"user-guide/pipeline-framework/#54-how-to-redefine-or-extend-edp-pipeline-stages-library","text":"Info Currently, the redefinition of Deploy pipeline stages is prohibited.","title":"5.4 How to Redefine or Extend EDP Pipeline Stages Library"},{"location":"user-guide/pipeline-framework/#55-using-edp-library-stages-in-the-pipeline","text":"In order to use the EDP stages, the created pipeline should fit some requirements, that`s why a developer has to do the following: import libraries - @Library(['edp-library-stages', 'edp-library-pipelines']) _ import reference EDP classes(See example below) define context Map \u2013 context = [:] define reference \"init\" stage After that, there is the ability to run any EDP stage beforehand by defining requirement context context.job.runStage(\"Deploy\", context) . For instance, the pipeline can look like : @Library ( [ ' edp - library - stages ' , ' edp - library - pipelines ' ] ) _ import com.epam.edp.stages.StageFactory import com.epam.edp.platform.PlatformFactory import com.epam.edp.platform.PlatformType import com.epam.edp.JobType context = [ : ] node ( ' master ' ) { stage ( \"Init\" ) { context . platform = new PlatformFactory (). getPlatformImpl ( PlatformType . OPENSHIFT , this ) context . job = new com . epam . edp . Job ( JobType . DEPLOY . value , context . platform , this ) context . job . init () context . job . initDeployJob () println ( \"[JENKINS][DEBUG] Created object job with type - ${context.job.type}\" ) context . nexus = new com . epam . edp . Nexus ( context . job , context . platform , this ) context . nexus . init () context . jenkins = new com . epam . edp . Jenkins ( context . job , context . platform , this ) context . jenkins . init () context . gerrit = new com . epam . edp . Gerrit ( context . job , context . platform , this ) context . gerrit . init () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . environment = new com . epam . edp . Environment ( context . job . deployProject , context . platform , this ) context . job . printDebugInfo ( context ) context . job . setDisplayName ( \"${currentBuild.displayName}-${context.job.deployProject}\" ) context . job . generateInputDataForDeployJob () } stage ( \"Pre Deploy Custom stage\" ) { println ( \"Some custom pre deploy logic\" ) } context . job . runStage ( \"Deploy\" , context ) stage ( \"Post Deploy Custom stage\" ) { println ( \"Some custom post deploy logic\" ) } } Or in a declarative way : @Library ( [ ' edp - library - stages ' , ' edp - library - pipelines ' ] ) _ import com.epam.edp.stages.StageFactory import com.epam.edp.platform.PlatformFactory import com.epam.edp.platform.PlatformType import com.epam.edp.JobType context = [ : ] pipeline { agent { label ' master ' } stages { stage ( ' Init ' ) { steps { script { context . platform = new PlatformFactory (). getPlatformImpl ( PlatformType . OPENSHIFT , this ) context . job = new com . epam . edp . Job ( JobType . DEPLOY . value , context . platform , this ) context . job . init () context . job . initDeployJob () println ( \"[JENKINS][DEBUG] Created object job with type - ${context.job.type}\" ) context . nexus = new com . epam . edp . Nexus ( context . job , context . platform , this ) context . nexus . init () context . jenkins = new com . epam . edp . Jenkins ( context . job , context . platform , this ) context . jenkins . init () context . gerrit = new com . epam . edp . Gerrit ( context . job , context . platform , this ) context . gerrit . init () context . factory = new StageFactory ( script : this ) context . factory . loadEdpStages (). each () { context . factory . add ( it ) } context . environment = new com . epam . edp . Environment ( context . job . deployProject , context . platform , this ) context . job . printDebugInfo ( context ) context . job . setDisplayName ( \"${currentBuild.displayName}-${context.job.deployProject}\" ) context . job . generateInputDataForDeployJob () } } } stage ( ' Deploy ' ) { steps { script { context . factory . getStage ( \"deploy\" ). run ( context ) } } } stage ( ' Custom stage ' ) { steps { println ( \"Some custom logic\" ) } } } }","title":"5.5 Using EDP Library Stages in the Pipeline"},{"location":"user-guide/pipeline-framework/#related-articles","text":"EDP Stages Use Terraform Library in EDP","title":"Related Articles"},{"location":"user-guide/pipeline-stages/","text":"Pipeline Stages \u2693\ufe0e Get acquainted with EDP CI/CD workflow and stages description. EDP CI/CD Workflow \u2693\ufe0e Within EDP, the pipeline framework comprises the following pipelines: Code Review; Build; Deploy. Note Please refer to the EDP Pipeline Framework page for details. The diagram below shows the commit path through these pipelines and the respective stages. Stages Description \u2693\ufe0e The table below provides the details on all the stages in the EDP pipeline framework: Name Dependency Description Pipeline Application Library Autotest Source code Documentation init Initiates information gathering Create Release, Code Review, Build + + Build.groovy checkout Performs for all files the checkout from a selected branch of the Git repository. For the main branch - from HEAD, for code review - from the commit Create Release, Build + + Checkout.groovy compile Compiles the code, includes individual groovy files for each type of app or lib (NPM, DotNet, Python, Maven, Gradle) Code Review, Build + + Compile tests Launches testing procedure, includes individual groovy files for each type of app or lib Code Review, Build + + + Tests sonar Launches testing via SonarQube scanner and includes individual groovy files for each type of app or lib Code Review, Build + + Sonar build Builds the application, includes individual groovy files for each type of app or lib (Go, Maven, Gradle, NPM) Code Review, Build + Build create-branch EDP create-release process Creates default branch in Gerrit during create and clone strategies Create Release + + + CreateBranch.groovy trigger-job EDP create-release process Triggers \"build\" job Create Release + + + TriggerJob.groovy gerrit-checkout Performs checkout to the current project branch in Gerrit Code Review + + + GerritCheckout.groovy commit-validate Optional in EDP Admin Console Takes Jira parameters, when \"Jira Integration\" is enabled for the project in the Admin Console. Code Review + + CommitValidate.groovy dockerfile-lint Launches linting tests for Dockerfile Code Review + LintDockerApplicationLibrary.groovy Use lint stages for code review dockerbuild-verify \"Build\" stage (if there are no \"COPY\" layers in Dockerfile) Launches build procedure for Dockerfile without pushing an image to the repository Code Review + BuildDockerfileApplicationLibrary.groovy Use lint stages for code review helm-lint Launches linting tests for deployment charts Code Review + LintHelmApplicationLibrary.groovy Use lint stages for code review get-version Defines the versioning of the project depending on the versioning schema selected in Admin Console Build + + GetVersion terraform-plan AWS credentials added to Jenkins Checks Terraform version, and installs default version if necessary, and launches terraform init, returns AWS username which used for action, and terraform plan command is called with an output of results to .tfplan file Build + TerraformPlan.groovy Use Terraform library in EDP terraform-apply AWS credentials added to Jenkins, the \"Terraform-plan\" stage Checks Terraform version, and installs default version if necessary, and launches terraform init, launches terraform plan from saves before .tfplan file, asks to approve, and run terraform apply from .tfplan file Build + TerraformApply.groovy Use Terraform library in EDP build-image-from-dockerfile Platform: OpenShift Builds Dockerfile Build + + .groovy files for building Dockerfile image build-image-kaniko Platform: k8s Builds Dockerfile using the Kaniko tool Build + BuildImageKaniko.groovy push Pushes an artifact to the Nexus repository Build + + Push create-Jira-issue-metadata \"get-version\" stage Creates a temporary CR in the namespace and after that pushes Jira Integration data to Jira ticket, and delete CR Build + + JiraIssueMetadata.groovy ecr-to-docker DockerHub credentials added to Jenkins Copies the docker image from the ECR project registry to DockerHub via the Crane tool after it is built Build + EcrToDocker.groovy Promote Docker images from ECR to Docker hub git-tag \"Get-version\" stage Creates a tag in SCM for the current build Build + + GitTagApplicationLibrary.groovy deploy Deploys the application Deploy + Deploy.groovy manual Works with the manual approve to proceed Deploy + ManualApprove.groovy promote-images Promotes docker images to the registry Deploy + PromoteImage.groovy promote-images-ecr Promotes docker images to ECR Deploy + PromoteImagesECR.groovy Note The Create Release pipeline is an internal EDP mechanism for adding, importing or cloning a codebase. It is not a part of the pipeline framework. Related Articles \u2693\ufe0e Add Job Provisioner GitLab Integration GitHub Integration Job Provisions for GCP Issues","title":"Overview"},{"location":"user-guide/pipeline-stages/#pipeline-stages","text":"Get acquainted with EDP CI/CD workflow and stages description.","title":"Pipeline Stages"},{"location":"user-guide/pipeline-stages/#edp-cicd-workflow","text":"Within EDP, the pipeline framework comprises the following pipelines: Code Review; Build; Deploy. Note Please refer to the EDP Pipeline Framework page for details. The diagram below shows the commit path through these pipelines and the respective stages.","title":"EDP CI/CD Workflow"},{"location":"user-guide/pipeline-stages/#stages-description","text":"The table below provides the details on all the stages in the EDP pipeline framework: Name Dependency Description Pipeline Application Library Autotest Source code Documentation init Initiates information gathering Create Release, Code Review, Build + + Build.groovy checkout Performs for all files the checkout from a selected branch of the Git repository. For the main branch - from HEAD, for code review - from the commit Create Release, Build + + Checkout.groovy compile Compiles the code, includes individual groovy files for each type of app or lib (NPM, DotNet, Python, Maven, Gradle) Code Review, Build + + Compile tests Launches testing procedure, includes individual groovy files for each type of app or lib Code Review, Build + + + Tests sonar Launches testing via SonarQube scanner and includes individual groovy files for each type of app or lib Code Review, Build + + Sonar build Builds the application, includes individual groovy files for each type of app or lib (Go, Maven, Gradle, NPM) Code Review, Build + Build create-branch EDP create-release process Creates default branch in Gerrit during create and clone strategies Create Release + + + CreateBranch.groovy trigger-job EDP create-release process Triggers \"build\" job Create Release + + + TriggerJob.groovy gerrit-checkout Performs checkout to the current project branch in Gerrit Code Review + + + GerritCheckout.groovy commit-validate Optional in EDP Admin Console Takes Jira parameters, when \"Jira Integration\" is enabled for the project in the Admin Console. Code Review + + CommitValidate.groovy dockerfile-lint Launches linting tests for Dockerfile Code Review + LintDockerApplicationLibrary.groovy Use lint stages for code review dockerbuild-verify \"Build\" stage (if there are no \"COPY\" layers in Dockerfile) Launches build procedure for Dockerfile without pushing an image to the repository Code Review + BuildDockerfileApplicationLibrary.groovy Use lint stages for code review helm-lint Launches linting tests for deployment charts Code Review + LintHelmApplicationLibrary.groovy Use lint stages for code review get-version Defines the versioning of the project depending on the versioning schema selected in Admin Console Build + + GetVersion terraform-plan AWS credentials added to Jenkins Checks Terraform version, and installs default version if necessary, and launches terraform init, returns AWS username which used for action, and terraform plan command is called with an output of results to .tfplan file Build + TerraformPlan.groovy Use Terraform library in EDP terraform-apply AWS credentials added to Jenkins, the \"Terraform-plan\" stage Checks Terraform version, and installs default version if necessary, and launches terraform init, launches terraform plan from saves before .tfplan file, asks to approve, and run terraform apply from .tfplan file Build + TerraformApply.groovy Use Terraform library in EDP build-image-from-dockerfile Platform: OpenShift Builds Dockerfile Build + + .groovy files for building Dockerfile image build-image-kaniko Platform: k8s Builds Dockerfile using the Kaniko tool Build + BuildImageKaniko.groovy push Pushes an artifact to the Nexus repository Build + + Push create-Jira-issue-metadata \"get-version\" stage Creates a temporary CR in the namespace and after that pushes Jira Integration data to Jira ticket, and delete CR Build + + JiraIssueMetadata.groovy ecr-to-docker DockerHub credentials added to Jenkins Copies the docker image from the ECR project registry to DockerHub via the Crane tool after it is built Build + EcrToDocker.groovy Promote Docker images from ECR to Docker hub git-tag \"Get-version\" stage Creates a tag in SCM for the current build Build + + GitTagApplicationLibrary.groovy deploy Deploys the application Deploy + Deploy.groovy manual Works with the manual approve to proceed Deploy + ManualApprove.groovy promote-images Promotes docker images to the registry Deploy + PromoteImage.groovy promote-images-ecr Promotes docker images to ECR Deploy + PromoteImagesECR.groovy Note The Create Release pipeline is an internal EDP mechanism for adding, importing or cloning a codebase. It is not a part of the pipeline framework.","title":"Stages Description"},{"location":"user-guide/pipeline-stages/#related-articles","text":"Add Job Provisioner GitLab Integration GitHub Integration Job Provisions for GCP Issues","title":"Related Articles"},{"location":"user-guide/run-functional-autotest/","text":"Run Functional Autotests \u2693\ufe0e This chapter describes the process of adding, configuring, and running autotests. Run the added autotest on the deployed environment or add it to a newly created CD pipeline stage. Explore the process of launching the autotest locally, review the successful and unsuccessful Allure reports, and resolve the encountered issue. The Predefined EDP Entities \u2693\ufe0e Explore the predefined EDP entities: Three applications: cart-service, order-service, and zuul: Every application has two branches: One CD pipeline with the DEV stage and a manual approve of the trigger type: The project deployment of the ms-master CD pipeline in OpenShift: The endpoint by the service status path of the master branch returns the following text: OK! The endpoint of the release-8.0 branch should return the following text: OK! Add and Configure an Autotest \u2693\ufe0e Follow the steps below to add an autotest using Admin Console and to configure it in Gerrit within the deployed environment: Open the Admin Console and add an autotest: Note To get more information on how to add autotests, please refer to the Add Autotest instruction. After the provisioning of the new autotest is successfully completed, implement the necessary configuration for the added autotest. Open Gerrit via the Admin Console overview page \u2192 select the created autotest: Navigate to the Branches tab and click the gitweb of the master branch: Switch to the tree tab and open the run.json file: Explore the run.json file where the stage is the key, the value is the command to run: { \"sit\": \"mvn test -Dendpoint=https://zuul-gk-ms-release-sit.dev.gk-edp.com -Dsurefire.suiteXmlFiles=testng-smoke-suite.xml\", \"codereview\": \"mvn test -Dendpoint=https://zuul-gk-ms-master-dev.dev.gk-edp.com -Dsurefire.suiteXmlFiles=testng-smoke-suite.xml\" } Info Continuous Integration is used when verifying the added to autotests changes that`s why the \"codereview\" key should be added. The \"codereview\" key is the mandatory and will be used during the Code Review pipeline processing to autotest itself by triggering when pushing a new code to the repository. Create change using the Gerrit web console: open the rest-autotests project \u2192 create change \u2192 click the Publish button \u2192 hit Edit and Add \u2192 type the run.json to open this file. Note To get more information on how to add a change using the Gerrit web console, please refer to the Creating a Change in Gerrit page. Open the deployed environment in OpenShift and copy the external URL address for zuul application: Define another command value by pasting the copied URL address, click Save and then Publish button to trigger the CI pipeline in Jenkins: Click the respective link to Jenkins in the History block to check that the Code Review pipeline is triggered: Wait for the Code Review pipeline to be successfully passed and click the Allure report link to verify the autotests results: Info The autotests pass only the Code Review pipeline. Explore the results in Allure by clicking the EDP SIT Tests link: Navigate between the included autotests and check the respective results: Return to Gerrit, which already displays the appropriate marks, and merge the changes by clicking the Code-Review+2 and Submit buttons. Local Launch of Autotests \u2693\ufe0e There is an ability to run the autotests locally using the IntelliJIDEA application. To launch the rest-autotests project for the local verification, perform the following steps: Clone the project to the local machine. Open the project in IntelliJIDEA and find the run.json file to copy out the necessary command value, then click the Add Configuration button, hit the plus sign \u2192 go to Maven and paste the copied command value into the Command line field\u2192 click Apply and OK \u2192 hit the necessary button to run the added command value: As a result, all launched tests will be successfully passed: Add an Autotest to a New CD Pipeline \u2693\ufe0e Add an additional CD pipeline and define two stages (SIT - System Integration Testing and QA - Quality Assurance) with the different Quality Gate types. Pay attention that the QA stage will be able to be launched only after the SIT stage is completed. Add a new CD pipeline (e.g. under the ms-release name). Select all three applications and specify the release-8.0 branch: Add SIT stage by defining the Autotests type in the Quality gate type field, and select the respective check box as well: Add the QA stage and define the Manual type in the Quality gate type field. As soon as the CD pipeline is provisioned, the details page will display the added stages with the corresponding quality gate types. Click the CD pipeline name on the top of the details page: Being navigated to Jenkins, select the SIT stage and trigger it by clicking the Build Now option from the left-side panel, and define the necessary version on the Initialization stage: Info To trigger the CD pipeline, first, be confident that all applications have passed the Build pipelines and autotests have passed the single Code Review pipelines. The SIT stage will not be passed successfully as the mentioned endpoint doesn`t exist. To resolve the issue, apply the configuration using IntelliJIDEA (_see above the additional information on how to make changes locally: Local Launch of Autotests ; Being in IntelliJIDEA, click Edit Configuration \u2192 Maven \u2192 type the name - ms-release-sit \u2192 click OK: Find the run.json file to copy out the necessary command value for SIT stage, then click the Edit Configuration button \u2192 go to Maven ms-release-sit and paste the copied command value into the Command line field\u2192 click Apply and OK: Push the changes to send them for review to Gerrit and submit the changes. After the changes are added, the SIT stage should be triggered one more time. Open Jenkins and click the Build Now option from the left-side panel, then select the latest version in the appeared notification during the Initialization stage. As a result, the triggered pipeline won`t be passed. Click the Allure link to get more information about the issues: The Allure report displays the failed tests and explains the failure reason. In the current case, the expected string doesn`t match the initial one: Deploy SIT Stage \u2693\ufe0e The pipeline can be failed due to the version that was selected for deployment. This version includes the new mentioned changes that affected the pipeline processing. In order to resolve this issue and successfully deploy the CD pipeline, a developer/user should choose the previous version without new changes. To do this, follow the steps below: Open Jenkins and trigger the SIT stage one more time, select the previous version on the Initialization stage: As soon as the automation-tests stage is passed, check the Allure report: After the Promote-images stage is completed on the SIT stage, the QA stage can be triggered and deployed as the Docker images were promoted to the next stage: As a result, the SIT stage will be deployed successfully, thus allowing to trigger the next QA stage.","title":"Run Functional Autotests"},{"location":"user-guide/run-functional-autotest/#run-functional-autotests","text":"This chapter describes the process of adding, configuring, and running autotests. Run the added autotest on the deployed environment or add it to a newly created CD pipeline stage. Explore the process of launching the autotest locally, review the successful and unsuccessful Allure reports, and resolve the encountered issue.","title":"Run Functional Autotests"},{"location":"user-guide/run-functional-autotest/#the-predefined-edp-entities","text":"Explore the predefined EDP entities: Three applications: cart-service, order-service, and zuul: Every application has two branches: One CD pipeline with the DEV stage and a manual approve of the trigger type: The project deployment of the ms-master CD pipeline in OpenShift: The endpoint by the service status path of the master branch returns the following text: OK! The endpoint of the release-8.0 branch should return the following text: OK!","title":"The Predefined EDP Entities"},{"location":"user-guide/run-functional-autotest/#add-and-configure-an-autotest","text":"Follow the steps below to add an autotest using Admin Console and to configure it in Gerrit within the deployed environment: Open the Admin Console and add an autotest: Note To get more information on how to add autotests, please refer to the Add Autotest instruction. After the provisioning of the new autotest is successfully completed, implement the necessary configuration for the added autotest. Open Gerrit via the Admin Console overview page \u2192 select the created autotest: Navigate to the Branches tab and click the gitweb of the master branch: Switch to the tree tab and open the run.json file: Explore the run.json file where the stage is the key, the value is the command to run: { \"sit\": \"mvn test -Dendpoint=https://zuul-gk-ms-release-sit.dev.gk-edp.com -Dsurefire.suiteXmlFiles=testng-smoke-suite.xml\", \"codereview\": \"mvn test -Dendpoint=https://zuul-gk-ms-master-dev.dev.gk-edp.com -Dsurefire.suiteXmlFiles=testng-smoke-suite.xml\" } Info Continuous Integration is used when verifying the added to autotests changes that`s why the \"codereview\" key should be added. The \"codereview\" key is the mandatory and will be used during the Code Review pipeline processing to autotest itself by triggering when pushing a new code to the repository. Create change using the Gerrit web console: open the rest-autotests project \u2192 create change \u2192 click the Publish button \u2192 hit Edit and Add \u2192 type the run.json to open this file. Note To get more information on how to add a change using the Gerrit web console, please refer to the Creating a Change in Gerrit page. Open the deployed environment in OpenShift and copy the external URL address for zuul application: Define another command value by pasting the copied URL address, click Save and then Publish button to trigger the CI pipeline in Jenkins: Click the respective link to Jenkins in the History block to check that the Code Review pipeline is triggered: Wait for the Code Review pipeline to be successfully passed and click the Allure report link to verify the autotests results: Info The autotests pass only the Code Review pipeline. Explore the results in Allure by clicking the EDP SIT Tests link: Navigate between the included autotests and check the respective results: Return to Gerrit, which already displays the appropriate marks, and merge the changes by clicking the Code-Review+2 and Submit buttons.","title":"Add and Configure an Autotest"},{"location":"user-guide/run-functional-autotest/#local-launch-of-autotests","text":"There is an ability to run the autotests locally using the IntelliJIDEA application. To launch the rest-autotests project for the local verification, perform the following steps: Clone the project to the local machine. Open the project in IntelliJIDEA and find the run.json file to copy out the necessary command value, then click the Add Configuration button, hit the plus sign \u2192 go to Maven and paste the copied command value into the Command line field\u2192 click Apply and OK \u2192 hit the necessary button to run the added command value: As a result, all launched tests will be successfully passed:","title":"Local Launch of Autotests"},{"location":"user-guide/run-functional-autotest/#add-an-autotest-to-a-new-cd-pipeline","text":"Add an additional CD pipeline and define two stages (SIT - System Integration Testing and QA - Quality Assurance) with the different Quality Gate types. Pay attention that the QA stage will be able to be launched only after the SIT stage is completed. Add a new CD pipeline (e.g. under the ms-release name). Select all three applications and specify the release-8.0 branch: Add SIT stage by defining the Autotests type in the Quality gate type field, and select the respective check box as well: Add the QA stage and define the Manual type in the Quality gate type field. As soon as the CD pipeline is provisioned, the details page will display the added stages with the corresponding quality gate types. Click the CD pipeline name on the top of the details page: Being navigated to Jenkins, select the SIT stage and trigger it by clicking the Build Now option from the left-side panel, and define the necessary version on the Initialization stage: Info To trigger the CD pipeline, first, be confident that all applications have passed the Build pipelines and autotests have passed the single Code Review pipelines. The SIT stage will not be passed successfully as the mentioned endpoint doesn`t exist. To resolve the issue, apply the configuration using IntelliJIDEA (_see above the additional information on how to make changes locally: Local Launch of Autotests ; Being in IntelliJIDEA, click Edit Configuration \u2192 Maven \u2192 type the name - ms-release-sit \u2192 click OK: Find the run.json file to copy out the necessary command value for SIT stage, then click the Edit Configuration button \u2192 go to Maven ms-release-sit and paste the copied command value into the Command line field\u2192 click Apply and OK: Push the changes to send them for review to Gerrit and submit the changes. After the changes are added, the SIT stage should be triggered one more time. Open Jenkins and click the Build Now option from the left-side panel, then select the latest version in the appeared notification during the Initialization stage. As a result, the triggered pipeline won`t be passed. Click the Allure link to get more information about the issues: The Allure report displays the failed tests and explains the failure reason. In the current case, the expected string doesn`t match the initial one:","title":"Add an Autotest to a New CD Pipeline"},{"location":"user-guide/run-functional-autotest/#deploy-sit-stage","text":"The pipeline can be failed due to the version that was selected for deployment. This version includes the new mentioned changes that affected the pipeline processing. In order to resolve this issue and successfully deploy the CD pipeline, a developer/user should choose the previous version without new changes. To do this, follow the steps below: Open Jenkins and trigger the SIT stage one more time, select the previous version on the Initialization stage: As soon as the automation-tests stage is passed, check the Allure report: After the Promote-images stage is completed on the SIT stage, the QA stage can be triggered and deployed as the Docker images were promoted to the next stage: As a result, the SIT stage will be deployed successfully, thus allowing to trigger the next QA stage.","title":"Deploy SIT Stage"},{"location":"user-guide/terraform-stages/","text":"CI Pipeline for Terraform \u2693\ufe0e EPAM Delivery Platform ensures the implemented Terraform support allowing to work with Terraform code that is processed by means of stages in the Code-Review and Build pipelines. These pipelines are expected to be created after the Terraform Library is added. Code-Review Pipeline Stages \u2693\ufe0e In the Code-Review pipeline, the following stages are available: checkout stage, a standard step during which all files are checked out from a selected branch of the Git repository. terraform-lint stage containing a script that performs the following actions: 2.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 2.2. Launches the terraform init command that initializes backend. 2.3. Launches the linters described below. Pay attention that if at least one of these checks is not true (returns with an error), the Code Review pipeline will fail on this step and will be displayed in red. * terraform fmt linter checks the formatting of the Terraform code; * tflint linter checks Terraform linters for possible errors and deprecated syntax; * terraform validate linter validates the Terraform code. Build Pipeline Stages \u2693\ufe0e In the Build pipeline, the following stages are available: checkout stage is a standard step during which all files are checked out from a master branch of Git repository. Note With the Default versioning, in the base directory of the project, create a file named 'VERSION' with a proper Terraform version (e.g.1.0.0). terraform-lint stage containing a script that performs the same actions as in the Code-Review pipeline, namely: 2.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 2.2. Launches the terraform init stage that initializes backend. 2.3. Launches the linters described below. Pay attention that if at least one of these checks is not true (returns with an error), the Build pipeline will fail on this step and will be displayed in red. - terraform fmt linter checks the formatting of the Terraform code; - tflint linter checks Terraform linters for possible errors and deprecated syntax; - terraform validate linter validates the Terraform code. terraform-plan stage containing a script that performs the following actions: 3.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 3.2. Launches the terraform init command that initializes backend. 3.3. Returns the name of the user, on behalf of whom the actions will be performed, with the help of awscliv2 . 3.4. Launches the terraform-plan command saving the results in the .tfplan file. Note EDP expects AWS credentials to be added in Jenkins under the name aws.user . To learn how to create credentials for the terraform-plan and terraform-apply stages, see the section Create AWS Credentials . terraform-apply stage containing a script that performs the following actions: 4.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 4.2. Launches the terraform init command that initializes backend. 4.3. Launches the terraform-plan command saving the results in the tfplan file. 4.4. Approves the application of Terraform code in your project by manually clicking the Proceed button. To decline the Terraform code, click the Abort button. If none of the buttons is selected within 30 minutes, by default the terraform-plan command will not be applied. 4.5. Launches the terraform-apply command. Create AWS Credentials \u2693\ufe0e To create credentials that will be used in terraform-plan and terraform-apply stages, perform the following steps: 1. Go to Jenkins -> Manage Jenkins -> Manage Credentials . In the Store scoped to Jenkins section select global as Domains . 2. Click the Add Credentials tab and select AWS Credentials in the Kind dropdown. 3. Enter the ID name. By default, EDP expects AWS credentials to be under the ID aws.user . 4. Enter values into the Access Key ID and Secret Access Key fields (credentials should belong to a user in AWS). 5. Click OK to save these credentials. Now the ID of the credentials is visible in the Global credentials table in Jenkins. Use Existing AWS Credentials \u2693\ufe0e To use other existing credentials (e.g. from other accounts) instead of the expected ones in the Build pipeline and in the terraform-plan and terraform-apply stages, perform the following steps: Navigate to the Build pipeline and select the Configure tab. Click the Add Parameter button and select the String Parameter option. Fill in the respective fields with the variable name AWS_CREDENTIALS , description, and the default value (e.g., aws.user , used previously in pipelines). Now during the launch of the Build pipeline, it is possible to select the desired credentials, added in Jenkins, in the AWS_CREDENTIALS field of the Build pipeline settings. Related Articles \u2693\ufe0e EDP Pipeline Framework Associate IAM Roles With Service Accounts","title":"Terraform"},{"location":"user-guide/terraform-stages/#ci-pipeline-for-terraform","text":"EPAM Delivery Platform ensures the implemented Terraform support allowing to work with Terraform code that is processed by means of stages in the Code-Review and Build pipelines. These pipelines are expected to be created after the Terraform Library is added.","title":"CI Pipeline for Terraform"},{"location":"user-guide/terraform-stages/#code-review-pipeline-stages","text":"In the Code-Review pipeline, the following stages are available: checkout stage, a standard step during which all files are checked out from a selected branch of the Git repository. terraform-lint stage containing a script that performs the following actions: 2.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 2.2. Launches the terraform init command that initializes backend. 2.3. Launches the linters described below. Pay attention that if at least one of these checks is not true (returns with an error), the Code Review pipeline will fail on this step and will be displayed in red. * terraform fmt linter checks the formatting of the Terraform code; * tflint linter checks Terraform linters for possible errors and deprecated syntax; * terraform validate linter validates the Terraform code.","title":"Code-Review Pipeline Stages"},{"location":"user-guide/terraform-stages/#build-pipeline-stages","text":"In the Build pipeline, the following stages are available: checkout stage is a standard step during which all files are checked out from a master branch of Git repository. Note With the Default versioning, in the base directory of the project, create a file named 'VERSION' with a proper Terraform version (e.g.1.0.0). terraform-lint stage containing a script that performs the same actions as in the Code-Review pipeline, namely: 2.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 2.2. Launches the terraform init stage that initializes backend. 2.3. Launches the linters described below. Pay attention that if at least one of these checks is not true (returns with an error), the Build pipeline will fail on this step and will be displayed in red. - terraform fmt linter checks the formatting of the Terraform code; - tflint linter checks Terraform linters for possible errors and deprecated syntax; - terraform validate linter validates the Terraform code. terraform-plan stage containing a script that performs the following actions: 3.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 3.2. Launches the terraform init command that initializes backend. 3.3. Returns the name of the user, on behalf of whom the actions will be performed, with the help of awscliv2 . 3.4. Launches the terraform-plan command saving the results in the .tfplan file. Note EDP expects AWS credentials to be added in Jenkins under the name aws.user . To learn how to create credentials for the terraform-plan and terraform-apply stages, see the section Create AWS Credentials . terraform-apply stage containing a script that performs the following actions: 4.1. Checks whether the repository contains the .terraform-version file, where the information about the Terraform version is stored. If there is no .terraform-version file, the default Terraform version (0.14.5) will be used on this stage. In order to install different versions of Terraform, use the Terraform version manager . 4.2. Launches the terraform init command that initializes backend. 4.3. Launches the terraform-plan command saving the results in the tfplan file. 4.4. Approves the application of Terraform code in your project by manually clicking the Proceed button. To decline the Terraform code, click the Abort button. If none of the buttons is selected within 30 minutes, by default the terraform-plan command will not be applied. 4.5. Launches the terraform-apply command.","title":"Build Pipeline Stages"},{"location":"user-guide/terraform-stages/#create-aws-credentials","text":"To create credentials that will be used in terraform-plan and terraform-apply stages, perform the following steps: 1. Go to Jenkins -> Manage Jenkins -> Manage Credentials . In the Store scoped to Jenkins section select global as Domains . 2. Click the Add Credentials tab and select AWS Credentials in the Kind dropdown. 3. Enter the ID name. By default, EDP expects AWS credentials to be under the ID aws.user . 4. Enter values into the Access Key ID and Secret Access Key fields (credentials should belong to a user in AWS). 5. Click OK to save these credentials. Now the ID of the credentials is visible in the Global credentials table in Jenkins.","title":"Create AWS Credentials"},{"location":"user-guide/terraform-stages/#use-existing-aws-credentials","text":"To use other existing credentials (e.g. from other accounts) instead of the expected ones in the Build pipeline and in the terraform-plan and terraform-apply stages, perform the following steps: Navigate to the Build pipeline and select the Configure tab. Click the Add Parameter button and select the String Parameter option. Fill in the respective fields with the variable name AWS_CREDENTIALS , description, and the default value (e.g., aws.user , used previously in pipelines). Now during the launch of the Build pipeline, it is possible to select the desired credentials, added in Jenkins, in the AWS_CREDENTIALS field of the Build pipeline settings.","title":"Use Existing AWS Credentials"},{"location":"user-guide/terraform-stages/#related-articles","text":"EDP Pipeline Framework Associate IAM Roles With Service Accounts","title":"Related Articles"}]}